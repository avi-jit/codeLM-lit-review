{"papers":[{"url":"https://www.semanticscholar.org/paper/270b015093073d3ba254928b6d736a59870d3fb1","title":"Chatbots As Fluent Polyglots: Revisiting Breakthrough Code Snippets","venue":"ArXiv","year":2023,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/01/2023","authors":"David Noever,Kevin Williams","citations":[],"references":[{"paperId":"6756fcd998caeb7b23702e08559e63710179334c","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"51d253814e85249a84bbe634b4a80d306b74fbd0","title":"\"It would work for me too\": How Online Communities Shape Software Developers' Trust in AI-Powered Code Generation Tools"},{"paperId":"8175ce2cbb99b6a394bdac152ae39d413f4f1380","title":"Codex Hacks HackerRank: Memorization Issues and a Framework for Code Synthesis Evaluation"},{"paperId":"0d08ffccc982781e310bb184397bbe64b9aef157","title":"Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models"},{"paperId":"979eb5c97c49d7979447ed684500895a24d75ac4","title":"The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming"},{"paperId":"6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a","title":"Few-Shot Semantic Parsing with Language Models Trained on Code"},{"paperId":"91260f73dd179487fb16713deb8267634ae14716","title":"CodexDB: Synthesizing Code for Query Processing from Natural Language Instructions using GPT-3 Codex"},{"paperId":"1444536496d8064f33e10b38b5820fecfab5b367","title":"Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"5ff9032d0f7f246d01ae7b2c231ab03469a7344a","title":"Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?"},{"paperId":null,"title":"Can artificial intelligence make software development more productive"},{"paperId":null,"title":"GPT-2 Output Detector Demo"},{"paperId":null,"title":"OpenAI Codex"},{"paperId":"b72c89500dd57f1a4ceadb97f3dbf5015948a5e7","title":"A Brief History of Chatbots"},{"paperId":null,"title":"The Lines of Code That Changed Everything"},{"paperId":"e9431029d8c7da7f55c3496c9d8a3cec542a858f","title":"COBOL to Java and Newspapers Still Get Delivered"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":null,"title":"Towards Making Legacy HPC Codes Maintainable: Two- Way Fortran-Python Transpilation with Python Type Hints (Unrefereed Workshop Manuscript)"}],"id":"270b015093073d3ba254928b6d736a59870d3fb1","summary":"The research applies AI-driven code assistants to analyze a selection of influential computer code that has shaped modern technology, including email, internet browsing, robotics, and malicious software to provide insights into obfuscated code or software lacking explanatory commentary."},{"url":"https://www.semanticscholar.org/paper/bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence","venue":"ArXiv","year":2022,"referenceCount":77,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Yichen Xu,Yanqiao Zhu","citations":[],"references":[{"paperId":"4c26a6a777f1d46ffe79e3f41f8fd1ef6a6bd8c9","title":"Automatically Generating CS Learning Materials with Large Language Models"},{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"c03fa01fbb9c77fe3d10609ba5f1dee33a723867","title":"ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"},{"paperId":"703f79763d534dbf9674132cec890f432dcc19ec","title":"A Survey of Deep Learning Models for Structural Code Understanding"},{"paperId":"4b27f18bff43d605805c92696a979714ced0b805","title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"979eb5c97c49d7979447ed684500895a24d75ac4","title":"The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"916a06a6d51aa93de27aac2f3e14faed08dd6706","title":"Formal Mathematics Statement Curriculum Learning"},{"paperId":null,"title":"Natural Language Processing with Transformers: Building Language Applications with Hugging Face"},{"paperId":"f7987fa2aadc0b368c185dc4d2fdb1337a202c32","title":"Solving Probability and Statistics Problems by Program Synthesis"},{"paperId":"4e40595d6ecba027cebb4f2e3b43ae44bcf51daf","title":"Solving Linear Algebra by Program Synthesis"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"0b077c9577f4297dcf3da835e253d21965bbc6e0","title":"CoTexT: Multi-task Learning with Code-Text Transformer"},{"paperId":"1c0752fb3e9ab5c9392f196225075422f26b5110","title":"How could Neural Networks understand Programs?"},{"paperId":"3a863672b072a37ac8d2eb6b780234f6b748a2d9","title":"Generating bug-fixes using pretrained transformers"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"f389c09ad85263bdd1bb2518d61c90a8a558441d","title":"MulCode: A Multi-task Learning Approach for Source Code Understanding"},{"paperId":"20d37eb44ad65735f243938961fde9ba5b4d26b7","title":"D2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using Differential Analysis"},{"paperId":"3a1efac4a8aa2f40a8128e034c01f18660652dd5","title":"DOBF: A Deobfuscation Pre-Training Objective for Programming Languages"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"8b4c857311c001f6ed0cd790cce4af4dfcfb6533","title":"Deep Graph Matching and Searching for Semantic Code Retrieval"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"021bbcefc993c389bad6c1daefd8ff92d0fc2441","title":"Contrastive Code Representation Learning"},{"paperId":"2a405483796dfedf5d95483aa8880c57626e0e9f","title":"Integrating Tree Path in Transformer for Code Representation"},{"paperId":null,"title":"Fsf-funded call for white papers on philosophical and legal questions around copilot"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"49cf6a22a5dac5bc98b653534af65ffa0bc0e76d","title":"Multi-task Learning based Pre-trained Language Model for Code Completion"},{"paperId":"406afe68e789fcb0d7e3d24ebea65b53d206f740","title":"Exploring Software Naturalness through Neural Language Models"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"37a23c43ddf09ea97b82b38e2827a2229cfae545","title":"Novel positional encodings to enable tree-based transformers"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"00549af4bc3270e0f688acbf694f912d7ee39cad","title":"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks"},{"paperId":"e3ef11877bdd08140fcabf358dd9fc5bef6b15e0","title":"Recommendations for Datasets for Source Code Summarization"},{"paperId":"516497e7fc39ebb126bed28dcc5c6129e500acb8","title":"An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation"},{"paperId":"98d1307bed619b58b4a44acd8e65ac58495776c2","title":"code2seq: Generating Sequences from Structured Representations of Code"},{"paperId":"73bade9162bc4df6ca6c9ab8a7eb2d54d35acad2","title":"code2vec: learning distributed representations of code"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e3deff41f9d3eb1e141207b69d6c7a4c007bd63e","title":"Program Synthesis Through Reinforcement Learning Guided Tree Search"},{"paperId":"41c8cabe475d85d7548c25935da19ea5b96dfe06","title":"Neural Code Comprehension: A Learnable Representation of Code Semantics"},{"paperId":"e3d772986d176057aca2f5e3eb783da53b559134","title":"Unsupervised Machine Translation Using Monolingual Corpora Only"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"32aa2517b03c871c11e521c2a3406f457833e2c3","title":"Summarizing Source Code using a Neural Attention Model"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"9277dc7dd5b348645e8fcf4a987e5b3fc9132c7c","title":"Towards a Big Data Curated Benchmark of Inter-project Code Clones"},{"paperId":"110b7719f16eae8d82af0f56fca6e802c842c181","title":"Mining source code repositories at massive scale using language modeling"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"9a56a9cea19b83bf46ab2d47b59bc1ea3020a2b1","title":"Experiences Using Static Analysis to Find Bugs"},{"paperId":"b7eceec1e8edfa769fdd095db16897a061b02a79","title":"Compilers: Principles, Techniques, and Tools (2nd Edition)"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"95d886a0d097f0a1a81db8f431e744996ecc3048","title":"Static Analysis versus Software Model Checking for Bug Finding"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"5a6dbe11f7bd7182ca008b0f94b75fe5cac57a08","title":"Types and programming languages"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"45510aef1c064bbd16361dc9413ddd2068bebe53","title":"Compiler Construction: Principles and Practice"},{"paperId":"f8d77bb8da085ec419866e0f87e4efc2577b6141","title":"Serial Order: A Parallel Distributed Processing Approach"},{"paperId":"111fd833a4ae576cfdbb27d87d2f8fc0640af355","title":"Learning internal representations by error propagation"},{"paperId":"fef8d6579c5ba743baf3aa4c8dda78516284c3d9","title":"The Cloze Procedure"},{"paperId":"e2e0dd827011bb9bcb09578efe95c973a1f413b5","title":"The Mythical Man-Month: Essays on Softw"},{"paperId":"766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd","title":"“Cloze Procedure”: A New Tool for Measuring Readability"}],"id":"bae76e1d13abe54f66dc140be53538b864578ba8","summary":"This paper presents a comprehensive survey of the NCI domain, including a thorough review of pretraining techniques, tasks, datasets, and model architectures, and hopes it will serve as a bridge between the natural language and programming language communities."},{"url":"https://www.semanticscholar.org/paper/acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code","venue":"ArXiv","year":2021,"referenceCount":119,"citationCount":484,"influentialCitationCount":139,"publicationDate":"07/07/2021","authors":"Mark Chen,Jerry Tworek,Heewoo Jun,Qiming Yuan,Henrique Ponde,Jared Kaplan,Harrison Edwards,Yura Burda,Nicholas Joseph,Greg Brockman,Alex Ray,Raul Puri,Gretchen Krueger,Michael Petrov,Heidy Khlaaf,Girish Sastry,Pamela Mishkin,Brooke Chan,Scott Gray,Nick Ryder,Mikhail Pavlov,Alethea Power,Lukasz Kaiser,Mohammad Bavarian,Clemens Winter,Philippe Tillet,F. Such,D. Cummings,Matthias Plappert,Fotios Chantzis,Elizabeth Barnes,Ariel Herbert-Voss,William H. Guss,Alex Nichol,I. Babuschkin,S. Balaji,Shantanu Jain,A. Carr,J. Leike,Joshua Achiam,Vedant Misra,Evan Morikawa,Alec Radford,M. Knight,Miles Brundage,Mira Murati,Katie Mayer,P. Welinder,Bob McGrew,Dario Amodei,Sam McCandlish,Ilya Sutskever,Wojciech Zaremba","citations":[{"paperId":"c532f1df90925e5c69789f0cd99248d8a2a2e5bc","title":"My AI Wants to Know if This Will Be on the Exam: Testing OpenAI’s Codex on CS2 Programming Exercises"},{"paperId":"288055fa8954c6ea6cdafee25cd523108b716d15","title":"Explicit or Implicit? On Feature Engineering for ML-based Variability-intensive Systems"},{"paperId":"d8405996b4d08c304098636aedd9e1c1a1e262ee","title":"The Premature Obituary of Programming"},{"paperId":"cb29cf52f0f7d2e4324c68690a55b22890f2212d","title":"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection"},{"paperId":"907a77639069bb7dd270f017068745706133cffc","title":"Inaccessible Neural Language Models Could Reinvigorate Linguistic Nativism"},{"paperId":"6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc","title":"See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning"},{"paperId":"1f22de83d912176cb8857efa1c6d65b14d6a2f5c","title":"ChatGPT is not all you need. A State of the Art Review of large Generative AI models"},{"paperId":"23c9c8a3abf9f17c176904faa97a7f233cb2c03f","title":"EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata"},{"paperId":"c27bbdd8968c11513a68383145f7935293a57c25","title":"Exploring the Approximation Capabilities of Multiplicative Neural Networks for Smooth Functions"},{"paperId":"a3e355b5de868f34fdfa2500415c5f74c69d2091","title":"Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models"},{"paperId":"c9ad9d69d7568110dd5527598a92c7f8b335eef4","title":"Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations"},{"paperId":"468992bf970c37bd1fef58b78a6c2fcd8c018868","title":"Scaling Laws for Generative Mixed-Modal Language Models"},{"paperId":"5435ed7c26f0c250493f244acffb69dd929d116b","title":"Structured Case-based Reasoning for Inference-time Adaptation of Text-to-SQL parsers"},{"paperId":"4221f29aec5ce9feedddc1644f074af19f8d110e","title":"FlashFill++: Scaling Programming by Example by Cutting to the Chase"},{"paperId":"e028ba59aacec72a55164e274e1d64896fea0256","title":"Efficient Mutation Testing via Pre-Trained Language Models"},{"paperId":"dae74645479f7c1fa3671066f9e24ec6c20c17ec","title":"TrojanPuzzle: Covertly Poisoning Code-Suggestion Models"},{"paperId":"490f1e8ff352bded30bcde01d5b4769d6c2d2dd5","title":"Adversarial Attacks on Neural Models of Code via Code Difference Reduction"},{"paperId":"b2a8b21062718f930dbb1662a93ae1f13298fa1f","title":"Serenity: Library Based Python Code Analysis for Code Completion and Automated Machine Learning"},{"paperId":"270b015093073d3ba254928b6d736a59870d3fb1","title":"Chatbots As Fluent Polyglots: Revisiting Breakthrough Code Snippets"},{"paperId":"0392d58335ce674a70f5e58ac8c438de296a0e6a","title":"Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models"},{"paperId":"69e330037afd18e5546fdcacbc9a9f7deb69bba9","title":"Memorization and Generalization in Neural Code Intelligence Models"},{"paperId":"a8e0ba16346b72c3a04dd0b1da84bc5f28900174","title":"Using GitHub Copilot to Solve Simple Programming Problems"},{"paperId":"ab8a53be2ff8a756776aa0245ad8da41189d60d6","title":"Fuzzing Deep-Learning Libraries via Large Language Models"},{"paperId":"b8d06dd769f89d08bdd9997d7bd363c89ede845b","title":"ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models"},{"paperId":"4bea09d4c897fb201c032b9eb605a943b1e70435","title":"CORRPUS: Detecting Story Inconsistencies via Codex-Bootstrapped Neurosymbolic Reasoning"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning"},{"paperId":"269df328eec08b56b7b1f38a7555797fe2b999b6","title":"ReCode: Robustness Evaluation of Code Generation Models"},{"paperId":"d3a7a4543d83f568f79d1febe8379465ff0140c9","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"0f38267a8ba32789f5d3b1b19820f86940fea052","title":"Generation-Augmented Query Expansion For Code Retrieval"},{"paperId":"3ee9c65366efbb17adf370c39f20dbef60d53670","title":"Towards Reasoning in Large Language Models: A Survey"},{"paperId":"5c32c653735b43a0a8923ca65ac191bd4bf15311","title":"Precise Zero-Shot Dense Retrieval without Relevance Labels"},{"paperId":"29be9045fb09f0c947fb24c76bd1136d47880d96","title":"Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions"},{"paperId":"3810345aef8d1146452196e26ac49bdc07b26d8b","title":"Can Current Task-oriented Dialogue Models Automate Real-world Scenarios in the Wild?"},{"paperId":"bf5fbd690f24f873df86d1b0a06579cf42f7dc36","title":"Dialog2API: Task-Oriented Dialogue with API Description and Example Programs"},{"paperId":"433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context"},{"paperId":"bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"8210cef990b8e5cddbc95000e46309bdd25337f7","title":"Asking Clarification Questions for Code Generation in General-Purpose Programming Language"},{"paperId":"6756fcd998caeb7b23702e08559e63710179334c","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"9e8f0d9ba4e7af673c8b214b3764e020706dd1f3","title":"Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"cef330bacf014d60daabbd489647b2006af130ca","title":"Discovering Language Model Behaviors with Model-Written Evaluations"},{"paperId":"42630c03d3817b1153d245f20742ad4b30a80b75","title":"JEMMA: An Extensible Java Dataset for ML4Code Applications"},{"paperId":"3b8ccc7ec80b8775de603e248ac1ca2b919d6b70","title":"Chatbots in a Botnet World"},{"paperId":"9cc5c25517c3a78183a052e8e93a44e85bb17432","title":"Improving Cross-task Generalization of Unified Table-to-text Models with Compositional Task Configurations"},{"paperId":"ae441f7305dc2cd58c708528b3ecee3501cc5c46","title":"Plansformer: Generating Symbolic Plans using Transformers"},{"paperId":"4290a70025f29d7054c550c75ae6b24c38a79d12","title":"SE Factual Knowledge in Frozen Giant Code Model: A Study on FQN and its Retrieval"},{"paperId":"e4f8cb7bb933d95bec8d6eaeeb9d1815ed095f21","title":"Benchmarking Large Language Models for Automated Verilog RTL Code Generation"},{"paperId":"6bc87e51018d6de55011e95a0d43c588dd44a1e8","title":"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages"},{"paperId":"2cbddba62a1d1185d364f876b35dc9d6689d60e4","title":"Diverse Demonstrations Improve In-context Compositional Generalization"},{"paperId":"7932b714e2ae1def5828df52b97f1decb9bebd32","title":"Considerations for Differentially Private Learning with Large-Scale Public Pretraining"},{"paperId":"f788c8f1e331dc6b648e9aae5a08e059352bfa13","title":"Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments"},{"paperId":"c548b05a9696023a6eb5ff6d93a9a00e850b1ea8","title":"Automated Variable Renaming: Are We There Yet?"},{"paperId":"9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming"},{"paperId":"4c26a6a777f1d46ffe79e3f41f8fd1ef6a6bd8c9","title":"Automatically Generating CS Learning Materials with Large Language Models"},{"paperId":"8bfd39e6e8f15531ffb071f2c6470e1e6e0a4aff","title":"LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models"},{"paperId":"97148858b395c5122e09d6e8a20dd7016a111aa3","title":"Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning"},{"paperId":"8175ce2cbb99b6a394bdac152ae39d413f4f1380","title":"Codex Hacks HackerRank: Memorization Issues and a Framework for Code Synthesis Evaluation"},{"paperId":"fe3ead702e8e8948d00caef9bc9dd075dc560236","title":"I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification"},{"paperId":"dca3bc28a7d404b28780a813ea7072eda809e6c0","title":"Programming Is Hard - Or at Least It Used to Be: Educational Opportunities And Challenges of AI Code Generation"},{"paperId":"720130cea1933e0d9ff4915e0a9a869bf1ab0221","title":"Automated Quantum Software Engineering: why? what? how?"},{"paperId":"c4607fdcc6bcbc214546147062104d9f50493810","title":"On Mixed-Initiative Content Creation for Video Games"},{"paperId":"be76b0f32e287d866bc7aefc700052f9825ed3ce","title":"BudgetLongformer: Can we Cheaply Pretrain a SotA Legal Language Model From Scratch?"},{"paperId":"32b58766a1bfcef7ebba07070a272687aa518206","title":"Explicit Knowledge Transfer for Weakly-Supervised Code Generation"},{"paperId":"1dee3eba1b0bd3f9074ee6a9e276f982835a58e0","title":"Genetic Programming with Local Scoring"},{"paperId":"6d4a12ea469ff08634eeb24c47b265a7dca2fce2","title":"PADL: Language-Directed Physics-Based Character Control"},{"paperId":"27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation"},{"paperId":"46d0a832fada6147bceb0bd4e39928e482733246","title":"How Important are Good Method Names in Neural Code Generation? A Model Robustness Perspective"},{"paperId":"477d0b2abf07ee92732698f9aeb3c784f28ffa88","title":"On the Security Vulnerabilities of Text-to-SQL Models"},{"paperId":"34009b5d7a3ab44863c8dfa0d8dc4383c86c3115","title":"Deep-Learning-based Vulnerability Detection in Binary Executables"},{"paperId":"8aa23a86603f7dd4eceda3d2e0337ba90dff7f4f","title":"CodeExp: Explanatory Code Document Generation"},{"paperId":"6d7b8a478801bd9d21df82d5f33ae6eced90da5e","title":"Solving math word problems with process- and outcome-based feedback"},{"paperId":"edcd520e553dc58c728eceb8433e3d155955a89a","title":"Complementary Explanations for Effective In-Context Learning"},{"paperId":"ce9397eaa3de0f791bead5d16f14b5dd4a15052f","title":"GitHub Considered Harmful? Analyzing Open-Source Projects for the Automatic Generation of Cryptographic API Call Sequences"},{"paperId":"30624a18720bf93a85dc3efe570df271a8c9f4c3","title":"Program Repair"},{"paperId":"2f29426556cb9d434f114b5a8472fd9d65e71d4f","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"6d56ab40eba28678c4f4d6b054f2ae4e7048c928","title":"CLAWSAT: Towards Both Robust and Accurate Code Models"},{"paperId":"05f09050118d82100d04e56ba8b54836753fa9b4","title":"Programming by Example and Text-to-Code Translation for Conversational Code Generation"},{"paperId":"ef3bbee11c03de1e7e3672121c58768b10d34436","title":"Exploring the Efficacy of Pre-trained Checkpoints in Text-to-Music Generation Task"},{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"ce47e99c1ebab6fc253b1be58bd9478a87d90288","title":"PAL: Program-aided Language Models"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"0b871a9f12e5c2da1b291a8b166c671256ebe1cd","title":"A Copy Mechanism for Handling Knowledge Base Elements in SPARQL Neural Machine Translation"},{"paperId":"e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"95915aa592fdfc73f039c13472a21d3e4220f129","title":"On the Compositional Generalization Gap of In-Context Learning"},{"paperId":"9dab4c20648cd4c7c6830e6274a95294b014aac9","title":"QAmeleon: Multilingual QA with Only 5 Examples"},{"paperId":"ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","title":"Evaluating How Fine-tuning on Bimodal Data Effects Code Generation"},{"paperId":"965e409a3e7b5670d609837fac9823b160d6639c","title":"Logical Tasks for Measuring Extrapolation and Rule Comprehension"},{"paperId":"5f9ca3133898bcc7b7d76caadb18598cc336b208","title":"Calibrated Interpretation: Confidence Estimation in Semantic Parsing"},{"paperId":"a4bdc300db297756f36bedee2859b62df8e268c2","title":"Follow the Wisdom of the Crowd: Effective Text Generation via Minimum Bayes Risk Decoding"},{"paperId":"fa4188bcc6728b580699f714a8a6fe5fc60d7dfe","title":"Rethinking data-driven networking with foundation models: challenges and opportunities"},{"paperId":"048ed70192de0232086eb32a95ffb3be8d336c76","title":"Metaphors We Learn By"},{"paperId":"632ab7663e6d64578ceda1d1df9ec525b503bacb","title":"Steps towards prompt-based creation of virtual worlds"},{"paperId":"327f1561b544b0a3b9d8d5d0e6d82c2a5911fca9","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"ac3d7ae8b4acb137492d4a8d8bcff480b89fa000","title":"Programming Pedagogy and Assessment in the Era of AI/ML: A Position Paper"},{"paperId":"2037eb819d08446a057634a851ebf9afb8d7ae4b","title":"Assessing the quality of GitHub copilot’s code generation"},{"paperId":"87bb0bc9195751351761b21c3adfa7d055a824ea","title":"Neural language models for code quality identification"},{"paperId":"71280dba5bda65c162f9deaffed7d3d20692ca0a","title":"SecurityEval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques"},{"paperId":"61ddf932488405ab1c7b275460d2b3c5dfa274a0","title":"Fixing Model Bugs with Natural Language Patches"},{"paperId":"2abed82162c47a0cc32cd62afcf46b0745541017","title":"Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book"},{"paperId":"ec7324a15009a9bd0b676f6b17762f759cf5dd9a","title":"Large Language Models Are Human-Level Prompt Engineers"},{"paperId":"511df4b4e44db2852228091c297dcb5c1d212017","title":"MPCFormer: fast, performant and private Transformer inference with MPC"},{"paperId":"3214fbf2a78c5d1bff4d2a2e67e3f22a341df460","title":"Hybrid Rule-based and Machine Learning System for Assertion Generation from Natural Language Specifications"},{"paperId":"afd834af31f043e7c1d348c6a51d299d029dafca","title":"SPEAK YOUR MIND: INTRODUCING APTLY, THE SOFTWARE PLATFORM THAT TURNS IDEAS INTO WORKING APPS"},{"paperId":"20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"3c74317647301b28234663bd3bf084498a647b6f","title":"Emergent Linguistic Structures in Neural Networks are Fragile"},{"paperId":"1819c5c8c556db205ec9a49d5ba4a8c2afb28a4a","title":"Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy"},{"paperId":"6b8f26678785ebd7b7b27984af3cb9a273b722b0","title":"Towards Zero-Shot and Few-Shot Table Question Answering using GPT-3"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"f5b3cb14e0947c62b470d2072483481f14258738","title":"A Solvable Model of Neural Scaling Laws"},{"paperId":"cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers"},{"paperId":"41f5e1ad7793593befc0b9c38f756836e8b07c98","title":"Code4ML: a Large-scale Dataset of annotated Machine Learning Code"},{"paperId":"99323bd786ee5be1e1aa589858e14e89630f207b","title":"Exploring the Learnability of Program Synthesizers by Novice Programmers"},{"paperId":"0566c1c3eeeef5c968fced6d80b77fe22d02bbd9","title":"Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language"},{"paperId":"0b340dd78fd04bbde2807d5efedb796d319355e3","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?"},{"paperId":"dcf3caaf797f4496241c7375c0e8ede8cc5616f1","title":"Learning to Configure Computer Networks with Neural Algorithmic Reasoning"},{"paperId":"a6df60ce44ab5d968bb6c649f30d1ba8e121fa98","title":"Multi-Viewpoint and Multi-Evaluation with Felicitous Inductive Bias Boost Machine Abstract Reasoning Ability"},{"paperId":"a593ce5d2a93f3113be7717d08d1ba8e62fd7ddf","title":"Benchmarking Language Models for Code Syntax Understanding"},{"paperId":"472d87be3dc298102e058be55a814cc6d2085b39","title":"Towards Deceptive Defense in Software Security with Chaff Bugs"},{"paperId":"5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","title":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming"},{"paperId":"f031ba42cf82f106200bb03fbb91dd5671a59b9c","title":"Practical Program Repair in the Era of Large Pre-trained Language Models"},{"paperId":"38e1a9c5599fc7597b7c5ffd37951ba5f528094c","title":"XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Semantic Parsing"},{"paperId":"50ee0be85da0e98d9ec0157c7b89f76b4b5d1516","title":"Contrastive Search Is What You Need For Neural Text Generation"},{"paperId":"152dc3042f5d4fc5a2686b5f4e0904f1e12a9207","title":"Code4Struct: Code Generation for Few-Shot Structured Prediction from Natural Language"},{"paperId":"dae5c4660dab4c2b0a0be586f8537db980925d4a","title":"When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks"},{"paperId":"aacf8a1bf2349b29159ab884ea82465330c2e256","title":"Combining OCL and natural language: a call for a community effort"},{"paperId":"d26f616699a122e5455a13189e276002ee4cf923","title":"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs"},{"paperId":"1acb761ec624104124eda4a8c681e59adf1bf2b9","title":"Formalizing Chemical Theory using the Lean Theorem Prover"},{"paperId":"a6ef5bad716091fb1888bf365f6129628ab3a5ee","title":"ObSynth: An Interactive Synthesis System for Generating Object Models from Natural Language Specifications"},{"paperId":"5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"4bef9d46209ac8988ea5ab83547149760d4af65e","title":"Automatic Document Selection for Efficient Encoder Pretraining"},{"paperId":"4c2534f9b03ac2f3810c07abc398a11bcf47258e","title":"Transformers Learn Shortcuts to Automata"},{"paperId":"4431a39f677fe59b07b3f0cfde7b10f7208cf46c","title":"N-Best Hypotheses Reranking for Text-To-SQL Systems"},{"paperId":"8ffe7b7eeddbd4c22d642b0a48379d17e61c3bab","title":"Soft-Labeled Contrastive Pre-training for Function-level Code Representation"},{"paperId":"663a41c866d49ce052801fbc88947d39764cad29","title":"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"},{"paperId":"711d5e8ddbb840ad31a9ffa3d38590603ba69a92","title":"Prompting GPT-3 To Be Reliable"},{"paperId":"706b1e0c7dcc58ac6ba80a0ca37efc2993e6e5ef","title":"CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling"},{"paperId":"a0e086754a9de168ae2674f472affe4c8d1502e6","title":"Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods"},{"paperId":"39e40821b7207125e54e6ed7112e55cd38c6f0c3","title":"Language Models of Code are Few-Shot Commonsense Learners"},{"paperId":"3f2fa77ce40d21a7cf5e9a4dbf594c85a31576d0","title":"Searching for Better Database Queries in the Outputs of Semantic Parsers"},{"paperId":"de7d334a543d077f4162ebcd8da7eee843b7b10a","title":"Predictive Querying for Autoregressive Neural Sequence Models"},{"paperId":"7619b0aad6f16a0328021c5fdd3d97239e362e97","title":"Visual Language Maps for Robot Navigation"},{"paperId":"045a6f92b58f08d5b305dd5d661316a506ee4d43","title":"Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation"},{"paperId":"285dcaad8a5da5159bf2550239f3365b5282bf05","title":"Next Syntactic-Unit Code Completion and Applications"},{"paperId":"825333b7efe2cade106eaf36c7e731f757974806","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot"},{"paperId":"0c78a473e33a81246d5c0fbbda7e7de168814c18","title":"FlexType: A Plug-and-Play Framework for Type Inference Models"},{"paperId":"dcff38de0e5fb47bdb31d472c21b0c2d88cbc4fc","title":"AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models"},{"paperId":"259b7a01700c39d5669e88d1434873ea38a13528","title":"In-Context Policy Iteration"},{"paperId":"90350aa626bed47b02d0c162462e5b0ca82be6b2","title":"Automatic Chain of Thought Prompting in Large Language Models"},{"paperId":"d35a6adf01f35a18f655234198a4778d3307487b","title":"GNM: A General Navigation Model to Drive Any Robot"},{"paperId":"62f0db3a5ad5c795ec18fc7a6e7b01836809df57","title":"Language Models are Multilingual Chain-of-Thought Reasoners"},{"paperId":"c140fe515de2f20d0c85c813c7b3ec1defc41f9d","title":"Binding Language Models in Symbolic Languages"},{"paperId":"1afe2799df238ca749534860552501eaf51c77eb","title":"TgDLF2.0: Theory-guided deep-learning for electrical load forecasting via Transformer and transfer learning"},{"paperId":"55e3fe05598be7c3dd357d51166869f6571b824f","title":"Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors"},{"paperId":"b69b84706fe84c4c614e4473760c57dffbfeb9a0","title":"Waveformer: Linear-Time Attention with Forward and Backward Wavelet Transform"},{"paperId":"dd12f12015220d4beb2967fe23860d575e7a9e53","title":"Grounding Language with Visual Affordances over Unstructured Data"},{"paperId":"04db926687fa9af8f7e9be04901f440bea135da0","title":"Guiding the PLMs with Semantic Anchors as Intermediate Supervision: Towards Interpretable Semantic Parsing"},{"paperId":"7d6f17706cbcfcca55f08485bcbf8c82e00c9279","title":"Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals"},{"paperId":"5c4bece777bbc7c52cdd4bbb6e222163f6a580dd","title":"Unveiling the Black Box of PLMs with Semantic Anchors: Towards Interpretable Neural Semantic Parsing"},{"paperId":"0b8772b7790c69f40897b5eb7f8fd57f24138f3d","title":"ContraGen: Effective Contrastive Learning For Causal Language Model"},{"paperId":"8a854331c593c6a766fa3b8037fb2ad1b95a6f06","title":"An Empirical Study of Code Smells in Transformer-based Code Generation Techniques"},{"paperId":"7002faf46875c84cb3ad148f362118b46946f163","title":"CodeDSI: Differentiable Code Search"},{"paperId":"2a0456b0408cd4c33f2ff4400374e7be2497a362","title":"Repairing Bugs in Python Assignments Using Large Language Models"},{"paperId":"95fa2b27ab7eb84738441ee16da97323538938f9","title":"I Speak, You Verify: Toward Trustworthy Neural Program Synthesis"},{"paperId":"cd6496bc404e18a24f634e3dded2ed1cdca03e0f","title":"Learning to Learn with Generative Models of Neural Network Checkpoints"},{"paperId":"363758e9e296adc9391ed731e834809cf5d4c19b","title":"Are machine programming systems using right source-code measures to select code repositories?"},{"paperId":"971e875e28f26240987d2c9470d1ee74ad204205","title":"Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction"},{"paperId":"c03fa01fbb9c77fe3d10609ba5f1dee33a723867","title":"ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"},{"paperId":"885676b8a05437868f7b83c134a99f991190a1de","title":"T5QL: Taming language models for SQL generation"},{"paperId":"7b2be89668e8f18ed20dc51be7646bd31cc0aff3","title":"Assisted Specification of Code Using Search"},{"paperId":"83d879a830ac4286945e628e670c30fefb1493c6","title":"NL2INTERFACE: Interactive Visualization Interface Generation from Natural Language Queries"},{"paperId":"a7435722d8ab595da5a9c70ac9160f57d0dcd75a","title":"Enabling Transformers to Understand Low-Level Programs"},{"paperId":"97e1903b4afc811a9b5fa9e55723e80ba48bf46f","title":"Code as Policies: Language Model Programs for Embodied Control"},{"paperId":"cd11837054a3396542daca1b9966b03a057fbe9f","title":"Malicious Source Code Detection Using Transformer"},{"paperId":"e54a6201ea31e3c11576a343ff04cab3cbba8ccb","title":"Multi-donor Neural Transfer Learning for Genetic Programming"},{"paperId":"ef29fb8cc6bdda4b011288f51da521d3c25fc53d","title":"Learning to Prevent Profitless Neural Code Completion"},{"paperId":"7d2c43a4a9e3198cd133bb0ab86ca69a5125f2c5","title":"Exploring Code Style Transfer with Neural Networks"},{"paperId":"ac2e15fbfe3ea338725f5d33d17a5a687609c431","title":"On The Computational Complexity of Self-Attention"},{"paperId":"372dd97de200970859315bce7e150fe50baecad5","title":"Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence"},{"paperId":"ace4d199aa72ab0808af0f30a61fc16727c95dec","title":"AudioLM: a Language Modeling Approach to Audio Generation"},{"paperId":"9360390b02b9a09ece9a2486055b17e18dc5d3f6","title":"Automatic Code Documentation Generation Using GPT-3"},{"paperId":"9afab8dc694269d205d769eaea549d8f7558d776","title":"Exploring the Verifiability of Code Generated by GitHub Copilot"},{"paperId":"6b5d1e50894b1f28e4798cf20e9ffa88b9ec011a","title":"How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models"},{"paperId":"5581bf85386737bd3378eec68189759a05280bea","title":"FOLIO: Natural Language Reasoning with First-Order Logic"},{"paperId":"0509c25103939d59ed4e27b1393e74ad5734c453","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot"},{"paperId":"befde3e07ce97f02f12fe92ab27e99f23ccd17aa","title":"Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation"},{"paperId":"1aac692ca061feb846cf32cd61a1d422f89593a1","title":"A Survey on Text-to-SQL Parsing: Concepts, Methods, and Future Directions"},{"paperId":"6032212d5790b6a580d68d469a9895aad6238c89","title":"Diverse Title Generation for Stack Overflow Posts with Multiple Sampling Enhanced Transformer"},{"paperId":"453a8fac3be9282be53908f0735160d0d21e0f48","title":"Repair Is Nearly Generation: Multilingual Program Repair with LLMs"},{"paperId":"1c8e15f15d67c5974445634bb971e2275e957aff","title":"Security Implications of Large Language Model Code Assistants: A User Study"},{"paperId":"9b61de7038290751377b64293baaf42f3e7cf441","title":"An Empirical Evaluation of Competitive Programming AI: A Case Study of AlphaCode"},{"paperId":"05ccfb058dc6788254742722278b0af2fc87f083","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"99f85119f113b5498517928eff74a904b69e37b7","title":"CCTEST: Testing and Repairing Code Completion Systems"},{"paperId":"512e16b9aef6ca6cb973d734b4cc66661ea33498","title":"Targeted Honeyword Generation with Language Models"},{"paperId":"53b3b15e8cbe2baf82cd0de3709e0a1e6f677415","title":"Limits of an AI program for solving college math problems"},{"paperId":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?"},{"paperId":"8c2d9d2aa891e3b53bbd9166c1b804a0741fc44a","title":"Finding Reusable Machine Learning Components to Build Programming Language Processing Pipelines"},{"paperId":"7018ff49ca3b81f2ed6228b097a471c2529986e4","title":"CoditT5: Pretraining for Source Code and Natural Language Editing"},{"paperId":"def2e28863338cb20782eb2015a39d32df697ed6","title":"Learning to Improve Code Efficiency"},{"paperId":"c067664a45dce31411b3052c635c044ad4587db4","title":"Generating Diverse Code Explanations using the GPT-3 Large Language Model"},{"paperId":"654cd297b307c0c6fa08732511ba852b8dce1977","title":"Out of the BLEU: how should we assess quality of the Code Generation models?"},{"paperId":"15bacb240e2598457af4ded3039b6988aa9706f0","title":"Few-shot Adaptation Works with UnpredicTable Data"},{"paperId":"63c670ba8018da0a7e33c34ffd84c2a3ca54b894","title":"Language Models Can Teach Themselves to Program Better"},{"paperId":"ec4c8d99eb1c028c43af6d8bbf727392d351cb59","title":"Efficient Training of Language Models to Fill in the Middle"},{"paperId":"dd112d4dbd4656223770989778f39700de3052bc","title":"A Hazard Analysis Framework for Code Synthesis Large Language Models"},{"paperId":"1e3ec5709b2ca43233c566cd77dbabbd6892bfa6","title":"Neurosymbolic repair for low-code formula languages"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"4f4326e69a1a34f68f7b14556077571a1752c319","title":"Artificial Intelligence and Deep Learning for Rheumatologists"},{"paperId":"14a797972c3a7045c8449f20d524234ffc36bf24","title":"Mimetic Models: Ethical Implications of AI that Acts Like You"},{"paperId":"e9fc39f56abbc6b8aed1e05496d985e70345a95a","title":"An extensive study on pre-trained models for program understanding and generation"},{"paperId":"fc994f026a04e4cfc8e24ee68994836700166421","title":"COBE: A Natural Language Code Search Robustness Benchmark"},{"paperId":"2f2750b48a6f958ff12cba90e99695123d1e2f47","title":"Using pre-trained language models to resolve textual and semantic merge conflicts (experience paper)"},{"paperId":"e37155d21818513bd40d64ee212099aac82bd6f8","title":"Less training, more repairing please: revisiting automated program repair via zero-shot learning"},{"paperId":"5d142f8b1dc67ec6307050d34dbcd6dfd4c0218c","title":"Active Data Pattern Extraction Attacks on Generative Language Models"},{"paperId":"75cb3efeb69e044cc07613a3eba64504483e999d","title":"Combing for Credentials: Active Pattern Extraction from Smart Reply"},{"paperId":"b3a54332a0791751fcf234f6264f242c42ac00d2","title":"DocPrompting: Generating Code by Retrieving the Docs"},{"paperId":"b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs"},{"paperId":"f3cf71c51b882fe3111d71c4bf104297d38197f8","title":"Inner Monologue: Embodied Reasoning through Planning with Language Models"},{"paperId":"142ebbf4760145f591166bde2564ac70c001e927","title":"Language Models (Mostly) Know What They Know"},{"paperId":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models"},{"paperId":"cdf54c147434c83a4a380916b6c1279b0ca19fc2","title":"LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action"},{"paperId":"53661ff6fdbfb8557c5b19895fad151792c62da7","title":"Few-shot training LLMs for project-specific code-summarization"},{"paperId":"cec43785ac6ede33cff208b6b828dc440cf43b2b","title":"Big Learning: A Universal Machine Learning Paradigm?"},{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"10bf4c1ca1531a49dae14d1226e53095306506ff","title":"LaMPost: Design and Evaluation of an AI-assisted Email Writing Prototype for Adults with Dyslexia"},{"paperId":"b17cc18e4130505b939f7d527082eb6be2a7fd5b","title":"Rationale-Augmented Ensembles in Language Models"},{"paperId":"61dd84f069f0329b1f3a84059be925cf7391140d","title":"Machine Learning for Aggregate Computing: a Research Roadmap"},{"paperId":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"52ca0f589fdcde1c45fd6dfb1b72248d4ecaefc0","title":"Code Translation with Compiler Representations"},{"paperId":"114eb5a35a3cd802cd1f46fff35c284e32ef6c54","title":"GitHub Copilot AI pair programmer: Asset or Liability?"},{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"1d160123cbbef972ea151a641dd435d57c727de8","title":"AixBench: A Code Generation Benchmark Dataset"},{"paperId":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code"},{"paperId":"f8f2b17083c10f730b711a938e2bb5da992086e7","title":"AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models"},{"paperId":"374bbb716aa007a65ac03f0220d3027fa724874d","title":"AI Challenges for Society and Ethics"},{"paperId":"6a53eeada90d83b9508e7e451d62fdc9d2476350","title":"Using cognitive psychology to understand GPT-3"},{"paperId":"f2c17758e74707d379b87372528221656d14b697","title":"Taxonomy of Risks posed by Language Models"},{"paperId":"e98799e709dc93a8ea721dd6b3e1398104797050","title":"Cracking the code: Co-coding with AI in creative programming education"},{"paperId":"660fde2f51e025638b8c937bf228ecaa5c5b649c","title":"XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence"},{"paperId":"2edc8efcda27c944a46f367acf6a5280b8f65525","title":"FixEval: Execution-based Evaluation of Program Fixes for Competitive Programming Problems"},{"paperId":"0d7a97961517e1d1289e913e81a93746a81ba117","title":"FixEval: Execution-based Evaluation of Program Fixes for Programming Problems"},{"paperId":"45263786d07f5751f7494fdeee3c8764836d02c4","title":"NatGen: generative pre-training by “naturalizing” source code"},{"paperId":"64c36adaeb803ff1d49771ed6a7ae7271e17b35d","title":"Training Discrete Deep Generative Models via Gapped Straight-Through Estimator"},{"paperId":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation"},{"paperId":"3ba793e937cb90ea3e82b4a6903ee4a95f307ddf","title":"X-Risk Analysis for AI Research"},{"paperId":"41f7bcca48c321071cbba7f9e7d735f65698dcce","title":"A Dataset and Benchmark for Automatically Answering and Generating Machine Learning Final Exams"},{"paperId":"647d81055d281f038b89a684db8d9c011e2a9bc0","title":"STable: Table Generation Framework for Encoder-Decoder Models"},{"paperId":"a64871352f8ac7f8aa226fda5cce70251a18a4fc","title":"Assessing Project-Level Fine-Tuning of ML4SE Models"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"40edfa97cd02268fccff75eb9c693b11c1a968b2","title":"Formal Specifications from Natural Language"},{"paperId":"0d08ffccc982781e310bb184397bbe64b9aef157","title":"Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models"},{"paperId":"1100dee3fd78655cddc4b7bfaef1161351d4fab5","title":"Automated Feedback Generation for Competition-Level Code"},{"paperId":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code"},{"paperId":"8c7fd98b6bc4f32772e76471afe8babc323f10d3","title":"CitySpec: An Intelligent Assistant System for Requirement Specification in Smart Cities"},{"paperId":"f1b6b34b4440a77ba86493f7062e8974062508c5","title":"Applying genetic programming to PSB2: the next generation program synthesis benchmark suite"},{"paperId":"4f161f3cf6a272061600c71cc2e8a325753a38f0","title":"Attention Flows for General Transformers"},{"paperId":"35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs"},{"paperId":"196cc546041cb6db167784f632037f0a1dcf4a79","title":"Generating Natural Language Proofs with Verifier-Guided Search"},{"paperId":"0efa0441da820b1905572666ba1974a06a9663fb","title":"NaturalProver: Grounded Mathematical Proof Generation with Language Models"},{"paperId":"4a4581003f56e8cb581ad6f383c037964765d3d5","title":"Active Programming by Example with a Natural Language Prior"},{"paperId":"047a8344e3cfa49c8354fc244387d57ef9d2f01d","title":"Memorization in NLP Fine-tuning Methods"},{"paperId":"8b7c11e773dc6ec32560d247193c9dd4c5109644","title":"Análise de Performance dos Modelos Gerais de Aprendizado de Máquina Pré-Treinados: BERT vs DistilBERT"},{"paperId":"8c90bfe05c06fd47eaec0f5b1662e06862572afe","title":"Looking for a Handsome Carpenter! Debiasing GPT-3 Job Advertisements"},{"paperId":"adce1da47d490dcdca254ccd43055ed4f4423bc2","title":"Summarize and Generate to Back-translate: Unsupervised Translation of Programming Languages"},{"paperId":"415be47b17f5214a1710010c7c18f4fafd3ef524","title":"AdaptivePaste: Code Adaptation through Learning Semantics-aware Variable Usage Representations"},{"paperId":"ea4749c6ed5f08b16bab31b1ca4fd3fdc259ae8f","title":"Improving automatically generated code from Codex via Automated Program Repair"},{"paperId":"0bcd59da541fdae66884afba8d25475a54a9da1a","title":"Automated Repair of Programs from Large Language Models"},{"paperId":"cbcd19395e4b5ad5e047e0476cb906ca6461df72","title":"Few-Shot Natural Language Inference Generation with PDD: Prompt and Dynamic Demonstration"},{"paperId":"d6c5aab433d9871cabc01ffb1e5e1ea89141155b","title":"KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation"},{"paperId":"58aacb967cc7fc25cfc9d51b7ad3e57ac00d119b","title":"Can Foundation Models Wrangle Your Data?"},{"paperId":"227966c9b4fe75271946d239507196408842a2f2","title":"Transformer-based Program Synthesis for Low-Data Environments"},{"paperId":"2ff6f2b4a175cbc533795b723d6f18d64cea3916","title":"A CLIP-Hitchhiker's Guide to Long Video Retrieval"},{"paperId":"c61ce808818308566124df2c8725c98d6bd38dc3","title":"A Precis of Language Models are not Models of Language"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"5922f437512158970c417f4413bface021df5f78","title":"A Generalist Agent"},{"paperId":"11d77a7a56381d9700befa86f5c93cf887108ecd","title":"Editorial: Theory of Mind in Humans and in Machines"},{"paperId":"7ef9aafc68511afab5b287e62b754576ea37b4ce","title":"Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks"},{"paperId":"9a7d4c2f4309a2a38ea8e5b23c6d616fa0952d44","title":"Neural language models for network configuration: Opportunities and reality check"},{"paperId":"d072b46a0504ac023d5035d8ec0c7876151245c4","title":"Playing Games with Ais: The Limits of GPT-3 and Similar Large Language Models"},{"paperId":"cdfe9580f63070f311151444f9df32818cc858bf","title":"An Empirical Evaluation of GitHub Copilot's Code Suggestions"},{"paperId":"3f400fff93d5ff352ee102d2c04d38f6214b4283","title":"CodePanorama: a language agnostic tool for visual code inspection"},{"paperId":"1f3a97d255a6d0b2d5535af75737ed43b1e96fc4","title":"Is GitHub Copilot a Substitute for Human Pair-programming? An Empirical Study"},{"paperId":"a8c072f6cbd4653a1f0888b3529898f66c0e6bb0","title":"Strategies for Reuse and Sharing among Data Scientists in Software Teams"},{"paperId":"06ce4c0d92ed51a2b5e6e363a6d3932b8f162e72","title":"Can Information Behaviour Inform Machine Learning?"},{"paperId":"fc32083203ce124375af2a225296a26576306f6c","title":"Language & Coding Creativity"},{"paperId":"779fa63ae731a1efaf0cb95cab8e47672a919581","title":"I Do Not Think It Means What You Think It Means: Artificial Intelligence, Cognitive Work & Scale"},{"paperId":"e5cbcb665486ae7077dc3a37a9fd6addeabca5cf","title":"PROPR: Property-Based Automatic Program Repair"},{"paperId":"f8aa0cab09bc0668276e2cb5690bae47fa50d350","title":"An Initial Look at Self-Reprogramming Artificial Intelligence"},{"paperId":"4bc040835fbff57ce6612306d794b8c6c8226086","title":"Discovering the Syntax and Strategies of Natural Language Programming with Generative Language Models"},{"paperId":"1fafaccebc4a74898a74c606f846318c4c2c7536","title":"On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"6050454e0446a3068617f73b0301453f3f67844d","title":"Stylette: Styling the Web with Natural Language"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"1f87dc41bdf2c4c78e2dce9c5c8adfef5e25a70c","title":"Passport: Improving Automated Formal Verification Using Identifiers"},{"paperId":"2ba7104f7b93d77940312664f3467b8f090d6d16","title":"On Distribution Shift in Learning-based Bug Detectors"},{"paperId":"07cd498aacfb4d39fa2e0e8d8a9c8ad881257300","title":"Prompt Engineering for Text-Based Generative Art"},{"paperId":"1676160139ca59c6728472f34092db69460567a8","title":"A Taxonomy of Prompt Modifiers for Text-To-Image Generation"},{"paperId":"bb7e46f316d319f9819c3554c99995ef8361ae9c","title":"CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex"},{"paperId":"012378718c34f0b17b3fcd7316371f8f4e4fdde2","title":"Addressing Leakage in Self-Supervised Contextualized Code Retrieval"},{"paperId":"a6b9a934fe039a5636a26e94fb47f872263d702c","title":"To What Extent do Deep Learning-based Code Recommenders Generate Predictions by Cloning Code from the Training Set?"},{"paperId":"9fcf3424eee7a607813980e114f1d9d2d3657960","title":"GLAD: Neural Predicate Synthesis to Repair Omission Faults"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"89e78d6f76b70c30804ecd3592fa05fccdc49b64","title":"Fix Bugs with Transformer through a Neural-Symbolic Edit Grammar"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"0286b2736a114198b25fb5553c671c33aed5d477","title":"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"},{"paperId":"c18624382afc57072446b056bf590ee891c078e7","title":"Is GitHub's Copilot as Bad As Humans at Introducing Vulnerabilities in Code?"},{"paperId":"6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis"},{"paperId":"a225d5d846ba5110232ed5bb32d54ea742b1c2d4","title":"KNN-Diffusion: Image Generation via Large-Scale Retrieval"},{"paperId":"e2baf81813d5a515f554ee60bcddcc57548f8c22","title":"Code Search: A Survey of Techniques for Finding Code"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"ada81a4de88a6ce474df2e2446ad11fea480616e","title":"Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language"},{"paperId":"d358ce242dacfd2ea738aa538779f06c79777f2b","title":"SELFIES and the future of molecular string representations"},{"paperId":"237f5ca6fcccef2b77a2212b34fb06a1dbd09b72","title":"Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting"},{"paperId":"af0899b397d5cf5974e939e5c1057abd7c54cdde","title":"MQDD: Pre-training of Multimodal Question Duplicity Detection for Software Engineering Domain"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"1a903282f7c19dbdb2714b852fb42dbb4675422b","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"365d0313908871118df59c469c4db10f4e1b96bd","title":"Linearizing Transformer with Key-Value Memory"},{"paperId":"fb5c11bbf63884f75d2da615fbf37a3bcfa2bd20","title":"Wordcraft: Story Writing With Large Language Models"},{"paperId":"79b88230fabb59a1d368641bbc822af0f09bf262","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"a522543180db6c5b21f47fe88abee44de158c85c","title":"Automatic Programming and Education"},{"paperId":"c347093e2dca530ce347526380b0b7aedf03a6b2","title":"CrossBeam: Learning to Search in Bottom-Up Program Synthesis"},{"paperId":"1d10023541f06701bf2f9ae8c91609e1055799ec","title":"Automating code review activities by large-scale pre-training"},{"paperId":"b951c0691a0d2ca65202a1eed2ccedf6e305d035","title":"CodeReviewer: Pre-Training for Automating Code Review Activities"},{"paperId":"590f6817b42407f96b079e82c935fae298196359","title":"Less is More: Summary of Long Instructions is Better for Program Synthesis"},{"paperId":"c96363c42bc8c465902c22b8c33c8704233f519e","title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages"},{"paperId":"b645e706651391eca1f692e7f560051c21b3dea4","title":"In-Context Learning for Few-Shot Dialogue State Tracking"},{"paperId":"51000d9f79be0eefd7972fe94e3c71dddc90d2c6","title":"Evaluating the Text-to-SQL Capabilities of Large Language Models"},{"paperId":"2b556fcf2ac634f03c1fb0ace5e602e829418e65","title":"ReACC: A Retrieval-Augmented Code Completion Framework"},{"paperId":"5dd7bc394e032eb0e982699a5f0c781fab9e3111","title":"Contrastive Visual Semantic Pretraining Magnifies the Semantics of Natural Language Representations"},{"paperId":"463c6944ddcb43280e1c9765c2245bc7b764ab68","title":"AI-Driven Development Is Here: Should You Worry?"},{"paperId":"1f11601f9eb8c0bee1d1d2cc7f4fa187e8c5e525","title":"The impact of lexical and grammatical processing on generating code from natural language"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"75b8444e1d82ebb6c6a32d8ea86af858e5de93e3","title":"Iterative Genetic Improvement: Scaling Stochastic Program Synthesis"},{"paperId":"d26fe2a7a7cc940d8485488e97460b144dc7d69e","title":"From Natural Language to Simulations: Applying GPT-3 Codex to Automate Simulation Modeling of Logistics Systems"},{"paperId":"76f023c3a819fc58989a064a1b50825b11fce95d","title":"Capturing Failures of Large Language Models via Human Cognitive Biases"},{"paperId":"4a6a65968a8eb8c09ffb57a7774ddabb596565b1","title":"COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics"},{"paperId":"7b5aa186ca8abc585607c5ec91562e127a398601","title":"Open-Ended Knowledge Tracing"},{"paperId":"b96cb62450cb77926c712d5cd50d1764101de605","title":"On the Implicit Bias Towards Minimal Depth of Deep Neural Networks"},{"paperId":"44d6c201a4056e260e9844bdbb01461ea9b1a011","title":"A data-driven approach for learning to control computers"},{"paperId":"6aae3ddbe142f7ae28f0f18bb6248dc7b3f41c00","title":"Probing Pretrained Models of Source Code"},{"paperId":"9cbc044e315cdefe9a255119037ac7c23e9abdd5","title":"Predictability and Surprise in Large Generative Models"},{"paperId":"adea6aa83b847752940129185428ea61935a0027","title":"Quantifying Memorization Across Neural Language Models"},{"paperId":"979eb5c97c49d7979447ed684500895a24d75ac4","title":"The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming"},{"paperId":"62d17b6f6ad77fd71ef9954c7784700d5e316f1f","title":"What Does it Mean for a Language Model to Preserve Privacy?"},{"paperId":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective"},{"paperId":"76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","title":"Investigating Explainability of Generative AI for Code through Scenario-based Design"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"5d49c7401c5f2337c4cc88d243ae39ed659afe64","title":"Red Teaming Language Models with Language Models"},{"paperId":"9ac5e1ffa8f837671a89354d4e298b1adeb08d79","title":"Enabling Automatic Repair of Source Code Vulnerabilities Using Data-Driven Methods"},{"paperId":"763792c655e591c5d61f67d7ac9cbecbcb5f4508","title":"Pop Quiz! Can a Large Language Model Help With Reverse Engineering?"},{"paperId":"0109c662c101723aea99e561937e3aca58563537","title":"Exploring Transformer Backbones for Heterogeneous Treatment Effect Estimation"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"ab18a9168b93def3145e118b6686e84241b39f42","title":"DeepRNG: Towards Deep Reinforcement Learning-Assisted Generative Testing of Software"},{"paperId":"5d0db797a45ce2453f821f7ded0b547d3fdab054","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"7428f9b16a82839e2cb6e6c7a77c1ffeab898813","title":"HEAT: Hyperedge Attention Networks"},{"paperId":"03eeff98d24383518ce0dacc0b3c4a38b6f1a514","title":"Recursive Decoding: A Situated Cognition Approach to Compositional Generation in Grounded Language Understanding"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"6d7d4fca9840504f630e9bea6acaa07322a6e889","title":"Text and Code Embeddings by Contrastive Pre-Training"},{"paperId":"3d5463a16d85d9d1d0d8ebb4117a31aca3c240ff","title":"Fooling MOSS Detection with Pretrained Language Models"},{"paperId":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"},{"paperId":"2b5d234efd26e7377698cf16c901601a3d3c4e56","title":"CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities"},{"paperId":"53c0abe83fe9b4fdaf2208295d8504fcf5241694","title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models"},{"paperId":"49c3f85573a3204c5e66317289e4cecfed50f38a","title":"Assemble Foundation Models for Automatic Code Summarization"},{"paperId":"7a54c01824a725a03f99411c8646d0a5674b2777","title":"Predictive synthesis of API-centric code"},{"paperId":"c0a98fca99e5f2d34330a15ab8ad4b9d692146d0","title":"The growing cost of deep learning for source code"},{"paperId":"f9838a3be5c94bb2674a0e224de349b50e18f3c4","title":"Learning To Retrieve Prompts for In-Context Learning"},{"paperId":"6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a","title":"Few-Shot Semantic Parsing with Language Models Trained on Code"},{"paperId":"52db8674337e5d86dcb96d013734befc8c3d4581","title":"Large Language Models are not Models of Natural Language: they are Corpus Models."},{"paperId":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis"},{"paperId":"ecb5a6fe2f5261e4e717ece1e82c464c63cb4862","title":"Controlling Conditional Language Models without Catastrophic Forgetting"},{"paperId":"8417d83e74a63b9d3874cd7695477ebad8b98f8a","title":"Choose your programming copilot: a comparison of the program synthesis performance of github copilot and genetic programming"},{"paperId":"231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models"},{"paperId":"1cbb3d96242c3f47c3f40aada33616d0f5c07737","title":"Inductive Biases and Variable Creation in Self-Attention Mechanisms"},{"paperId":"a421ba0a9150cd35e231dddc323bdd9a59b3af93","title":"Coherence boosting: When your pretrained language model is not paying enough attention"},{"paperId":"1aed58bd07026492194672adec494dc37c894a28","title":"Leveraging Automated Unit Tests for Unsupervised Code Translation"},{"paperId":"c6bb04f3d8000b7e800f6359082de39548c7da79","title":"Capturing Structural Locality in Non-parametric Language Models"},{"paperId":"2672777d25562c9df6fc13b653181db62d39bece","title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"},{"paperId":"77d956cdab4508d569ae5741549b78e715fd0749","title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"a1e1297fb132d7769dda3f7917e57757e6e22605","title":"Impact of Evaluation Methodologies on Code Summarization"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions"},{"paperId":"8cf3a454556060d6e9aa86dbabf221bd10bf9759","title":"On the Effectiveness of Transfer Learning for Code Search"},{"paperId":"75e36bb95023e55f7dec95d1af557e219ba3d349","title":"CORAL: COde RepresentAtion learning with weakly-supervised transformers for analyzing data analysis"},{"paperId":"56e6d62c638a24411f12d15cdc8821a31fc495c8","title":"Source Code Generation from Descriptions in a Natural Language"},{"paperId":"78863000eb70945cc8d791d45d4a3fe8a6521cb6","title":"Open-Ended Knowledge Tracing for Computer Science Education"},{"paperId":"ba5d21b7c65c6598c7bd39a5d992308c205df374","title":"A S YSTEMATIC E VALUATION OF L ARGE L ANGUAGE M ODELS OF C ODE"},{"paperId":"b2c4fdb49bdb23e6ad0ac3272029324157046ea7","title":"Automated Repair of Code from Language Models"},{"paperId":"25c402db512d327f1da143de3b8e797ad6fbfe5b","title":"P ROG P ROMPT : Generating Situated Robot Task Plans using Large Language Models"},{"paperId":"660ca9e15e19409903a0605f0584d0f263c35c67","title":"S YNCHROMESH : R ELIABLE C ODE G ENERATION FROM P RE - TRAINED L ANGUAGE M ODELS"},{"paperId":"4797e960f8e7a2b47ae0d95c7071ef84fa5d4b5b","title":"Code Generation Using Machine Learning: A Systematic Review"},{"paperId":"15ef2d1b88f54fa32a32927463a7116219b89529","title":"SUPEROPTIMIZE REAL-WORLD PROGRAMS"},{"paperId":"a2b6e1f7d8a7963d321f29fca7c01eeb1ebd7f0f","title":"P ATCH G ENERATION WITH L ANGUAGE M ODELS : F EASIBILITY AND S CALING B EHAVIOR"},{"paperId":"8a4299a61cc44b60e5be32fef35341c3bc7b2a0d","title":"The Hole Story: Type-Directed Synthesis and Repair"},{"paperId":"60043104ca33a1fc905af57ead32768e52c69103","title":"C3PO: A Lightweight Copying Mechanism for Translating Pseudocode to Code"},{"paperId":"ae10c4b220a0bc0999bf169d5c219086d1f1aeed","title":"Edinburgh Research Explorer Taxonomy of risks posed by language models"},{"paperId":"a85c5d7272371345e28a9910080224cad799972e","title":"Schema Matching using Pre-Trained Language Models"},{"paperId":"f9d38e03c97562b5f5942f3a0c43bdb751b9dc1c","title":"Wordplay 2022 The 3rd Wordplay: When Language Meets Games Workshop Proceedings of the Workshop"},{"paperId":"b98d6fe8f0ef02ec0d1bb2bcfb924c8f01feb7d4","title":"Convergent Representations of Computer Programs in Human and Artificial Neural Networks"},{"paperId":"e002bb8dae5a18a5ea1e7e1aafa16e19ad545662","title":"Solving Math Word Problems with Process-based and Outcome-based Feedback"},{"paperId":"0180d35b85dd4daead90e0652b64b1339e754684","title":"Assistance with large language models"},{"paperId":"cd155729180ea707dea251f8e9654db241ffd808","title":"Is GPT-3 all you need for machine learning for chemistry?"},{"paperId":"a1ef81e17a9ca41e09aba802040a2eca2744716f","title":"Generation Probabilities are Not Enough: Improving Error Highlighting for AI Code Suggestions"},{"paperId":"15e767aa26a14455da95a2b2f11e3d59f2c250f6","title":"Autoformalization for Neural Theorem Proving"},{"paperId":"2443179d421e1faf7474add557b45add554723c7","title":"Formal Premise Selection With Language Models"},{"paperId":"ddab94478a7647ee136b1f6b5076417db3074d0f","title":"Machine Programming: Turning Data into Programmer Productivity"},{"paperId":"9bf75110ea0923bbed49256b5491f1ec284019ec","title":"From BERT to GPT-3 Codex: Harnessing the Potential of Very Large Language Models for Data Management"},{"paperId":"91260f73dd179487fb16713deb8267634ae14716","title":"CodexDB: Synthesizing Code for Query Processing from Natural Language Instructions using GPT-3 Codex"},{"paperId":"3b0cf543a730e674d4213d344ebc857fada76ead","title":"Understanding High-Level Properties of Low-Level Programs Through Transformers"},{"paperId":"09e14c4c80e20e80c052e0adb0d49df51aff718d","title":"Yes, You Can Make an App Too: A Systematic Study of Prompt Engineering in the Automatic Generation of Mobile Applications from User Queries"},{"paperId":"b562be15b076b494023b8ac24fc8c459f4fdf80a","title":"Craft an Iron Sword: Dynamically Generating Interactive Game Characters by Prompting Large Language Models Tuned on Code"},{"paperId":null,"title":"Materials ExerciseTeacher Student Attempt Feedback"},{"paperId":"24c6982a25c0114bc98805d368b06d1a4f6d8fd5","title":"Understanding AI alignment research: A Systematic Analysis"},{"paperId":"f01e316d3b28ccecda25b4d57926f496a9b17d3d","title":"How Robust are Neural Code Completion Models to Source Code Transformation?"},{"paperId":"6f84d0cc33c7c58f74b28ddcc1cbda91ea608c9f","title":"C ODE S UMMARIZATION : D O T RANSFORMERS R E ALLY U C"},{"paperId":"2e5b29457ff45b8faba69bc2eaf05521584a7bec","title":"B UG F IX G ENERATION USING G RAPH T RANS"},{"paperId":"57c31c709792949bfbb9d4aaee941048aa07cc4b","title":"How to Give Imperfect Automated Guidance to Learners: A Case-Study in Workplace Learning"},{"paperId":"bee0be592c314435048599281bcd9c72bf63b735","title":"CueBot: Cue-Controlled Response Generation for Assistive Interaction Usages"},{"paperId":"8a4dce5735a101ff8f64c2b676afb8c24950a5d8","title":"Zero-shot Mathematical Problem Solving via Generative Pre-trained Transformers"},{"paperId":"75c8f2916a2067f7549cf58ea8c9061565eb1dab","title":"C ROSS B EAM : L EARNING TO S EARCH IN B OTTOM -U P P ROGRAM S YNTHESIS"},{"paperId":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"DECOMPOSITION IN NEURAL PROGRAM SYNTHESIS"},{"paperId":"a3564f3cf954c05844c757505325a50b4d858e22","title":"Generating Infrastructure-as-Code from Natural Language: Evaluating Fine-Tuned GPT-3 Models for Cloud Infrastructure Provisioning"},{"paperId":"78fd8185c5cd55830c31aa718a9909827e20774e","title":"A Research Agenda for Assessing the Economic Impacts of Code Generation Models"},{"paperId":"856d2c0f7b3f80dcf1f68d1dc0dcbf5c6fe5679a","title":"Interpreting docstrings without using common sense: the private science of very large language models∗"},{"paperId":"1b94afca9d6688cc584a744734126473283cbc93","title":"Can Transformers be Strong Treatment Effect Estimators?"},{"paperId":"58dd9a3da16c10f5c3cdbb9760a9ff378847bf76","title":"Self-supervision of wearable sensors time-series data for influenza detection"},{"paperId":"6ccc0ca964ddab19705e4832758e6a2447325348","title":"End to End Software Engineering Research"},{"paperId":"091fa84bdc07dcb22a34060c3996d8c58d71cd20","title":"Towards Neural Functional Program Evaluation"},{"paperId":"ee042a3e299a32c413532e64603de8d3ddb6aa87","title":"Automap: Towards Ergonomic Automated Parallelism for ML Models"},{"paperId":"827a67bbb96c8ed34c0f79e2ea811c5b53a6896b","title":"Controllable Response Generation for Assistive Use-cases"},{"paperId":"73e6f5a7e2760b3f9ab077a450e445e12bfd7061","title":"Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?"},{"paperId":"a5731122200fbb8b37f048010a1e1ca4474aa606","title":"Examining Zero-Shot Vulnerability Repair with Large Language Models"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"21ab011a3adccbd912aea58f76b84b7873c41df3","title":"Machines&Influence: An Information Systems Lens"},{"paperId":"cecc913290736a5a368642c5b59a130eddd1fa7b","title":"Can Pre-trained Language Models be Used to Resolve Textual and Semantic Merge Conflicts?"},{"paperId":"04db9b694280134f09af5fa787a306907edba29d","title":"How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN"},{"paperId":"f7987fa2aadc0b368c185dc4d2fdb1337a202c32","title":"Solving Probability and Statistics Problems by Program Synthesis"},{"paperId":"4e40595d6ecba027cebb4f2e3b43ae44bcf51daf","title":"Solving Linear Algebra by Program Synthesis"},{"paperId":"1444536496d8064f33e10b38b5820fecfab5b367","title":"Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs"},{"paperId":"c23d9d44e8bc68408cea9f305d1f24d915bc0d0d","title":"Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey"},{"paperId":"9f260bdd4030af5297a9c1cbb817c75701ac8c83","title":"The 5th Recognizing Families in the Wild Data Challenge: Predicting Kinship from Faces"},{"paperId":"570a6a5b8ec2827c3f33bb1b1bd027190a0d3e07","title":"Neural Program Generation Modulo Static Analysis"},{"paperId":"6a269b1abccdbf57e79b3f115a97bff14b435ad9","title":"Automated Support for Unit Test Generation: A Tutorial Book Chapter"},{"paperId":"21e8e76386aaaa00e0971af70ce84a8a544e1aa1","title":"Cascaded Fast and Slow Models for Efficient Semantic Code Search"},{"paperId":"dace03e57056d736f9e24937bdf486e894f8e866","title":"Is neural machine translation approach accurate enough for coding assistance?"},{"paperId":"8091e51ebbcd2424a1c5b50c036bae5295090525","title":"Top 3 in FG 2021 Families In the Wild Kinship Verification Challenge"},{"paperId":"21bc4ead8ea415579ab40e437fcbc274929f17c8","title":"Solving the Families In the Wild Kinship Verification Challenge by Program Synthesis"},{"paperId":"9991bb2eb7e7d7e9d831e257ae77ba2eaeaba3dc","title":"Applying quantum approximate optimization to the heterogeneous vehicle routing problem"},{"paperId":"360e0197378799d890f473893cc0c773b8182b4e","title":"Searching for Replacement Classes"},{"paperId":"24e775b20adf21e9b5b95c6a9b7a5c164d055849","title":"M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining"},{"paperId":"6c2d43e71e240e354b5790a38da78a291ceffe7c","title":"Learning to Superoptimize Real-world Programs"},{"paperId":"05c2e1ee203be217f100d2da05bdcc52004f00b6","title":"Unsolved Problems in ML Safety"},{"paperId":"bc9598dc4ed0472d8b59b87ed3a139f8347d40ee","title":"Towards A Measure Of General Machine Intelligence"},{"paperId":"b8b21c2ddcd7d1c23d7ccfceabb63cb1a05bcfca","title":"HYDRA - Hyper Dependency Representation Attentions"},{"paperId":"a8863de15a5ee8eed98107f423138a1a8f5a2ba8","title":"Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a176b0de62840f7118006277d94bbc1547162a4d","title":"Learning to Synthesize Programs as Interpretable and Generalizable Policies"},{"paperId":"70087677fd1a6309829b42968934575d05a95f92","title":"What do pre-trained code models know about code?"},{"paperId":"3f97c2067cde9377e50b3160bbd7982c94abd88a","title":"An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions"},{"paperId":"4885e616e85d420576196b2578525cbc501137ec","title":"Programming and execution models for next generation code intelligence systems (keynote)"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"5436193122dff271796bca07df7cecb7a8d6dea6","title":"Natural language-guided programming"},{"paperId":"26450917d41c828b470ec8818d49f59516a5b9c0","title":"Towards Universality in Multilingual Text Rewriting"},{"paperId":"58a6ca2ae28a618126f71a07262cb958a8c37904","title":"Latent Execution for Neural Program Synthesis"},{"paperId":"98485ce6532d69f34a8ec67de6b09a39532bd221","title":"Communicating Natural Programs to Humans and Machines"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"09279dc8018a8131e11d527cebb06d0a43c67cff","title":"Creativity and Machine Learning: A Survey"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"4f278ab5ad629267e06196e273252262854c1c57","title":"BF++: a language for general-purpose program synthesis"},{"paperId":"021bbcefc993c389bad6c1daefd8ff92d0fc2441","title":"Contrastive Code Representation Learning"},{"paperId":"4da830b6d84e117cb147ff71f205e71500ebbbb1","title":"Machines and Influence"},{"paperId":"007153d786caa906255fba2ca265fd67994f8b44","title":"Tracking Blobs in the Turbulent Edge Plasma of Tokamak Fusion Reactors"},{"paperId":"a3c2b81d8bb5ac69771ed7830d009310d98ac9dc","title":"A First Approach to AGI-based Robot Task Planning"},{"paperId":"d66e80224cda0c1d5a4c1be3798df6a6bfe3713c","title":"GPT-3 for Few-Shot Dialogue State Tracking"},{"paperId":"b874faa9c6cfb5d7e87e3d79650007ade1394958","title":"Creating new Program Proofs by Combining Abductive and Deductive Reasoning"},{"paperId":"bab6893ee48d168d27c227c3b0867f6d471fbea8","title":"Language Models are not Models of Language"},{"paperId":"4ff14a0580f3b36a9564ec18a51d2ce1d4eafebc","title":"Learning Methods for Solving Astronomy Course Problems"},{"paperId":"a63535ebbf90d0c51408252c23b85ffaf87f09ae","title":"Towards an AI assistant for power grid operators"},{"paperId":"8a9e437b2e2d813b402ac560c852ef0ab2f1cd3c","title":"Recognizing Families In the Wild (RFIW): The 4th Edition"}],"references":[{"paperId":"113fba4c88e2eb74f49544a161ef70e9745e969a","title":"Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses"},{"paperId":"fc963363bcddfc1aeca07d44d7a9d0e53485662d","title":"Atlas of AI: power, politics, and the planetary costs of artificial intelligence."},{"paperId":"dab46dd3985d1de5cd6549319797ab3705b6a801","title":"BEiT: BERT Pre-Training of Image Transformers"},{"paperId":"06510ee15cabae97647ceb647dce6f5a1820d524","title":"Women’s Participation in Open Source Software: A Survey of the Literature"},{"paperId":"4a160efbe80c38cd5eb2f92c7c095b49b113397d","title":"In-IDE Code Generation from Natural Language: Promise and Challenges"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"79b8ef3905a42b771248719495a2117271906445","title":"Carbon Emissions and Large Neural Network Training"},{"paperId":"3a863672b072a37ac8d2eb6b780234f6b748a2d9","title":"Generating bug-fixes using pretrained transformers"},{"paperId":"49f905eb03958c7cfae52ac759ea8978b8b2a6ea","title":"Alignment of Language Agents"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","title":"Zero-Shot Text-to-Image Generation"},{"paperId":"56fa0b9cba4d9aee5ccc327365b3b3a721031c69","title":"Calibrate Before Use: Improving Few-Shot Performance of Language Models"},{"paperId":"4c2733d191e347753bb28afa46a1c55c65e085be","title":"Persistent Anti-Muslim Bias in Large Language Models"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"021bbcefc993c389bad6c1daefd8ff92d0fc2441","title":"Contrastive Code Representation Learning"},{"paperId":"1ec69f1a1a9d4ff5c5bc70db0e5087157b620570","title":"You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"18a93dc1558bf9d7534d0b416633cebaf75c1145","title":"Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/ kingoflolz/mesh-transformer-jax"},{"paperId":null,"title":"A first look at rote learning in github copilot suggestions., Jun 2021. URL https://docs.github.com/en/ github/copilot/research-recitation"},{"paperId":"72730829313a9a907a875da6e8ab66af92ce62b2","title":"Multimodal Neural Script Knowledge Models"},{"paperId":null,"title":"Learning autocompletion from realworld datasets"},{"paperId":null,"title":"Supplemental Bias Analysis Generative models have been shown to encode bias in modalities such as natural language (Brown et al., 2020; Blodgett et al., 2020) and images (Radford et al., 2021)"},{"paperId":null,"title":"15-1252.00 -software developers"},{"paperId":null,"title":"Fun and dystopia with ai-based code generation using gpt-j-6b, June 2021"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"15a60130ea9cb4a00e0dac6d11a368894974cedf","title":"Unit Test Case Generation with Transformers and Focal Context"},{"paperId":"053b1d7b97eb2c91fc3921d589c160b0923c70b1","title":"Learning to summarize from human feedback"},{"paperId":"bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8","title":"Generative Pretraining From Pixels"},{"paperId":"49a049dc85e2380dde80501a984878341dd8efdf","title":"wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"d47a682723f710395454687319bb55635e653105","title":"Language (Technology) is Power: A Critical Survey of “Bias” in NLP"},{"paperId":"737189319865d7a363e38d957a8845c519953556","title":"SourceFinder: Finding Malware Source-Code from Publicly Available Repositories"},{"paperId":"4c7183fb2109271405e4a0fe23b5e827520a9f68","title":"Backstabber’s Knife Collection: A Review of Open Source Software Supply Chain Attacks"},{"paperId":"67dea28495cab71703993d0d52ca4733b9a66077","title":"Jukebox: A Generative Model for Music"},{"paperId":"70082b428b794d1d5e47c04ed4c3167a75aed22f","title":"Recalibrating global data center energy-use estimates"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"be9aa6c68d4df56a55444b15a83ff1e62c869bb5","title":"Robots and Jobs: Evidence from US Labor Markets"},{"paperId":null,"title":"Language models are few-shot"},{"paperId":null,"title":"formers is to fine-tune large pre-trained models with curated or human-generated datasets of the desired behavior (e.g., Raffel et al"},{"paperId":null,"title":"Working in public: the making and maintenance of open source software"},{"paperId":null,"title":"2020) for an analysis of conventional language models"},{"paperId":"5ca5c4c46c7729270746f885ff43953cb407fc6d","title":"What distinguishes great software engineers?"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"e8955a546425ef14a66da848f0bd174a33179373","title":"Automatic programming: The open issue?"},{"paperId":"75acc731bdd2b626edc74672a30da3bc51010ae8","title":"CTRL: A Conditional Transformer Language Model for Controllable Generation"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"433fcb584902e716ee25a44175e380267616b54e","title":"Learning Compositional Neural Programs with Recursive Tree Search and Planning"},{"paperId":"21da617a0f79aabf94272107184606cefe90ab75","title":"Generating Long Sequences with Sparse Transformers"},{"paperId":"79af328616d2440c77449d038f72d053c64d8f1f","title":"Unified rational protein engineering with sequence-based deep representation learning"},{"paperId":"e1799998d9e0932617aee2949167b040ebc8bec7","title":"The Wrong Kind of Ai? Artificial Intelligence and the Future of Labor Demand"},{"paperId":"516497e7fc39ebb126bed28dcc5c6129e500acb8","title":"An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation"},{"paperId":"7d3ab2a839b077a318022f7842225db55033b2c3","title":"Generating High Fidelity Images with Subscale Pixel Networks and Multidimensional Upscaling"},{"paperId":"98d1307bed619b58b4a44acd8e65ac58495776c2","title":"code2seq: Generating Sequences from Structured Representations of Code"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"Improving the standard risk matrix: Part 1"},{"paperId":null,"title":"Improving the standard risk matrix: Part 1. 2019"},{"paperId":null,"title":"Search-based pseudocode to code"},{"paperId":null,"title":"Comment regarding request for comments on intellectual property protection for artificial intelligence innovation. Before the United States Patent and Trademark Office Department of Commerce"},{"paperId":null,"title":"A common approach to adjusting the behavior of Transformers is to fine-tune large pre-trained models with curated or human-generated datasets of the desired behavior (e.g., Raffel et al"},{"paperId":null,"title":"Comment regarding request for comments on intellectual property protection for artificial intelligence innovation"},{"paperId":"b227f3e4c0dc96e5ac5426b85485a70f2175a205","title":"Representation Learning with Contrastive Predictive Coding"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"bec0b38f0083bd351d25daf8b31e6e049c376f94","title":"Improving Neural Program Synthesis with Inferred Execution Traces"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":null,"title":"Open-sourcing gvisor, a sandboxed container runtime"},{"paperId":null,"title":"Open-sourcing gvisor, a sandboxed container runtime, 2018"},{"paperId":null,"title":"Clarifying ”ai alignment"},{"paperId":null,"title":"Protecting applications with automated software diversity, Sep 2018"},{"paperId":"75f06defa60e069b863853afc6ac6f5feaca9450","title":"On the difficulty of benchmarking inductive program synthesis methods"},{"paperId":"3807a517403cfc7ed67fdfea64baf62b9948bcfe","title":"A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"3ff0af64279929a952ee340e645256b7e0580f65","title":"RobustFill: Neural Program Learning under Noisy I/O"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":null,"title":"A6:2017-security misconfiguration"},{"paperId":null,"title":"A syntactic neural model for generalpurpose code generation"},{"paperId":null,"title":"The trouble with bias"},{"paperId":"784ee73d5363c711118f784428d1ab89f019daa5","title":"Hybrid computing using a neural network with dynamic external memory"},{"paperId":"df0402517a7338ae28bc54acaac400de6b456a46","title":"WaveNet: A Generative Model for Raw Audio"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"41f1d50c85d3180476c4c7b3eea121278b0d8474","title":"Pixel Recurrent Neural Networks"},{"paperId":"b59d91e0699d4e1896a15bae13fd180bdaf77ea5","title":"Neural Programmer-Interpreters"},{"paperId":"4aa9f5150b46320f534de4747a2dd0cd7f3fe292","title":"Semi-supervised Sequence Learning"},{"paperId":"23f9a5ec68f4e947d168e2a8dfc99771ab3e81f2","title":"General Program Synthesis Benchmark Suite"},{"paperId":"193136b86539bd6df3f57a3685629c049a037418","title":"An analysis of patch plausibility and correctness for generate-and-validate patch generation systems"},{"paperId":"ac39029550eda5efbc6df754d74ec0e378eea4a8","title":"Representation Learning"},{"paperId":"0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a","title":"Learning to Execute"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"215999a0e155a21255e4655d4eac312858058a84","title":"Structured Generative Models of Natural Source Code"},{"paperId":"d1e8e4e952d6d9b49dd4c5d4fed9dedb22ed9946","title":"Temporal Logics for Hyperproperties"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"304f7fd4af9cc82355eaf912a8f120f7429362de","title":"Spreadsheet data manipulation using examples"},{"paperId":"546d3b90ef0e4ab5f4a61dcda59184bc951672ec","title":"A systematic study of automated program repair: Fixing 55 out of 105 bugs for $8 each"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"56dd0149688e9d196ddc4c833f864e6ee85e81ac","title":"The Economics of Software Quality"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"dd892c577eb45f3237c1e4d2513b1674cbc2043c","title":"BugFix: A learning-based tool to assist developers in fixing bugs"},{"paperId":null,"title":"Cwe-780: Use of rsa algorithm without oaep"},{"paperId":null,"title":"Use of a broken or risky cryptographic algorithm"},{"paperId":"bc876b7d314af5507fb52ffdb9279c83bae20035","title":"The technology trap"},{"paperId":"92b286e00a26f059c278a66d00e087c01306648f","title":"The economic impacts of inadequate infrastructure for software testing"},{"paperId":"3da678cf38575a81e0634d564919da3865b12fc6","title":"USENIX Association"},{"paperId":null,"title":"Online; accessed 29-June-2000"},{"paperId":"0361f0a1b710d4101d190b56c85e1d07cd96bae5","title":"Genetic Programming III - Darwinian Invention and Problem Solving"},{"paperId":"19b3b95f3dc0a3bf1b22e5f08df918bcc0079486","title":"Application of Dynamic Slicing in Program Debugging"},{"paperId":"83721103a6fd5535e943b1b575cf70862c2322a8","title":"Handbook of Applied Cryptography"},{"paperId":"028af662ebf7203017714e2d257faed94a566961","title":"Fault localization using execution slices and dataflow tests"},{"paperId":"94d9cbfc474cb6ce961de45a06233b2853ca6724","title":"Toward automatic program synthesis"},{"paperId":"eb8d64c0f34610597cc02d31ec10d5d6bcd6d39c","title":"Experiments with a Heuristic Compiler"},{"paperId":null,"title":"Fun and dystopia with ai-based code generation using gpt-j-6b"}],"id":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","summary":"It is found that repeated sampling from the GPT language model is a surprisingly effective strategy for producing working solutions to difﬁcult prompts, and the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics are discussed."},{"url":"https://www.semanticscholar.org/paper/b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code","venue":"MAPS@PLDI","year":2022,"referenceCount":37,"citationCount":47,"influentialCitationCount":8,"publicationDate":"26/02/2022","authors":"Frank F. Xu,Uri Alon,Graham Neubig,V. Hellendoorn","citations":[{"paperId":"907a77639069bb7dd270f017068745706133cffc","title":"Inaccessible Neural Language Models Could Reinvigorate Linguistic Nativism"},{"paperId":"a3e355b5de868f34fdfa2500415c5f74c69d2091","title":"Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models"},{"paperId":"b2f256a1ec54325102119a57cb2359840fe68586","title":"SantaCoder: don't reach for the stars!"},{"paperId":"69e330037afd18e5546fdcacbc9a9f7deb69bba9","title":"Memorization and Generalization in Neural Code Intelligence Models"},{"paperId":"ab8a53be2ff8a756776aa0245ad8da41189d60d6","title":"Fuzzing Deep-Learning Libraries via Large Language Models"},{"paperId":"433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context"},{"paperId":"239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning"},{"paperId":"bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence"},{"paperId":"8210cef990b8e5cddbc95000e46309bdd25337f7","title":"Asking Clarification Questions for Code Generation in General-Purpose Programming Language"},{"paperId":"6756fcd998caeb7b23702e08559e63710179334c","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"6bc87e51018d6de55011e95a0d43c588dd44a1e8","title":"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages"},{"paperId":"32b58766a1bfcef7ebba07070a272687aa518206","title":"Explicit Knowledge Transfer for Weakly-Supervised Code Generation"},{"paperId":"05f09050118d82100d04e56ba8b54836753fa9b4","title":"Programming by Example and Text-to-Code Translation for Conversational Code Generation"},{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","title":"Evaluating How Fine-tuning on Bimodal Data Effects Code Generation"},{"paperId":"cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers"},{"paperId":"41f5e1ad7793593befc0b9c38f756836e8b07c98","title":"Code4ML: a Large-scale Dataset of annotated Machine Learning Code"},{"paperId":"20e4ca6368a0731d3cbbd2e68aa90cef7384dc72","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"39e40821b7207125e54e6ed7112e55cd38c6f0c3","title":"Language Models of Code are Few-Shot Commonsense Learners"},{"paperId":"2cfd0dacfa267a64a23392332c358d4e3ec6fbd4","title":"The COVID That Wasn’t: Counterfactual Journalism Using GPT"},{"paperId":"0c78a473e33a81246d5c0fbbda7e7de168814c18","title":"FlexType: A Plug-and-Play Framework for Type Inference Models"},{"paperId":"97e1903b4afc811a9b5fa9e55723e80ba48bf46f","title":"Code as Policies: Language Model Programs for Embodied Control"},{"paperId":"e5993b3afe6384b5e6f90093989773ad1f868f71","title":"Towards Top-Down Deep Code Generation in Limited Scopes"},{"paperId":"6032212d5790b6a580d68d469a9895aad6238c89","title":"Diverse Title Generation for Stack Overflow Posts with Multiple Sampling Enhanced Transformer"},{"paperId":"05ccfb058dc6788254742722278b0af2fc87f083","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?"},{"paperId":"1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs"},{"paperId":"b3a54332a0791751fcf234f6264f242c42ac00d2","title":"DocPrompting: Generating Code by Retrieving the Docs"},{"paperId":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code"},{"paperId":"82d9f1db6db43cb61fe4b0b26a489a2e72628675","title":"A Test for Evaluating Performance in Human-Computer Systems"},{"paperId":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code"},{"paperId":"35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs"},{"paperId":"58aacb967cc7fc25cfc9d51b7ad3e57ac00d119b","title":"Can Foundation Models Wrangle Your Data?"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"3d5463a16d85d9d1d0d8ebb4117a31aca3c240ff","title":"Fooling MOSS Detection with Pretrained Language Models"},{"paperId":"cdec75f901a93c75ee5386a98abbe44746286e80","title":"Delivering Fairness in Human Resources AI: Mutual Information to the Rescue"},{"paperId":"a2b6e1f7d8a7963d321f29fca7c01eeb1ebd7f0f","title":"P ATCH G ENERATION WITH L ANGUAGE M ODELS : F EASIBILITY AND S CALING B EHAVIOR"},{"paperId":"73e6f5a7e2760b3f9ab077a450e445e12bfd7061","title":"Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?"},{"paperId":"a5731122200fbb8b37f048010a1e1ca4474aa606","title":"Examining Zero-Shot Vulnerability Repair with Large Language Models"}],"references":[{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"c0a98fca99e5f2d34330a15ab8ad4b9d692146d0","title":"The growing cost of deep learning for source code"},{"paperId":"92d211c65ae8c8006c07d34dfa0587e278d0ac00","title":"An Empirical Study of C++ Vulnerabilities in Crowd-Sourced Code Examples"},{"paperId":null,"title":"Natural Language Processing with Transformers"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"feba0c47bf12a02c3a725174bb53df78658a72a8","title":"Pre-Trained Models: Past, Present and Future"},{"paperId":"7571ed4cf1bbdcf891b576a0da12c910b1f0c72f","title":"Concealed Data Poisoning Attacks on NLP Models"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"05e396e79a2f88f0b8f8d99f5ab36ab3efa95c14","title":"Language-Agnostic Representation Learning of Source Code from Structure and Context"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"3944354c42ddfff7414ad06022f96c72858d5fa6","title":"Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"db392858262b17aa9c8ff8659738f68fbf832ebe","title":"Structural Language Models of Code"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"d170bd486e4c0fe82601e322b0e9e0dde63ab299","title":"Adaptive Input Representations for Neural Language Modeling"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"b4a26011eb7c26d71cd074f182c4f093da24308b","title":"Are deep neural networks the best choice for modeling source code?"},{"paperId":"022788ecf8685ed30e3f69f9121b5a3d242a22d6","title":"On the naturalness of software"},{"paperId":"2579e826f22da988fbc1b5d545aa59ab68bde4f3","title":"Program Synthesis Using Natural Language"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"6c2b28f9354f667cd5bd07afc0471d8334430da7","title":"A Neural Probabilistic Language Model"}],"id":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","summary":"This work finds that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language modeling, and identifies an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code."},{"url":"https://www.semanticscholar.org/paper/7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":6,"influentialCitationCount":0,"publicationDate":"02/06/2022","authors":"Patrick Bareiss,Beatriz Souza,Marcelo d’Amorim,Michael Pradel","citations":[{"paperId":"a3e355b5de868f34fdfa2500415c5f74c69d2091","title":"Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models"},{"paperId":"0b340dd78fd04bbde2807d5efedb796d319355e3","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?"},{"paperId":"317208b423d24d52ba04221cfb46956962364e22","title":"Extracting Meaningful Attention on Source Code: An Empirical Study of Developer and Neural Model Code Exploration"},{"paperId":"45a37f351bb275d22354b712c78df65715a37cc5","title":"CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code"},{"paperId":"453a8fac3be9282be53908f0735160d0d21e0f48","title":"Repair Is Nearly Generation: Multilingual Program Repair with LLMs"},{"paperId":"205ac1373eb7981aca2d08f2ab651871a001271e","title":"CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code"}],"references":[{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"4413f763efcd34498ec1c4439a9c4c9395689024","title":"µBert: Mutation Testing using Pre-Trained Language Models"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5d0db797a45ce2453f821f7ded0b547d3fdab054","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"7cbc2a7843411a1768ab762930707af0a3c33a19","title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"},{"paperId":"c0a98fca99e5f2d34330a15ab8ad4b9d692146d0","title":"The growing cost of deep learning for source code"},{"paperId":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis"},{"paperId":"26f1c97c9e38ed0da329e3197ed554553b305d18","title":"Neural software analysis"},{"paperId":"209333bef25395800997a7411c905757baf38553","title":"MeMo: Automatically identifying metamorphic relations in Javadoc comments for test automation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"c2c7233293d55f201fe5b496234bed1914eea70e","title":"Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"1311adebe439ccd9e373d594a8df1d720a685190","title":"Code Prediction by Feeding Trees to Transformers"},{"paperId":"49cf6a22a5dac5bc98b653534af65ffa0bc0e76d","title":"Multi-task Learning based Pre-trained Language Model for Code Completion"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"27724bd19946d6a824d06cdca3cdfe5d40f71003","title":"A structural model for contextual code changes"},{"paperId":"9a4bfc410eb7efac0557025fdb00503eb5a2adc0","title":"Learning semantic program embeddings with graph interval neural network"},{"paperId":"08066c80919620397e8e4e5372ff84caf401e675","title":"Global Relational Models of Source Code"},{"paperId":"6acf0d7b24a93953269a4d52c6b2f0c7d11f9ae4","title":"Hoppity: Learning Graph Transformations to Detect and Fix Bugs in Programs"},{"paperId":"29fc7ff02e68537a86d32978bfd2e8e45258e77d","title":"LambdaNet: Probabilistic Type Inference using Graph Neural Networks"},{"paperId":"4ddb9135fc6fdf3f9e01fdfcd115fdbf6f7486b4","title":"Sequence Model Design for Code Completion in the Modern IDE"},{"paperId":"0dfe706526e5234338411489e1826b4060acc4e8","title":"CC2Vec: Distributed Representations of Code Changes"},{"paperId":"4578871d2b271e4b5473c9cb81d431d6bf58c607","title":"Metamorphic Testing: A New Approach for Generating Next Test Cases"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"a429c6c8407b10a89c9610ab5e9682ec89772356","title":"TypeWriter: neural type prediction with search-based validation"},{"paperId":"deabe3945e100ec42c80582652aa149c2e1b66b0","title":"Automatic Software Repair: a Bibliography"},{"paperId":"b5abecf78c1bd975449e34350289a904e6ef35cc","title":"Automated program repair"},{"paperId":"1432c8378b1cafa3f91f09fa743082d154fdab92","title":"A Novel Neural Source Code Representation Based on Abstract Syntax Tree"},{"paperId":"d2c7cbd578878ff0064857240426e605c2e959d2","title":"On the Effectiveness of Manual and Automatic Unit Test Generation: Ten Years Later"},{"paperId":"8e71af3fbec666e67c0fcfefedb38881027a0254","title":"NL2Type: Inferring JavaScript Function Types from Natural Language Information"},{"paperId":"d02285385fd2b53a6100b8d62421bba6693b788c","title":"When Code Completion Fails: A Case Study on Real-World Completions"},{"paperId":"98d1307bed619b58b4a44acd8e65ac58495776c2","title":"code2seq: Generating Sequences from Structured Representations of Code"},{"paperId":"73bade9162bc4df6ca6c9ab8a7eb2d54d35acad2","title":"code2vec: learning distributed representations of code"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"d461ab9482b7fb5eadd7e6cd2c6dffced8ede8b8","title":"Chapter Six - Mutation Testing Advances: An Analysis and Survey"},{"paperId":"ea1b62a03ab16c4acca6f7041c6f096bedec244b","title":"Deep learning type inference"},{"paperId":"06fedb27a62aac78dda4f852e284d09d264844f8","title":"Translating code comments to procedure specifications"},{"paperId":"8b1aa4727eb2a83db1bd3ae54e078b0b7ce5eccb","title":"Retrieval on source code: a neural code search"},{"paperId":"e033a0b29af2939fd44e5765d03380c08897a9e8","title":"Deep Code Search"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"554c9b3aa8c3b7504810270b18b942a0d22156e7","title":"DeepFix: Fixing Common C Language Errors by Deep Learning"},{"paperId":"a60bbe22c11454661ab56dbbd1ceff1e83d94dea","title":"Automatic generation of oracles for exceptional behaviors"},{"paperId":"10d5cee2e9e3a5c7df8548f95eb7791f9d79d626","title":"The major mutation framework: efficient and scalable mutation analysis for Java"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"1fab8830baef8f353dce995c0269a32fd3105f64","title":"An orchestrated survey of methodologies for automated software test case generation"},{"paperId":"216b98bb3d9221d5f5d261864975612e4d0faaa6","title":"EvoSuite: automatic test suite generation for object-oriented software"},{"paperId":"494fbdd61eb5d9fc3b0d7c4b0e557d9b4996fae6","title":"An Analysis and Survey of the Development of Mutation Testing"},{"paperId":"ec316064d25c1f1dadc32a8570b56049aa109f78","title":"Inferring Resource Specifications from Natural Language API Documentation"},{"paperId":"af5a7be7bfc592e400e2f12a24ad3e6a5a44b58f","title":"Learning from examples to improve code completion systems"},{"paperId":"6dc7017e357767dbc210bcb27080260fb0816701","title":"Feedback-Directed Random Test Generation"},{"paperId":"208a35b667ac0b17333f593f9c68cc8d8603df1e","title":"JCrasher: an automatic robustness tester for Java"}],"id":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","summary":"This paper studies to what extent a state-of-the-art, pre-trained language model of code, Codex, may serve this purpose, and concludes that few-shot language models are surprisingly effective."},{"url":"https://www.semanticscholar.org/paper/8210cef990b8e5cddbc95000e46309bdd25337f7","title":"Asking Clarification Questions for Code Generation in General-Purpose Programming Language","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Haau-Sing Li,Mohsen Mesgar,André F. T. Martins,Iryna Gurevych","citations":[],"references":[{"paperId":"1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization"},{"paperId":"b3a54332a0791751fcf234f6264f242c42ac00d2","title":"DocPrompting: Generating Code by Retrieving the Docs"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"1a903282f7c19dbdb2714b852fb42dbb4675422b","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"906f9b3a5cd1912158528c7c6bd2c94b76c791bf","title":"Generating Clarifying Questions for Query Refinement in Source Code Search"},{"paperId":"f2a5af2e2fd7fabaefbb21ca211a500bca7cf6b7","title":"Pseudo Ambiguous and Clarifying Questions Based on Sentence Structures Toward Clarifying Question Answering System"},{"paperId":null,"title":"Natural Language Processing with Transformers: Building Language Applications with Hugging Face"},{"paperId":null,"title":"Competition-level code generation"},{"paperId":"267d9a093ae7e8388fd1e25d2b5e4cfe91c71226","title":"Building and Evaluating Open-Domain Dialogue Corpora with Clarifying Questions"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"c56aced0f0c5cfebefadb530cb08d736c3ac5c05","title":"Retrieval Augmented Code Generation and Summarization"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"b255a1aeed73c236fc1ae209743e32e805795168","title":"Ask what’s missing and what’s useful: Improving Clarification Question Generation using Global Knowledge"},{"paperId":"6e2b1038682cd116b2e38bec19b5721196c41eea","title":"HAConvGNN: Hierarchical Attention Based Convolutional Graph Neural Network for Code Documentation Generation in Jupyter Notebooks"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"6714dff86284cbde6be351e3e1f8ddee1bfadb9c","title":"A Toolkit for Generating Code Knowledge Graphs"},{"paperId":"1af678b040ce638aedf8b582212937f0921ccc1d","title":"Abg-CoQA: Clarifying Ambiguity in Conversational Question Answering"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"77910e51a40d17157fc798325d06edfa6cff18d6","title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation"},{"paperId":"42e863f93203d37a2518da381beaf06e4c70fb3d","title":"Stanza: A Python Natural Language Processing Toolkit for Many Human Languages"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"9cb32bdd43f64b36cb447ba1307869c5d8bf675c","title":"YAKE! Keyword extraction from single documents using multiple local features"},{"paperId":"517f770a38c6f09b1d0ee03793887912a844e69e","title":"Asking Clarification Questions in Knowledge-Based Question Answering"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"4a75598125bd0be3600d840aec04de40bf03c33b","title":"Asking Clarifying Questions in Open-Domain Information-Seeking Conversations"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"SentenceBERT: Sentence embeddings using Siamese BERTnetworks"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"7aca31a3bf370ad63af2eefa8e1fb146f7988eb0","title":"Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"3ade4d3be53981a1678b1e3a736d01547f7d3b9e","title":"Dialog for Language to Code"},{"paperId":"3807a517403cfc7ed67fdfea64baf62b9948bcfe","title":"A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation"},{"paperId":null,"title":"Asian Federation of Natural Language Processing"},{"paperId":"b5defb8729294614d40e0247eabfdc483bf1a992","title":"Bimodal Modelling of Source Code and Natural Language"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"d2071c1e4a6030dc0005dbfeefdd196a8b293e84","title":"Okapi at TREC-3"},{"paperId":null,"title":"Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language model for code with multi"},{"paperId":null,"title":"Curtis von Veh, Madanlal Musuvathi"}],"id":"8210cef990b8e5cddbc95000e46309bdd25337f7","summary":"The empirical results support the hypothesis that clariﬁcations result in more precise generated code, as shown by an improve-ment of 17.52 in BLEU, 12.72 in CodeBLEU, and 7.7% in the exact match."},{"url":"https://www.semanticscholar.org/paper/9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":3,"influentialCitationCount":0,"publicationDate":"26/06/2022","authors":"Disha Shrivastava,H. Larochelle,Daniel Tarlow","citations":[{"paperId":"433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"317208b423d24d52ba04221cfb46956962364e22","title":"Extracting Meaningful Attention on Source Code: An Empirical Study of Developer and Neural Model Code Exploration"}],"references":[{"paperId":"f75a0ccb3c9b7e12dfb8b9c9dc3d35ca4518a643","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"4bc040835fbff57ce6612306d794b8c6c8226086","title":"Discovering the Syntax and Strategies of Natural Language Programming with Generative Language Models"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"0e802c0739771acf70e60d59c2df51cd7e8c50c0","title":"Memorizing Transformers"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","title":"Investigating Explainability of Generative AI for Code through Scenario-based Design"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"5d0db797a45ce2453f821f7ded0b547d3fdab054","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"0271d1dbc01eda68c2f0291c62a956fca3092864","title":"PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization"},{"paperId":"c6bb04f3d8000b7e800f6359082de39548c7da79","title":"Capturing Structural Locality in Non-parametric Language Models"},{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"59641c10ed7431a3cf841f308367dc2dc0281b74","title":"What Makes Good In-Context Examples for GPT-3?"},{"paperId":"7bbfe2586d10d56081915a9edc44be2d29bbf8dc","title":"CodeTrek: Flexible Modeling of Code using an Extensible Relational Representation"},{"paperId":null,"title":"Natural Language Processing with Transformers"},{"paperId":null,"title":"A generalist agent, 2022"},{"paperId":null,"title":"Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"2ee03e28208a9310a9be4032c2b04ebdddb83cc7","title":"FLEX: Unifying Evaluation for Few-Shot NLP"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"209f9bde2dee7cf1677801586562ffe56d435d38","title":"Learning How to Ask: Querying LMs with Mixtures of Soft Prompts"},{"paperId":"a711a240ccf37b68b86e97c46aaed3d2cd256f75","title":"Learning to Generate Code Comments from Class Hierarchies"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"ac3cdb50606f7770eef8e4cd951840a4f71287a0","title":"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"},{"paperId":"85e7d63f75c0916bd350a229e040c5fbb1472e7a","title":"Making Pre-trained Language Models Better Few-shot Learners"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"8ae9a17c87a4518b513e860683a0ef7824be994d","title":"Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"},{"paperId":"18a93dc1558bf9d7534d0b416633cebaf75c1145","title":"Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"2934145c4ab42bfee022067b1b6b213acd836a2a","title":"On-the-Fly Adaptation of Source Code Models using Meta-Learning"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"7be8c119dbe065c52125ee7716601751f3116844","title":"Generalization through Memorization: Nearest Neighbor Language Models"},{"paperId":null,"title":"AutoPrompt: Eliciting knowledge from language models with automatically generated prompts"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"b4a26011eb7c26d71cd074f182c4f093da24308b","title":"Are deep neural networks the best choice for modeling source code?"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"34f25a8704614163c4095b3ee2fc969b60de4698","title":"Dropout: a simple way to prevent neural networks from overfitting"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"},{"paperId":null,"title":"For the example shown in Figure 1 of the main paper"},{"paperId":null,"title":"Type Identifiers (TI): Type Identifiers define the type of an identifier. For example, in the code snippet class DPAffinityPropagation extends AffinityPropagation"},{"paperId":null,"title":"Method Names and Bodies (MNB): Take all the method names along with their signatures and corresponding bodies used in the rule context location"},{"paperId":null,"title":"This was done by splitting the filenames based on either camel-case or underscore. If the sub-parts of two files match"},{"paperId":null,"title":"String Literals (SL): Take all the string literals used in the rule context location"},{"paperId":null,"title":"Identifiers (I): Identifiers are the names of variables used in the code. For example, for the rule context taken from the imported file shown in Figure 1 in the main paper"}],"id":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","summary":"This work proposes a framework called Repo-Level Prompt Generator that learns to generate example-speciﬁc prompts using prompt proposals, and demonstrates that an oracle constructed from these prompt proposals gives a remarkably high relative improvement over Codex, showing the quality of these proposals."},{"url":"https://www.semanticscholar.org/paper/b3a54332a0791751fcf234f6264f242c42ac00d2","title":"DocPrompting: Generating Code by Retrieving the Docs","venue":"","year":2022,"referenceCount":38,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/07/2022","authors":"Shuyan Zhou,Uri Alon,Frank F. Xu,Zhiruo Wang,Zhengbao Jiang,Graham Neubig","citations":[{"paperId":"8210cef990b8e5cddbc95000e46309bdd25337f7","title":"Asking Clarification Questions for Code Generation in General-Purpose Programming Language"}],"references":[{"paperId":"bcd4c46e4d75ddedb6138cfd77600c6d964a9aa8","title":"Natural Language to Code Translation with Execution"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"1f11601f9eb8c0bee1d1d2cc7f4fa187e8c5e525","title":"The impact of lexical and grammatical processing on generating code from natural language"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"a0e92f6e9564b8c38b6649ae71b892ddfb988faa","title":"Controllable Semantic Parsing via Retrieval Augmentation"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"c56aced0f0c5cfebefadb530cb08d736c3ac5c05","title":"Retrieval Augmented Code Generation and Summarization"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"c26759e6c701201af2f62f7ee4eb68742b5bf085","title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"bde0c85ed3d61de2a8874ddad70497b3d68bc8ad","title":"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"},{"paperId":"45793225c6593db60e9efd95bce1d70bf4844198","title":"Evaluation Paradigms in Question Answering"},{"paperId":null,"title":"Human-annotated unit tests"},{"paperId":"58ed1fbaabe027345f7bb3a6312d41c5aac63e22","title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"paperId":"77910e51a40d17157fc798325d06edfa6cff18d6","title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation"},{"paperId":"79cd9f77e5258f62c0e15d11534aea6393ef73fe","title":"Dense Passage Retrieval for Open-Domain Question Answering"},{"paperId":"34733eaf66007516347a40ad5d9bbe1cc9dacb6b","title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"832fff14d2ed50eb7969c4c4b976c35776548f56","title":"REALM: Retrieval-Augmented Language Model Pre-Training"},{"paperId":"3c9fa66674e1053eb9de12076c48f13e46422c29","title":"CLAI: A Platform for AI Skills on the Command Line"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2b88aefb70cd52a4d6899020f4be97c669a5edcb","title":"RTFM: Generalising to Novel Environment Dynamics via Reading"},{"paperId":"db392858262b17aa9c8ff8659738f68fbf832ebe","title":"Structural Language Models of Code"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"75c0d369c7151b925155cfe1b3f01dd7d0503981","title":"Generative Code Modeling with Graphs"},{"paperId":"d3e13d2514edaf74b863bfbe45a739c32a7689e1","title":"Retrieval-Based Neural Code Generation"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"429b65937d4922578a81e1f0ef5aeab7361ae36b","title":"NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System"},{"paperId":"2c1e874c3b67510a3215e535f5646b362de5bc89","title":"Abstract Syntax Networks for Code Generation and Semantic Parsing"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"b5defb8729294614d40e0247eabfdc483bf1a992","title":"Bimodal Modelling of Source Code and Natural Language"},{"paperId":"3170bb1affffeddfe9d3262c9a0787c0b29cb18a","title":"How do professional developers comprehend software?"},{"paperId":"99f1921a8ba5ceecb18e5ca7ff19b7f95c4e250d","title":"Learning to Win by Reading Manuals in a Monte-Carlo Framework"},{"paperId":"d744c7cf1f5e9e77c7de55db8df5a918ee1f41d7","title":"How software engineers use documentation: the state of the practice"},{"paperId":"3046cf5c122149b80915ce5bb99153d9cbd0366e","title":"The relevance of software documentation, tools and technologies: a survey"},{"paperId":"660f2447218bdf884fb39f2802383824c575dc43","title":"What programmers really want: results of a needs assessment for SDK documentation"},{"paperId":"f6e3e57567e9803718623ec088cd7fea65cfbc9d","title":"Relevance weighting of search terms"}],"id":"b3a54332a0791751fcf234f6264f242c42ac00d2","summary":"DocPrompting is a natural-language-to-code generation approach that explicitly leverages code documentation by retrieving the relevant documentation pieces given a natural language (NL) intent, and generating code based on the NL intent and the retrieved documentation."},{"url":"https://www.semanticscholar.org/paper/1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":46,"citationCount":74,"influentialCitationCount":20,"publicationDate":"20/05/2021","authors":"Dan Hendrycks,Steven Basart,Saurav Kadavath,Mantas Mazeika,Akul Arora,Ethan Guo,Collin Burns,Samir Puranik,Horace He,D. Song,J. Steinhardt","citations":[{"paperId":"b2f256a1ec54325102119a57cb2359840fe68586","title":"SantaCoder: don't reach for the stars!"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning"},{"paperId":"bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"ae441f7305dc2cd58c708528b3ecee3501cc5c46","title":"Plansformer: Generating Symbolic Plans using Transformers"},{"paperId":"9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming"},{"paperId":"27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models"},{"paperId":"20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers"},{"paperId":"20e4ca6368a0731d3cbbd2e68aa90cef7384dc72","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"0b340dd78fd04bbde2807d5efedb796d319355e3","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?"},{"paperId":"5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","title":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming"},{"paperId":"cb123f1afd67fb8bae15dc876709c842b626c49c","title":"SimSCOOD: Systematic Analysis of Out-of-Distribution Behavior of Source Code Models"},{"paperId":"a4c216d2ce9dd245c84771acc574722055967fd6","title":"Enhancing Code Classification by Mixup-Based Data Augmentation"},{"paperId":"1c1ca2392155ddf30408a442e6b504b5d60d4f2a","title":"When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment"},{"paperId":"8a854331c593c6a766fa3b8037fb2ad1b95a6f06","title":"An Empirical Study of Code Smells in Transformer-based Code Generation Techniques"},{"paperId":"a7435722d8ab595da5a9c70ac9160f57d0dcd75a","title":"Enabling Transformers to Understand Low-Level Programs"},{"paperId":"6032212d5790b6a580d68d469a9895aad6238c89","title":"Diverse Title Generation for Stack Overflow Posts with Multiple Sampling Enhanced Transformer"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"05ccfb058dc6788254742722278b0af2fc87f083","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"},{"paperId":"63c670ba8018da0a7e33c34ffd84c2a3ca54b894","title":"Language Models Can Teach Themselves to Program Better"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"1d160123cbbef972ea151a641dd435d57c727de8","title":"AixBench: A Code Generation Benchmark Dataset"},{"paperId":"2edc8efcda27c944a46f367acf6a5280b8f65525","title":"FixEval: Execution-based Evaluation of Program Fixes for Competitive Programming Problems"},{"paperId":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"1100dee3fd78655cddc4b7bfaef1161351d4fab5","title":"Automated Feedback Generation for Competition-Level Code"},{"paperId":"0bcd59da541fdae66884afba8d25475a54a9da1a","title":"Automated Repair of Programs from Large Language Models"},{"paperId":"ea4749c6ed5f08b16bab31b1ca4fd3fdc259ae8f","title":"Improving automatically generated code from Codex via Automated Program Repair"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"6050454e0446a3068617f73b0301453f3f67844d","title":"Stylette: Styling the Web with Natural Language"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"1a903282f7c19dbdb2714b852fb42dbb4675422b","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"590f6817b42407f96b079e82c935fae298196359","title":"Less is More: Summary of Long Instructions is Better for Program Synthesis"},{"paperId":"c96363c42bc8c465902c22b8c33c8704233f519e","title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages"},{"paperId":"1f11601f9eb8c0bee1d1d2cc7f4fa187e8c5e525","title":"The impact of lexical and grammatical processing on generating code from natural language"},{"paperId":"76f023c3a819fc58989a064a1b50825b11fce95d","title":"Capturing Failures of Large Language Models via Human Cognitive Biases"},{"paperId":"7b5aa186ca8abc585607c5ec91562e127a398601","title":"Open-Ended Knowledge Tracing"},{"paperId":"04ff95e0edc3759fc5d18a1b929b3ccf79b032b2","title":"Deconstructing Distributions: A Pointwise Framework of Learning"},{"paperId":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective"},{"paperId":"76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","title":"Investigating Explainability of Generative AI for Code through Scenario-based Design"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"3d5463a16d85d9d1d0d8ebb4117a31aca3c240ff","title":"Fooling MOSS Detection with Pretrained Language Models"},{"paperId":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"},{"paperId":"52db8674337e5d86dcb96d013734befc8c3d4581","title":"Large Language Models are not Models of Natural Language: they are Corpus Models."},{"paperId":"1aed58bd07026492194672adec494dc37c894a28","title":"Leveraging Automated Unit Tests for Unsupervised Code Translation"},{"paperId":"407b9e9478ba6bff43ce4b20e8b6cb2b303477d2","title":"P LANNING WITH L ARGE L ANGUAGE M ODELS FOR C ODE G ENERATION"},{"paperId":"b2c4fdb49bdb23e6ad0ac3272029324157046ea7","title":"Automated Repair of Code from Language Models"},{"paperId":"78863000eb70945cc8d791d45d4a3fe8a6521cb6","title":"Open-Ended Knowledge Tracing for Computer Science Education"},{"paperId":"3b0cf543a730e674d4213d344ebc857fada76ead","title":"Understanding High-Level Properties of Low-Level Programs Through Transformers"},{"paperId":"7497360b0f411a44aa6afbd8b830050c40ec8aed","title":"Dataset of Student Solutions to Algorithm and Data Structure Programming Assignments"},{"paperId":"6f84d0cc33c7c58f74b28ddcc1cbda91ea608c9f","title":"C ODE S UMMARIZATION : D O T RANSFORMERS R E ALLY U C"},{"paperId":"27d16a7f2ce8f2b787b34ff1f9b4fece079700c3","title":"Figuring out Figures: Using Textual References to Caption Scientific Figures"},{"paperId":"a3564f3cf954c05844c757505325a50b4d858e22","title":"Generating Infrastructure-as-Code from Natural Language: Evaluating Fine-Tuned GPT-3 Models for Cloud Infrastructure Provisioning"},{"paperId":"570a6a5b8ec2827c3f33bb1b1bd027190a0d3e07","title":"Neural Program Generation Modulo Static Analysis"},{"paperId":"a8863de15a5ee8eed98107f423138a1a8f5a2ba8","title":"Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"58a6ca2ae28a618126f71a07262cb958a8c37904","title":"Latent Execution for Neural Program Synthesis"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"6a1b25f7a67395ad1e676027322913acbb0a0635","title":"CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review"},{"paperId":"27e3ca4fd7b8290a0e12ca4fd2b7ad5bcd5900f2","title":"Are Transformers All That Karel Needs?"},{"paperId":"f5eb526492798dd7a53fe78f28431f5f489192da","title":"A Survey on Semantic Parsing for Machine Programming"},{"paperId":"642e280df732665249315d6c144871f0e2ceeae6","title":"NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash Commands"}],"references":[{"paperId":"c317f4d5ddeb11da9a5c8fea1e3cdec2f7de485d","title":"Deep Learning Based Program Generation From Requirements Text: Are We There Yet?"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"10bb7e2c54b947fa50e7bb65b0b5c700fe998044","title":"Measuring Massive Multitask Language Understanding"},{"paperId":"ce7091d011be560aca0fba8511e5a41a1436f5d7","title":"Aligning AI With Shared Human Values"},{"paperId":"0df347f5e3118fac7c351917e3a497899b071d1e","title":"Datasheets for datasets"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"5fe0a4af3bd1479d5e39fbda2215c86bce54722b","title":"Generative Language Modeling for Automated Theorem Proving"},{"paperId":"725264948d7b6946259af5b8d966e996b9570f99","title":"DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"},{"paperId":"70f2c1567ef94fdf4581e1290bf7667cc9a4dcfc","title":"LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"0071ee55f1749e6be586645bab08f0385ee856b4","title":"Graph-based, Self-Supervised Program Repair from Diagnostic Feedback"},{"paperId":"04f4e55e14150b7c48b0287ba77c7443df76ed45","title":"PIQA: Reasoning about Physical Commonsense in Natural Language"},{"paperId":"0c5bc409e62e65f86838968a2a7cdae5fa0b288b","title":"RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers"},{"paperId":"00c957711b12468cb38424caccdf5291bb354033","title":"ZeRO: Memory optimizations Toward Training Trillion Parameter Models"},{"paperId":null,"title":"2020a) do a smaller-scale analysis of code generation but with their limited"},{"paperId":"66117f82def0c69a3b9cc77eb3e2694b0245ca86","title":"Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad","title":"HellaSwag: Can a Machine Really Finish Your Sentence?"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"2016) introduce datasets based on Hearthstone and Magic the Gathering card games for code generation"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":null,"title":"2019c) use Spider"},{"paperId":null,"title":"2018) introduce the NAPS dataset for converting"},{"paperId":null,"title":"Gathering card games for code"},{"paperId":null,"title":"Natural program synthesis dataset"},{"paperId":null,"title":"Number of Programs’ refers to the number of human-written programs"},{"paperId":"6b024162f81e8ff7aa34c3a43d601a912d012c78","title":"Making Neural Programming Architectures Generalize via Recursion"},{"paperId":null,"title":"Attention is all you"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"2579e826f22da988fbc1b5d545aa59ab68bde4f3","title":"Program Synthesis Using Natural Language"},{"paperId":null,"title":"2018) by also facilitating the synthesis of database queries, though more recent program synthesis works such as Wang et al. (2019c) use Spider from Yu et al. (2018)"},{"paperId":null,"title":"A Additional Dataset Information Expanded Dataset Comparisons. We compared to several datasets in the (Kulal et al., 2019"},{"paperId":"27e1dbe9f7c71cd6cc1b0357f49aef497e572d09","title":"Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"},{"paperId":"e2d2099379af74d7be80a1964830f99edddaeb11","title":"Compositional Program Synthesis from Natural Language and Examples"},{"paperId":"110b7719f16eae8d82af0f56fca6e802c842c181","title":"Mining source code repositories at massive scale using language modeling"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"f60d8dd8ca3a7dfa7d0a14988af73084ad93619d","title":"Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing"},{"paperId":"b7c0e47f8b768258b7d536c21b218e6c46ab8791","title":"Learning to Parse Database Queries Using Inductive Logic Programming"}],"id":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","summary":"APPS is introduced, a benchmark for code generation that measures the ability of models to take an arbitrary natural language speciﬁcation and generate satisfactory Python code and shows that machine learning models are now beginning to learn how to code."}]}