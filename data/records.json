{"papers":[{"url":"https://www.semanticscholar.org/paper/270b015093073d3ba254928b6d736a59870d3fb1","title":"Chatbots As Fluent Polyglots: Revisiting Breakthrough Code Snippets","venue":"ArXiv","year":2023,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/01/2023","authors":"David Noever,Kevin Williams","citations":[],"references":[{"paperId":"6756fcd998caeb7b23702e08559e63710179334c","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"51d253814e85249a84bbe634b4a80d306b74fbd0","title":"\"It would work for me too\": How Online Communities Shape Software Developers' Trust in AI-Powered Code Generation Tools"},{"paperId":"8175ce2cbb99b6a394bdac152ae39d413f4f1380","title":"Codex Hacks HackerRank: Memorization Issues and a Framework for Code Synthesis Evaluation"},{"paperId":"0d08ffccc982781e310bb184397bbe64b9aef157","title":"Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models"},{"paperId":"979eb5c97c49d7979447ed684500895a24d75ac4","title":"The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming"},{"paperId":"6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a","title":"Few-Shot Semantic Parsing with Language Models Trained on Code"},{"paperId":"91260f73dd179487fb16713deb8267634ae14716","title":"CodexDB: Synthesizing Code for Query Processing from Natural Language Instructions using GPT-3 Codex"},{"paperId":"1444536496d8064f33e10b38b5820fecfab5b367","title":"Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"5ff9032d0f7f246d01ae7b2c231ab03469a7344a","title":"Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?"},{"paperId":null,"title":"Can artificial intelligence make software development more productive"},{"paperId":null,"title":"GPT-2 Output Detector Demo"},{"paperId":null,"title":"OpenAI Codex"},{"paperId":"b72c89500dd57f1a4ceadb97f3dbf5015948a5e7","title":"A Brief History of Chatbots"},{"paperId":null,"title":"The Lines of Code That Changed Everything"},{"paperId":"e9431029d8c7da7f55c3496c9d8a3cec542a858f","title":"COBOL to Java and Newspapers Still Get Delivered"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":null,"title":"Towards Making Legacy HPC Codes Maintainable: Two- Way Fortran-Python Transpilation with Python Type Hints (Unrefereed Workshop Manuscript)"}],"id":"270b015093073d3ba254928b6d736a59870d3fb1","summary":"The research applies AI-driven code assistants to analyze a selection of influential computer code that has shaped modern technology, including email, internet browsing, robotics, and malicious software to provide insights into obfuscated code or software lacking explanatory commentary."},{"url":"https://www.semanticscholar.org/paper/bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence","venue":"ArXiv","year":2022,"referenceCount":77,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Yichen Xu,Yanqiao Zhu","citations":[],"references":[{"paperId":"4c26a6a777f1d46ffe79e3f41f8fd1ef6a6bd8c9","title":"Automatically Generating CS Learning Materials with Large Language Models"},{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"c03fa01fbb9c77fe3d10609ba5f1dee33a723867","title":"ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"},{"paperId":"703f79763d534dbf9674132cec890f432dcc19ec","title":"A Survey of Deep Learning Models for Structural Code Understanding"},{"paperId":"4b27f18bff43d605805c92696a979714ced0b805","title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"979eb5c97c49d7979447ed684500895a24d75ac4","title":"The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"916a06a6d51aa93de27aac2f3e14faed08dd6706","title":"Formal Mathematics Statement Curriculum Learning"},{"paperId":null,"title":"Natural Language Processing with Transformers: Building Language Applications with Hugging Face"},{"paperId":"f7987fa2aadc0b368c185dc4d2fdb1337a202c32","title":"Solving Probability and Statistics Problems by Program Synthesis"},{"paperId":"4e40595d6ecba027cebb4f2e3b43ae44bcf51daf","title":"Solving Linear Algebra by Program Synthesis"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"0b077c9577f4297dcf3da835e253d21965bbc6e0","title":"CoTexT: Multi-task Learning with Code-Text Transformer"},{"paperId":"1c0752fb3e9ab5c9392f196225075422f26b5110","title":"How could Neural Networks understand Programs?"},{"paperId":"3a863672b072a37ac8d2eb6b780234f6b748a2d9","title":"Generating bug-fixes using pretrained transformers"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"f389c09ad85263bdd1bb2518d61c90a8a558441d","title":"MulCode: A Multi-task Learning Approach for Source Code Understanding"},{"paperId":"20d37eb44ad65735f243938961fde9ba5b4d26b7","title":"D2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using Differential Analysis"},{"paperId":"3a1efac4a8aa2f40a8128e034c01f18660652dd5","title":"DOBF: A Deobfuscation Pre-Training Objective for Programming Languages"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"8b4c857311c001f6ed0cd790cce4af4dfcfb6533","title":"Deep Graph Matching and Searching for Semantic Code Retrieval"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"021bbcefc993c389bad6c1daefd8ff92d0fc2441","title":"Contrastive Code Representation Learning"},{"paperId":"2a405483796dfedf5d95483aa8880c57626e0e9f","title":"Integrating Tree Path in Transformer for Code Representation"},{"paperId":null,"title":"Fsf-funded call for white papers on philosophical and legal questions around copilot"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"49cf6a22a5dac5bc98b653534af65ffa0bc0e76d","title":"Multi-task Learning based Pre-trained Language Model for Code Completion"},{"paperId":"406afe68e789fcb0d7e3d24ebea65b53d206f740","title":"Exploring Software Naturalness through Neural Language Models"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"37a23c43ddf09ea97b82b38e2827a2229cfae545","title":"Novel positional encodings to enable tree-based transformers"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"00549af4bc3270e0f688acbf694f912d7ee39cad","title":"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks"},{"paperId":"e3ef11877bdd08140fcabf358dd9fc5bef6b15e0","title":"Recommendations for Datasets for Source Code Summarization"},{"paperId":"516497e7fc39ebb126bed28dcc5c6129e500acb8","title":"An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation"},{"paperId":"98d1307bed619b58b4a44acd8e65ac58495776c2","title":"code2seq: Generating Sequences from Structured Representations of Code"},{"paperId":"73bade9162bc4df6ca6c9ab8a7eb2d54d35acad2","title":"code2vec: learning distributed representations of code"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e3deff41f9d3eb1e141207b69d6c7a4c007bd63e","title":"Program Synthesis Through Reinforcement Learning Guided Tree Search"},{"paperId":"41c8cabe475d85d7548c25935da19ea5b96dfe06","title":"Neural Code Comprehension: A Learnable Representation of Code Semantics"},{"paperId":"e3d772986d176057aca2f5e3eb783da53b559134","title":"Unsupervised Machine Translation Using Monolingual Corpora Only"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"32aa2517b03c871c11e521c2a3406f457833e2c3","title":"Summarizing Source Code using a Neural Attention Model"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"9277dc7dd5b348645e8fcf4a987e5b3fc9132c7c","title":"Towards a Big Data Curated Benchmark of Inter-project Code Clones"},{"paperId":"110b7719f16eae8d82af0f56fca6e802c842c181","title":"Mining source code repositories at massive scale using language modeling"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"9a56a9cea19b83bf46ab2d47b59bc1ea3020a2b1","title":"Experiences Using Static Analysis to Find Bugs"},{"paperId":"b7eceec1e8edfa769fdd095db16897a061b02a79","title":"Compilers: Principles, Techniques, and Tools (2nd Edition)"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"95d886a0d097f0a1a81db8f431e744996ecc3048","title":"Static Analysis versus Software Model Checking for Bug Finding"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"5a6dbe11f7bd7182ca008b0f94b75fe5cac57a08","title":"Types and programming languages"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"45510aef1c064bbd16361dc9413ddd2068bebe53","title":"Compiler Construction: Principles and Practice"},{"paperId":"f8d77bb8da085ec419866e0f87e4efc2577b6141","title":"Serial Order: A Parallel Distributed Processing Approach"},{"paperId":"111fd833a4ae576cfdbb27d87d2f8fc0640af355","title":"Learning internal representations by error propagation"},{"paperId":"fef8d6579c5ba743baf3aa4c8dda78516284c3d9","title":"The Cloze Procedure"},{"paperId":"e2e0dd827011bb9bcb09578efe95c973a1f413b5","title":"The Mythical Man-Month: Essays on Softw"},{"paperId":"766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd","title":"“Cloze Procedure”: A New Tool for Measuring Readability"}],"id":"bae76e1d13abe54f66dc140be53538b864578ba8","summary":"This paper presents a comprehensive survey of the NCI domain, including a thorough review of pretraining techniques, tasks, datasets, and model architectures, and hopes it will serve as a bridge between the natural language and programming language communities."},{"url":"https://www.semanticscholar.org/paper/acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code","venue":"ArXiv","year":2021,"referenceCount":119,"citationCount":499,"influentialCitationCount":139,"publicationDate":"07/07/2021","authors":"Mark Chen,Jerry Tworek,Heewoo Jun,Qiming Yuan,Henrique Ponde,Jared Kaplan,Harrison Edwards,Yura Burda,Nicholas Joseph,Greg Brockman,Alex Ray,Raul Puri,Gretchen Krueger,Michael Petrov,Heidy Khlaaf,Girish Sastry,Pamela Mishkin,Brooke Chan,Scott Gray,Nick Ryder,Mikhail Pavlov,Alethea Power,Lukasz Kaiser,Mohammad Bavarian,Clemens Winter,Philippe Tillet,F. Such,D. Cummings,Matthias Plappert,Fotios Chantzis,Elizabeth Barnes,Ariel Herbert-Voss,William H. Guss,Alex Nichol,I. Babuschkin,S. Balaji,Shantanu Jain,A. Carr,J. Leike,Joshua Achiam,Vedant Misra,Evan Morikawa,Alec Radford,M. Knight,Miles Brundage,Mira Murati,Katie Mayer,P. Welinder,Bob McGrew,Dario Amodei,Sam McCandlish,Ilya Sutskever,Wojciech Zaremba","citations":[{"paperId":"c532f1df90925e5c69789f0cd99248d8a2a2e5bc","title":"My AI Wants to Know if This Will Be on the Exam: Testing OpenAI’s Codex on CS2 Programming Exercises"},{"paperId":"7196f4940459b7a3bdf685ab2ab4047f918c0df8","title":"User-customizable Transpilation of Scripting Languages"},{"paperId":"572e82dfdde0f72f9448caf72fdc68a233da6659","title":"Open Problems in Applied Deep Learning"},{"paperId":"0192638593c430e15e2804a74a3e8a7ecb52d435","title":"On the Design of AI-powered Code Assistants for Notebooks"},{"paperId":"72ceff6b004efe8724d59d51f12a0b60623ceed4","title":"Causal Reasoning of Entities and Events in Procedural Texts"},{"paperId":"0885556b71b24a641b4ffe139afd4d2712228cff","title":"Beware of the Unexpected: Bimodal Taint Analysis"},{"paperId":"288055fa8954c6ea6cdafee25cd523108b716d15","title":"Explicit or Implicit? On Feature Engineering for ML-based Variability-intensive Systems"},{"paperId":"1338b3771c27090dee722cc5b351ace179ebae76","title":"Mathematics, word problems, common sense, and artificial intelligence"},{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"},{"paperId":"f4ee6feae75757c6c706ffeaa31c01cf62a2c5e8","title":"Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL Robustness"},{"paperId":"9f530ebf624bf58e91b2a1f20b0799a45ca48f9a","title":"An Analysis of the Automatic Bug Fixing Performance of ChatGPT"},{"paperId":"d8405996b4d08c304098636aedd9e1c1a1e262ee","title":"The Premature Obituary of Programming"},{"paperId":"e659fa1e79a2a151be331125c14339988542aac3","title":"Batch Prompting: Efficient Inference with Large Language Model APIs"},{"paperId":"cb29cf52f0f7d2e4324c68690a55b22890f2212d","title":"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection"},{"paperId":"a20875e70a38cb053cd34e170038c4746f85dac9","title":"A Case Study in Engineering a Conversational Programming Assistant's Persona"},{"paperId":"907a77639069bb7dd270f017068745706133cffc","title":"Inaccessible Neural Language Models Could Reinvigorate Linguistic Nativism"},{"paperId":"6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc","title":"See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning"},{"paperId":"1f22de83d912176cb8857efa1c6d65b14d6a2f5c","title":"ChatGPT is not all you need. A State of the Art Review of large Generative AI models"},{"paperId":"23c9c8a3abf9f17c176904faa97a7f233cb2c03f","title":"EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata"},{"paperId":"c27bbdd8968c11513a68383145f7935293a57c25","title":"Exploring the Approximation Capabilities of Multiplicative Neural Networks for Smooth Functions"},{"paperId":"a3e355b5de868f34fdfa2500415c5f74c69d2091","title":"Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models"},{"paperId":"c9ad9d69d7568110dd5527598a92c7f8b335eef4","title":"Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations"},{"paperId":"468992bf970c37bd1fef58b78a6c2fcd8c018868","title":"Scaling Laws for Generative Mixed-Modal Language Models"},{"paperId":"5435ed7c26f0c250493f244acffb69dd929d116b","title":"Structured Case-based Reasoning for Inference-time Adaptation of Text-to-SQL parsers"},{"paperId":"4221f29aec5ce9feedddc1644f074af19f8d110e","title":"FlashFill++: Scaling Programming by Example by Cutting to the Chase"},{"paperId":"e028ba59aacec72a55164e274e1d64896fea0256","title":"Efficient Mutation Testing via Pre-Trained Language Models"},{"paperId":"dae74645479f7c1fa3671066f9e24ec6c20c17ec","title":"TrojanPuzzle: Covertly Poisoning Code-Suggestion Models"},{"paperId":"490f1e8ff352bded30bcde01d5b4769d6c2d2dd5","title":"Adversarial Attacks on Neural Models of Code via Code Difference Reduction"},{"paperId":"b2a8b21062718f930dbb1662a93ae1f13298fa1f","title":"Serenity: Library Based Python Code Analysis for Code Completion and Automated Machine Learning"},{"paperId":"270b015093073d3ba254928b6d736a59870d3fb1","title":"Chatbots As Fluent Polyglots: Revisiting Breakthrough Code Snippets"},{"paperId":"4431a39f677fe59b07b3f0cfde7b10f7208cf46c","title":"N-Best Hypotheses Reranking for Text-To-SQL Systems"},{"paperId":"0392d58335ce674a70f5e58ac8c438de296a0e6a","title":"Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models"},{"paperId":"69e330037afd18e5546fdcacbc9a9f7deb69bba9","title":"Memorization and Generalization in Neural Code Intelligence Models"},{"paperId":"a8e0ba16346b72c3a04dd0b1da84bc5f28900174","title":"Using GitHub Copilot to Solve Simple Programming Problems"},{"paperId":"ab8a53be2ff8a756776aa0245ad8da41189d60d6","title":"Fuzzing Deep-Learning Libraries via Large Language Models"},{"paperId":"b8d06dd769f89d08bdd9997d7bd363c89ede845b","title":"ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models"},{"paperId":"4bea09d4c897fb201c032b9eb605a943b1e70435","title":"CORRPUS: Detecting Story Inconsistencies via Codex-Bootstrapped Neurosymbolic Reasoning"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"269df328eec08b56b7b1f38a7555797fe2b999b6","title":"ReCode: Robustness Evaluation of Code Generation Models"},{"paperId":"d3a7a4543d83f568f79d1febe8379465ff0140c9","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"0f38267a8ba32789f5d3b1b19820f86940fea052","title":"Generation-Augmented Query Expansion For Code Retrieval"},{"paperId":"3ee9c65366efbb17adf370c39f20dbef60d53670","title":"Towards Reasoning in Large Language Models: A Survey"},{"paperId":"5c32c653735b43a0a8923ca65ac191bd4bf15311","title":"Precise Zero-Shot Dense Retrieval without Relevance Labels"},{"paperId":"29be9045fb09f0c947fb24c76bd1136d47880d96","title":"Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions"},{"paperId":"3810345aef8d1146452196e26ac49bdc07b26d8b","title":"Can Current Task-oriented Dialogue Models Automate Real-world Scenarios in the Wild?"},{"paperId":"bf5fbd690f24f873df86d1b0a06579cf42f7dc36","title":"Dialog2API: Task-Oriented Dialogue with API Description and Example Programs"},{"paperId":"433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context"},{"paperId":"bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"8210cef990b8e5cddbc95000e46309bdd25337f7","title":"Asking Clarification Questions for Code Generation in General-Purpose Programming Language"},{"paperId":"6756fcd998caeb7b23702e08559e63710179334c","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"9e8f0d9ba4e7af673c8b214b3764e020706dd1f3","title":"Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"cef330bacf014d60daabbd489647b2006af130ca","title":"Discovering Language Model Behaviors with Model-Written Evaluations"},{"paperId":"42630c03d3817b1153d245f20742ad4b30a80b75","title":"JEMMA: An Extensible Java Dataset for ML4Code Applications"},{"paperId":"3b8ccc7ec80b8775de603e248ac1ca2b919d6b70","title":"Chatbots in a Botnet World"},{"paperId":"9cc5c25517c3a78183a052e8e93a44e85bb17432","title":"Improving Cross-task Generalization of Unified Table-to-text Models with Compositional Task Configurations"},{"paperId":"ae441f7305dc2cd58c708528b3ecee3501cc5c46","title":"Plansformer: Generating Symbolic Plans using Transformers"},{"paperId":"4290a70025f29d7054c550c75ae6b24c38a79d12","title":"SE Factual Knowledge in Frozen Giant Code Model: A Study on FQN and its Retrieval"},{"paperId":"e4f8cb7bb933d95bec8d6eaeeb9d1815ed095f21","title":"Benchmarking Large Language Models for Automated Verilog RTL Code Generation"},{"paperId":"6bc87e51018d6de55011e95a0d43c588dd44a1e8","title":"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages"},{"paperId":"2cbddba62a1d1185d364f876b35dc9d6689d60e4","title":"Diverse Demonstrations Improve In-context Compositional Generalization"},{"paperId":"7932b714e2ae1def5828df52b97f1decb9bebd32","title":"Considerations for Differentially Private Learning with Large-Scale Public Pretraining"},{"paperId":"f788c8f1e331dc6b648e9aae5a08e059352bfa13","title":"Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments"},{"paperId":"c548b05a9696023a6eb5ff6d93a9a00e850b1ea8","title":"Automated Variable Renaming: Are We There Yet?"},{"paperId":"9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming"},{"paperId":"4c26a6a777f1d46ffe79e3f41f8fd1ef6a6bd8c9","title":"Automatically Generating CS Learning Materials with Large Language Models"},{"paperId":"8bfd39e6e8f15531ffb071f2c6470e1e6e0a4aff","title":"LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models"},{"paperId":"97148858b395c5122e09d6e8a20dd7016a111aa3","title":"Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning"},{"paperId":"8175ce2cbb99b6a394bdac152ae39d413f4f1380","title":"Codex Hacks HackerRank: Memorization Issues and a Framework for Code Synthesis Evaluation"},{"paperId":"fe3ead702e8e8948d00caef9bc9dd075dc560236","title":"I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification"},{"paperId":"dca3bc28a7d404b28780a813ea7072eda809e6c0","title":"Programming Is Hard - Or at Least It Used to Be: Educational Opportunities And Challenges of AI Code Generation"},{"paperId":"720130cea1933e0d9ff4915e0a9a869bf1ab0221","title":"Automated Quantum Software Engineering: why? what? how?"},{"paperId":"c4607fdcc6bcbc214546147062104d9f50493810","title":"On Mixed-Initiative Content Creation for Video Games"},{"paperId":"be76b0f32e287d866bc7aefc700052f9825ed3ce","title":"BudgetLongformer: Can we Cheaply Pretrain a SotA Legal Language Model From Scratch?"},{"paperId":"32b58766a1bfcef7ebba07070a272687aa518206","title":"Explicit Knowledge Transfer for Weakly-Supervised Code Generation"},{"paperId":"ea55ae4c82b45e3857f37406c94e9f642a2b3d84","title":"Genetic Programming with Local Scoring"},{"paperId":"6d4a12ea469ff08634eeb24c47b265a7dca2fce2","title":"PADL: Language-Directed Physics-Based Character Control"},{"paperId":"27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation"},{"paperId":"46d0a832fada6147bceb0bd4e39928e482733246","title":"How Important are Good Method Names in Neural Code Generation? A Model Robustness Perspective"},{"paperId":"477d0b2abf07ee92732698f9aeb3c784f28ffa88","title":"On the Security Vulnerabilities of Text-to-SQL Models"},{"paperId":"34009b5d7a3ab44863c8dfa0d8dc4383c86c3115","title":"Deep-Learning-based Vulnerability Detection in Binary Executables"},{"paperId":"8aa23a86603f7dd4eceda3d2e0337ba90dff7f4f","title":"CodeExp: Explanatory Code Document Generation"},{"paperId":"6d7b8a478801bd9d21df82d5f33ae6eced90da5e","title":"Solving math word problems with process- and outcome-based feedback"},{"paperId":"edcd520e553dc58c728eceb8433e3d155955a89a","title":"Complementary Explanations for Effective In-Context Learning"},{"paperId":"ce9397eaa3de0f791bead5d16f14b5dd4a15052f","title":"GitHub Considered Harmful? Analyzing Open-Source Projects for the Automatic Generation of Cryptographic API Call Sequences"},{"paperId":"30624a18720bf93a85dc3efe570df271a8c9f4c3","title":"Program Repair"},{"paperId":"2f29426556cb9d434f114b5a8472fd9d65e71d4f","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"6d56ab40eba28678c4f4d6b054f2ae4e7048c928","title":"CLAWSAT: Towards Both Robust and Accurate Code Models"},{"paperId":"05f09050118d82100d04e56ba8b54836753fa9b4","title":"Programming by Example and Text-to-Code Translation for Conversational Code Generation"},{"paperId":"ef3bbee11c03de1e7e3672121c58768b10d34436","title":"Exploring the Efficacy of Pre-trained Checkpoints in Text-to-Music Generation Task"},{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"ce47e99c1ebab6fc253b1be58bd9478a87d90288","title":"PAL: Program-aided Language Models"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"0b871a9f12e5c2da1b291a8b166c671256ebe1cd","title":"A Copy Mechanism for Handling Knowledge Base Elements in SPARQL Neural Machine Translation"},{"paperId":"e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"95915aa592fdfc73f039c13472a21d3e4220f129","title":"On the Compositional Generalization Gap of In-Context Learning"},{"paperId":"9dab4c20648cd4c7c6830e6274a95294b014aac9","title":"QAmeleon: Multilingual QA with Only 5 Examples"},{"paperId":"ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","title":"Evaluating How Fine-tuning on Bimodal Data Effects Code Generation"},{"paperId":"965e409a3e7b5670d609837fac9823b160d6639c","title":"Logical Tasks for Measuring Extrapolation and Rule Comprehension"},{"paperId":"5f9ca3133898bcc7b7d76caadb18598cc336b208","title":"Calibrated Interpretation: Confidence Estimation in Semantic Parsing"},{"paperId":"a4bdc300db297756f36bedee2859b62df8e268c2","title":"Follow the Wisdom of the Crowd: Effective Text Generation via Minimum Bayes Risk Decoding"},{"paperId":"fa4188bcc6728b580699f714a8a6fe5fc60d7dfe","title":"Rethinking data-driven networking with foundation models: challenges and opportunities"},{"paperId":"048ed70192de0232086eb32a95ffb3be8d336c76","title":"Metaphors We Learn By"},{"paperId":"632ab7663e6d64578ceda1d1df9ec525b503bacb","title":"Steps towards prompt-based creation of virtual worlds"},{"paperId":"327f1561b544b0a3b9d8d5d0e6d82c2a5911fca9","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"ac3d7ae8b4acb137492d4a8d8bcff480b89fa000","title":"Programming Pedagogy and Assessment in the Era of AI/ML: A Position Paper"},{"paperId":"2037eb819d08446a057634a851ebf9afb8d7ae4b","title":"Assessing the quality of GitHub copilot’s code generation"},{"paperId":"87bb0bc9195751351761b21c3adfa7d055a824ea","title":"Neural language models for code quality identification"},{"paperId":"71280dba5bda65c162f9deaffed7d3d20692ca0a","title":"SecurityEval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques"},{"paperId":"61ddf932488405ab1c7b275460d2b3c5dfa274a0","title":"Fixing Model Bugs with Natural Language Patches"},{"paperId":"2abed82162c47a0cc32cd62afcf46b0745541017","title":"Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book"},{"paperId":"ec7324a15009a9bd0b676f6b17762f759cf5dd9a","title":"Large Language Models Are Human-Level Prompt Engineers"},{"paperId":"511df4b4e44db2852228091c297dcb5c1d212017","title":"MPCFormer: fast, performant and private Transformer inference with MPC"},{"paperId":"3214fbf2a78c5d1bff4d2a2e67e3f22a341df460","title":"Hybrid Rule-based and Machine Learning System for Assertion Generation from Natural Language Specifications"},{"paperId":"afd834af31f043e7c1d348c6a51d299d029dafca","title":"SPEAK YOUR MIND: INTRODUCING APTLY, THE SOFTWARE PLATFORM THAT TURNS IDEAS INTO WORKING APPS"},{"paperId":"22071947891c505d7a4879aa172f8e0fcd9f9f44","title":"A methodology for refined evaluation of neural code completion approaches"},{"paperId":"20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"3c74317647301b28234663bd3bf084498a647b6f","title":"Emergent Linguistic Structures in Neural Networks are Fragile"},{"paperId":"1819c5c8c556db205ec9a49d5ba4a8c2afb28a4a","title":"Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy"},{"paperId":"6b8f26678785ebd7b7b27984af3cb9a273b722b0","title":"Towards Zero-Shot and Few-Shot Table Question Answering using GPT-3"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"f5b3cb14e0947c62b470d2072483481f14258738","title":"A Solvable Model of Neural Scaling Laws"},{"paperId":"cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers"},{"paperId":"41f5e1ad7793593befc0b9c38f756836e8b07c98","title":"Code4ML: a Large-scale Dataset of annotated Machine Learning Code"},{"paperId":"99323bd786ee5be1e1aa589858e14e89630f207b","title":"Exploring the Learnability of Program Synthesizers by Novice Programmers"},{"paperId":"0566c1c3eeeef5c968fced6d80b77fe22d02bbd9","title":"Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language"},{"paperId":"0b340dd78fd04bbde2807d5efedb796d319355e3","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?"},{"paperId":"dcf3caaf797f4496241c7375c0e8ede8cc5616f1","title":"Learning to Configure Computer Networks with Neural Algorithmic Reasoning"},{"paperId":"a6df60ce44ab5d968bb6c649f30d1ba8e121fa98","title":"Multi-Viewpoint and Multi-Evaluation with Felicitous Inductive Bias Boost Machine Abstract Reasoning Ability"},{"paperId":"a593ce5d2a93f3113be7717d08d1ba8e62fd7ddf","title":"Benchmarking Language Models for Code Syntax Understanding"},{"paperId":"472d87be3dc298102e058be55a814cc6d2085b39","title":"Towards Deceptive Defense in Software Security with Chaff Bugs"},{"paperId":"5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","title":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming"},{"paperId":"f031ba42cf82f106200bb03fbb91dd5671a59b9c","title":"Practical Program Repair in the Era of Large Pre-trained Language Models"},{"paperId":"38e1a9c5599fc7597b7c5ffd37951ba5f528094c","title":"XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Semantic Parsing"},{"paperId":"50ee0be85da0e98d9ec0157c7b89f76b4b5d1516","title":"Contrastive Search Is What You Need For Neural Text Generation"},{"paperId":"152dc3042f5d4fc5a2686b5f4e0904f1e12a9207","title":"Code4Struct: Code Generation for Few-Shot Structured Prediction from Natural Language"},{"paperId":"dae5c4660dab4c2b0a0be586f8537db980925d4a","title":"When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks"},{"paperId":"aacf8a1bf2349b29159ab884ea82465330c2e256","title":"Combining OCL and natural language: a call for a community effort"},{"paperId":"d26f616699a122e5455a13189e276002ee4cf923","title":"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs"},{"paperId":"1acb761ec624104124eda4a8c681e59adf1bf2b9","title":"Formalizing Chemical Theory using the Lean Theorem Prover"},{"paperId":"a6ef5bad716091fb1888bf365f6129628ab3a5ee","title":"ObSynth: An Interactive Synthesis System for Generating Object Models from Natural Language Specifications"},{"paperId":"5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"4bef9d46209ac8988ea5ab83547149760d4af65e","title":"Automatic Document Selection for Efficient Encoder Pretraining"},{"paperId":"4c2534f9b03ac2f3810c07abc398a11bcf47258e","title":"Transformers Learn Shortcuts to Automata"},{"paperId":"8ffe7b7eeddbd4c22d642b0a48379d17e61c3bab","title":"Soft-Labeled Contrastive Pre-training for Function-level Code Representation"},{"paperId":"663a41c866d49ce052801fbc88947d39764cad29","title":"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"},{"paperId":"711d5e8ddbb840ad31a9ffa3d38590603ba69a92","title":"Prompting GPT-3 To Be Reliable"},{"paperId":"706b1e0c7dcc58ac6ba80a0ca37efc2993e6e5ef","title":"CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling"},{"paperId":"e66f0f822d4c4853b39b27daaafa2993005fd55e","title":"Large Language Models are few(1)-shot Table Reasoners"},{"paperId":"a0e086754a9de168ae2674f472affe4c8d1502e6","title":"Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods"},{"paperId":"39e40821b7207125e54e6ed7112e55cd38c6f0c3","title":"Language Models of Code are Few-Shot Commonsense Learners"},{"paperId":"3f2fa77ce40d21a7cf5e9a4dbf594c85a31576d0","title":"Searching for Better Database Queries in the Outputs of Semantic Parsers"},{"paperId":"de7d334a543d077f4162ebcd8da7eee843b7b10a","title":"Predictive Querying for Autoregressive Neural Sequence Models"},{"paperId":"7619b0aad6f16a0328021c5fdd3d97239e362e97","title":"Visual Language Maps for Robot Navigation"},{"paperId":"045a6f92b58f08d5b305dd5d661316a506ee4d43","title":"Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation"},{"paperId":"285dcaad8a5da5159bf2550239f3365b5282bf05","title":"Next Syntactic-Unit Code Completion and Applications"},{"paperId":"825333b7efe2cade106eaf36c7e731f757974806","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot"},{"paperId":"0c78a473e33a81246d5c0fbbda7e7de168814c18","title":"FlexType: A Plug-and-Play Framework for Type Inference Models"},{"paperId":"dcff38de0e5fb47bdb31d472c21b0c2d88cbc4fc","title":"AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models"},{"paperId":"259b7a01700c39d5669e88d1434873ea38a13528","title":"In-Context Policy Iteration"},{"paperId":"90350aa626bed47b02d0c162462e5b0ca82be6b2","title":"Automatic Chain of Thought Prompting in Large Language Models"},{"paperId":"d35a6adf01f35a18f655234198a4778d3307487b","title":"GNM: A General Navigation Model to Drive Any Robot"},{"paperId":"62f0db3a5ad5c795ec18fc7a6e7b01836809df57","title":"Language Models are Multilingual Chain-of-Thought Reasoners"},{"paperId":"c140fe515de2f20d0c85c813c7b3ec1defc41f9d","title":"Binding Language Models in Symbolic Languages"},{"paperId":"1afe2799df238ca749534860552501eaf51c77eb","title":"TgDLF2.0: Theory-guided deep-learning for electrical load forecasting via Transformer and transfer learning"},{"paperId":"55e3fe05598be7c3dd357d51166869f6571b824f","title":"Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors"},{"paperId":"b69b84706fe84c4c614e4473760c57dffbfeb9a0","title":"Waveformer: Linear-Time Attention with Forward and Backward Wavelet Transform"},{"paperId":"dd12f12015220d4beb2967fe23860d575e7a9e53","title":"Grounding Language with Visual Affordances over Unstructured Data"},{"paperId":"04db926687fa9af8f7e9be04901f440bea135da0","title":"Guiding the PLMs with Semantic Anchors as Intermediate Supervision: Towards Interpretable Semantic Parsing"},{"paperId":"7d6f17706cbcfcca55f08485bcbf8c82e00c9279","title":"Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals"},{"paperId":"5c4bece777bbc7c52cdd4bbb6e222163f6a580dd","title":"Unveiling the Black Box of PLMs with Semantic Anchors: Towards Interpretable Neural Semantic Parsing"},{"paperId":"0b8772b7790c69f40897b5eb7f8fd57f24138f3d","title":"ContraGen: Effective Contrastive Learning For Causal Language Model"},{"paperId":"8a854331c593c6a766fa3b8037fb2ad1b95a6f06","title":"An Empirical Study of Code Smells in Transformer-based Code Generation Techniques"},{"paperId":"7002faf46875c84cb3ad148f362118b46946f163","title":"CodeDSI: Differentiable Code Search"},{"paperId":"2a0456b0408cd4c33f2ff4400374e7be2497a362","title":"Repairing Bugs in Python Assignments Using Large Language Models"},{"paperId":"95fa2b27ab7eb84738441ee16da97323538938f9","title":"I Speak, You Verify: Toward Trustworthy Neural Program Synthesis"},{"paperId":"cd6496bc404e18a24f634e3dded2ed1cdca03e0f","title":"Learning to Learn with Generative Models of Neural Network Checkpoints"},{"paperId":"363758e9e296adc9391ed731e834809cf5d4c19b","title":"Are machine programming systems using right source-code measures to select code repositories?"},{"paperId":"971e875e28f26240987d2c9470d1ee74ad204205","title":"Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction"},{"paperId":"c03fa01fbb9c77fe3d10609ba5f1dee33a723867","title":"ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"},{"paperId":"885676b8a05437868f7b83c134a99f991190a1de","title":"T5QL: Taming language models for SQL generation"},{"paperId":"7b2be89668e8f18ed20dc51be7646bd31cc0aff3","title":"Assisted Specification of Code Using Search"},{"paperId":"83d879a830ac4286945e628e670c30fefb1493c6","title":"NL2INTERFACE: Interactive Visualization Interface Generation from Natural Language Queries"},{"paperId":"a7435722d8ab595da5a9c70ac9160f57d0dcd75a","title":"Enabling Transformers to Understand Low-Level Programs"},{"paperId":"97e1903b4afc811a9b5fa9e55723e80ba48bf46f","title":"Code as Policies: Language Model Programs for Embodied Control"},{"paperId":"cd11837054a3396542daca1b9966b03a057fbe9f","title":"Malicious Source Code Detection Using Transformer"},{"paperId":"e54a6201ea31e3c11576a343ff04cab3cbba8ccb","title":"Multi-donor Neural Transfer Learning for Genetic Programming"},{"paperId":"ef29fb8cc6bdda4b011288f51da521d3c25fc53d","title":"Learning to Prevent Profitless Neural Code Completion"},{"paperId":"7d2c43a4a9e3198cd133bb0ab86ca69a5125f2c5","title":"Exploring Code Style Transfer with Neural Networks"},{"paperId":"ac2e15fbfe3ea338725f5d33d17a5a687609c431","title":"On The Computational Complexity of Self-Attention"},{"paperId":"372dd97de200970859315bce7e150fe50baecad5","title":"Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence"},{"paperId":"ace4d199aa72ab0808af0f30a61fc16727c95dec","title":"AudioLM: a Language Modeling Approach to Audio Generation"},{"paperId":"9360390b02b9a09ece9a2486055b17e18dc5d3f6","title":"Automatic Code Documentation Generation Using GPT-3"},{"paperId":"9afab8dc694269d205d769eaea549d8f7558d776","title":"Exploring the Verifiability of Code Generated by GitHub Copilot"},{"paperId":"6b5d1e50894b1f28e4798cf20e9ffa88b9ec011a","title":"How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models"},{"paperId":"5581bf85386737bd3378eec68189759a05280bea","title":"FOLIO: Natural Language Reasoning with First-Order Logic"},{"paperId":"0509c25103939d59ed4e27b1393e74ad5734c453","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot"},{"paperId":"befde3e07ce97f02f12fe92ab27e99f23ccd17aa","title":"Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation"},{"paperId":"1aac692ca061feb846cf32cd61a1d422f89593a1","title":"A Survey on Text-to-SQL Parsing: Concepts, Methods, and Future Directions"},{"paperId":"6032212d5790b6a580d68d469a9895aad6238c89","title":"Diverse Title Generation for Stack Overflow Posts with Multiple Sampling Enhanced Transformer"},{"paperId":"453a8fac3be9282be53908f0735160d0d21e0f48","title":"Repair Is Nearly Generation: Multilingual Program Repair with LLMs"},{"paperId":"1c8e15f15d67c5974445634bb971e2275e957aff","title":"Security Implications of Large Language Model Code Assistants: A User Study"},{"paperId":"9b61de7038290751377b64293baaf42f3e7cf441","title":"An Empirical Evaluation of Competitive Programming AI: A Case Study of AlphaCode"},{"paperId":"05ccfb058dc6788254742722278b0af2fc87f083","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"99f85119f113b5498517928eff74a904b69e37b7","title":"CCTEST: Testing and Repairing Code Completion Systems"},{"paperId":"916bfe3378b2b1828b181477720957e3a6395b26","title":"CommitBART: A Large Pre-trained Model for GitHub Commits"},{"paperId":"512e16b9aef6ca6cb973d734b4cc66661ea33498","title":"Targeted Honeyword Generation with Language Models"},{"paperId":"53b3b15e8cbe2baf82cd0de3709e0a1e6f677415","title":"Limits of an AI program for solving college math problems"},{"paperId":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?"},{"paperId":"8c2d9d2aa891e3b53bbd9166c1b804a0741fc44a","title":"Finding Reusable Machine Learning Components to Build Programming Language Processing Pipelines"},{"paperId":"7018ff49ca3b81f2ed6228b097a471c2529986e4","title":"CoditT5: Pretraining for Source Code and Natural Language Editing"},{"paperId":"def2e28863338cb20782eb2015a39d32df697ed6","title":"Learning to Improve Code Efficiency"},{"paperId":"c067664a45dce31411b3052c635c044ad4587db4","title":"Generating Diverse Code Explanations using the GPT-3 Large Language Model"},{"paperId":"654cd297b307c0c6fa08732511ba852b8dce1977","title":"Out of the BLEU: how should we assess quality of the Code Generation models?"},{"paperId":"15bacb240e2598457af4ded3039b6988aa9706f0","title":"Few-shot Adaptation Works with UnpredicTable Data"},{"paperId":"63c670ba8018da0a7e33c34ffd84c2a3ca54b894","title":"Language Models Can Teach Themselves to Program Better"},{"paperId":"ec4c8d99eb1c028c43af6d8bbf727392d351cb59","title":"Efficient Training of Language Models to Fill in the Middle"},{"paperId":"dd112d4dbd4656223770989778f39700de3052bc","title":"A Hazard Analysis Framework for Code Synthesis Large Language Models"},{"paperId":"1e3ec5709b2ca43233c566cd77dbabbd6892bfa6","title":"Neurosymbolic repair for low-code formula languages"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"4f4326e69a1a34f68f7b14556077571a1752c319","title":"Artificial Intelligence and Deep Learning for Rheumatologists"},{"paperId":"14a797972c3a7045c8449f20d524234ffc36bf24","title":"Mimetic Models: Ethical Implications of AI that Acts Like You"},{"paperId":"e9fc39f56abbc6b8aed1e05496d985e70345a95a","title":"An extensive study on pre-trained models for program understanding and generation"},{"paperId":"fc994f026a04e4cfc8e24ee68994836700166421","title":"COBE: A Natural Language Code Search Robustness Benchmark"},{"paperId":"2f2750b48a6f958ff12cba90e99695123d1e2f47","title":"Using pre-trained language models to resolve textual and semantic merge conflicts (experience paper)"},{"paperId":"e37155d21818513bd40d64ee212099aac82bd6f8","title":"Less training, more repairing please: revisiting automated program repair via zero-shot learning"},{"paperId":"5d142f8b1dc67ec6307050d34dbcd6dfd4c0218c","title":"Active Data Pattern Extraction Attacks on Generative Language Models"},{"paperId":"75cb3efeb69e044cc07613a3eba64504483e999d","title":"Combing for Credentials: Active Pattern Extraction from Smart Reply"},{"paperId":"b3a54332a0791751fcf234f6264f242c42ac00d2","title":"DocPrompting: Generating Code by Retrieving the Docs"},{"paperId":"b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs"},{"paperId":"f3cf71c51b882fe3111d71c4bf104297d38197f8","title":"Inner Monologue: Embodied Reasoning through Planning with Language Models"},{"paperId":"142ebbf4760145f591166bde2564ac70c001e927","title":"Language Models (Mostly) Know What They Know"},{"paperId":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models"},{"paperId":"cdf54c147434c83a4a380916b6c1279b0ca19fc2","title":"LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action"},{"paperId":"53661ff6fdbfb8557c5b19895fad151792c62da7","title":"Few-shot training LLMs for project-specific code-summarization"},{"paperId":"cec43785ac6ede33cff208b6b828dc440cf43b2b","title":"Big Learning: A Universal Machine Learning Paradigm?"},{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"10bf4c1ca1531a49dae14d1226e53095306506ff","title":"LaMPost: Design and Evaluation of an AI-assisted Email Writing Prototype for Adults with Dyslexia"},{"paperId":"b17cc18e4130505b939f7d527082eb6be2a7fd5b","title":"Rationale-Augmented Ensembles in Language Models"},{"paperId":"f220cd8740627a39d78ecae109f30dbceffff1ea","title":"Natural Language Interface for Data Visualization with Deep Learning Based Language Models"},{"paperId":"61dd84f069f0329b1f3a84059be925cf7391140d","title":"Machine Learning for Aggregate Computing: a Research Roadmap"},{"paperId":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"52ca0f589fdcde1c45fd6dfb1b72248d4ecaefc0","title":"Code Translation with Compiler Representations"},{"paperId":"114eb5a35a3cd802cd1f46fff35c284e32ef6c54","title":"GitHub Copilot AI pair programmer: Asset or Liability?"},{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"1d160123cbbef972ea151a641dd435d57c727de8","title":"AixBench: A Code Generation Benchmark Dataset"},{"paperId":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code"},{"paperId":"f8f2b17083c10f730b711a938e2bb5da992086e7","title":"AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models"},{"paperId":"374bbb716aa007a65ac03f0220d3027fa724874d","title":"AI Challenges for Society and Ethics"},{"paperId":"6a53eeada90d83b9508e7e451d62fdc9d2476350","title":"Using cognitive psychology to understand GPT-3"},{"paperId":"f2c17758e74707d379b87372528221656d14b697","title":"Taxonomy of Risks posed by Language Models"},{"paperId":"e98799e709dc93a8ea721dd6b3e1398104797050","title":"Cracking the code: Co-coding with AI in creative programming education"},{"paperId":"660fde2f51e025638b8c937bf228ecaa5c5b649c","title":"XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence"},{"paperId":"2edc8efcda27c944a46f367acf6a5280b8f65525","title":"FixEval: Execution-based Evaluation of Program Fixes for Competitive Programming Problems"},{"paperId":"0d7a97961517e1d1289e913e81a93746a81ba117","title":"FixEval: Execution-based Evaluation of Program Fixes for Programming Problems"},{"paperId":"45263786d07f5751f7494fdeee3c8764836d02c4","title":"NatGen: generative pre-training by “naturalizing” source code"},{"paperId":"64c36adaeb803ff1d49771ed6a7ae7271e17b35d","title":"Training Discrete Deep Generative Models via Gapped Straight-Through Estimator"},{"paperId":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation"},{"paperId":"3ba793e937cb90ea3e82b4a6903ee4a95f307ddf","title":"X-Risk Analysis for AI Research"},{"paperId":"41f7bcca48c321071cbba7f9e7d735f65698dcce","title":"A Dataset and Benchmark for Automatically Answering and Generating Machine Learning Final Exams"},{"paperId":"647d81055d281f038b89a684db8d9c011e2a9bc0","title":"STable: Table Generation Framework for Encoder-Decoder Models"},{"paperId":"a64871352f8ac7f8aa226fda5cce70251a18a4fc","title":"Assessing Project-Level Fine-Tuning of ML4SE Models"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"40edfa97cd02268fccff75eb9c693b11c1a968b2","title":"Formal Specifications from Natural Language"},{"paperId":"0d08ffccc982781e310bb184397bbe64b9aef157","title":"Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models"},{"paperId":"1100dee3fd78655cddc4b7bfaef1161351d4fab5","title":"Automated Feedback Generation for Competition-Level Code"},{"paperId":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code"},{"paperId":"8c7fd98b6bc4f32772e76471afe8babc323f10d3","title":"CitySpec: An Intelligent Assistant System for Requirement Specification in Smart Cities"},{"paperId":"f1b6b34b4440a77ba86493f7062e8974062508c5","title":"Applying genetic programming to PSB2: the next generation program synthesis benchmark suite"},{"paperId":"4f161f3cf6a272061600c71cc2e8a325753a38f0","title":"Attention Flows for General Transformers"},{"paperId":"35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs"},{"paperId":"196cc546041cb6db167784f632037f0a1dcf4a79","title":"Generating Natural Language Proofs with Verifier-Guided Search"},{"paperId":"0efa0441da820b1905572666ba1974a06a9663fb","title":"NaturalProver: Grounded Mathematical Proof Generation with Language Models"},{"paperId":"4a4581003f56e8cb581ad6f383c037964765d3d5","title":"Active Programming by Example with a Natural Language Prior"},{"paperId":"047a8344e3cfa49c8354fc244387d57ef9d2f01d","title":"Memorization in NLP Fine-tuning Methods"},{"paperId":"8b7c11e773dc6ec32560d247193c9dd4c5109644","title":"Análise de Performance dos Modelos Gerais de Aprendizado de Máquina Pré-Treinados: BERT vs DistilBERT"},{"paperId":"8c90bfe05c06fd47eaec0f5b1662e06862572afe","title":"Looking for a Handsome Carpenter! Debiasing GPT-3 Job Advertisements"},{"paperId":"adce1da47d490dcdca254ccd43055ed4f4423bc2","title":"Summarize and Generate to Back-translate: Unsupervised Translation of Programming Languages"},{"paperId":"415be47b17f5214a1710010c7c18f4fafd3ef524","title":"AdaptivePaste: Code Adaptation through Learning Semantics-aware Variable Usage Representations"},{"paperId":"ea4749c6ed5f08b16bab31b1ca4fd3fdc259ae8f","title":"Improving automatically generated code from Codex via Automated Program Repair"},{"paperId":"0bcd59da541fdae66884afba8d25475a54a9da1a","title":"Automated Repair of Programs from Large Language Models"},{"paperId":"cbcd19395e4b5ad5e047e0476cb906ca6461df72","title":"Few-Shot Natural Language Inference Generation with PDD: Prompt and Dynamic Demonstration"},{"paperId":"d6c5aab433d9871cabc01ffb1e5e1ea89141155b","title":"KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation"},{"paperId":"58aacb967cc7fc25cfc9d51b7ad3e57ac00d119b","title":"Can Foundation Models Wrangle Your Data?"},{"paperId":"227966c9b4fe75271946d239507196408842a2f2","title":"Transformer-based Program Synthesis for Low-Data Environments"},{"paperId":"2ff6f2b4a175cbc533795b723d6f18d64cea3916","title":"A CLIP-Hitchhiker's Guide to Long Video Retrieval"},{"paperId":"c61ce808818308566124df2c8725c98d6bd38dc3","title":"A Precis of Language Models are not Models of Language"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"5922f437512158970c417f4413bface021df5f78","title":"A Generalist Agent"},{"paperId":"11d77a7a56381d9700befa86f5c93cf887108ecd","title":"Editorial: Theory of Mind in Humans and in Machines"},{"paperId":"7ef9aafc68511afab5b287e62b754576ea37b4ce","title":"Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks"},{"paperId":"9a7d4c2f4309a2a38ea8e5b23c6d616fa0952d44","title":"Neural language models for network configuration: Opportunities and reality check"},{"paperId":"d072b46a0504ac023d5035d8ec0c7876151245c4","title":"Playing Games with Ais: The Limits of GPT-3 and Similar Large Language Models"},{"paperId":"cdfe9580f63070f311151444f9df32818cc858bf","title":"An Empirical Evaluation of GitHub Copilot's Code Suggestions"},{"paperId":"3f400fff93d5ff352ee102d2c04d38f6214b4283","title":"CodePanorama: a language agnostic tool for visual code inspection"},{"paperId":"1f3a97d255a6d0b2d5535af75737ed43b1e96fc4","title":"Is GitHub Copilot a Substitute for Human Pair-programming? An Empirical Study"},{"paperId":"a8c072f6cbd4653a1f0888b3529898f66c0e6bb0","title":"Strategies for Reuse and Sharing among Data Scientists in Software Teams"},{"paperId":"06ce4c0d92ed51a2b5e6e363a6d3932b8f162e72","title":"Can Information Behaviour Inform Machine Learning?"},{"paperId":"fc32083203ce124375af2a225296a26576306f6c","title":"Language & Coding Creativity"},{"paperId":"779fa63ae731a1efaf0cb95cab8e47672a919581","title":"I Do Not Think It Means What You Think It Means: Artificial Intelligence, Cognitive Work & Scale"},{"paperId":"e5cbcb665486ae7077dc3a37a9fd6addeabca5cf","title":"PROPR: Property-Based Automatic Program Repair"},{"paperId":"f8aa0cab09bc0668276e2cb5690bae47fa50d350","title":"An Initial Look at Self-Reprogramming Artificial Intelligence"},{"paperId":"4bc040835fbff57ce6612306d794b8c6c8226086","title":"Discovering the Syntax and Strategies of Natural Language Programming with Generative Language Models"},{"paperId":"1fafaccebc4a74898a74c606f846318c4c2c7536","title":"On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"6050454e0446a3068617f73b0301453f3f67844d","title":"Stylette: Styling the Web with Natural Language"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"1f87dc41bdf2c4c78e2dce9c5c8adfef5e25a70c","title":"Passport: Improving Automated Formal Verification Using Identifiers"},{"paperId":"2ba7104f7b93d77940312664f3467b8f090d6d16","title":"On Distribution Shift in Learning-based Bug Detectors"},{"paperId":"07cd498aacfb4d39fa2e0e8d8a9c8ad881257300","title":"Prompt Engineering for Text-Based Generative Art"},{"paperId":"1676160139ca59c6728472f34092db69460567a8","title":"A Taxonomy of Prompt Modifiers for Text-To-Image Generation"},{"paperId":"bb7e46f316d319f9819c3554c99995ef8361ae9c","title":"CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex"},{"paperId":"012378718c34f0b17b3fcd7316371f8f4e4fdde2","title":"Addressing Leakage in Self-Supervised Contextualized Code Retrieval"},{"paperId":"a6b9a934fe039a5636a26e94fb47f872263d702c","title":"To What Extent do Deep Learning-based Code Recommenders Generate Predictions by Cloning Code from the Training Set?"},{"paperId":"9fcf3424eee7a607813980e114f1d9d2d3657960","title":"GLAD: Neural Predicate Synthesis to Repair Omission Faults"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"89e78d6f76b70c30804ecd3592fa05fccdc49b64","title":"Fix Bugs with Transformer through a Neural-Symbolic Edit Grammar"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"0286b2736a114198b25fb5553c671c33aed5d477","title":"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"},{"paperId":"c18624382afc57072446b056bf590ee891c078e7","title":"Is GitHub's Copilot as Bad As Humans at Introducing Vulnerabilities in Code?"},{"paperId":"6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis"},{"paperId":"a225d5d846ba5110232ed5bb32d54ea742b1c2d4","title":"KNN-Diffusion: Image Generation via Large-Scale Retrieval"},{"paperId":"e2baf81813d5a515f554ee60bcddcc57548f8c22","title":"Code Search: A Survey of Techniques for Finding Code"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"ada81a4de88a6ce474df2e2446ad11fea480616e","title":"Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language"},{"paperId":"d358ce242dacfd2ea738aa538779f06c79777f2b","title":"SELFIES and the future of molecular string representations"},{"paperId":"237f5ca6fcccef2b77a2212b34fb06a1dbd09b72","title":"Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting"},{"paperId":"af0899b397d5cf5974e939e5c1057abd7c54cdde","title":"MQDD: Pre-training of Multimodal Question Duplicity Detection for Software Engineering Domain"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"1a903282f7c19dbdb2714b852fb42dbb4675422b","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"365d0313908871118df59c469c4db10f4e1b96bd","title":"Linearizing Transformer with Key-Value Memory"},{"paperId":"fb5c11bbf63884f75d2da615fbf37a3bcfa2bd20","title":"Wordcraft: Story Writing With Large Language Models"},{"paperId":"79b88230fabb59a1d368641bbc822af0f09bf262","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"a522543180db6c5b21f47fe88abee44de158c85c","title":"Automatic Programming and Education"},{"paperId":"c347093e2dca530ce347526380b0b7aedf03a6b2","title":"CrossBeam: Learning to Search in Bottom-Up Program Synthesis"},{"paperId":"1d10023541f06701bf2f9ae8c91609e1055799ec","title":"Automating code review activities by large-scale pre-training"},{"paperId":"b951c0691a0d2ca65202a1eed2ccedf6e305d035","title":"CodeReviewer: Pre-Training for Automating Code Review Activities"},{"paperId":"590f6817b42407f96b079e82c935fae298196359","title":"Less is More: Summary of Long Instructions is Better for Program Synthesis"},{"paperId":"c96363c42bc8c465902c22b8c33c8704233f519e","title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages"},{"paperId":"b645e706651391eca1f692e7f560051c21b3dea4","title":"In-Context Learning for Few-Shot Dialogue State Tracking"},{"paperId":"51000d9f79be0eefd7972fe94e3c71dddc90d2c6","title":"Evaluating the Text-to-SQL Capabilities of Large Language Models"},{"paperId":"2b556fcf2ac634f03c1fb0ace5e602e829418e65","title":"ReACC: A Retrieval-Augmented Code Completion Framework"},{"paperId":"5dd7bc394e032eb0e982699a5f0c781fab9e3111","title":"Contrastive Visual Semantic Pretraining Magnifies the Semantics of Natural Language Representations"},{"paperId":"463c6944ddcb43280e1c9765c2245bc7b764ab68","title":"AI-Driven Development Is Here: Should You Worry?"},{"paperId":"1f11601f9eb8c0bee1d1d2cc7f4fa187e8c5e525","title":"The impact of lexical and grammatical processing on generating code from natural language"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"75b8444e1d82ebb6c6a32d8ea86af858e5de93e3","title":"Iterative Genetic Improvement: Scaling Stochastic Program Synthesis"},{"paperId":"d26fe2a7a7cc940d8485488e97460b144dc7d69e","title":"From Natural Language to Simulations: Applying GPT-3 Codex to Automate Simulation Modeling of Logistics Systems"},{"paperId":"76f023c3a819fc58989a064a1b50825b11fce95d","title":"Capturing Failures of Large Language Models via Human Cognitive Biases"},{"paperId":"4a6a65968a8eb8c09ffb57a7774ddabb596565b1","title":"COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics"},{"paperId":"7b5aa186ca8abc585607c5ec91562e127a398601","title":"Open-Ended Knowledge Tracing"},{"paperId":"b96cb62450cb77926c712d5cd50d1764101de605","title":"On the Implicit Bias Towards Minimal Depth of Deep Neural Networks"},{"paperId":"44d6c201a4056e260e9844bdbb01461ea9b1a011","title":"A data-driven approach for learning to control computers"},{"paperId":"6aae3ddbe142f7ae28f0f18bb6248dc7b3f41c00","title":"Probing Pretrained Models of Source Code"},{"paperId":"9cbc044e315cdefe9a255119037ac7c23e9abdd5","title":"Predictability and Surprise in Large Generative Models"},{"paperId":"adea6aa83b847752940129185428ea61935a0027","title":"Quantifying Memorization Across Neural Language Models"},{"paperId":"979eb5c97c49d7979447ed684500895a24d75ac4","title":"The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming"},{"paperId":"62d17b6f6ad77fd71ef9954c7784700d5e316f1f","title":"What Does it Mean for a Language Model to Preserve Privacy?"},{"paperId":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective"},{"paperId":"76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","title":"Investigating Explainability of Generative AI for Code through Scenario-based Design"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"5d49c7401c5f2337c4cc88d243ae39ed659afe64","title":"Red Teaming Language Models with Language Models"},{"paperId":"9ac5e1ffa8f837671a89354d4e298b1adeb08d79","title":"Enabling Automatic Repair of Source Code Vulnerabilities Using Data-Driven Methods"},{"paperId":"763792c655e591c5d61f67d7ac9cbecbcb5f4508","title":"Pop Quiz! Can a Large Language Model Help With Reverse Engineering?"},{"paperId":"0109c662c101723aea99e561937e3aca58563537","title":"Exploring Transformer Backbones for Heterogeneous Treatment Effect Estimation"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"ab18a9168b93def3145e118b6686e84241b39f42","title":"DeepRNG: Towards Deep Reinforcement Learning-Assisted Generative Testing of Software"},{"paperId":"5d0db797a45ce2453f821f7ded0b547d3fdab054","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"7428f9b16a82839e2cb6e6c7a77c1ffeab898813","title":"HEAT: Hyperedge Attention Networks"},{"paperId":"03eeff98d24383518ce0dacc0b3c4a38b6f1a514","title":"Recursive Decoding: A Situated Cognition Approach to Compositional Generation in Grounded Language Understanding"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"6d7d4fca9840504f630e9bea6acaa07322a6e889","title":"Text and Code Embeddings by Contrastive Pre-Training"},{"paperId":"3d5463a16d85d9d1d0d8ebb4117a31aca3c240ff","title":"Fooling MOSS Detection with Pretrained Language Models"},{"paperId":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"},{"paperId":"2b5d234efd26e7377698cf16c901601a3d3c4e56","title":"CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities"},{"paperId":"53c0abe83fe9b4fdaf2208295d8504fcf5241694","title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models"},{"paperId":"49c3f85573a3204c5e66317289e4cecfed50f38a","title":"Assemble Foundation Models for Automatic Code Summarization"},{"paperId":"7a54c01824a725a03f99411c8646d0a5674b2777","title":"Predictive synthesis of API-centric code"},{"paperId":"c0a98fca99e5f2d34330a15ab8ad4b9d692146d0","title":"The growing cost of deep learning for source code"},{"paperId":"f9838a3be5c94bb2674a0e224de349b50e18f3c4","title":"Learning To Retrieve Prompts for In-Context Learning"},{"paperId":"6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a","title":"Few-Shot Semantic Parsing with Language Models Trained on Code"},{"paperId":"52db8674337e5d86dcb96d013734befc8c3d4581","title":"Large Language Models are not Models of Natural Language: they are Corpus Models."},{"paperId":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis"},{"paperId":"ecb5a6fe2f5261e4e717ece1e82c464c63cb4862","title":"Controlling Conditional Language Models without Catastrophic Forgetting"},{"paperId":"8417d83e74a63b9d3874cd7695477ebad8b98f8a","title":"Choose your programming copilot: a comparison of the program synthesis performance of github copilot and genetic programming"},{"paperId":"231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models"},{"paperId":"1cbb3d96242c3f47c3f40aada33616d0f5c07737","title":"Inductive Biases and Variable Creation in Self-Attention Mechanisms"},{"paperId":"a421ba0a9150cd35e231dddc323bdd9a59b3af93","title":"Coherence boosting: When your pretrained language model is not paying enough attention"},{"paperId":"1aed58bd07026492194672adec494dc37c894a28","title":"Leveraging Automated Unit Tests for Unsupervised Code Translation"},{"paperId":"c6bb04f3d8000b7e800f6359082de39548c7da79","title":"Capturing Structural Locality in Non-parametric Language Models"},{"paperId":"2672777d25562c9df6fc13b653181db62d39bece","title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"},{"paperId":"77d956cdab4508d569ae5741549b78e715fd0749","title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"a1e1297fb132d7769dda3f7917e57757e6e22605","title":"Impact of Evaluation Methodologies on Code Summarization"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions"},{"paperId":"8cf3a454556060d6e9aa86dbabf221bd10bf9759","title":"On the Effectiveness of Transfer Learning for Code Search"},{"paperId":"a63535ebbf90d0c51408252c23b85ffaf87f09ae","title":"Towards an AI Assistant for Power Grid Operators"},{"paperId":"75e36bb95023e55f7dec95d1af557e219ba3d349","title":"CORAL: COde RepresentAtion learning with weakly-supervised transformers for analyzing data analysis"},{"paperId":"239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning"},{"paperId":"56e6d62c638a24411f12d15cdc8821a31fc495c8","title":"Source Code Generation from Descriptions in a Natural Language"},{"paperId":"78863000eb70945cc8d791d45d4a3fe8a6521cb6","title":"Open-Ended Knowledge Tracing for Computer Science Education"},{"paperId":"ba5d21b7c65c6598c7bd39a5d992308c205df374","title":"A S YSTEMATIC E VALUATION OF L ARGE L ANGUAGE M ODELS OF C ODE"},{"paperId":"b2c4fdb49bdb23e6ad0ac3272029324157046ea7","title":"Automated Repair of Code from Language Models"},{"paperId":"25c402db512d327f1da143de3b8e797ad6fbfe5b","title":"P ROG P ROMPT : Generating Situated Robot Task Plans using Large Language Models"},{"paperId":"660ca9e15e19409903a0605f0584d0f263c35c67","title":"S YNCHROMESH : R ELIABLE C ODE G ENERATION FROM P RE - TRAINED L ANGUAGE M ODELS"},{"paperId":"4797e960f8e7a2b47ae0d95c7071ef84fa5d4b5b","title":"Code Generation Using Machine Learning: A Systematic Review"},{"paperId":"15ef2d1b88f54fa32a32927463a7116219b89529","title":"SUPEROPTIMIZE REAL-WORLD PROGRAMS"},{"paperId":"a2b6e1f7d8a7963d321f29fca7c01eeb1ebd7f0f","title":"P ATCH G ENERATION WITH L ANGUAGE M ODELS : F EASIBILITY AND S CALING B EHAVIOR"},{"paperId":"8a4299a61cc44b60e5be32fef35341c3bc7b2a0d","title":"The Hole Story: Type-Directed Synthesis and Repair"},{"paperId":"60043104ca33a1fc905af57ead32768e52c69103","title":"C3PO: A Lightweight Copying Mechanism for Translating Pseudocode to Code"},{"paperId":"ae10c4b220a0bc0999bf169d5c219086d1f1aeed","title":"Edinburgh Research Explorer Taxonomy of risks posed by language models"},{"paperId":"a85c5d7272371345e28a9910080224cad799972e","title":"Schema Matching using Pre-Trained Language Models"},{"paperId":"f9d38e03c97562b5f5942f3a0c43bdb751b9dc1c","title":"Wordplay 2022 The 3rd Wordplay: When Language Meets Games Workshop Proceedings of the Workshop"},{"paperId":"b98d6fe8f0ef02ec0d1bb2bcfb924c8f01feb7d4","title":"Convergent Representations of Computer Programs in Human and Artificial Neural Networks"},{"paperId":"e002bb8dae5a18a5ea1e7e1aafa16e19ad545662","title":"Solving Math Word Problems with Process-based and Outcome-based Feedback"},{"paperId":"0180d35b85dd4daead90e0652b64b1339e754684","title":"Assistance with large language models"},{"paperId":"cd155729180ea707dea251f8e9654db241ffd808","title":"Is GPT-3 all you need for machine learning for chemistry?"},{"paperId":"a1ef81e17a9ca41e09aba802040a2eca2744716f","title":"Generation Probabilities are Not Enough: Improving Error Highlighting for AI Code Suggestions"},{"paperId":"15e767aa26a14455da95a2b2f11e3d59f2c250f6","title":"Autoformalization for Neural Theorem Proving"},{"paperId":"2443179d421e1faf7474add557b45add554723c7","title":"Formal Premise Selection With Language Models"},{"paperId":"ddab94478a7647ee136b1f6b5076417db3074d0f","title":"Machine Programming: Turning Data into Programmer Productivity"},{"paperId":"9bf75110ea0923bbed49256b5491f1ec284019ec","title":"From BERT to GPT-3 Codex: Harnessing the Potential of Very Large Language Models for Data Management"},{"paperId":"91260f73dd179487fb16713deb8267634ae14716","title":"CodexDB: Synthesizing Code for Query Processing from Natural Language Instructions using GPT-3 Codex"},{"paperId":"3b0cf543a730e674d4213d344ebc857fada76ead","title":"Understanding High-Level Properties of Low-Level Programs Through Transformers"},{"paperId":"09e14c4c80e20e80c052e0adb0d49df51aff718d","title":"Yes, You Can Make an App Too: A Systematic Study of Prompt Engineering in the Automatic Generation of Mobile Applications from User Queries"},{"paperId":"b562be15b076b494023b8ac24fc8c459f4fdf80a","title":"Craft an Iron Sword: Dynamically Generating Interactive Game Characters by Prompting Large Language Models Tuned on Code"},{"paperId":null,"title":"Materials ExerciseTeacher Student Attempt Feedback"},{"paperId":"24c6982a25c0114bc98805d368b06d1a4f6d8fd5","title":"Understanding AI alignment research: A Systematic Analysis"},{"paperId":"f01e316d3b28ccecda25b4d57926f496a9b17d3d","title":"How Robust are Neural Code Completion Models to Source Code Transformation?"},{"paperId":"6f84d0cc33c7c58f74b28ddcc1cbda91ea608c9f","title":"C ODE S UMMARIZATION : D O T RANSFORMERS R E ALLY U C"},{"paperId":"2e5b29457ff45b8faba69bc2eaf05521584a7bec","title":"B UG F IX G ENERATION USING G RAPH T RANS"},{"paperId":"57c31c709792949bfbb9d4aaee941048aa07cc4b","title":"How to Give Imperfect Automated Guidance to Learners: A Case-Study in Workplace Learning"},{"paperId":"bee0be592c314435048599281bcd9c72bf63b735","title":"CueBot: Cue-Controlled Response Generation for Assistive Interaction Usages"},{"paperId":"8a4dce5735a101ff8f64c2b676afb8c24950a5d8","title":"Zero-shot Mathematical Problem Solving via Generative Pre-trained Transformers"},{"paperId":"75c8f2916a2067f7549cf58ea8c9061565eb1dab","title":"C ROSS B EAM : L EARNING TO S EARCH IN B OTTOM -U P P ROGRAM S YNTHESIS"},{"paperId":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"DECOMPOSITION IN NEURAL PROGRAM SYNTHESIS"},{"paperId":"a3564f3cf954c05844c757505325a50b4d858e22","title":"Generating Infrastructure-as-Code from Natural Language: Evaluating Fine-Tuned GPT-3 Models for Cloud Infrastructure Provisioning"},{"paperId":"78fd8185c5cd55830c31aa718a9909827e20774e","title":"A Research Agenda for Assessing the Economic Impacts of Code Generation Models"},{"paperId":"856d2c0f7b3f80dcf1f68d1dc0dcbf5c6fe5679a","title":"Interpreting docstrings without using common sense: the private science of very large language models∗"},{"paperId":"1b94afca9d6688cc584a744734126473283cbc93","title":"Can Transformers be Strong Treatment Effect Estimators?"},{"paperId":"58dd9a3da16c10f5c3cdbb9760a9ff378847bf76","title":"Self-supervision of wearable sensors time-series data for influenza detection"},{"paperId":"6ccc0ca964ddab19705e4832758e6a2447325348","title":"End to End Software Engineering Research"},{"paperId":"091fa84bdc07dcb22a34060c3996d8c58d71cd20","title":"Towards Neural Functional Program Evaluation"},{"paperId":"ee042a3e299a32c413532e64603de8d3ddb6aa87","title":"Automap: Towards Ergonomic Automated Parallelism for ML Models"},{"paperId":"827a67bbb96c8ed34c0f79e2ea811c5b53a6896b","title":"Controllable Response Generation for Assistive Use-cases"},{"paperId":"73e6f5a7e2760b3f9ab077a450e445e12bfd7061","title":"Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?"},{"paperId":"a5731122200fbb8b37f048010a1e1ca4474aa606","title":"Examining Zero-Shot Vulnerability Repair with Large Language Models"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"21ab011a3adccbd912aea58f76b84b7873c41df3","title":"Machines&Influence: An Information Systems Lens"},{"paperId":"cecc913290736a5a368642c5b59a130eddd1fa7b","title":"Can Pre-trained Language Models be Used to Resolve Textual and Semantic Merge Conflicts?"},{"paperId":"04db9b694280134f09af5fa787a306907edba29d","title":"How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN"},{"paperId":"f7987fa2aadc0b368c185dc4d2fdb1337a202c32","title":"Solving Probability and Statistics Problems by Program Synthesis"},{"paperId":"4e40595d6ecba027cebb4f2e3b43ae44bcf51daf","title":"Solving Linear Algebra by Program Synthesis"},{"paperId":"1444536496d8064f33e10b38b5820fecfab5b367","title":"Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs"},{"paperId":"c23d9d44e8bc68408cea9f305d1f24d915bc0d0d","title":"Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey"},{"paperId":"9f260bdd4030af5297a9c1cbb817c75701ac8c83","title":"The 5th Recognizing Families in the Wild Data Challenge: Predicting Kinship from Faces"},{"paperId":"570a6a5b8ec2827c3f33bb1b1bd027190a0d3e07","title":"Neural Program Generation Modulo Static Analysis"},{"paperId":"6a269b1abccdbf57e79b3f115a97bff14b435ad9","title":"Automated Support for Unit Test Generation: A Tutorial Book Chapter"},{"paperId":"21e8e76386aaaa00e0971af70ce84a8a544e1aa1","title":"Cascaded Fast and Slow Models for Efficient Semantic Code Search"},{"paperId":"dace03e57056d736f9e24937bdf486e894f8e866","title":"Is neural machine translation approach accurate enough for coding assistance?"},{"paperId":"8091e51ebbcd2424a1c5b50c036bae5295090525","title":"Top 3 in FG 2021 Families In the Wild Kinship Verification Challenge"},{"paperId":"21bc4ead8ea415579ab40e437fcbc274929f17c8","title":"Solving the Families In the Wild Kinship Verification Challenge by Program Synthesis"},{"paperId":"9991bb2eb7e7d7e9d831e257ae77ba2eaeaba3dc","title":"Applying quantum approximate optimization to the heterogeneous vehicle routing problem"},{"paperId":"360e0197378799d890f473893cc0c773b8182b4e","title":"Searching for Replacement Classes"},{"paperId":"24e775b20adf21e9b5b95c6a9b7a5c164d055849","title":"M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining"},{"paperId":"6c2d43e71e240e354b5790a38da78a291ceffe7c","title":"Learning to Superoptimize Real-world Programs"},{"paperId":"05c2e1ee203be217f100d2da05bdcc52004f00b6","title":"Unsolved Problems in ML Safety"},{"paperId":"bc9598dc4ed0472d8b59b87ed3a139f8347d40ee","title":"Towards A Measure Of General Machine Intelligence"},{"paperId":"b8b21c2ddcd7d1c23d7ccfceabb63cb1a05bcfca","title":"HYDRA - Hyper Dependency Representation Attentions"},{"paperId":"a8863de15a5ee8eed98107f423138a1a8f5a2ba8","title":"Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a176b0de62840f7118006277d94bbc1547162a4d","title":"Learning to Synthesize Programs as Interpretable and Generalizable Policies"},{"paperId":"70087677fd1a6309829b42968934575d05a95f92","title":"What do pre-trained code models know about code?"},{"paperId":"3f97c2067cde9377e50b3160bbd7982c94abd88a","title":"An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions"},{"paperId":"4885e616e85d420576196b2578525cbc501137ec","title":"Programming and execution models for next generation code intelligence systems (keynote)"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"5436193122dff271796bca07df7cecb7a8d6dea6","title":"Natural language-guided programming"},{"paperId":"26450917d41c828b470ec8818d49f59516a5b9c0","title":"Towards Universality in Multilingual Text Rewriting"},{"paperId":"58a6ca2ae28a618126f71a07262cb958a8c37904","title":"Latent Execution for Neural Program Synthesis"},{"paperId":"98485ce6532d69f34a8ec67de6b09a39532bd221","title":"Communicating Natural Programs to Humans and Machines"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"09279dc8018a8131e11d527cebb06d0a43c67cff","title":"Creativity and Machine Learning: A Survey"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"4f278ab5ad629267e06196e273252262854c1c57","title":"BF++: a language for general-purpose program synthesis"},{"paperId":"021bbcefc993c389bad6c1daefd8ff92d0fc2441","title":"Contrastive Code Representation Learning"},{"paperId":"4da830b6d84e117cb147ff71f205e71500ebbbb1","title":"Machines and Influence"},{"paperId":"007153d786caa906255fba2ca265fd67994f8b44","title":"Tracking Blobs in the Turbulent Edge Plasma of Tokamak Fusion Reactors"},{"paperId":"a3c2b81d8bb5ac69771ed7830d009310d98ac9dc","title":"A First Approach to AGI-based Robot Task Planning"},{"paperId":"d66e80224cda0c1d5a4c1be3798df6a6bfe3713c","title":"GPT-3 for Few-Shot Dialogue State Tracking"},{"paperId":"b874faa9c6cfb5d7e87e3d79650007ade1394958","title":"Creating new Program Proofs by Combining Abductive and Deductive Reasoning"},{"paperId":"bab6893ee48d168d27c227c3b0867f6d471fbea8","title":"Language Models are not Models of Language"},{"paperId":"4ff14a0580f3b36a9564ec18a51d2ce1d4eafebc","title":"Learning Methods for Solving Astronomy Course Problems"},{"paperId":"8a9e437b2e2d813b402ac560c852ef0ab2f1cd3c","title":"Recognizing Families In the Wild (RFIW): The 4th Edition"}],"references":[{"paperId":"113fba4c88e2eb74f49544a161ef70e9745e969a","title":"Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses"},{"paperId":"fc963363bcddfc1aeca07d44d7a9d0e53485662d","title":"Atlas of AI: power, politics, and the planetary costs of artificial intelligence."},{"paperId":"dab46dd3985d1de5cd6549319797ab3705b6a801","title":"BEiT: BERT Pre-Training of Image Transformers"},{"paperId":"06510ee15cabae97647ceb647dce6f5a1820d524","title":"Women’s Participation in Open Source Software: A Survey of the Literature"},{"paperId":"4a160efbe80c38cd5eb2f92c7c095b49b113397d","title":"In-IDE Code Generation from Natural Language: Promise and Challenges"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"79b8ef3905a42b771248719495a2117271906445","title":"Carbon Emissions and Large Neural Network Training"},{"paperId":"3a863672b072a37ac8d2eb6b780234f6b748a2d9","title":"Generating bug-fixes using pretrained transformers"},{"paperId":"49f905eb03958c7cfae52ac759ea8978b8b2a6ea","title":"Alignment of Language Agents"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","title":"Zero-Shot Text-to-Image Generation"},{"paperId":"56fa0b9cba4d9aee5ccc327365b3b3a721031c69","title":"Calibrate Before Use: Improving Few-Shot Performance of Language Models"},{"paperId":"4c2733d191e347753bb28afa46a1c55c65e085be","title":"Persistent Anti-Muslim Bias in Large Language Models"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"021bbcefc993c389bad6c1daefd8ff92d0fc2441","title":"Contrastive Code Representation Learning"},{"paperId":"1ec69f1a1a9d4ff5c5bc70db0e5087157b620570","title":"You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"18a93dc1558bf9d7534d0b416633cebaf75c1145","title":"Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/ kingoflolz/mesh-transformer-jax"},{"paperId":null,"title":"A first look at rote learning in github copilot suggestions., Jun 2021. URL https://docs.github.com/en/ github/copilot/research-recitation"},{"paperId":"72730829313a9a907a875da6e8ab66af92ce62b2","title":"Multimodal Neural Script Knowledge Models"},{"paperId":null,"title":"Learning autocompletion from realworld datasets"},{"paperId":null,"title":"Supplemental Bias Analysis Generative models have been shown to encode bias in modalities such as natural language (Brown et al., 2020; Blodgett et al., 2020) and images (Radford et al., 2021)"},{"paperId":null,"title":"15-1252.00 -software developers"},{"paperId":null,"title":"Fun and dystopia with ai-based code generation using gpt-j-6b, June 2021"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"15a60130ea9cb4a00e0dac6d11a368894974cedf","title":"Unit Test Case Generation with Transformers and Focal Context"},{"paperId":"053b1d7b97eb2c91fc3921d589c160b0923c70b1","title":"Learning to summarize from human feedback"},{"paperId":"bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8","title":"Generative Pretraining From Pixels"},{"paperId":"49a049dc85e2380dde80501a984878341dd8efdf","title":"wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"d47a682723f710395454687319bb55635e653105","title":"Language (Technology) is Power: A Critical Survey of “Bias” in NLP"},{"paperId":"737189319865d7a363e38d957a8845c519953556","title":"SourceFinder: Finding Malware Source-Code from Publicly Available Repositories"},{"paperId":"4c7183fb2109271405e4a0fe23b5e827520a9f68","title":"Backstabber’s Knife Collection: A Review of Open Source Software Supply Chain Attacks"},{"paperId":"67dea28495cab71703993d0d52ca4733b9a66077","title":"Jukebox: A Generative Model for Music"},{"paperId":"70082b428b794d1d5e47c04ed4c3167a75aed22f","title":"Recalibrating global data center energy-use estimates"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"be9aa6c68d4df56a55444b15a83ff1e62c869bb5","title":"Robots and Jobs: Evidence from US Labor Markets"},{"paperId":null,"title":"Language models are few-shot"},{"paperId":null,"title":"formers is to fine-tune large pre-trained models with curated or human-generated datasets of the desired behavior (e.g., Raffel et al"},{"paperId":null,"title":"Working in public: the making and maintenance of open source software"},{"paperId":null,"title":"2020) for an analysis of conventional language models"},{"paperId":"5ca5c4c46c7729270746f885ff43953cb407fc6d","title":"What distinguishes great software engineers?"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"e8955a546425ef14a66da848f0bd174a33179373","title":"Automatic programming: The open issue?"},{"paperId":"75acc731bdd2b626edc74672a30da3bc51010ae8","title":"CTRL: A Conditional Transformer Language Model for Controllable Generation"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"433fcb584902e716ee25a44175e380267616b54e","title":"Learning Compositional Neural Programs with Recursive Tree Search and Planning"},{"paperId":"21da617a0f79aabf94272107184606cefe90ab75","title":"Generating Long Sequences with Sparse Transformers"},{"paperId":"79af328616d2440c77449d038f72d053c64d8f1f","title":"Unified rational protein engineering with sequence-based deep representation learning"},{"paperId":"e1799998d9e0932617aee2949167b040ebc8bec7","title":"The Wrong Kind of Ai? Artificial Intelligence and the Future of Labor Demand"},{"paperId":"516497e7fc39ebb126bed28dcc5c6129e500acb8","title":"An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation"},{"paperId":"7d3ab2a839b077a318022f7842225db55033b2c3","title":"Generating High Fidelity Images with Subscale Pixel Networks and Multidimensional Upscaling"},{"paperId":"98d1307bed619b58b4a44acd8e65ac58495776c2","title":"code2seq: Generating Sequences from Structured Representations of Code"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"Improving the standard risk matrix: Part 1"},{"paperId":null,"title":"Improving the standard risk matrix: Part 1. 2019"},{"paperId":null,"title":"Search-based pseudocode to code"},{"paperId":null,"title":"Comment regarding request for comments on intellectual property protection for artificial intelligence innovation. Before the United States Patent and Trademark Office Department of Commerce"},{"paperId":null,"title":"A common approach to adjusting the behavior of Transformers is to fine-tune large pre-trained models with curated or human-generated datasets of the desired behavior (e.g., Raffel et al"},{"paperId":null,"title":"Comment regarding request for comments on intellectual property protection for artificial intelligence innovation"},{"paperId":"b227f3e4c0dc96e5ac5426b85485a70f2175a205","title":"Representation Learning with Contrastive Predictive Coding"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"bec0b38f0083bd351d25daf8b31e6e049c376f94","title":"Improving Neural Program Synthesis with Inferred Execution Traces"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":null,"title":"Open-sourcing gvisor, a sandboxed container runtime"},{"paperId":null,"title":"Open-sourcing gvisor, a sandboxed container runtime, 2018"},{"paperId":null,"title":"Clarifying ”ai alignment"},{"paperId":null,"title":"Protecting applications with automated software diversity, Sep 2018"},{"paperId":"75f06defa60e069b863853afc6ac6f5feaca9450","title":"On the difficulty of benchmarking inductive program synthesis methods"},{"paperId":"3807a517403cfc7ed67fdfea64baf62b9948bcfe","title":"A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"3ff0af64279929a952ee340e645256b7e0580f65","title":"RobustFill: Neural Program Learning under Noisy I/O"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":null,"title":"A6:2017-security misconfiguration"},{"paperId":null,"title":"A syntactic neural model for generalpurpose code generation"},{"paperId":null,"title":"The trouble with bias"},{"paperId":"784ee73d5363c711118f784428d1ab89f019daa5","title":"Hybrid computing using a neural network with dynamic external memory"},{"paperId":"df0402517a7338ae28bc54acaac400de6b456a46","title":"WaveNet: A Generative Model for Raw Audio"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"41f1d50c85d3180476c4c7b3eea121278b0d8474","title":"Pixel Recurrent Neural Networks"},{"paperId":"b59d91e0699d4e1896a15bae13fd180bdaf77ea5","title":"Neural Programmer-Interpreters"},{"paperId":"4aa9f5150b46320f534de4747a2dd0cd7f3fe292","title":"Semi-supervised Sequence Learning"},{"paperId":"23f9a5ec68f4e947d168e2a8dfc99771ab3e81f2","title":"General Program Synthesis Benchmark Suite"},{"paperId":"193136b86539bd6df3f57a3685629c049a037418","title":"An analysis of patch plausibility and correctness for generate-and-validate patch generation systems"},{"paperId":"ac39029550eda5efbc6df754d74ec0e378eea4a8","title":"Representation Learning"},{"paperId":"0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a","title":"Learning to Execute"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"215999a0e155a21255e4655d4eac312858058a84","title":"Structured Generative Models of Natural Source Code"},{"paperId":"d1e8e4e952d6d9b49dd4c5d4fed9dedb22ed9946","title":"Temporal Logics for Hyperproperties"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"304f7fd4af9cc82355eaf912a8f120f7429362de","title":"Spreadsheet data manipulation using examples"},{"paperId":"546d3b90ef0e4ab5f4a61dcda59184bc951672ec","title":"A systematic study of automated program repair: Fixing 55 out of 105 bugs for $8 each"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"56dd0149688e9d196ddc4c833f864e6ee85e81ac","title":"The Economics of Software Quality"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"dd892c577eb45f3237c1e4d2513b1674cbc2043c","title":"BugFix: A learning-based tool to assist developers in fixing bugs"},{"paperId":null,"title":"Cwe-780: Use of rsa algorithm without oaep"},{"paperId":null,"title":"Use of a broken or risky cryptographic algorithm"},{"paperId":"bc876b7d314af5507fb52ffdb9279c83bae20035","title":"The technology trap"},{"paperId":"92b286e00a26f059c278a66d00e087c01306648f","title":"The economic impacts of inadequate infrastructure for software testing"},{"paperId":"3da678cf38575a81e0634d564919da3865b12fc6","title":"USENIX Association"},{"paperId":null,"title":"Online; accessed 29-June-2000"},{"paperId":"0361f0a1b710d4101d190b56c85e1d07cd96bae5","title":"Genetic Programming III - Darwinian Invention and Problem Solving"},{"paperId":"19b3b95f3dc0a3bf1b22e5f08df918bcc0079486","title":"Application of Dynamic Slicing in Program Debugging"},{"paperId":"83721103a6fd5535e943b1b575cf70862c2322a8","title":"Handbook of Applied Cryptography"},{"paperId":"028af662ebf7203017714e2d257faed94a566961","title":"Fault localization using execution slices and dataflow tests"},{"paperId":"94d9cbfc474cb6ce961de45a06233b2853ca6724","title":"Toward automatic program synthesis"},{"paperId":"eb8d64c0f34610597cc02d31ec10d5d6bcd6d39c","title":"Experiments with a Heuristic Compiler"},{"paperId":null,"title":"Fun and dystopia with ai-based code generation using gpt-j-6b"}],"id":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","summary":"It is found that repeated sampling from the GPT language model is a surprisingly effective strategy for producing working solutions to difﬁcult prompts, and the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics are discussed."},{"url":"https://www.semanticscholar.org/paper/b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code","venue":"MAPS@PLDI","year":2022,"referenceCount":37,"citationCount":47,"influentialCitationCount":8,"publicationDate":"26/02/2022","authors":"Frank F. Xu,Uri Alon,Graham Neubig,V. Hellendoorn","citations":[{"paperId":"907a77639069bb7dd270f017068745706133cffc","title":"Inaccessible Neural Language Models Could Reinvigorate Linguistic Nativism"},{"paperId":"a3e355b5de868f34fdfa2500415c5f74c69d2091","title":"Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models"},{"paperId":"b2f256a1ec54325102119a57cb2359840fe68586","title":"SantaCoder: don't reach for the stars!"},{"paperId":"69e330037afd18e5546fdcacbc9a9f7deb69bba9","title":"Memorization and Generalization in Neural Code Intelligence Models"},{"paperId":"ab8a53be2ff8a756776aa0245ad8da41189d60d6","title":"Fuzzing Deep-Learning Libraries via Large Language Models"},{"paperId":"433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context"},{"paperId":"bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence"},{"paperId":"8210cef990b8e5cddbc95000e46309bdd25337f7","title":"Asking Clarification Questions for Code Generation in General-Purpose Programming Language"},{"paperId":"6756fcd998caeb7b23702e08559e63710179334c","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"6bc87e51018d6de55011e95a0d43c588dd44a1e8","title":"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages"},{"paperId":"32b58766a1bfcef7ebba07070a272687aa518206","title":"Explicit Knowledge Transfer for Weakly-Supervised Code Generation"},{"paperId":"05f09050118d82100d04e56ba8b54836753fa9b4","title":"Programming by Example and Text-to-Code Translation for Conversational Code Generation"},{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","title":"Evaluating How Fine-tuning on Bimodal Data Effects Code Generation"},{"paperId":"cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers"},{"paperId":"41f5e1ad7793593befc0b9c38f756836e8b07c98","title":"Code4ML: a Large-scale Dataset of annotated Machine Learning Code"},{"paperId":"20e4ca6368a0731d3cbbd2e68aa90cef7384dc72","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"39e40821b7207125e54e6ed7112e55cd38c6f0c3","title":"Language Models of Code are Few-Shot Commonsense Learners"},{"paperId":"2cfd0dacfa267a64a23392332c358d4e3ec6fbd4","title":"The COVID That Wasn’t: Counterfactual Journalism Using GPT"},{"paperId":"0c78a473e33a81246d5c0fbbda7e7de168814c18","title":"FlexType: A Plug-and-Play Framework for Type Inference Models"},{"paperId":"97e1903b4afc811a9b5fa9e55723e80ba48bf46f","title":"Code as Policies: Language Model Programs for Embodied Control"},{"paperId":"e5993b3afe6384b5e6f90093989773ad1f868f71","title":"Towards Top-Down Deep Code Generation in Limited Scopes"},{"paperId":"6032212d5790b6a580d68d469a9895aad6238c89","title":"Diverse Title Generation for Stack Overflow Posts with Multiple Sampling Enhanced Transformer"},{"paperId":"05ccfb058dc6788254742722278b0af2fc87f083","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?"},{"paperId":"1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs"},{"paperId":"b3a54332a0791751fcf234f6264f242c42ac00d2","title":"DocPrompting: Generating Code by Retrieving the Docs"},{"paperId":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code"},{"paperId":"82d9f1db6db43cb61fe4b0b26a489a2e72628675","title":"A Test for Evaluating Performance in Human-Computer Systems"},{"paperId":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code"},{"paperId":"35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs"},{"paperId":"58aacb967cc7fc25cfc9d51b7ad3e57ac00d119b","title":"Can Foundation Models Wrangle Your Data?"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"3d5463a16d85d9d1d0d8ebb4117a31aca3c240ff","title":"Fooling MOSS Detection with Pretrained Language Models"},{"paperId":"239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning"},{"paperId":"cdec75f901a93c75ee5386a98abbe44746286e80","title":"Delivering Fairness in Human Resources AI: Mutual Information to the Rescue"},{"paperId":"a2b6e1f7d8a7963d321f29fca7c01eeb1ebd7f0f","title":"P ATCH G ENERATION WITH L ANGUAGE M ODELS : F EASIBILITY AND S CALING B EHAVIOR"},{"paperId":"73e6f5a7e2760b3f9ab077a450e445e12bfd7061","title":"Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?"},{"paperId":"a5731122200fbb8b37f048010a1e1ca4474aa606","title":"Examining Zero-Shot Vulnerability Repair with Large Language Models"}],"references":[{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"c0a98fca99e5f2d34330a15ab8ad4b9d692146d0","title":"The growing cost of deep learning for source code"},{"paperId":"92d211c65ae8c8006c07d34dfa0587e278d0ac00","title":"An Empirical Study of C++ Vulnerabilities in Crowd-Sourced Code Examples"},{"paperId":null,"title":"Natural Language Processing with Transformers"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"feba0c47bf12a02c3a725174bb53df78658a72a8","title":"Pre-Trained Models: Past, Present and Future"},{"paperId":"7571ed4cf1bbdcf891b576a0da12c910b1f0c72f","title":"Concealed Data Poisoning Attacks on NLP Models"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"05e396e79a2f88f0b8f8d99f5ab36ab3efa95c14","title":"Language-Agnostic Representation Learning of Source Code from Structure and Context"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"3944354c42ddfff7414ad06022f96c72858d5fa6","title":"Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"db392858262b17aa9c8ff8659738f68fbf832ebe","title":"Structural Language Models of Code"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"d170bd486e4c0fe82601e322b0e9e0dde63ab299","title":"Adaptive Input Representations for Neural Language Modeling"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"b4a26011eb7c26d71cd074f182c4f093da24308b","title":"Are deep neural networks the best choice for modeling source code?"},{"paperId":"022788ecf8685ed30e3f69f9121b5a3d242a22d6","title":"On the naturalness of software"},{"paperId":"2579e826f22da988fbc1b5d545aa59ab68bde4f3","title":"Program Synthesis Using Natural Language"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"6c2b28f9354f667cd5bd07afc0471d8334430da7","title":"A Neural Probabilistic Language Model"}],"id":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","summary":"This work finds that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language modeling, and identifies an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code."},{"url":"https://www.semanticscholar.org/paper/7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":7,"influentialCitationCount":0,"publicationDate":"02/06/2022","authors":"Patrick Bareiss,Beatriz Souza,Marcelo d’Amorim,Michael Pradel","citations":[{"paperId":"0885556b71b24a641b4ffe139afd4d2712228cff","title":"Beware of the Unexpected: Bimodal Taint Analysis"},{"paperId":"a3e355b5de868f34fdfa2500415c5f74c69d2091","title":"Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models"},{"paperId":"0b340dd78fd04bbde2807d5efedb796d319355e3","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?"},{"paperId":"317208b423d24d52ba04221cfb46956962364e22","title":"Extracting Meaningful Attention on Source Code: An Empirical Study of Developer and Neural Model Code Exploration"},{"paperId":"45a37f351bb275d22354b712c78df65715a37cc5","title":"CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code"},{"paperId":"453a8fac3be9282be53908f0735160d0d21e0f48","title":"Repair Is Nearly Generation: Multilingual Program Repair with LLMs"},{"paperId":"205ac1373eb7981aca2d08f2ab651871a001271e","title":"CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code"}],"references":[{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"4413f763efcd34498ec1c4439a9c4c9395689024","title":"µBert: Mutation Testing using Pre-Trained Language Models"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5d0db797a45ce2453f821f7ded0b547d3fdab054","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"7cbc2a7843411a1768ab762930707af0a3c33a19","title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"},{"paperId":"c0a98fca99e5f2d34330a15ab8ad4b9d692146d0","title":"The growing cost of deep learning for source code"},{"paperId":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis"},{"paperId":"26f1c97c9e38ed0da329e3197ed554553b305d18","title":"Neural software analysis"},{"paperId":"209333bef25395800997a7411c905757baf38553","title":"MeMo: Automatically identifying metamorphic relations in Javadoc comments for test automation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"c2c7233293d55f201fe5b496234bed1914eea70e","title":"Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"1311adebe439ccd9e373d594a8df1d720a685190","title":"Code Prediction by Feeding Trees to Transformers"},{"paperId":"49cf6a22a5dac5bc98b653534af65ffa0bc0e76d","title":"Multi-task Learning based Pre-trained Language Model for Code Completion"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"27724bd19946d6a824d06cdca3cdfe5d40f71003","title":"A structural model for contextual code changes"},{"paperId":"9a4bfc410eb7efac0557025fdb00503eb5a2adc0","title":"Learning semantic program embeddings with graph interval neural network"},{"paperId":"08066c80919620397e8e4e5372ff84caf401e675","title":"Global Relational Models of Source Code"},{"paperId":"6acf0d7b24a93953269a4d52c6b2f0c7d11f9ae4","title":"Hoppity: Learning Graph Transformations to Detect and Fix Bugs in Programs"},{"paperId":"29fc7ff02e68537a86d32978bfd2e8e45258e77d","title":"LambdaNet: Probabilistic Type Inference using Graph Neural Networks"},{"paperId":"4ddb9135fc6fdf3f9e01fdfcd115fdbf6f7486b4","title":"Sequence Model Design for Code Completion in the Modern IDE"},{"paperId":"0dfe706526e5234338411489e1826b4060acc4e8","title":"CC2Vec: Distributed Representations of Code Changes"},{"paperId":"4578871d2b271e4b5473c9cb81d431d6bf58c607","title":"Metamorphic Testing: A New Approach for Generating Next Test Cases"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"a429c6c8407b10a89c9610ab5e9682ec89772356","title":"TypeWriter: neural type prediction with search-based validation"},{"paperId":"deabe3945e100ec42c80582652aa149c2e1b66b0","title":"Automatic Software Repair: a Bibliography"},{"paperId":"b5abecf78c1bd975449e34350289a904e6ef35cc","title":"Automated program repair"},{"paperId":"1432c8378b1cafa3f91f09fa743082d154fdab92","title":"A Novel Neural Source Code Representation Based on Abstract Syntax Tree"},{"paperId":"d2c7cbd578878ff0064857240426e605c2e959d2","title":"On the Effectiveness of Manual and Automatic Unit Test Generation: Ten Years Later"},{"paperId":"8e71af3fbec666e67c0fcfefedb38881027a0254","title":"NL2Type: Inferring JavaScript Function Types from Natural Language Information"},{"paperId":"d02285385fd2b53a6100b8d62421bba6693b788c","title":"When Code Completion Fails: A Case Study on Real-World Completions"},{"paperId":"98d1307bed619b58b4a44acd8e65ac58495776c2","title":"code2seq: Generating Sequences from Structured Representations of Code"},{"paperId":"73bade9162bc4df6ca6c9ab8a7eb2d54d35acad2","title":"code2vec: learning distributed representations of code"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"d461ab9482b7fb5eadd7e6cd2c6dffced8ede8b8","title":"Chapter Six - Mutation Testing Advances: An Analysis and Survey"},{"paperId":"ea1b62a03ab16c4acca6f7041c6f096bedec244b","title":"Deep learning type inference"},{"paperId":"06fedb27a62aac78dda4f852e284d09d264844f8","title":"Translating code comments to procedure specifications"},{"paperId":"8b1aa4727eb2a83db1bd3ae54e078b0b7ce5eccb","title":"Retrieval on source code: a neural code search"},{"paperId":"e033a0b29af2939fd44e5765d03380c08897a9e8","title":"Deep Code Search"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"554c9b3aa8c3b7504810270b18b942a0d22156e7","title":"DeepFix: Fixing Common C Language Errors by Deep Learning"},{"paperId":"a60bbe22c11454661ab56dbbd1ceff1e83d94dea","title":"Automatic generation of oracles for exceptional behaviors"},{"paperId":"10d5cee2e9e3a5c7df8548f95eb7791f9d79d626","title":"The major mutation framework: efficient and scalable mutation analysis for Java"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"1fab8830baef8f353dce995c0269a32fd3105f64","title":"An orchestrated survey of methodologies for automated software test case generation"},{"paperId":"216b98bb3d9221d5f5d261864975612e4d0faaa6","title":"EvoSuite: automatic test suite generation for object-oriented software"},{"paperId":"494fbdd61eb5d9fc3b0d7c4b0e557d9b4996fae6","title":"An Analysis and Survey of the Development of Mutation Testing"},{"paperId":"ec316064d25c1f1dadc32a8570b56049aa109f78","title":"Inferring Resource Specifications from Natural Language API Documentation"},{"paperId":"af5a7be7bfc592e400e2f12a24ad3e6a5a44b58f","title":"Learning from examples to improve code completion systems"},{"paperId":"6dc7017e357767dbc210bcb27080260fb0816701","title":"Feedback-Directed Random Test Generation"},{"paperId":"208a35b667ac0b17333f593f9c68cc8d8603df1e","title":"JCrasher: an automatic robustness tester for Java"}],"id":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","summary":"This paper studies to what extent a state-of-the-art, pre-trained language model of code, Codex, may serve this purpose, and concludes that few-shot language models are surprisingly effective."},{"url":"https://www.semanticscholar.org/paper/8210cef990b8e5cddbc95000e46309bdd25337f7","title":"Asking Clarification Questions for Code Generation in General-Purpose Programming Language","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Haau-Sing Li,Mohsen Mesgar,André F. T. Martins,Iryna Gurevych","citations":[],"references":[{"paperId":"1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization"},{"paperId":"b3a54332a0791751fcf234f6264f242c42ac00d2","title":"DocPrompting: Generating Code by Retrieving the Docs"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"1a903282f7c19dbdb2714b852fb42dbb4675422b","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"906f9b3a5cd1912158528c7c6bd2c94b76c791bf","title":"Generating Clarifying Questions for Query Refinement in Source Code Search"},{"paperId":"f2a5af2e2fd7fabaefbb21ca211a500bca7cf6b7","title":"Pseudo Ambiguous and Clarifying Questions Based on Sentence Structures Toward Clarifying Question Answering System"},{"paperId":null,"title":"Natural Language Processing with Transformers: Building Language Applications with Hugging Face"},{"paperId":null,"title":"Competition-level code generation"},{"paperId":"267d9a093ae7e8388fd1e25d2b5e4cfe91c71226","title":"Building and Evaluating Open-Domain Dialogue Corpora with Clarifying Questions"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"c56aced0f0c5cfebefadb530cb08d736c3ac5c05","title":"Retrieval Augmented Code Generation and Summarization"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"b255a1aeed73c236fc1ae209743e32e805795168","title":"Ask what’s missing and what’s useful: Improving Clarification Question Generation using Global Knowledge"},{"paperId":"6e2b1038682cd116b2e38bec19b5721196c41eea","title":"HAConvGNN: Hierarchical Attention Based Convolutional Graph Neural Network for Code Documentation Generation in Jupyter Notebooks"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"6714dff86284cbde6be351e3e1f8ddee1bfadb9c","title":"A Toolkit for Generating Code Knowledge Graphs"},{"paperId":"1af678b040ce638aedf8b582212937f0921ccc1d","title":"Abg-CoQA: Clarifying Ambiguity in Conversational Question Answering"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"77910e51a40d17157fc798325d06edfa6cff18d6","title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation"},{"paperId":"42e863f93203d37a2518da381beaf06e4c70fb3d","title":"Stanza: A Python Natural Language Processing Toolkit for Many Human Languages"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"9cb32bdd43f64b36cb447ba1307869c5d8bf675c","title":"YAKE! Keyword extraction from single documents using multiple local features"},{"paperId":"517f770a38c6f09b1d0ee03793887912a844e69e","title":"Asking Clarification Questions in Knowledge-Based Question Answering"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"4a75598125bd0be3600d840aec04de40bf03c33b","title":"Asking Clarifying Questions in Open-Domain Information-Seeking Conversations"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"SentenceBERT: Sentence embeddings using Siamese BERTnetworks"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"7aca31a3bf370ad63af2eefa8e1fb146f7988eb0","title":"Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"3ade4d3be53981a1678b1e3a736d01547f7d3b9e","title":"Dialog for Language to Code"},{"paperId":"3807a517403cfc7ed67fdfea64baf62b9948bcfe","title":"A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation"},{"paperId":null,"title":"Asian Federation of Natural Language Processing"},{"paperId":"b5defb8729294614d40e0247eabfdc483bf1a992","title":"Bimodal Modelling of Source Code and Natural Language"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"d2071c1e4a6030dc0005dbfeefdd196a8b293e84","title":"Okapi at TREC-3"},{"paperId":null,"title":"Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language model for code with multi"},{"paperId":null,"title":"Curtis von Veh, Madanlal Musuvathi"}],"id":"8210cef990b8e5cddbc95000e46309bdd25337f7","summary":"The empirical results support the hypothesis that clariﬁcations result in more precise generated code, as shown by an improve-ment of 17.52 in BLEU, 12.72 in CodeBLEU, and 7.7% in the exact match."},{"url":"https://www.semanticscholar.org/paper/9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":3,"influentialCitationCount":0,"publicationDate":"26/06/2022","authors":"Disha Shrivastava,H. Larochelle,Daniel Tarlow","citations":[{"paperId":"433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"317208b423d24d52ba04221cfb46956962364e22","title":"Extracting Meaningful Attention on Source Code: An Empirical Study of Developer and Neural Model Code Exploration"}],"references":[{"paperId":"f75a0ccb3c9b7e12dfb8b9c9dc3d35ca4518a643","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"4bc040835fbff57ce6612306d794b8c6c8226086","title":"Discovering the Syntax and Strategies of Natural Language Programming with Generative Language Models"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"0e802c0739771acf70e60d59c2df51cd7e8c50c0","title":"Memorizing Transformers"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","title":"Investigating Explainability of Generative AI for Code through Scenario-based Design"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"5d0db797a45ce2453f821f7ded0b547d3fdab054","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"0271d1dbc01eda68c2f0291c62a956fca3092864","title":"PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization"},{"paperId":"c6bb04f3d8000b7e800f6359082de39548c7da79","title":"Capturing Structural Locality in Non-parametric Language Models"},{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"59641c10ed7431a3cf841f308367dc2dc0281b74","title":"What Makes Good In-Context Examples for GPT-3?"},{"paperId":"7bbfe2586d10d56081915a9edc44be2d29bbf8dc","title":"CodeTrek: Flexible Modeling of Code using an Extensible Relational Representation"},{"paperId":null,"title":"Natural Language Processing with Transformers"},{"paperId":null,"title":"A generalist agent, 2022"},{"paperId":null,"title":"Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"2ee03e28208a9310a9be4032c2b04ebdddb83cc7","title":"FLEX: Unifying Evaluation for Few-Shot NLP"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"209f9bde2dee7cf1677801586562ffe56d435d38","title":"Learning How to Ask: Querying LMs with Mixtures of Soft Prompts"},{"paperId":"a711a240ccf37b68b86e97c46aaed3d2cd256f75","title":"Learning to Generate Code Comments from Class Hierarchies"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"ac3cdb50606f7770eef8e4cd951840a4f71287a0","title":"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"},{"paperId":"85e7d63f75c0916bd350a229e040c5fbb1472e7a","title":"Making Pre-trained Language Models Better Few-shot Learners"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"8ae9a17c87a4518b513e860683a0ef7824be994d","title":"Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"},{"paperId":"18a93dc1558bf9d7534d0b416633cebaf75c1145","title":"Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"2934145c4ab42bfee022067b1b6b213acd836a2a","title":"On-the-Fly Adaptation of Source Code Models using Meta-Learning"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"7be8c119dbe065c52125ee7716601751f3116844","title":"Generalization through Memorization: Nearest Neighbor Language Models"},{"paperId":null,"title":"AutoPrompt: Eliciting knowledge from language models with automatically generated prompts"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"b4a26011eb7c26d71cd074f182c4f093da24308b","title":"Are deep neural networks the best choice for modeling source code?"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"34f25a8704614163c4095b3ee2fc969b60de4698","title":"Dropout: a simple way to prevent neural networks from overfitting"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"},{"paperId":null,"title":"For the example shown in Figure 1 of the main paper"},{"paperId":null,"title":"Type Identifiers (TI): Type Identifiers define the type of an identifier. For example, in the code snippet class DPAffinityPropagation extends AffinityPropagation"},{"paperId":null,"title":"Method Names and Bodies (MNB): Take all the method names along with their signatures and corresponding bodies used in the rule context location"},{"paperId":null,"title":"This was done by splitting the filenames based on either camel-case or underscore. If the sub-parts of two files match"},{"paperId":null,"title":"String Literals (SL): Take all the string literals used in the rule context location"},{"paperId":null,"title":"Identifiers (I): Identifiers are the names of variables used in the code. For example, for the rule context taken from the imported file shown in Figure 1 in the main paper"}],"id":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","summary":"This work proposes a framework called Repo-Level Prompt Generator that learns to generate example-speciﬁc prompts using prompt proposals, and demonstrates that an oracle constructed from these prompt proposals gives a remarkably high relative improvement over Codex, showing the quality of these proposals."},{"url":"https://www.semanticscholar.org/paper/b3a54332a0791751fcf234f6264f242c42ac00d2","title":"DocPrompting: Generating Code by Retrieving the Docs","venue":"","year":2022,"referenceCount":38,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/07/2022","authors":"Shuyan Zhou,Uri Alon,Frank F. Xu,Zhiruo Wang,Zhengbao Jiang,Graham Neubig","citations":[{"paperId":"8210cef990b8e5cddbc95000e46309bdd25337f7","title":"Asking Clarification Questions for Code Generation in General-Purpose Programming Language"}],"references":[{"paperId":"bcd4c46e4d75ddedb6138cfd77600c6d964a9aa8","title":"Natural Language to Code Translation with Execution"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"1f11601f9eb8c0bee1d1d2cc7f4fa187e8c5e525","title":"The impact of lexical and grammatical processing on generating code from natural language"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"a0e92f6e9564b8c38b6649ae71b892ddfb988faa","title":"Controllable Semantic Parsing via Retrieval Augmentation"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"c56aced0f0c5cfebefadb530cb08d736c3ac5c05","title":"Retrieval Augmented Code Generation and Summarization"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"c26759e6c701201af2f62f7ee4eb68742b5bf085","title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"bde0c85ed3d61de2a8874ddad70497b3d68bc8ad","title":"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"},{"paperId":"45793225c6593db60e9efd95bce1d70bf4844198","title":"Evaluation Paradigms in Question Answering"},{"paperId":null,"title":"Human-annotated unit tests"},{"paperId":"58ed1fbaabe027345f7bb3a6312d41c5aac63e22","title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"paperId":"77910e51a40d17157fc798325d06edfa6cff18d6","title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation"},{"paperId":"79cd9f77e5258f62c0e15d11534aea6393ef73fe","title":"Dense Passage Retrieval for Open-Domain Question Answering"},{"paperId":"34733eaf66007516347a40ad5d9bbe1cc9dacb6b","title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"832fff14d2ed50eb7969c4c4b976c35776548f56","title":"REALM: Retrieval-Augmented Language Model Pre-Training"},{"paperId":"3c9fa66674e1053eb9de12076c48f13e46422c29","title":"CLAI: A Platform for AI Skills on the Command Line"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2b88aefb70cd52a4d6899020f4be97c669a5edcb","title":"RTFM: Generalising to Novel Environment Dynamics via Reading"},{"paperId":"db392858262b17aa9c8ff8659738f68fbf832ebe","title":"Structural Language Models of Code"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"75c0d369c7151b925155cfe1b3f01dd7d0503981","title":"Generative Code Modeling with Graphs"},{"paperId":"d3e13d2514edaf74b863bfbe45a739c32a7689e1","title":"Retrieval-Based Neural Code Generation"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"429b65937d4922578a81e1f0ef5aeab7361ae36b","title":"NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System"},{"paperId":"2c1e874c3b67510a3215e535f5646b362de5bc89","title":"Abstract Syntax Networks for Code Generation and Semantic Parsing"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"b5defb8729294614d40e0247eabfdc483bf1a992","title":"Bimodal Modelling of Source Code and Natural Language"},{"paperId":"3170bb1affffeddfe9d3262c9a0787c0b29cb18a","title":"How do professional developers comprehend software?"},{"paperId":"99f1921a8ba5ceecb18e5ca7ff19b7f95c4e250d","title":"Learning to Win by Reading Manuals in a Monte-Carlo Framework"},{"paperId":"d744c7cf1f5e9e77c7de55db8df5a918ee1f41d7","title":"How software engineers use documentation: the state of the practice"},{"paperId":"3046cf5c122149b80915ce5bb99153d9cbd0366e","title":"The relevance of software documentation, tools and technologies: a survey"},{"paperId":"660f2447218bdf884fb39f2802383824c575dc43","title":"What programmers really want: results of a needs assessment for SDK documentation"},{"paperId":"f6e3e57567e9803718623ec088cd7fea65cfbc9d","title":"Relevance weighting of search terms"}],"id":"b3a54332a0791751fcf234f6264f242c42ac00d2","summary":"DocPrompting is a natural-language-to-code generation approach that explicitly leverages code documentation by retrieving the relevant documentation pieces given a natural language (NL) intent, and generating code based on the NL intent and the retrieved documentation."},{"url":"https://www.semanticscholar.org/paper/1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":46,"citationCount":75,"influentialCitationCount":20,"publicationDate":"20/05/2021","authors":"Dan Hendrycks,Steven Basart,Saurav Kadavath,Mantas Mazeika,Akul Arora,Ethan Guo,Collin Burns,Samir Puranik,Horace He,D. Song,J. Steinhardt","citations":[{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"},{"paperId":"b2f256a1ec54325102119a57cb2359840fe68586","title":"SantaCoder: don't reach for the stars!"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"ae441f7305dc2cd58c708528b3ecee3501cc5c46","title":"Plansformer: Generating Symbolic Plans using Transformers"},{"paperId":"9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming"},{"paperId":"27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models"},{"paperId":"20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers"},{"paperId":"20e4ca6368a0731d3cbbd2e68aa90cef7384dc72","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"0b340dd78fd04bbde2807d5efedb796d319355e3","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?"},{"paperId":"5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","title":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming"},{"paperId":"cb123f1afd67fb8bae15dc876709c842b626c49c","title":"SimSCOOD: Systematic Analysis of Out-of-Distribution Behavior of Source Code Models"},{"paperId":"a4c216d2ce9dd245c84771acc574722055967fd6","title":"Enhancing Code Classification by Mixup-Based Data Augmentation"},{"paperId":"1c1ca2392155ddf30408a442e6b504b5d60d4f2a","title":"When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment"},{"paperId":"8a854331c593c6a766fa3b8037fb2ad1b95a6f06","title":"An Empirical Study of Code Smells in Transformer-based Code Generation Techniques"},{"paperId":"a7435722d8ab595da5a9c70ac9160f57d0dcd75a","title":"Enabling Transformers to Understand Low-Level Programs"},{"paperId":"6032212d5790b6a580d68d469a9895aad6238c89","title":"Diverse Title Generation for Stack Overflow Posts with Multiple Sampling Enhanced Transformer"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"05ccfb058dc6788254742722278b0af2fc87f083","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"},{"paperId":"63c670ba8018da0a7e33c34ffd84c2a3ca54b894","title":"Language Models Can Teach Themselves to Program Better"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"1d160123cbbef972ea151a641dd435d57c727de8","title":"AixBench: A Code Generation Benchmark Dataset"},{"paperId":"2edc8efcda27c944a46f367acf6a5280b8f65525","title":"FixEval: Execution-based Evaluation of Program Fixes for Competitive Programming Problems"},{"paperId":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"1100dee3fd78655cddc4b7bfaef1161351d4fab5","title":"Automated Feedback Generation for Competition-Level Code"},{"paperId":"0bcd59da541fdae66884afba8d25475a54a9da1a","title":"Automated Repair of Programs from Large Language Models"},{"paperId":"ea4749c6ed5f08b16bab31b1ca4fd3fdc259ae8f","title":"Improving automatically generated code from Codex via Automated Program Repair"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"6050454e0446a3068617f73b0301453f3f67844d","title":"Stylette: Styling the Web with Natural Language"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"1a903282f7c19dbdb2714b852fb42dbb4675422b","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"590f6817b42407f96b079e82c935fae298196359","title":"Less is More: Summary of Long Instructions is Better for Program Synthesis"},{"paperId":"c96363c42bc8c465902c22b8c33c8704233f519e","title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages"},{"paperId":"1f11601f9eb8c0bee1d1d2cc7f4fa187e8c5e525","title":"The impact of lexical and grammatical processing on generating code from natural language"},{"paperId":"76f023c3a819fc58989a064a1b50825b11fce95d","title":"Capturing Failures of Large Language Models via Human Cognitive Biases"},{"paperId":"7b5aa186ca8abc585607c5ec91562e127a398601","title":"Open-Ended Knowledge Tracing"},{"paperId":"04ff95e0edc3759fc5d18a1b929b3ccf79b032b2","title":"Deconstructing Distributions: A Pointwise Framework of Learning"},{"paperId":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective"},{"paperId":"76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","title":"Investigating Explainability of Generative AI for Code through Scenario-based Design"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"3d5463a16d85d9d1d0d8ebb4117a31aca3c240ff","title":"Fooling MOSS Detection with Pretrained Language Models"},{"paperId":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"},{"paperId":"52db8674337e5d86dcb96d013734befc8c3d4581","title":"Large Language Models are not Models of Natural Language: they are Corpus Models."},{"paperId":"1aed58bd07026492194672adec494dc37c894a28","title":"Leveraging Automated Unit Tests for Unsupervised Code Translation"},{"paperId":"407b9e9478ba6bff43ce4b20e8b6cb2b303477d2","title":"P LANNING WITH L ARGE L ANGUAGE M ODELS FOR C ODE G ENERATION"},{"paperId":"239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning"},{"paperId":"b2c4fdb49bdb23e6ad0ac3272029324157046ea7","title":"Automated Repair of Code from Language Models"},{"paperId":"78863000eb70945cc8d791d45d4a3fe8a6521cb6","title":"Open-Ended Knowledge Tracing for Computer Science Education"},{"paperId":"3b0cf543a730e674d4213d344ebc857fada76ead","title":"Understanding High-Level Properties of Low-Level Programs Through Transformers"},{"paperId":"7497360b0f411a44aa6afbd8b830050c40ec8aed","title":"Dataset of Student Solutions to Algorithm and Data Structure Programming Assignments"},{"paperId":"6f84d0cc33c7c58f74b28ddcc1cbda91ea608c9f","title":"C ODE S UMMARIZATION : D O T RANSFORMERS R E ALLY U C"},{"paperId":"27d16a7f2ce8f2b787b34ff1f9b4fece079700c3","title":"Figuring out Figures: Using Textual References to Caption Scientific Figures"},{"paperId":"a3564f3cf954c05844c757505325a50b4d858e22","title":"Generating Infrastructure-as-Code from Natural Language: Evaluating Fine-Tuned GPT-3 Models for Cloud Infrastructure Provisioning"},{"paperId":"570a6a5b8ec2827c3f33bb1b1bd027190a0d3e07","title":"Neural Program Generation Modulo Static Analysis"},{"paperId":"a8863de15a5ee8eed98107f423138a1a8f5a2ba8","title":"Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"58a6ca2ae28a618126f71a07262cb958a8c37904","title":"Latent Execution for Neural Program Synthesis"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"6a1b25f7a67395ad1e676027322913acbb0a0635","title":"CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review"},{"paperId":"27e3ca4fd7b8290a0e12ca4fd2b7ad5bcd5900f2","title":"Are Transformers All That Karel Needs?"},{"paperId":"f5eb526492798dd7a53fe78f28431f5f489192da","title":"A Survey on Semantic Parsing for Machine Programming"},{"paperId":"642e280df732665249315d6c144871f0e2ceeae6","title":"NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash Commands"}],"references":[{"paperId":"c317f4d5ddeb11da9a5c8fea1e3cdec2f7de485d","title":"Deep Learning Based Program Generation From Requirements Text: Are We There Yet?"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"10bb7e2c54b947fa50e7bb65b0b5c700fe998044","title":"Measuring Massive Multitask Language Understanding"},{"paperId":"ce7091d011be560aca0fba8511e5a41a1436f5d7","title":"Aligning AI With Shared Human Values"},{"paperId":"0df347f5e3118fac7c351917e3a497899b071d1e","title":"Datasheets for datasets"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"5fe0a4af3bd1479d5e39fbda2215c86bce54722b","title":"Generative Language Modeling for Automated Theorem Proving"},{"paperId":"725264948d7b6946259af5b8d966e996b9570f99","title":"DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"},{"paperId":"70f2c1567ef94fdf4581e1290bf7667cc9a4dcfc","title":"LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"0071ee55f1749e6be586645bab08f0385ee856b4","title":"Graph-based, Self-Supervised Program Repair from Diagnostic Feedback"},{"paperId":"04f4e55e14150b7c48b0287ba77c7443df76ed45","title":"PIQA: Reasoning about Physical Commonsense in Natural Language"},{"paperId":"0c5bc409e62e65f86838968a2a7cdae5fa0b288b","title":"RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers"},{"paperId":"00c957711b12468cb38424caccdf5291bb354033","title":"ZeRO: Memory optimizations Toward Training Trillion Parameter Models"},{"paperId":null,"title":"2020a) do a smaller-scale analysis of code generation but with their limited"},{"paperId":"66117f82def0c69a3b9cc77eb3e2694b0245ca86","title":"Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad","title":"HellaSwag: Can a Machine Really Finish Your Sentence?"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"2016) introduce datasets based on Hearthstone and Magic the Gathering card games for code generation"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":null,"title":"2019c) use Spider"},{"paperId":null,"title":"2018) introduce the NAPS dataset for converting"},{"paperId":null,"title":"Gathering card games for code"},{"paperId":null,"title":"Natural program synthesis dataset"},{"paperId":null,"title":"Number of Programs’ refers to the number of human-written programs"},{"paperId":"6b024162f81e8ff7aa34c3a43d601a912d012c78","title":"Making Neural Programming Architectures Generalize via Recursion"},{"paperId":null,"title":"Attention is all you"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"2579e826f22da988fbc1b5d545aa59ab68bde4f3","title":"Program Synthesis Using Natural Language"},{"paperId":null,"title":"2018) by also facilitating the synthesis of database queries, though more recent program synthesis works such as Wang et al. (2019c) use Spider from Yu et al. (2018)"},{"paperId":null,"title":"A Additional Dataset Information Expanded Dataset Comparisons. We compared to several datasets in the (Kulal et al., 2019"},{"paperId":"27e1dbe9f7c71cd6cc1b0357f49aef497e572d09","title":"Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"},{"paperId":"e2d2099379af74d7be80a1964830f99edddaeb11","title":"Compositional Program Synthesis from Natural Language and Examples"},{"paperId":"110b7719f16eae8d82af0f56fca6e802c842c181","title":"Mining source code repositories at massive scale using language modeling"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"f60d8dd8ca3a7dfa7d0a14988af73084ad93619d","title":"Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing"},{"paperId":"b7c0e47f8b768258b7d536c21b218e6c46ab8791","title":"Learning to Parse Database Queries Using Inductive Logic Programming"}],"id":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","summary":"APPS is introduced, a benchmark for code generation that measures the ability of models to take an arbitrary natural language speciﬁcation and generate satisfactory Python code and shows that machine learning models are now beginning to learn how to code."},{"url":"https://www.semanticscholar.org/paper/9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey","venue":"ArXiv","year":2022,"referenceCount":156,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Daoguang Zan,Bei Chen,Fengji Zhang,Di Lu,Bingchao Wu,Bei Guan,Yongji Wang,Jian-Guang Lou","citations":[],"references":[{"paperId":"6bc87e51018d6de55011e95a0d43c588dd44a1e8","title":"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages"},{"paperId":"4c26a6a777f1d46ffe79e3f41f8fd1ef6a6bd8c9","title":"Automatically Generating CS Learning Materials with Large Language Models"},{"paperId":"5fd0533ba1068af1cb075d4eb71ee551691a71ac","title":"ExploitGen: Template-augmented exploit code generation based on CodeBERT"},{"paperId":"598d9b235f5ab148fc757240d9bc39a47b8eaf72","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","title":"Evaluating How Fine-tuning on Bimodal Data Effects Code Generation"},{"paperId":"71280dba5bda65c162f9deaffed7d3d20692ca0a","title":"SecurityEval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"20e4ca6368a0731d3cbbd2e68aa90cef7384dc72","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"152dc3042f5d4fc5a2686b5f4e0904f1e12a9207","title":"Code4Struct: Code Generation for Few-Shot Structured Prediction from Natural Language"},{"paperId":"39e40821b7207125e54e6ed7112e55cd38c6f0c3","title":"Language Models of Code are Few-Shot Commonsense Learners"},{"paperId":"2a0456b0408cd4c33f2ff4400374e7be2497a362","title":"Repairing Bugs in Python Assignments Using Large Language Models"},{"paperId":"453a8fac3be9282be53908f0735160d0d21e0f48","title":"Repair Is Nearly Generation: Multilingual Program Repair with LLMs"},{"paperId":"eed359a8a3ffdc45253e698b352443f0271a9666","title":"Compressing Pre-trained Models of Code into 3 MB"},{"paperId":"53b3b15e8cbe2baf82cd0de3709e0a1e6f677415","title":"Limits of an AI program for solving college math problems"},{"paperId":"c067664a45dce31411b3052c635c044ad4587db4","title":"Generating Diverse Code Explanations using the GPT-3 Large Language Model"},{"paperId":"654cd297b307c0c6fa08732511ba852b8dce1977","title":"Out of the BLEU: how should we assess quality of the Code Generation models?"},{"paperId":"ec4c8d99eb1c028c43af6d8bbf727392d351cb59","title":"Efficient Training of Language Models to Fill in the Middle"},{"paperId":"df1cc92fba512ce7d28d1d608ea19f18cda185ca","title":"No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"f0631f7928b99ee51f8164acd04889219b2bcdbb","title":"Diet code is healthy: simplifying programs for pre-trained models of code"},{"paperId":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code"},{"paperId":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation"},{"paperId":"9aed848be4e9e401b0e61a2e5d60dbdafa0c6cc1","title":"A Dataset and Benchmark for Automatically Answering and Generating Machine Learning Final Exams"},{"paperId":"0d08ffccc982781e310bb184397bbe64b9aef157","title":"Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models"},{"paperId":"cdfe9580f63070f311151444f9df32818cc858bf","title":"An Empirical Evaluation of GitHub Copilot's Code Suggestions"},{"paperId":"1f3a97d255a6d0b2d5535af75737ed43b1e96fc4","title":"Is GitHub Copilot a Substitute for Human Pair-programming? An Empirical Study"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"0c6c5e0fb38c28e50909cd5e165737636abf804b","title":"WhyGen: Explaining ML-powered Code Generation by Referring to Training Examples"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"d358ce242dacfd2ea738aa538779f06c79777f2b","title":"SELFIES and the future of molecular string representations"},{"paperId":"a522543180db6c5b21f47fe88abee44de158c85c","title":"Automatic Programming and Education"},{"paperId":"1d10023541f06701bf2f9ae8c91609e1055799ec","title":"Automating code review activities by large-scale pre-training"},{"paperId":"51000d9f79be0eefd7972fe94e3c71dddc90d2c6","title":"Evaluating the Text-to-SQL Capabilities of Large Language Models"},{"paperId":"2b556fcf2ac634f03c1fb0ace5e602e829418e65","title":"ReACC: A Retrieval-Augmented Code Completion Framework"},{"paperId":"bf81b2a50009fc7370d25f2ae6f8acc09c7da5d9","title":"GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models"},{"paperId":"7a1069dafaeb484e22f2473d5545f1e45ce30656","title":"Compilable Neural Code Generation with Compiler Feedback"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"463c6944ddcb43280e1c9765c2245bc7b764ab68","title":"AI-Driven Development Is Here: Should You Worry?"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"76f023c3a819fc58989a064a1b50825b11fce95d","title":"Capturing Failures of Large Language Models via Human Cognitive Biases"},{"paperId":"81d1af99082e60ae8135b335a1ffe208868b6bba","title":"Locating and Editing Factual Associations in GPT"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"23e8ed7568454e11d9a6fecb8242e1d16b1828d5","title":"Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"5d0db797a45ce2453f821f7ded0b547d3fdab054","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"6d7d4fca9840504f630e9bea6acaa07322a6e889","title":"Text and Code Embeddings by Contrastive Pre-Training"},{"paperId":"166b64f2ae8e52f5779682fab756cbd617a6e74b","title":"A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level"},{"paperId":"8417d83e74a63b9d3874cd7695477ebad8b98f8a","title":"Choose your programming copilot: a comparison of the program synthesis performance of github copilot and genetic programming"},{"paperId":"a341bf8acc0cb13c06f747db6543cac9f2120065","title":"Natural language processing models that automate programming will transform chemistry research and teaching"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"},{"paperId":"4a160efbe80c38cd5eb2f92c7c095b49b113397d","title":"In-IDE Code Generation from Natural Language: Promise and Challenges"},{"paperId":"37bf0bf34603145246c3311df19e2afdf6e0270a","title":"JAKET: Joint Pre-training of Knowledge Graph and Language Understanding"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs"},{"paperId":"737daa49c4234a8897b1f5b466c004db56241d83","title":"S2QL: Retrieval Augmented Zero-Shot Question Answering over Knowledge Graph"},{"paperId":"e11d61b669bdf256d4021e92fae980152591bf45","title":"Are NLP Metrics Suitable for Evaluating Generated Code?"},{"paperId":"91260f73dd179487fb16713deb8267634ae14716","title":"CodexDB: Synthesizing Code for Query Processing from Natural Language Instructions using GPT-3 Codex"},{"paperId":"1b4c19168410fb2690d285b205ab2281793db81a","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"},{"paperId":null,"title":"2022d. MCoNaLa: A"},{"paperId":null,"title":"2022a. A systematic evaluation"},{"paperId":"f7987fa2aadc0b368c185dc4d2fdb1337a202c32","title":"Solving Probability and Statistics Problems by Program Synthesis"},{"paperId":"4e40595d6ecba027cebb4f2e3b43ae44bcf51daf","title":"Solving Linear Algebra by Program Synthesis"},{"paperId":"1444536496d8064f33e10b38b5820fecfab5b367","title":"Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs"},{"paperId":"92add61e7c3dd745321fc8a6e113dae8199668ac","title":"Improving Text-to-Code Generation with Features of Code Graph on GPT-2"},{"paperId":"2016e814eed00b0c0a9358e193e29854e0ed526f","title":"Long-Range Modeling of Source Code Files with eWASH: Extended Window Access by Syntax Hierarchy"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"cc9ad384ec0d0176ce05865d3866b44d2519bd68","title":"Reading StackOverflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"0b077c9577f4297dcf3da835e253d21965bbc6e0","title":"CoTexT: Multi-task Learning with Code-Text Transformer"},{"paperId":"240b0caabb415578bdea4da7d0a32bdff2e8163f","title":"Editing Factual Knowledge in Language Models"},{"paperId":"969e8c2c7cdf26c35e6c3fc19a9a56b3e7fcd6f9","title":"Generating Code with the Help of Retrieved Template Functions and Stack Overflow Answers"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"bc4f195d2f0937dab16bf50a24bf385c10781dd7","title":"MLIR: Scaling Compiler Infrastructure for Domain Specific Computation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"10d5a71da740e4d709914c450fc70fee1959b196","title":"Self-Supervised Contrastive Learning for Code Retrieval and Summarization via Semantic-Preserving Transformations"},{"paperId":"021bbcefc993c389bad6c1daefd8ff92d0fc2441","title":"Contrastive Code Representation Learning"},{"paperId":"5d4cbdd2172039b84b8628f1a2f77b83ba1fa551","title":"Enriching contextualized language model from knowledge graph for biomedical information extraction"},{"paperId":"1728cb805a9573b59330890ba9723e73d6c3c974","title":"Knowledge Distillation: A Survey"},{"paperId":"375b9b36ef68678185f2b6e4dbbbe7bbfad6535a","title":"Large-Scale Relation Learning for Question Answering over Knowledge Bases with Pre-trained Language Models"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"49cf6a22a5dac5bc98b653534af65ffa0bc0e76d","title":"Multi-task Learning based Pre-trained Language Model for Code Completion"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"77910e51a40d17157fc798325d06edfa6cff18d6","title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation"},{"paperId":"21d9d58ab9537d10927b749eaa1c8b8c5a970579","title":"Leveraging Code Generation to Improve Code Retrieval and Summarization via Dual Learning"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"dfc08920427d7013e793fd54b6fdef6b76615b41","title":"GANCoder: An Automatic Natural Language-to-Programming Language Translation Approach Based on GAN"},{"paperId":"b7af363ff612f41e0c1071ae24ce68a836aaa678","title":"Code Generation as a Dual Task of Code Summarization"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"9e955da40c62b2543b963454b90928ad6cbd5f8a","title":"Does BLEU Score Work for Code Migration?"},{"paperId":"9c2ab4707e902f6174910847e3f94f3b85017ec3","title":"A Grammar-Based Structural CNN Decoder for Code Generation"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"c2a6a4b6be2dabfcae7e17f2b02b6b29e03a8277","title":"CodeAlchemist: Semantics-Aware Code Generation to Find Vulnerabilities in JavaScript Engines"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"077ea3a496bd76bd22d76c432b79b9fa40e136e8","title":"Improving Automatic Source Code Summarization via Deep Reinforcement Learning"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"d3e13d2514edaf74b863bfbe45a739c32a7689e1","title":"Retrieval-Based Neural Code Generation"},{"paperId":"d6b7abd5c17f9e7d83c5b709c27da3a82a9788f2","title":"NAPS: Natural Program Synthesis Dataset"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"f6a4bf043af1a9ec7f104a7b7ab56806b241ceda","title":"Model compression via distillation and quantization"},{"paperId":"429b65937d4922578a81e1f0ef5aeab7361ae36b","title":"NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System"},{"paperId":"4df7bbe3ca7806f39a490c99f17867a0ac299bc3","title":"Learning Explanatory Rules from Noisy Data"},{"paperId":"ebeb5026cc5c6cf496441887f8b5bd0e36ff987b","title":"Systematic mapping study of template-based code generation"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":null,"title":"and Illia Polosukhin"},{"paperId":"956c52bf738517a8827f63be976e1291750e0ebc","title":"Learning to select examples for program synthesis"},{"paperId":"b4a26011eb7c26d71cd074f182c4f093da24308b","title":"Are deep neural networks the best choice for modeling source code?"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2c1e874c3b67510a3215e535f5646b362de5bc89","title":"Abstract Syntax Networks for Code Generation and Semantic Parsing"},{"paperId":"de7e3537d974c2ca28e3ca130d531e5eb23eb79f","title":"Program Synthesis for Character Level Language Modeling"},{"paperId":"8ec6abfdc5009b4e490e975991c871dfeec05434","title":"Program Synthesis from Natural Language Using Recurrent Neural Networks"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"ecc5492d00b1c53ed30d830136c300d381ca2770","title":"Automatic code generation of convolutional neural networks in FPGA implementation"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"a1b6b07157442f9ca2b5af3533a42b4caa100307","title":"A deep language model for software code"},{"paperId":"32aa2517b03c871c11e521c2a3406f457833e2c3","title":"Summarizing Source Code using a Neural Attention Model"},{"paperId":"a62731619b2b09a7d166b9e805f4a1a9c4c7c2d3","title":"PHOG: Probabilistic Model for Code"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"db02cd07726371790a825208cec377ec15f5b5f1","title":"Tree-to-Sequence Attentional Neural Machine Translation"},{"paperId":"5a4cc911a2b3d50d974aea6f9e3dbdae76d9df98","title":"An analysis of tools for automatic software development and automatic code generation"},{"paperId":"27e1dbe9f7c71cd6cc1b0357f49aef497e572d09","title":"Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"},{"paperId":"285c165c81fc9275955147a892b9a039ec8b1052","title":"chrF: character n-gram F-score for automatic MT evaluation"},{"paperId":"cd7ee037c029b25e691650a26cec7538d255405c","title":"Suggesting accurate method and class names"},{"paperId":"b5defb8729294614d40e0247eabfdc483bf1a992","title":"Bimodal Modelling of Source Code and Natural Language"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"26adb749fc5d80502a6d889966e50b31391560d3","title":"Meteor Universal: Language Specific Translation Evaluation for Any Target Language"},{"paperId":"7f013f172a45824d907f68481e92a22e0188ea0b","title":"Mining idioms from source code"},{"paperId":"215999a0e155a21255e4655d4eac312858058a84","title":"Structured Generative Models of Natural Source Code"},{"paperId":"9769e24c45c87e0daa5cff39991e0313882213fd","title":"A statistical semantic language model for source code"},{"paperId":"2813afed488061326bc42d3d297dd5db37c9744f","title":"Complete completion using types and weights"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"18b8ef71bc01b8658b4ef2c8b9a9e4e6e5c2a07b","title":"Dimensions in program synthesis"},{"paperId":"fd07159f48b358e912d0bd6b81ca87710f463734","title":"Inducing Tree-Substitution Grammars"},{"paperId":"0228810a988f6b8f06337e14f564e2fd3f6e1056","title":"The Recurrent Temporal Restricted Boltzmann Machine"},{"paperId":"3960dda299e0f8615a7db675b8e6905b375ecf8a","title":"Z3: An Efficient SMT Solver"},{"paperId":"bd7d93193aad6c4b71cc8942e808753019e87706","title":"Three new graphical models for statistical language modelling"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"1459ed38b154648d1375b29f39891f94d459c64c","title":"A Formalism for Dependency Grammar Based on Tree Adjoining Grammar"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"e41498c05d4c68e4750fb84a380317a112d97b01","title":"Connectionist language modeling for large vocabulary continuous speech recognition"}],"id":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","summary":"This survey focuses on how does neural network (NN) solves NL2Code and proposes a comprehensive framework, which is able to cover all studies in this task, and in-depth parse the existing studies into this framework."},{"url":"https://www.semanticscholar.org/paper/a08a3b08a5a1de6462a7da2906b1cd81691d6c18","title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation","venue":"International Joint Conference on Artificial Intelligence","year":2022,"referenceCount":27,"citationCount":4,"influentialCitationCount":0,"publicationDate":"14/06/2022","authors":"Daoguang Zan,Bei Chen,Dejian Yang,Zeqi Lin,Minsu Kim,Bei Guan,Yongji Wang,Weizhu Chen,Jian-Guang Lou","citations":[{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"045a6f92b58f08d5b305dd5d661316a506ee4d43","title":"Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation"},{"paperId":"1d160123cbbef972ea151a641dd435d57c727de8","title":"AixBench: A Code Generation Benchmark Dataset"}],"references":[{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax"},{"paperId":"ed2a639dd4a6b3abdcc0040733a264cb1dbcd7d4","title":"PlotCoder: Hierarchical Decoding for Synthesizing Visualization Code in Programmatic Context"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"1fa9ed2bea208511ae698a967875e943049f16b6","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"6c72cff44294e050f75b47f1027889f539e11347","title":"Learning to Infer Program Sketches"},{"paperId":"9c2ab4707e902f6174910847e3f94f3b85017ec3","title":"A Grammar-Based Structural CNN Decoder for Code Generation"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"fd04e31c25451f9103a0ac2220ac8d7e7884c343","title":"Coarse-to-Fine Decoding for Neural Semantic Parsing"},{"paperId":"d11777ca327c6d91de34d1d2ac50316b905578b2","title":"Neural Sketch Learning for Conditional Program Generation"},{"paperId":"ae271f8b7ae47e83bcd8af7d567df433a36c5dce","title":"API usage pattern recommendation for software development"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"1737bbf048409b19cfda6d0d18a4262dbb57a194","title":"Mining succinct and high-coverage API usage patterns from source code"},{"paperId":"fed132d312a9f618329238ac6542d0148a0ff157","title":"MAPO: Mining and Recommending API Usage Patterns"}],"id":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","summary":"This paper investigates how to leverage an unlabelled code corpus to train a model for library-oriented code generation, and observes that library- oriented code snippets are more likely to share similar code sketches."},{"url":"https://www.semanticscholar.org/paper/876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":11,"influentialCitationCount":3,"publicationDate":"21/07/2022","authors":"Bei Chen,Fengji Zhang,A. Nguyen,Daoguang Zan,Zeqi Lin,Jian-Guang Lou,Weizhu Chen","citations":[{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"46d0a832fada6147bceb0bd4e39928e482733246","title":"How Important are Good Method Names in Neural Code Generation? A Model Robustness Perspective"},{"paperId":"971e875e28f26240987d2c9470d1ee74ad204205","title":"Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"}],"references":[{"paperId":"1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization"},{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"4288d44c2b8e6a89607780caf1272061028f6f97","title":"On the Advance of Making Language Models Better Reasoners"},{"paperId":"4e0e5eb19f97ee19f05420b89c3e447bf297b8df","title":"Fault-Aware Neural Code Rankers"},{"paperId":"0d08ffccc982781e310bb184397bbe64b9aef157","title":"Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"79b88230fabb59a1d368641bbc822af0f09bf262","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"eadcf2f484b2d326f4d32ba4a897b009e4de1784","title":"Pynguin: Automated Unit Test Generation for Python"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"c783e1fb3ce8514f981925ee590c00884660ee4e","title":"CM3: A Causal Masked Multimodal Model of the Internet"},{"paperId":"166b64f2ae8e52f5779682fab756cbd617a6e74b","title":"A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level"},{"paperId":"1aed58bd07026492194672adec494dc37c894a28","title":"Leveraging Automated Unit Tests for Unsupervised Code Translation"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"4698fc4712f0212c8a3810fd67b41ee8b8896aba","title":"Generate & Rank: A Multi-task Framework for Math Word Problems"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"5ff9032d0f7f246d01ae7b2c231ab03469a7344a","title":"Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax"},{"paperId":"15a60130ea9cb4a00e0dac6d11a368894974cedf","title":"Unit Test Case Generation with Transformers and Focal Context"},{"paperId":"74998707fd7d2886ef53206bd3c3c7536d2f3a94","title":"BiRank: Fast and Flexible Ranking on Bipartite Networks with R and Python"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"1fa9ed2bea208511ae698a967875e943049f16b6","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"f669b5406320a83e94b4dd439806211c8b1e95fc","title":"Write, Execute, Assess: Program Synthesis with a REPL"},{"paperId":"6c41bedc4637f3fd504c68baa3b3d8881e056ac1","title":"Execution-Guided Neural Program Synthesis"},{"paperId":"0943d656221144bf063a538f6c667ac006c303cd","title":"Automated Test Case Generation as a Many-Objective Optimisation Problem with Dynamic Selection of the Targets"},{"paperId":"b28abc70eee3a5b628d20ba7ffa96dd54c29beb6","title":"Many Independent Objective (MIO) Algorithm for Test Suite Generation"},{"paperId":"728f470d67c5b2d6e3ea1fa13b3b3a23545b3f65","title":"BiRank: Towards Ranking on Bipartite Graphs"},{"paperId":"07315439e66dd8c6709a1834cb15a673ab9a19bc","title":"Reformulating Branch Coverage as a Many-Objective Optimization Problem"},{"paperId":"216b98bb3d9221d5f5d261864975612e4d0faaa6","title":"EvoSuite: automatic test suite generation for object-oriented software"},{"paperId":"6dc7017e357767dbc210bcb27080260fb0816701","title":"Feedback-Directed Random Test Generation"},{"paperId":"058861dd838ddf4a6a36860f54e82e11f8945b32","title":"Authoritative sources in a hyperlinked environment"},{"paperId":"4f37468a95ccc62debb9e5a4cb0d73489ca61190","title":"Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography"},{"paperId":null,"title":"Natural language processing with transformers . ” O ’ Reilly Media , Inc . ” , 2022 . Ben Wang and Aran Komatsuzaki"}],"id":"876eb375cb7b365475040046df669c039ad54202","summary":"A novel method, C ODE T, leverages the same pre-trained language models to test cases for the code samples, thus reducing the human effort and increasing the coverage of the test scenarios, achieving remarkable and consistent gains across different models and benchmarks."},{"url":"https://www.semanticscholar.org/paper/780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation","venue":"","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/08/2022","authors":"Federico Cassano,John Gouwar,Daniel Nguyen,S. Nguyen,Luna Phipps-Costin,Donald Pinckney,Ming-Ho Yee,Yangtian Zi,Carolyn Jane Anderson,Molly Q. Feldman,Arjun Guha,M. Greenberg,Abhinav Jangda","citations":[],"references":[{"paperId":"20e4ca6368a0731d3cbbd2e68aa90cef7384dc72","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"1a903282f7c19dbdb2714b852fb42dbb4675422b","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"c96363c42bc8c465902c22b8c33c8704233f519e","title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"c783e1fb3ce8514f981925ee590c00884660ee4e","title":"CM3: A Causal Masked Multimodal Model of the Internet"},{"paperId":"166b64f2ae8e52f5779682fab756cbd617a6e74b","title":"A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level"},{"paperId":"775a9c722262c7b656876a5fef20f4577afd8981","title":"Multilingual training for Software Engineering"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":null,"title":"Natural Language Processing with Transformers"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"0df347f5e3118fac7c351917e3a497899b071d1e","title":"Datasheets for datasets"},{"paperId":"b48be6bdd1c59ae965144fc8e449cbde10941d48","title":"Neurosymbolic Programming"},{"paperId":null,"title":"Openai codex"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"15a60130ea9cb4a00e0dac6d11a368894974cedf","title":"Unit Test Case Generation with Transformers and Focal Context"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"29fc7ff02e68537a86d32978bfd2e8e45258e77d","title":"LambdaNet: Probabilistic Type Inference using Graph Neural Networks"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"a429c6c8407b10a89c9610ab5e9682ec89772356","title":"TypeWriter: neural type prediction with search-based validation"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"ea1b62a03ab16c4acca6f7041c6f096bedec244b","title":"Deep learning type inference"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"116a877c774969b53399b1ccaa34d51bfb492ee4","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"9b536c7b0f45d4b512f337d0acda09ca3e4cd953","title":"Fitting Linear Mixed-Effects Models Using lme4"},{"paperId":"f7ba81057c8ec4f1c039027232034143a5afe6bf","title":"Syntax-guided synthesis"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"2fb9729c0a3b046c390ad6748773527eae6cfc84","title":"What Is Program Synthesis?"}],"id":"780f7eebde16b1ae5843df3a79a7772899ef6a71","summary":"This work creates the first massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages and evaluates the multi-language performance of three state-of-the-art code generation models."},{"url":"https://www.semanticscholar.org/paper/0b340dd78fd04bbde2807d5efedb796d319355e3","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Jean-Baptiste Döderlein,M. Acher,D. Khelladi,B. Combemale","citations":[],"references":[{"paperId":"7d180ff2a86d259794aa85466d87c215d86b5c92","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"34503c0b6a615124eaf82cb0e4a1dab2866e8980","title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"},{"paperId":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code"},{"paperId":"f75a0ccb3c9b7e12dfb8b9c9dc3d35ca4518a643","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"cdfe9580f63070f311151444f9df32818cc858bf","title":"An Empirical Evaluation of GitHub Copilot's Code Suggestions"},{"paperId":"c5b2dc81baeaba0595c6e8a2ed84a42ce706cab5","title":"Improving Machine Translation Systems via Isotopic Replacement"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"a6b9a934fe039a5636a26e94fb47f872263d702c","title":"To What Extent do Deep Learning-based Code Recommenders Generate Predictions by Cloning Code from the Training Set?"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"79b88230fabb59a1d368641bbc822af0f09bf262","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"80d0116d77beeded0c23cf48946d9d10d4faee14","title":"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"},{"paperId":"8417d83e74a63b9d3874cd7695477ebad8b98f8a","title":"Choose your programming copilot: a comparison of the program synthesis performance of github copilot and genetic programming"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"},{"paperId":"0adec918885dff698acf359988ed79a543157f80","title":"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity"},{"paperId":"f7987fa2aadc0b368c185dc4d2fdb1337a202c32","title":"Solving Probability and Statistics Problems by Program Synthesis"},{"paperId":"4e40595d6ecba027cebb4f2e3b43ae44bcf51daf","title":"Solving Linear Algebra by Program Synthesis"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"ac3cdb50606f7770eef8e4cd951840a4f71287a0","title":"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"399e7d8129c60818ee208f236c8dda17e876d21f","title":"RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85","title":"How Can We Know What Language Models Know?"},{"paperId":"93d63ec754f29fa22572615320afe0521f7ec66d","title":"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"5082a1a13daea5c7026706738f8528391a1e6d59","title":"A Neural Attention Model for Abstractive Sentence Summarization"},{"paperId":"80864f25239fa181e38c5293a7f1b76dc3396ea9","title":"A Survey on Document Clustering with Similarity Measures"},{"paperId":null,"title":"Auto- Prompt: Eliciting knowledge from language models with automatically generated prompts"},{"paperId":null,"title":"Research recitation"},{"paperId":null,"title":"GitHub copilot · your AI pair programmer"},{"paperId":null,"title":"IntelliSense in visual studio code"},{"paperId":null,"title":"Natural language processing with transformers"},{"paperId":null,"title":"Large language models are human - level prompt engineers , ” in Submitted to The Eleventh International Conference on Learning Representations , 2023 , under review"}],"id":"0b340dd78fd04bbde2807d5efedb796d319355e3","summary":"Investigation of the various input parameters of two language models shows that varying the input parameters can improve the performance of language models, but there is a tight dependency when varying the temperature, the prompt and the number of generated solutions, making potentially hard to properly control the parameters to obtain an optimal result."},{"url":"https://www.semanticscholar.org/paper/cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":1,"influentialCitationCount":0,"publicationDate":"29/10/2022","authors":"Victor C. Dibia,Adam Fourney,Gagan Bansal,Forough Poursabzi-Sangdeh,Han Liu,Saleema Amershi","citations":[{"paperId":"9431181f8115a2360621df5ed76e1a23b88e3b2f","title":"Evaluating Human-Language Model Interaction"}],"references":[{"paperId":"a38af38baeb1b5e25c089b699ab5072823ae6b4c","title":"The Fallacy of AI Functionality"},{"paperId":"4c59d10c24cd1948a6e88174564b5766b4e5c530","title":"Deconstructing NLG Evaluation: Evaluation Practices, Assumptions, and Their Implications"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"2fc583fbd1df9be20580db003d26d995b95f4eec","title":"Reliance on metrics is a fundamental challenge for AI"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":null,"title":"Quantifying GitHub Copilot’s impact on developer productivity and happiness"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"2a36402451c4fcab33c9e7f59fc027167c070aa6","title":"Perfection Not Required? Human-AI Partnerships in Code Translation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"681f4fbce872f138cbac9cdd92e8f6ed89ba6f8d","title":"Measurement and Fairness"},{"paperId":"28797c288403828f29d3faca6ab9d64d134a4ea3","title":"The SPACE of Developer Productivity: There's more to it than you think"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"868207797e69df5055f5c3fd4aa78a33e5a7ca45","title":"Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":null,"title":"Beyond accuracy: Grounding evaluation metrics for human-machine learning systems"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"d02285385fd2b53a6100b8d62421bba6693b788c","title":"When Code Completion Fails: A Case Study on Real-World Completions"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"71bb158c45d80da5b4408321f79a47789f9663ad","title":"Will They Like This? Evaluating Code Contributions with Language Models"},{"paperId":null,"title":"TensorFlow: Large-scale machine learning on heterogeneous systems"},{"paperId":"93ff001eb7ddd019c107879943126c74a973993b","title":"Learning natural coding conventions"},{"paperId":"d12fbc23cff452074a286b099cd475fdd3dcd91a","title":"Classifier Technology and the Illusion of Progress"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"cba98048f3e85a974c287b271692bf6c197db940","summary":"A simple hybrid metric is proposed, which combines functional correctness and similarity- based metrics to capture different dimensions of what programmers might value and shows that this hybrid metric more accurately captures effort."},{"url":"https://www.semanticscholar.org/paper/8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation","venue":"ArXiv","year":2022,"referenceCount":30,"citationCount":5,"influentialCitationCount":2,"publicationDate":"18/11/2022","authors":"Yuhang Lai,Chengxi Li,Yiming Wang,Tianyi Zhang,Ruiqi Zhong,Luke Zettlemoyer,S. Yih,Daniel Fried,Si-yi Wang,Tao Yu","citations":[{"paperId":"b2f256a1ec54325102119a57cb2359840fe68586","title":"SantaCoder: don't reach for the stars!"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation"}],"references":[{"paperId":"ec4c8d99eb1c028c43af6d8bbf727392d351cb59","title":"Efficient Training of Language Models to Fill in the Middle"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"c783e1fb3ce8514f981925ee590c00884660ee4e","title":"CM3: A Causal Masked Multimodal Model of the Internet"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"f40aeae3e522ada1f6a9f326841b01ef5c8657b6","title":"Unifying Language Learning Paradigms"},{"paperId":"5fbcfccd3736969d95ed660d8e6962c86b7a9113","title":"PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"56fa0b9cba4d9aee5ccc327365b3b3a721031c69","title":"Calibrate Before Use: Improving Few-Shot Performance of Language Models"},{"paperId":"ed2a639dd4a6b3abdcc0040733a264cb1dbcd7d4","title":"PlotCoder: Hierarchical Decoding for Synthesizing Visualization Code in Programmatic Context"},{"paperId":"e6e8a2c56243847b77b604259cde9e10a2daccb8","title":"Semantic Evaluation for Text-to-SQL with Distilled Test Suite"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"15a60130ea9cb4a00e0dac6d11a368894974cedf","title":"Unit Test Case Generation with Transformers and Focal Context"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"133bcd7488a3c07cb0f493a87564c30e5433768c","title":"Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"f3318491a55590e00dfe45d68708f515822e343a","title":"Cosette: An Automated Prover for SQL"},{"paperId":"9ba08d45d60130c7e5880f63a980b185a86e177c","title":"A Big Data Guide to Understanding Climate Change: The Case for Theory-Guided Data Science"},{"paperId":"b29447ba499507a259ae9d8f685d60cc1597d7d3","title":"Semantic Parsing on Freebase from Question-Answer Pairs"},{"paperId":"c73b0424e1a4ab2574cfce2e41c505f71f46940e","title":"Data mining in education"},{"paperId":null,"title":"Learning DependencyBased Compositional Semantics"},{"paperId":"774113732db34ce0b797fc3dcceded811fb6edbc","title":"Online Learning of Relaxed CCG Grammars for Parsing to Logical Form"},{"paperId":"b7c0e47f8b768258b7d536c21b218e6c46ab8791","title":"Learning to Parse Database Queries Using Inductive Logic Programming"},{"paperId":null,"title":"Reference SoluHon # df: pd.DataFrame as input result = df.replace(df"}],"id":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","summary":"This work introduces DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas, and proactively defends against memorization by slightly modifying the problems to be different from the original StackOverﬂow source."},{"url":"https://www.semanticscholar.org/paper/815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks","venue":"ArXiv","year":2022,"referenceCount":60,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Pengcheng Yin,Wen-Ding Li,Kefan Xiao,A. Rao,Yeming Wen,Kensen Shi,Joshua Howland,Paige Bailey,Michele Catasta,H. Michalewski,Alex Polozov,Charles Sutton","citations":[{"paperId":"239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning"}],"references":[{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"ce47e99c1ebab6fc253b1be58bd9478a87d90288","title":"PAL: Program-aided Language Models"},{"paperId":"e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models"},{"paperId":"4af2891ce1aab624c4917e8a69fcee5c8a1f41db","title":"NL2Viz: natural language to visualization via constrained syntax-guided synthesis"},{"paperId":"c140fe515de2f20d0c85c813c7b3ec1defc41f9d","title":"Binding Language Models in Symbolic Languages"},{"paperId":"f75a0ccb3c9b7e12dfb8b9c9dc3d35ca4518a643","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"79b88230fabb59a1d368641bbc822af0f09bf262","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"51000d9f79be0eefd7972fe94e3c71dddc90d2c6","title":"Evaluating the Text-to-SQL Capabilities of Large Language Models"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"5d0db797a45ce2453f821f7ded0b547d3fdab054","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"53c0abe83fe9b4fdaf2208295d8504fcf5241694","title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models"},{"paperId":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis"},{"paperId":"fe5cb4375f213abdbd34d25b02dc7e48794d286d","title":"AutoML to Date and Beyond: Challenges and Opportunities"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":null,"title":"We also report training data composition in Tab. 7. Motivation For what purpose was the dataset created? Who created the dataset? Who funded the creation"},{"paperId":null,"title":"Neuralsymbolic inference for robust autoregressive graph parsing via compositional uncertainty quantification"},{"paperId":null,"title":"Natural language processing with transformers"},{"paperId":null,"title":"2022a. Documentation matters: Human-centered ai system to assist data science code documentation"},{"paperId":"5436193122dff271796bca07df7cecb7a8d6dea6","title":"Natural language-guided programming"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"9cd3d6eef7c574830be410598c3024191ee974d4","title":"KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers"},{"paperId":"84b26030b648b6d79177bdafd3e896b1dda9f91e","title":"Towards Robustness of Text-to-SQL Models against Synonym Substitution"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"d377e9a167996d18eaaa35786cfcadecbc8b241e","title":"Reactive, reproducible, collaborative: computational notebooks evolve."},{"paperId":"66c10bf1f11bc1b2d92204d8f8391d087f6de1c4","title":"RoFormer: Enhanced Transformer with Rotary Position Embedding"},{"paperId":"7282f5c9d84cd47c516a6a66c5a6b8f1e2cf44b6","title":"AutoDS: Towards Human-Centered Automation of Data Science"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"5593676873d799a4727123a2cbffb231d3b4eb80","title":"NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries"},{"paperId":"3a7fa673ff8ec4ec2f322473de005f3cd09ea820","title":"AutoML: A Survey of the State-of-the-Art"},{"paperId":"330b5844d170b6b77f5f9fa4c2024150cef2af18","title":"Benchmark and Survey of Automated Machine Learning Frameworks"},{"paperId":"ed2a639dd4a6b3abdcc0040733a264cb1dbcd7d4","title":"PlotCoder: Hierarchical Decoding for Synthesizing Visualization Code in Programmatic Context"},{"paperId":null,"title":"2021b. How much automation does a data scientist want? arXiv preprint arXiv:2101.03970"},{"paperId":null,"title":"GPT-J6B: A 6 Billion Parameter Autoregressive Language Model"},{"paperId":null,"title":"GPT-Neo: Large Scale"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"72e0344605bde4707d24d2f9b0b4e1e0aa953604","title":"Quda: Natural Language Queries for Visual Data Analytics"},{"paperId":"8f8532a193313b9b956a8df402dd7f879bbe1377","title":"Data Engineering for Data Analytics: A Classification of the Issues, and Case Studies"},{"paperId":"4e80a42d330331d7b7e88f39ccc802fe6656ac5a","title":"How can AI Automate End-to-End Data Science?"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"545e60873b0fad25407bb6d647f92c905bd2483d","title":"CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases"},{"paperId":"f79af5e9e96f9c777e8759d791e33e9da83ffa65","title":"SParC: Cross-Domain Semantic Parsing in Context"},{"paperId":"30228f5e3ecd19452a2a5388b23086569e6233f4","title":"A Large-Scale Study About Quality and Reproducibility of Jupyter Notebooks"},{"paperId":"2012c176d7b003eb57a282bfd8681190704fb965","title":"Learning to Map Context-Dependent Sentences to Executable Formal Queries"},{"paperId":"f56425ec56586dcfd2694ab83643e9e76f314e91","title":"50 Years of Data Science"},{"paperId":"8ff54aa8045b1e30c348cf2ca42259c946cd7a9e","title":"Search-based Neural Structured Learning for Sequential Question Answering"},{"paperId":"554c9b3aa8c3b7504810270b18b942a0d22156e7","title":"DeepFix: Fixing Common C Language Errors by Deep Learning"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"7306437b2145677fe7bf3b7711ac8aa25989f1e3","title":"Simpler Context-Dependent Logical Forms via Model Projections"},{"paperId":"722e01d5ba05083f7a091f3188cfdfcf183a325d","title":"Larger-Context Language Modelling with Recurrent Neural Network"},{"paperId":"775a4e375cc79b53b94e37fa3eedff481823e4a6","title":"Efficient and Robust Automated Machine Learning"},{"paperId":"b41e95c8c97846d5ca4c11ef79d7814499cc9663","title":"Compositional Semantic Parsing on Semi-Structured Tables"},{"paperId":"4c67851b77766ba4ad9f1ac0bd4c9491c327574e","title":"Wrangler: interactive visual specification of data transformation scripts"},{"paperId":"07216ee1119f61b351b69e94b2e7c3698d96b026","title":"Learning Context-Dependent Mappings from Sentences to Logical Form"},{"paperId":"180e548520d2091beb8bb039473bd542e7de5aec","title":"Low-level components of analytic activity in information visualization"}],"id":"815c6ca281536d18ec0eb408b6e46e72a0826163","summary":"P A C H - I NC O, a 62B code language model for Python computational notebooks, which outperforms public code LMs and explores few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions."},{"url":"https://www.semanticscholar.org/paper/433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Yangruibo Ding,Zijian Wang,Wasi Uddin Ahmad,M. Ramanathan,Ramesh Nallapati,Parminder Bhatia,D. Roth,Bing Xiang","citations":[],"references":[{"paperId":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"1a903282f7c19dbdb2714b852fb42dbb4675422b","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"4b27f18bff43d605805c92696a979714ced0b805","title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"467306672c89d4a0a7c6bc733814605c53bbfa97","title":"Towards Learning (Dis)-Similarity of Source Code from Program Contrasts"},{"paperId":"5fde5c44197473ad2ac0645f643e577188b316c4","title":"Can Machines Read Coding Manuals Yet? - A Benchmark for Building Better Language Models for Code Understanding"},{"paperId":"b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"91670df8aae26546da4fac58599e6ecc14708776","title":"CoCoSum: Contextual Code Summarization with Multi-Relational Graph Neural Network"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"6714dff86284cbde6be351e3e1f8ddee1bfadb9c","title":"A Toolkit for Generating Code Knowledge Graphs"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"00549af4bc3270e0f688acbf694f912d7ee39cad","title":"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks"},{"paperId":"516497e7fc39ebb126bed28dcc5c6129e500acb8","title":"An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"07c4549be429a52274bc0ec083bf5598a3e5c365","title":"Modeling and Discovering Vulnerabilities with Code Property Graphs"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"346ccc20d5b8b8dec2a627af4d15a49db92d2f4e","title":"Using structural context to recommend source code examples"},{"paperId":"289baa69e259aee340352eb71831f1f81c3c47f2","title":"Automatic method completion"},{"paperId":"be51aaa0f8a5a12274082e9d554d736373405a08","title":"Component rank: relative significance rank for software component search"},{"paperId":"fc041feb5271efca959d0f54722338d419330cf0","title":"Hipikat: recommending pertinent software development artifacts"},{"paperId":"90e9139cc21dc074767e0f8927f73a597dd1fd3d","title":"Supporting reuse by delivering task-relevant and personalized information"},{"paperId":"1bc13b40746ccc95967746b73e11d837e3425154","title":"The structure and value of modularity in software design"},{"paperId":"aaaacfab112e465baebb4648a6808fe821f67e88","title":"CodeWeb: data mining library reuse patterns"},{"paperId":"8ced2d5d52413c2104b808c7bbc751c611daa75a","title":"Integrating active information delivery and reuse repository systems"},{"paperId":"a9e14144a391131c849b5496f04698fd71646843","title":"The reuse of uses in Smalltalk programming"},{"paperId":"f7376b069da8e403f0f8ebd54d02a8c44ff89aad","title":"Retrieving software objects in an example-based programming environment"},{"paperId":"ece80f049a527954af1c153d61cafee5789c2afe","title":"The program dependence graph and its use in optimization"},{"paperId":"3dfa629816bae8a681cc47de6a737abf8858f4fe","title":"The Modular Structure of Complex Systems"},{"paperId":"877e314d3a9f9317c162309c9ee0c660878a4bdb","title":"On the criteria to be used in decomposing systems into modules"}],"id":"433def684b5a9de5a9163f50b9004a44a11128b1","summary":"A framework that incorporates cross-file context to learn the in-file and cross- file context jointly on top of pretrained code LMs is proposed, COCOMIC, which successfully improves the existing code LM with a 19.30% relative increase in exact match and a 15.41%relative increase in identifier matching for code completion when the cross-line context is provided."},{"url":"https://www.semanticscholar.org/paper/20e4ca6368a0731d3cbbd2e68aa90cef7384dc72","title":"Multi-lingual Evaluation of Code Generation Models","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":4,"influentialCitationCount":2,"publicationDate":"26/10/2022","authors":"Ben Athiwaratkun,Sanjay Krishna Gouda,Zijian Wang,Xiaopeng Li,Yuchen Tian,Ming Tan,Wasi Uddin Ahmad,Shiqi Wang,Qing Sun,Mingyue Shang,Sujan Kumar Gonugondla,Hantian Ding,Varun Kumar,Nathan Fulton,A. Farahani,Siddharth Jain,Robert Giaquinto,Haifeng Qian,M. Ramanathan,Ramesh Nallapati,Baishakhi Ray,Parminder Bhatia,Sudipta Sengupta,D. Roth,Bing Xiang","citations":[{"paperId":"b2f256a1ec54325102119a57cb2359840fe68586","title":"SantaCoder: don't reach for the stars!"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning"}],"references":[{"paperId":"7018ff49ca3b81f2ed6228b097a471c2529986e4","title":"CoditT5: Pretraining for Source Code and Natural Language Editing"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"011a4019aa0d0ce3edfa56bb2ca1e7586eb43fb2","title":"Training Compute-Optimal Large Language Models"},{"paperId":"c96363c42bc8c465902c22b8c33c8704233f519e","title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"c6801d553a43530b192309ef4364a43e33e4067f","title":"Data augmentation using back-translation for context-aware neural machine translation"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"8323c591e119eb09b28b29fd6c7bc76bd889df7a","title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"7d41dbfa22eb5a425fc0ff27db41aaeb75b1201c","title":"Introducing MathQA - A Math-Aware Question Answering System"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"554c9b3aa8c3b7504810270b18b942a0d22156e7","title":"DeepFix: Fixing Common C Language Errors by Deep Learning"},{"paperId":null,"title":"Attention is all you need. CoRR, abs/1706.03762"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"4755b856dc08ac024ae935e7a6f9df325b00ae53","title":"Phrase-Based Statistical Translation of Programming Languages"},{"paperId":null,"title":"You are an expert Ruby programmer, and here is your task"},{"paperId":null,"title":"Exception --test case 0 did not pass"},{"paperId":null,"title":"12)){} else { throw 'Error at 2th assert statement. Value = ' + JSON.stringify(x ) }"},{"paperId":null,"title":"18 for (let j = 0; j <= n; j++) { 19 dp"},{"paperId":null,"title":"Error at 3th assert statement"},{"paperId":null,"title":"* Write a function to find the minimum cost path to reach (m, n) from (0, 0) for the given cost matrix cost"},{"paperId":null,"title":"28 var arg01 : Int = 2 29 var arg02 : Int = 2 30 var x0 : Int = minCost(cost : arg00, m : arg01, n : arg02) 31 var v0 : Int = 8 32 assert(x0 == v0"}],"id":"20e4ca6368a0731d3cbbd2e68aa90cef7384dc72","summary":"This work presents MBXP, an execution-based code completion benchmark in 10+ programming languages that is able to evaluate code generation models in a multi-lingual fashion, and discovers generalization ability of language models on out-of-domain languages, advantages of large multi-lingsual models over mono-lingUAL, benefits of few-shot prompting, and zero-shot translation abilities."},{"url":"https://www.semanticscholar.org/paper/7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":45,"citationCount":66,"influentialCitationCount":21,"publicationDate":"25/05/2021","authors":"Ruchi Puri,David S. Kung,G. Janssen,Wei Zhang,Giacomo Domeniconi,Vladmir Zolotov,Julian Dolby,Jie Chen,M. Choudhury,Lindsey Decker,Veronika Thost,Luca Buratti,Saurabh Pujar,Ulrich Finkler","citations":[{"paperId":"d8405996b4d08c304098636aedd9e1c1a1e262ee","title":"The Premature Obituary of Programming"},{"paperId":"bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence"},{"paperId":"42630c03d3817b1153d245f20742ad4b30a80b75","title":"JEMMA: An Extensible Java Dataset for ML4Code Applications"},{"paperId":"9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming"},{"paperId":"00aacec39159bcd92a412aa314b376c3378c49cb","title":"Improvement of Vulnerable Code Dataset Based on Program Equivalence Transformation"},{"paperId":"20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"41f5e1ad7793593befc0b9c38f756836e8b07c98","title":"Code4ML: a Large-scale Dataset of annotated Machine Learning Code"},{"paperId":"0b340dd78fd04bbde2807d5efedb796d319355e3","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?"},{"paperId":"8ffe7b7eeddbd4c22d642b0a48379d17e61c3bab","title":"Soft-Labeled Contrastive Pre-training for Function-level Code Representation"},{"paperId":"8b58130ecb302a2f0e78e9ffb7115cb4906cb966","title":"Towards Robust Models of Code via Energy-Based Learning on Auxiliary Datasets"},{"paperId":"05f225085154e4326af07f7c8f273156f132aa70","title":"Geração Automática de Benchmarks para Compilação Preditiva"},{"paperId":"0b8772b7790c69f40897b5eb7f8fd57f24138f3d","title":"ContraGen: Effective Contrastive Learning For Causal Language Model"},{"paperId":"363758e9e296adc9391ed731e834809cf5d4c19b","title":"Are machine programming systems using right source-code measures to select code repositories?"},{"paperId":"939b4b1ff5a21108bb2f8c81117f1d5b230180a9","title":"Extreme Multi-Domain, Multi-Task Learning With Unified Text-to-Text Transfer Transformers"},{"paperId":"a7435722d8ab595da5a9c70ac9160f57d0dcd75a","title":"Enabling Transformers to Understand Low-Level Programs"},{"paperId":"89b58765614bd6c52baca0006d67f64985d2204e","title":"Topical: Learning Repository Embeddings from Source Code using Attention"},{"paperId":"da78bab10019e530a93584f60b9224e353d90f2a","title":"A Tree-structured Transformer for Program Representation Learning"},{"paperId":"1444ed03083523a4413d9f15f2200007447771db","title":"A Library for Representing Python Programs as Graphs for Machine Learning"},{"paperId":"713bd2971116098211ef06336dfbe91a69854404","title":"Probing Semantic Grounding in Language Models of Code with Representational Similarity Analysis"},{"paperId":"8bf89cf8f18f08a43fd3d058687987666996b995","title":"PST: Measuring Skill Proficiency in Programming Exercise Process via Programming Skill Tracing"},{"paperId":"52ca0f589fdcde1c45fd6dfb1b72248d4ecaefc0","title":"Code Translation with Compiler Representations"},{"paperId":"62851a515ea0ee3a547d94e8a493d978c22d0be9","title":"Multilingual Code Snippets Training for Program Translation"},{"paperId":"2edc8efcda27c944a46f367acf6a5280b8f65525","title":"FixEval: Execution-based Evaluation of Program Fixes for Competitive Programming Problems"},{"paperId":"c6e6cb19e7055f3d0616c3314a85b0914132ae40","title":"CodeS: A Distribution Shift Benchmark Dataset for Source Code Learning"},{"paperId":"1100dee3fd78655cddc4b7bfaef1161351d4fab5","title":"Automated Feedback Generation for Competition-Level Code"},{"paperId":"5514b87e34db2b34bd9a9b995894243f91435efc","title":"Learning to Represent Programs with Code Hierarchies"},{"paperId":"6fb1a9d5278b85dfbbb0be3731d480ab36a0372e","title":"CodeAttack: Code-based Adversarial Attacks for Pre-Trained Programming Language Models"},{"paperId":"adce1da47d490dcdca254ccd43055ed4f4423bc2","title":"Summarize and Generate to Back-translate: Unsupervised Translation of Programming Languages"},{"paperId":"0bcd59da541fdae66884afba8d25475a54a9da1a","title":"Automated Repair of Programs from Large Language Models"},{"paperId":"ea4749c6ed5f08b16bab31b1ca4fd3fdc259ae8f","title":"Improving automatically generated code from Codex via Automated Program Repair"},{"paperId":"c1ddf0006e1aa0d5551e1ba1ad734ec0ecf27fd0","title":"CV4Code: Sourcecode Understanding via Visual Code Representations"},{"paperId":"0121c151c96b32cd7851e4bcda2a468b279c1e6f","title":"CODE-MVP: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training"},{"paperId":"703f79763d534dbf9674132cec890f432dcc19ec","title":"A Survey of Deep Learning Models for Structural Code Understanding"},{"paperId":"80b2b006ed2f26ec3ddc91e303dc9861fb456a26","title":"On The Cross-Modal Transfer from Natural Language to Code through Adapter Modules"},{"paperId":"c765091cb8bec8448669351f3662101c307c03c4","title":"Zero-Shot Program Representation Learning"},{"paperId":"1413acc991434ee36248b282b4cedac77ade1737","title":"Evaluating few shot and Contrastive learning Methods for Code Clone Detection"},{"paperId":"5e5a7f8423e0b990bbe1c85a999da86f16ee68a3","title":"LaF: Labeling-Free Model Selection for Automated Deep Neural Network Reusing"},{"paperId":"8c8bf30828bc789be679f29ee08cc6cdebd36600","title":"Characterizing and Understanding the Behavior of Quantized Models for Reliable Deployment"},{"paperId":"2b556fcf2ac634f03c1fb0ace5e602e829418e65","title":"ReACC: A Retrieval-Augmented Code Completion Framework"},{"paperId":"4b27f18bff43d605805c92696a979714ced0b805","title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation"},{"paperId":"c125b0be73c8493ebc27beb572f6c1b21d6b4ae4","title":"Static Prediction of Runtime Errors by Learning to Execute Programs with External Resource Descriptions"},{"paperId":"7438626a757c5442b9c0fb37b54ec0fe7e1889c3","title":"Better Together? An Evaluation of AI-Supported Code Translation"},{"paperId":"76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","title":"Investigating Explainability of Generative AI for Code through Scenario-based Design"},{"paperId":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"9f0852ce9338c00135fe39426d893a36a289e5d5","title":"GraphCode2Vec: Generic Code Embedding via Lexical and Program Dependence Analyses"},{"paperId":"b2c0e903b79835b6ee8fd553c2213ea8abbf7864","title":"Senatus - A Fast and Accurate Code-to-Code Recommendation Engine"},{"paperId":"cc80244d69c2c5bb7fe79fc6ca2561f2bed830ca","title":"Learning to Represent Programs with Heterogeneous Graphs"},{"paperId":"7d500d6bd3ae49fa3acb213fd25d5b11566e64fd","title":"Labeling-Free Comparison Testing of Deep Learning Models"},{"paperId":"6f84d0cc33c7c58f74b28ddcc1cbda91ea608c9f","title":"C ODE S UMMARIZATION : D O T RANSFORMERS R E ALLY U C"},{"paperId":"b98d6fe8f0ef02ec0d1bb2bcfb924c8f01feb7d4","title":"Convergent Representations of Computer Programs in Human and Artificial Neural Networks"},{"paperId":"b2c4fdb49bdb23e6ad0ac3272029324157046ea7","title":"Automated Repair of Code from Language Models"},{"paperId":"56e6d62c638a24411f12d15cdc8821a31fc495c8","title":"Source Code Generation from Descriptions in a Natural Language"},{"paperId":"ddab94478a7647ee136b1f6b5076417db3074d0f","title":"Machine Programming: Turning Data into Programmer Productivity"},{"paperId":"3b0cf543a730e674d4213d344ebc857fada76ead","title":"Understanding High-Level Properties of Low-Level Programs Through Transformers"},{"paperId":"d4f16dae4ab1d21db3c0b0bd7b54f4b62e2d1b85","title":"The Effectiveness of Transformer Models for Analyzing Low-Level Programs"},{"paperId":"eeadabea580953c14bb00ca99b41ee9b2cef6300","title":"Energy-bounded Learning for Robust Models of Code"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"7fba3100768fa3aac0fbf961d5e894b2f629e6e6","title":"Federated Data Science to Break Down Silos [Vision]"},{"paperId":"4e7de32c8da8c910285acdaf397347dc94ca3594","title":"Many Heads but One Brain: Fusion Brain -- a Competition and a Single Multimodal Multitask Architecture"},{"paperId":"58b142663367ef6ed67507e3d7591b6e384a6937","title":"Deep Distilling: automated code generation using explainable deep learning"},{"paperId":"21363c1ac138d8df80b20af4848b5113bd3bf6f8","title":"Clone-advisor: recommending code tokens and clone methods with deep learning and information retrieval"},{"paperId":"0c7cb854756f6b69f070a925cd497c2970b136f2","title":"DeSkew-LSH based Code-to-Code Recommendation Engine"},{"paperId":"4885e616e85d420576196b2578525cbc501137ec","title":"Programming and execution models for next generation code intelligence systems (keynote)"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"fd0bccf5e7c1fb5dadd75972e3212554fb255fe2","title":"MISIM: A Neural Code Semantics Similarity System Using the Context-Aware Semantics Structure"}],"references":[{"paperId":"bea6af010fc02f5ac29edfc17096be6078edab46","title":"A Survey on Deep Learning for Software Engineering"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"a989f5daa7c438d6b8433efbb1461f1b0af1aa8e","title":"Break-It-Fix-It: Unsupervised Learning for Program Repair"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"20d37eb44ad65735f243938961fde9ba5b4d26b7","title":"D2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using Differential Analysis"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"c6337dc83db09c9648ae850c71937eb8e5fd7a43","title":"Directed Acyclic Graph Neural Networks"},{"paperId":"59cc0871534a20ac98db0663539c6675ab63566f","title":"SequenceR: Sequence-to-Sequence Learning for End-to-End Program Repair"},{"paperId":"406afe68e789fcb0d7e3d24ebea65b53d206f740","title":"Exploring Software Naturalness through Neural Language Models"},{"paperId":"0d93d1943ebd0372d46ec1bdff4cb27c44a237a2","title":"MISIM: A Novel Code Similarity System."},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"597bd2e45427563cdf025e53a3239006aa364cfc","title":"Open Graph Benchmark: Datasets for Machine Learning on Graphs"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":null,"title":"A metric learning reality"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"00549af4bc3270e0f688acbf694f912d7ee39cad","title":"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks"},{"paperId":"311caedb65fff281470100585d90f2c672a9f97f","title":"Cyber Security Threats Detection in Internet of Things Using Deep Learning Approach"},{"paperId":"3b99ae042727b7bf679a8e4120b28999d6530ff6","title":"Graph Matching Networks for Learning the Similarity of Graph Structured Objects"},{"paperId":"63a513832f56addb67be81a2fa399b233f3030fc","title":"Fast Graph Representation Learning with PyTorch Geometric"},{"paperId":"516497e7fc39ebb126bed28dcc5c6129e500acb8","title":"An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"def6c6912c0120b40b7fefb24aa68708bb357d50","title":"Aroma: code recommendation via structural code search"},{"paperId":"62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9","title":"How Powerful are Graph Neural Networks?"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"41c8cabe475d85d7548c25935da19ea5b96dfe06","title":"Neural Code Comprehension: A Learnable Representation of Code Semantics"},{"paperId":"6989e13df80edfc6e638e8d8502cb0739d494ca6","title":"Machine Learning in Compiler Optimization"},{"paperId":"6c6170ffb39cdc8cfffbeda9c7a2259eda5875f2","title":"Tree-to-tree Neural Networks for Program Translation"},{"paperId":"36652428740cd30d245d55889f01a7fb04a91c93","title":"Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning"},{"paperId":"e3d772986d176057aca2f5e3eb783da53b559134","title":"Unsupervised Machine Translation Using Monolingual Corpora Only"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"554c9b3aa8c3b7504810270b18b942a0d22156e7","title":"DeepFix: Fixing Common C Language Errors by Deep Learning"},{"paperId":"36eff562f65125511b5dfab68ce7f7a943c27478","title":"Semi-Supervised Classification with Graph Convolutional Networks"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"993d2f3c404f1f7af879ee1afddf09f943b01508","title":"Large-Scale Code Clone Detection"},{"paperId":"a85012088ae447fca09c0642cc89f6e0cc0619ac","title":"オンラインジャッジの開発と運用 -Aizu Online Judge-"},{"paperId":"3c902294cb3230f81c504b63afffbad41cc302a1","title":"TBCNN: A Tree-Based Convolutional Neural Network for Programming Language Processing"},{"paperId":"110b7719f16eae8d82af0f56fca6e802c842c181","title":"Mining source code repositories at massive scale using language modeling"},{"paperId":"4b2d92d6e04cbdfd0dc9a10fa6f134fd5aa477c3","title":"The Definitive ANTLR 4 Reference"},{"paperId":null,"title":"Belongie , and Ser - Nam Lim . A metric learning reality check"},{"paperId":null,"title":"Each sample is pre-processed in the same way as the training samples and one token (never a padding) is arbitrarily replaced by the"},{"paperId":null,"title":"end-to-end masked language modeling with bert"},{"paperId":null,"title":"530 Clement , Dawn Drain , Daxin Jiang , Duyu Tang , Ge Li , Lidong Zhou , Linjun Shou , Long Zhou"},{"paperId":null,"title":"Women in data science"}],"id":"7547680408358916e66917d03436fca7540a7528","summary":"Project CodeNet is a first-of-its-kind, very large scale, diverse, and high-quality dataset to accelerate the algorithmic advancements in AI for Code, which consists of 14M code samples and about 500M lines of code in 55 different programming languages."},{"url":"https://www.semanticscholar.org/paper/a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models","venue":"ArXiv","year":2021,"referenceCount":102,"citationCount":152,"influentialCitationCount":35,"publicationDate":"16/08/2021","authors":"Jacob Austin,Augustus Odena,Maxwell Nye,Maarten Bosma,H. Michalewski,David Dohan,Ellen Jiang,Carrie J. Cai,Michael Terry,Quoc V. Le,Charles Sutton","citations":[{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"},{"paperId":"d8405996b4d08c304098636aedd9e1c1a1e262ee","title":"The Premature Obituary of Programming"},{"paperId":"9b5fb07df99b0dd65f3058701d7f017c3a70c144","title":"Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data"},{"paperId":"b2f256a1ec54325102119a57cb2359840fe68586","title":"SantaCoder: don't reach for the stars!"},{"paperId":"dae74645479f7c1fa3671066f9e24ec6c20c17ec","title":"TrojanPuzzle: Covertly Poisoning Code-Suggestion Models"},{"paperId":"ab8a53be2ff8a756776aa0245ad8da41189d60d6","title":"Fuzzing Deep-Learning Libraries via Large Language Models"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"d3a7a4543d83f568f79d1febe8379465ff0140c9","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"269df328eec08b56b7b1f38a7555797fe2b999b6","title":"ReCode: Robustness Evaluation of Code Generation Models"},{"paperId":"bf5fbd690f24f873df86d1b0a06579cf42f7dc36","title":"Dialog2API: Task-Oriented Dialogue with API Description and Example Programs"},{"paperId":"bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"9e8f0d9ba4e7af673c8b214b3764e020706dd1f3","title":"Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments"},{"paperId":"6bc87e51018d6de55011e95a0d43c588dd44a1e8","title":"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages"},{"paperId":"9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming"},{"paperId":"8175ce2cbb99b6a394bdac152ae39d413f4f1380","title":"Codex Hacks HackerRank: Memorization Issues and a Framework for Code Synthesis Evaluation"},{"paperId":"32b58766a1bfcef7ebba07070a272687aa518206","title":"Explicit Knowledge Transfer for Weakly-Supervised Code Generation"},{"paperId":"27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation"},{"paperId":"05f09050118d82100d04e56ba8b54836753fa9b4","title":"Programming by Example and Text-to-Code Translation for Conversational Code Generation"},{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models"},{"paperId":"ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","title":"Evaluating How Fine-tuning on Bimodal Data Effects Code Generation"},{"paperId":"2abed82162c47a0cc32cd62afcf46b0745541017","title":"Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book"},{"paperId":"ec7324a15009a9bd0b676f6b17762f759cf5dd9a","title":"Large Language Models Are Human-Level Prompt Engineers"},{"paperId":"3cba16fc46ac5b35c1cc72a822208aa0097384cc","title":"CodePAD: Sequence-based Code Generation with Pushdown Automaton"},{"paperId":"ca1df79eeb278aacaa20c11451e7c5f47c932ff6","title":"CODEP: Grammatical Seq2Seq Model for General-Purpose Code Generation"},{"paperId":"20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"538288d24bdad73d831dfed44b706958287ed318","title":"Generating Sequences by Learning to Self-Correct"},{"paperId":"cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers"},{"paperId":"41f5e1ad7793593befc0b9c38f756836e8b07c98","title":"Code4ML: a Large-scale Dataset of annotated Machine Learning Code"},{"paperId":"e7d0a8eb7e98863f37b51d89b5ca305b04aaba99","title":"SemanticOn: Specifying Content-Based Semantic Conditions for Web Automation Programs"},{"paperId":"0566c1c3eeeef5c968fced6d80b77fe22d02bbd9","title":"Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language"},{"paperId":"e8db669c8cb1c07557ede15e2771968f9370330b","title":"Large language models are not zero-shot communicators"},{"paperId":"0b340dd78fd04bbde2807d5efedb796d319355e3","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?"},{"paperId":"472d87be3dc298102e058be55a814cc6d2085b39","title":"Towards Deceptive Defense in Software Security with Chaff Bugs"},{"paperId":"a6ef5bad716091fb1888bf365f6129628ab3a5ee","title":"ObSynth: An Interactive Synthesis System for Generating Object Models from Natural Language Specifications"},{"paperId":"663a41c866d49ce052801fbc88947d39764cad29","title":"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"},{"paperId":"13d3733e0dabbb4ccdd036a5f04fd5b3e39eecb0","title":"Vision Transformers provably learn spatial structure"},{"paperId":"39e40821b7207125e54e6ed7112e55cd38c6f0c3","title":"Language Models of Code are Few-Shot Commonsense Learners"},{"paperId":"045a6f92b58f08d5b305dd5d661316a506ee4d43","title":"Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation"},{"paperId":"0c78a473e33a81246d5c0fbbda7e7de168814c18","title":"FlexType: A Plug-and-Play Framework for Type Inference Models"},{"paperId":"3ee3f425482cf86989d809155cc8cf2bf8d8113e","title":"Understanding HTML with Large Language Models"},{"paperId":"62f0db3a5ad5c795ec18fc7a6e7b01836809df57","title":"Language Models are Multilingual Chain-of-Thought Reasoners"},{"paperId":"8a854331c593c6a766fa3b8037fb2ad1b95a6f06","title":"An Empirical Study of Code Smells in Transformer-based Code Generation Techniques"},{"paperId":"95fa2b27ab7eb84738441ee16da97323538938f9","title":"I Speak, You Verify: Toward Trustworthy Neural Program Synthesis"},{"paperId":"97e1903b4afc811a9b5fa9e55723e80ba48bf46f","title":"Code as Policies: Language Model Programs for Embodied Control"},{"paperId":"6b5d1e50894b1f28e4798cf20e9ffa88b9ec011a","title":"How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models"},{"paperId":"18a831422a4b4c89bbf1cc4baaa2cfcbf29daaf1","title":"Exploring and evaluating personalized models for code generation"},{"paperId":"05ccfb058dc6788254742722278b0af2fc87f083","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?"},{"paperId":"1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization"},{"paperId":"def2e28863338cb20782eb2015a39d32df697ed6","title":"Learning to Improve Code Efficiency"},{"paperId":"63c670ba8018da0a7e33c34ffd84c2a3ca54b894","title":"Language Models Can Teach Themselves to Program Better"},{"paperId":"1e3ec5709b2ca43233c566cd77dbabbd6892bfa6","title":"Neurosymbolic repair for low-code formula languages"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"e37155d21818513bd40d64ee212099aac82bd6f8","title":"Less training, more repairing please: revisiting automated program repair via zero-shot learning"},{"paperId":"713bd2971116098211ef06336dfbe91a69854404","title":"Probing Semantic Grounding in Language Models of Code with Representational Similarity Analysis"},{"paperId":"b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs"},{"paperId":"b3a54332a0791751fcf234f6264f242c42ac00d2","title":"DocPrompting: Generating Code by Retrieving the Docs"},{"paperId":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models"},{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code"},{"paperId":"0f86d5ae106a53f40f89b60dff24074f6c2cd127","title":"The Case for a Single Model that can Both Generate Continuations and Fill in the Blank"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code"},{"paperId":"f1b6b34b4440a77ba86493f7062e8974062508c5","title":"Applying genetic programming to PSB2: the next generation program synthesis benchmark suite"},{"paperId":"35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs"},{"paperId":"0efa0441da820b1905572666ba1974a06a9663fb","title":"NaturalProver: Grounded Mathematical Proof Generation with Language Models"},{"paperId":"6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"},{"paperId":"ea4749c6ed5f08b16bab31b1ca4fd3fdc259ae8f","title":"Improving automatically generated code from Codex via Automated Program Repair"},{"paperId":"0bcd59da541fdae66884afba8d25475a54a9da1a","title":"Automated Repair of Programs from Large Language Models"},{"paperId":"c61ce808818308566124df2c8725c98d6bd38dc3","title":"A Precis of Language Models are not Models of Language"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"16168520c7efbfa84bcb609a05362916b04022bb","title":"Context-Aware Abbreviation Expansion Using Large Language Models"},{"paperId":"9a7d4c2f4309a2a38ea8e5b23c6d616fa0952d44","title":"Neural language models for network configuration: Opportunities and reality check"},{"paperId":"cdfe9580f63070f311151444f9df32818cc858bf","title":"An Empirical Evaluation of GitHub Copilot's Code Suggestions"},{"paperId":"fd7c3c8fbe8cf88bd967ead02738b43081e306a7","title":"Training Language Models with Language Feedback"},{"paperId":"4bc040835fbff57ce6612306d794b8c6c8226086","title":"Discovering the Syntax and Strategies of Natural Language Programming with Generative Language Models"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"1f87dc41bdf2c4c78e2dce9c5c8adfef5e25a70c","title":"Passport: Improving Automated Formal Verification Using Identifiers"},{"paperId":"2ba7104f7b93d77940312664f3467b8f090d6d16","title":"On Distribution Shift in Learning-based Bug Detectors"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"9a6730534295335247eebdec59b7decdeb83d59a","title":"On the Transferability of Pre-trained Language Models for Low-Resource Programming Languages"},{"paperId":"237f5ca6fcccef2b77a2212b34fb06a1dbd09b72","title":"Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"1a903282f7c19dbdb2714b852fb42dbb4675422b","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"fb5c11bbf63884f75d2da615fbf37a3bcfa2bd20","title":"Wordcraft: Story Writing With Large Language Models"},{"paperId":"c347093e2dca530ce347526380b0b7aedf03a6b2","title":"CrossBeam: Learning to Search in Bottom-Up Program Synthesis"},{"paperId":"0e802c0739771acf70e60d59c2df51cd7e8c50c0","title":"Memorizing Transformers"},{"paperId":"c96363c42bc8c465902c22b8c33c8704233f519e","title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages"},{"paperId":"c125b0be73c8493ebc27beb572f6c1b21d6b4ae4","title":"Static Prediction of Runtime Errors by Learning to Execute Programs with External Resource Descriptions"},{"paperId":"1f11601f9eb8c0bee1d1d2cc7f4fa187e8c5e525","title":"The impact of lexical and grammatical processing on generating code from natural language"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"9cbc044e315cdefe9a255119037ac7c23e9abdd5","title":"Predictability and Surprise in Large Generative Models"},{"paperId":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective"},{"paperId":"76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","title":"Investigating Explainability of Generative AI for Code through Scenario-based Design"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"7428f9b16a82839e2cb6e6c7a77c1ffeab898813","title":"HEAT: Hyperedge Attention Networks"},{"paperId":"5d0db797a45ce2453f821f7ded0b547d3fdab054","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"3d5463a16d85d9d1d0d8ebb4117a31aca3c240ff","title":"Fooling MOSS Detection with Pretrained Language Models"},{"paperId":"53c0abe83fe9b4fdaf2208295d8504fcf5241694","title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models"},{"paperId":"edc07f490c1c1b773094f236157219677f6a2f71","title":"Better Modeling the Programming World with Code Concept Graphs-augmented Multi-modal Learning"},{"paperId":"6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a","title":"Few-Shot Semantic Parsing with Language Models Trained on Code"},{"paperId":"52db8674337e5d86dcb96d013734befc8c3d4581","title":"Large Language Models are not Models of Natural Language: they are Corpus Models."},{"paperId":"05af6c968ef8c7f9b07b0d67f138780179f29511","title":"Sparks: Inspiration for Science Writing using Language Models"},{"paperId":"1aed58bd07026492194672adec494dc37c894a28","title":"Leveraging Automated Unit Tests for Unsupervised Code Translation"},{"paperId":"d3640eb3b542eaf36fee2261f037a6bf0d8eac9c","title":"AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot's Code Contributions"},{"paperId":"660ca9e15e19409903a0605f0584d0f263c35c67","title":"S YNCHROMESH : R ELIABLE C ODE G ENERATION FROM P RE - TRAINED L ANGUAGE M ODELS"},{"paperId":"239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning"},{"paperId":"b2c4fdb49bdb23e6ad0ac3272029324157046ea7","title":"Automated Repair of Code from Language Models"},{"paperId":"407b9e9478ba6bff43ce4b20e8b6cb2b303477d2","title":"P LANNING WITH L ARGE L ANGUAGE M ODELS FOR C ODE G ENERATION"},{"paperId":"8d282bf295d655e38b63ddbc7cdc94b188df8bb2","title":"G ENERATING S EQUENCES BY L EARNING TO [S ELF -]C ORRECT"},{"paperId":"2443179d421e1faf7474add557b45add554723c7","title":"Formal Premise Selection With Language Models"},{"paperId":"6df98ac2300c6e9c232440147ba976b4f501ca67","title":"C ODEX HACKS H ACKER R ANK : B ENEFITS AND R ISKS OF L ARGE -S CALE S OURCE C ODE M ODELS"},{"paperId":"6f84d0cc33c7c58f74b28ddcc1cbda91ea608c9f","title":"C ODE S UMMARIZATION : D O T RANSFORMERS R E ALLY U C"},{"paperId":"ba5d21b7c65c6598c7bd39a5d992308c205df374","title":"A S YSTEMATIC E VALUATION OF L ARGE L ANGUAGE M ODELS OF C ODE"},{"paperId":"a2b6e1f7d8a7963d321f29fca7c01eeb1ebd7f0f","title":"P ATCH G ENERATION WITH L ANGUAGE M ODELS : F EASIBILITY AND S CALING B EHAVIOR"},{"paperId":"2aec574791fd33e9be32fd5191a66734f805a6a1","title":"Training Language Models with Natural Language Feedback"},{"paperId":"75c8f2916a2067f7549cf58ea8c9061565eb1dab","title":"C ROSS B EAM : L EARNING TO S EARCH IN B OTTOM -U P P ROGRAM S YNTHESIS"},{"paperId":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"DECOMPOSITION IN NEURAL PROGRAM SYNTHESIS"},{"paperId":"54d00fc330248b3b2f89193da31bb17851ebd2b7","title":"M EMORIZING T RANSFORMERS"},{"paperId":"091fa84bdc07dcb22a34060c3996d8c58d71cd20","title":"Towards Neural Functional Program Evaluation"},{"paperId":"73e6f5a7e2760b3f9ab077a450e445e12bfd7061","title":"Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?"},{"paperId":"a5731122200fbb8b37f048010a1e1ca4474aa606","title":"Examining Zero-Shot Vulnerability Repair with Large Language Models"},{"paperId":"4b70f356f50d3e8e1f72c4a10f0ce2a26da95b5a","title":"Controlling Conditional Language Models with Distributional Policy Gradients"},{"paperId":"3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e","title":"A General Language Assistant as a Laboratory for Alignment"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"21e8e76386aaaa00e0971af70ce84a8a544e1aa1","title":"Cascaded Fast and Slow Models for Efficient Semantic Code Search"},{"paperId":"1093adc2a47212ef599e4708fe3e41ff7c9ec6d0","title":"GenLine and GenForm: Two Tools for Interacting with Generative Language Models in a Code Editor"},{"paperId":"05c2e1ee203be217f100d2da05bdcc52004f00b6","title":"Unsolved Problems in ML Safety"},{"paperId":"bc9598dc4ed0472d8b59b87ed3a139f8347d40ee","title":"Towards A Measure Of General Machine Intelligence"},{"paperId":"a176b0de62840f7118006277d94bbc1547162a4d","title":"Learning to Synthesize Programs as Interpretable and Generalizable Policies"},{"paperId":"3f97c2067cde9377e50b3160bbd7982c94abd88a","title":"An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions"},{"paperId":"98485ce6532d69f34a8ec67de6b09a39532bd221","title":"Communicating Natural Programs to Humans and Machines"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"abd77509ef1cc739c0757ae657025fcee13c98dc","title":"Program Synthesis Guided Reinforcement Learning for Partially Observed Environments"},{"paperId":"be09ed6cd73654a23f78416433a1b23ea623ea79","title":"Symbolic Behaviour in Artificial Intelligence"},{"paperId":"2ad6b59ff14d6bec3f14d8b6480025bbebc50e46","title":"Optimal Neural Program Synthesis from Multimodal Specifications"},{"paperId":"27e3ca4fd7b8290a0e12ca4fd2b7ad5bcd5900f2","title":"Are Transformers All That Karel Needs?"},{"paperId":"bab6893ee48d168d27c227c3b0867f6d471fbea8","title":"Language Models are not Models of Language"}],"references":[{"paperId":"34503c0b6a615124eaf82cb0e4a1dab2866e8980","title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"},{"paperId":"1093adc2a47212ef599e4708fe3e41ff7c9ec6d0","title":"GenLine and GenForm: Two Tools for Interacting with Generative Language Models in a Code Editor"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"ac879df2cc36f3f824fa24149517622b6bc7bd09","title":"Implicit Representations of Meaning in Neural Language Models"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"c2c7233293d55f201fe5b496234bed1914eea70e","title":"Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks"},{"paperId":"62d1a3137b01a69443bebf4d92c1990ec512a6a1","title":"Extracting Training Data from Large Language Models"},{"paperId":"03b6e258168796f96f1c40d32411bd699b6de922","title":"Deep Just-In-Time Inconsistency Detection Between Comments and Source Code"},{"paperId":"30a156f17ca8f54aa14d01d32c2315c11fcbe723","title":"BUSTLE: Bottom-up program-Synthesis Through Learning-guided Exploration"},{"paperId":"1ec69f1a1a9d4ff5c5bc70db0e5087157b620570","title":"You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion"},{"paperId":"1311adebe439ccd9e373d594a8df1d720a685190","title":"Code Prediction by Feeding Trees to Transformers"},{"paperId":"59cc0871534a20ac98db0663539c6675ab63566f","title":"SequenceR: Sequence-to-Sequence Learning for End-to-End Program Repair"},{"paperId":"8990154f515dadf6d1bfda745e62a67dc3b0e709","title":"A large-scale benchmark for few-shot program induction and synthesis"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"5f818ecbfce3bc44325a4f8ef2d744bc94006d6c","title":"Learning to Execute Programs with Instruction Pointer Attention Graph Neural Networks"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"02eaaf87f9cae34cca398fed146079e6eeb1f868","title":"Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data"},{"paperId":"be188977f1791faa60128d5f8a85470cad93f0f9","title":"Where should I comment my code? A dataset and model for predicting locations that need comments"},{"paperId":"ef2bbcd928749978b4395460a96c9869833c9c89","title":"DreamCoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0071ee55f1749e6be586645bab08f0385ee856b4","title":"Graph-based, Self-Supervised Program Repair from Diagnostic Feedback"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"08066c80919620397e8e4e5372ff84caf401e675","title":"Global Relational Models of Source Code"},{"paperId":"6acf0d7b24a93953269a4d52c6b2f0c7d11f9ae4","title":"Hoppity: Learning Graph Transformations to Detect and Fix Bugs in Programs"},{"paperId":"29fc7ff02e68537a86d32978bfd2e8e45258e77d","title":"LambdaNet: Probabilistic Type Inference using Graph Neural Networks"},{"paperId":"270897709b9e32eccb4b8968402be1700d1e28e6","title":"OptTyper: Probabilistic Type Inference by Optimising Logical and Natural Constraints"},{"paperId":"3944354c42ddfff7414ad06022f96c72858d5fa6","title":"Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"e41452747ac0674a7b6534e78be33134fe8ef650","title":"Learning to Represent Programs with Property Signatures"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"a429c6c8407b10a89c9610ab5e9682ec89772356","title":"TypeWriter: neural type prediction with search-based validation"},{"paperId":"78f4de568564b6b5bb9058779ead6b3be8548e53","title":"Learning to Fix Build Errors with Graph2Diff Neural Networks"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"f669b5406320a83e94b4dd439806211c8b1e95fc","title":"Write, Execute, Assess: Program Synthesis with a REPL"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"6c41bedc4637f3fd504c68baa3b3d8881e056ac1","title":"Execution-Guided Neural Program Synthesis"},{"paperId":"73bade9162bc4df6ca6c9ab8a7eb2d54d35acad2","title":"code2vec: learning distributed representations of code"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"ea1b62a03ab16c4acca6f7041c6f096bedec244b","title":"Deep learning type inference"},{"paperId":"980e4fe84fe5feec42c2a2eea7cc738e1af8acdf","title":"Automatic Program Synthesis of Long Programs with a Learned Garbage Collector"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"d6b7abd5c17f9e7d83c5b709c27da3a82a9788f2","title":"NAPS: Natural Program Synthesis Dataset"},{"paperId":"c70f11da5a8bb2df36def1d99c4c08df315e2233","title":"DeepBugs: a learning approach to name-based bug detection"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"1e077413b25c4d34945cc2707e17e46ed4fe784a","title":"Universal Language Model Fine-tuning for Text Classification"},{"paperId":"d0f2d7236e43f129744e88130fb71f8f872d2b31","title":"Learning to Represent Programs with Graphs"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"d11777ca327c6d91de34d1d2ac50316b905578b2","title":"Neural Sketch Learning for Conditional Program Generation"},{"paperId":"57a30c0a013bc36b4b5181b33c308c00d98b7a9d","title":"Learning Libraries of Subroutines for Neurally-Guided Bayesian Program Induction"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"3ff0af64279929a952ee340e645256b7e0580f65","title":"RobustFill: Neural Program Learning under Noisy I/O"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"784ee73d5363c711118f784428d1ab89f019daa5","title":"Hybrid computing using a neural network with dynamic external memory"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"484ed558a5863060b373e8a2cb7cb302b8c36116","title":"A Convolutional Attention Network for Extreme Summarization of Source Code"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"50358e3f30ab8c99f9d383e5683b31c2311e5651","title":"Automatic patch generation by learning correct code"},{"paperId":"5e4eb58d5b47ac1c73f4cf189497170e75ae6237","title":"Neural GPUs Learn Algorithms"},{"paperId":"a2499dd426c46c645ee805d7594b6687547c72d4","title":"Neural Random Access Machines"},{"paperId":"4aa9f5150b46320f534de4747a2dd0cd7f3fe292","title":"Semi-supervised Sequence Learning"},{"paperId":"be6665462fd6c56e8756dadf23373bb0c90b2b19","title":"Predicting Program Properties from \"Big Code\""},{"paperId":"0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a","title":"Learning to Execute"},{"paperId":"4755b856dc08ac024ae935e7a6f9df325b00ae53","title":"Phrase-Based Statistical Translation of Programming Languages"},{"paperId":"9277dc7dd5b348645e8fcf4a987e5b3fc9132c7c","title":"Towards a Big Data Curated Benchmark of Inter-project Code Clones"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"93ff001eb7ddd019c107879943126c74a973993b","title":"Learning natural coding conventions"},{"paperId":"215999a0e155a21255e4655d4eac312858058a84","title":"Structured Generative Models of Natural Source Code"},{"paperId":null,"title":"Neural turing machines. CoRR, abs/1410"},{"paperId":"69c42a8da4c52b90ee27f9b6c0df37f2731ac890","title":"Growing solver-aided languages with rosette"},{"paperId":"f7ba81057c8ec4f1c039027232034143a5afe6bf","title":"Syntax-guided synthesis"},{"paperId":"5447a3b8701d59f3a2f1a7f7af030f687ba495c3","title":"Lexical statistical machine translation for language migration"},{"paperId":"110b7719f16eae8d82af0f56fca6e802c842c181","title":"Mining source code repositories at massive scale using language modeling"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"3aa35c213730e688fa191a1ce241db31087956f0","title":"GenProg: A Generic Method for Automatic Software Repair"},{"paperId":null,"title":"Alan Turing’s Electronic Brain: The Struggle to Build the ACE, the World’s Fastest Computer"},{"paperId":"e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de","title":"Generating Text with Recurrent Neural Networks"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"9819b600a828a57e1cde047bbe710d3446b30da5","title":"Recurrent neural network based language model"},{"paperId":"1ef301c1b275091b6a50d620b41df4722f2108f0","title":"Combinatorial sketching for finite programs"},{"paperId":"96902d95f53358ea23ff401b5198c7315addeb43","title":"On the synthesis of a reactive module"},{"paperId":"2fb9729c0a3b046c390ad6748773527eae6cfc84","title":"What Is Program Synthesis?"},{"paperId":"dd7abc005846b18c9a78ab80467cbbaedb643456","title":"A Methodology for LISP Program Construction from Examples"},{"paperId":"e6d8af09e433d93cc17548d6cab4a6afcc7090b4","title":"Inferring LISP Programs From Examples"},{"paperId":"d163709460265aa4901ac41a2b903793ad24b3c2","title":"Knowledge and Reasoning in Program Synthesis"},{"paperId":"94d9cbfc474cb6ce961de45a06233b2853ca6724","title":"Toward automatic program synthesis"},{"paperId":"577e96521d62b9ebb5fd67412a21b02e9cd67b90","title":"PROW: A Step Toward Automatic Program Writing"},{"paperId":"8bcc2e1ceaf091239b0b2c0bb354185580587172","title":"The FORTRAN automatic coding system"},{"paperId":null,"title":"Make sure the function signature is not unusual"}],"id":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","summary":"The limits of the current generation of large language models for program synthesis in general purpose programming languages are explored, finding that even the best models are generally unable to predict the output of a program given a speciﬁc input."},{"url":"https://www.semanticscholar.org/paper/a3564f3cf954c05844c757505325a50b4d858e22","title":"Generating Infrastructure-as-Code from Natural Language: Evaluating Fine-Tuned GPT-3 Models for Cloud Infrastructure Provisioning","venue":"","year":2022,"referenceCount":8,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","citations":[],"references":[{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"4891cc27e296e0ead23407a835bcd3bbb802ce67","title":"Automatic Code Generation using Pre-Trained Language Models"},{"paperId":"4cd8cea2f0302acffe1dc509d5f6458d8eb1e234","title":"Pseudocode to Code Translation Using Transformers"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"}],"id":"a3564f3cf954c05844c757505325a50b4d858e22","summary":"A transformer-based model to generate Infrastructure-as-Code from natural language is introduced, which allows both technical and nontechnical users to dynamically generate IaC artifacts, enabling them to request and receive cloud resources using conversational interfaces such as chat bots, SMS, etc."},{"url":"https://www.semanticscholar.org/paper/55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant","venue":"ArXiv","year":2022,"referenceCount":22,"citationCount":7,"influentialCitationCount":2,"publicationDate":"30/01/2022","authors":"Shubham Chandel,Colin B. Clement,Guillermo Serrato,Neel Sundaresan","citations":[{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs"}],"references":[{"paperId":"ad18af95ba8125d2c0eb9f9941205678cad38ad2","title":"Generating Accurate Assert Statements for Unit Test Cases using Pretrained Transformers"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"3a863672b072a37ac8d2eb6b780234f6b748a2d9","title":"Generating bug-fixes using pretrained transformers"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"dd1ef7e7dc6ab885d9d64218148f08354c3c6fdb","title":"CPC: Automatically Classifying and Propagating Natural Language Comments via Program Analysis"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"75acc731bdd2b626edc74672a30da3bc51010ae8","title":"CTRL: A Conditional Transformer Language Model for Controllable Generation"},{"paperId":"79b7a61abd4a4b4cf62c26cec815b55b0778f48d","title":"Pythia: AI-assisted Code Completion System"},{"paperId":"7436d86cd6b572a3f52caa7820c07e7bfcf16f86","title":"Gmail Smart Compose: Real-Time Assisted Writing"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"af5a7be7bfc592e400e2f12a24ad3e6a5a44b58f","title":"Learning from examples to improve code completion systems"}],"id":"55ad5e818cfed72317576027fb33a9609210d592","summary":"This work studies the feasibility of a Data Science assistant powered by a sequence-to-sequence transformer by training a new model JuPyT5 on all publicly available Jupyter Notebook GitHub repositories and developing a new metric: Data Science Problems (DSP)."},{"url":"https://www.semanticscholar.org/paper/5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode","venue":"Science","year":2022,"referenceCount":81,"citationCount":127,"influentialCitationCount":28,"publicationDate":"08/02/2022","authors":"Yujia Li,David H. Choi,Junyoung Chung,Nate Kushman,Julian Schrittwieser,Rémi Leblond,Tom,Eccles,James Keeling,Felix Gimeno,Agustin Dal Lago,T. Hubert,Peter Choy,Cyprien de,Masson d’Autume,I. Babuschkin,Xinyun Chen,Po-Sen Huang,Johannes Welbl,Sven Gowal,Alexey,Cherepanov,James Molloy,D. Mankowitz,Esme Sutherland Robson,Pushmeet Kohli,Nando de,Freitas,K. Kavukcuoglu,Oriol Vinyals","citations":[{"paperId":"c532f1df90925e5c69789f0cd99248d8a2a2e5bc","title":"My AI Wants to Know if This Will Be on the Exam: Testing OpenAI’s Codex on CS2 Programming Exercises"},{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"},{"paperId":"d8405996b4d08c304098636aedd9e1c1a1e262ee","title":"The Premature Obituary of Programming"},{"paperId":"a20875e70a38cb053cd34e170038c4746f85dac9","title":"A Case Study in Engineering a Conversational Programming Assistant's Persona"},{"paperId":"1f22de83d912176cb8857efa1c6d65b14d6a2f5c","title":"ChatGPT is not all you need. A State of the Art Review of large Generative AI models"},{"paperId":"dae74645479f7c1fa3671066f9e24ec6c20c17ec","title":"TrojanPuzzle: Covertly Poisoning Code-Suggestion Models"},{"paperId":"490f1e8ff352bded30bcde01d5b4769d6c2d2dd5","title":"Adversarial Attacks on Neural Models of Code via Code Difference Reduction"},{"paperId":"b7823997fb185f208b6a6723b60413ff179d2639","title":"Standing on the Shoulders of AI Giants"},{"paperId":"a8e0ba16346b72c3a04dd0b1da84bc5f28900174","title":"Using GitHub Copilot to Solve Simple Programming Problems"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"0f38267a8ba32789f5d3b1b19820f86940fea052","title":"Generation-Augmented Query Expansion For Code Retrieval"},{"paperId":"bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence"},{"paperId":"433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"9e8f0d9ba4e7af673c8b214b3764e020706dd1f3","title":"Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"e8aa5f51aaf29344174f90d7edca49cc153a6b00","title":"Economic impacts of AI-augmented R&D"},{"paperId":"9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming"},{"paperId":"000b8567d17dd982ae226c29505027ed692911dd","title":"AlphaCode and “data-driven” programming"},{"paperId":"3d3012bfcc8bc7e4dc84c177e94650e66f03bc5b","title":"Are ChatGPT and AlphaCode going to replace programmers?"},{"paperId":"8175ce2cbb99b6a394bdac152ae39d413f4f1380","title":"Codex Hacks HackerRank: Memorization Issues and a Framework for Code Synthesis Evaluation"},{"paperId":"dca3bc28a7d404b28780a813ea7072eda809e6c0","title":"Programming Is Hard - Or at Least It Used to Be: Educational Opportunities And Challenges of AI Code Generation"},{"paperId":"ea55ae4c82b45e3857f37406c94e9f642a2b3d84","title":"Genetic Programming with Local Scoring"},{"paperId":"27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation"},{"paperId":"30624a18720bf93a85dc3efe570df271a8c9f4c3","title":"Program Repair"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","title":"Evaluating How Fine-tuning on Bimodal Data Effects Code Generation"},{"paperId":"a4bdc300db297756f36bedee2859b62df8e268c2","title":"Follow the Wisdom of the Crowd: Effective Text Generation via Minimum Bayes Risk Decoding"},{"paperId":"b1eebb2df3b9ff7ff2b00fb1a786f6ada2caebce","title":"Towards a Mathematics Formalisation Assistant using Large Language Models"},{"paperId":"048ed70192de0232086eb32a95ffb3be8d336c76","title":"Metaphors We Learn By"},{"paperId":"632ab7663e6d64578ceda1d1df9ec525b503bacb","title":"Steps towards prompt-based creation of virtual worlds"},{"paperId":"ac3d7ae8b4acb137492d4a8d8bcff480b89fa000","title":"Programming Pedagogy and Assessment in the Era of AI/ML: A Position Paper"},{"paperId":"327f1561b544b0a3b9d8d5d0e6d82c2a5911fca9","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"4af2891ce1aab624c4917e8a69fcee5c8a1f41db","title":"NL2Viz: natural language to visualization via constrained syntax-guided synthesis"},{"paperId":"71280dba5bda65c162f9deaffed7d3d20692ca0a","title":"SecurityEval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques"},{"paperId":"ec7324a15009a9bd0b676f6b17762f759cf5dd9a","title":"Large Language Models Are Human-Level Prompt Engineers"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers"},{"paperId":"0b340dd78fd04bbde2807d5efedb796d319355e3","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?"},{"paperId":"5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","title":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming"},{"paperId":"f031ba42cf82f106200bb03fbb91dd5671a59b9c","title":"Practical Program Repair in the Era of Large Pre-trained Language Models"},{"paperId":"d400a649f0f0a3de22b89a268f48aff2dcb06a09","title":"Entailer: Answering Questions with Faithful and Truthful Chains of Reasoning"},{"paperId":"d26f616699a122e5455a13189e276002ee4cf923","title":"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs"},{"paperId":"5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"a6ef5bad716091fb1888bf365f6129628ab3a5ee","title":"ObSynth: An Interactive Synthesis System for Generating Object Models from Natural Language Specifications"},{"paperId":"4c2534f9b03ac2f3810c07abc398a11bcf47258e","title":"Transformers Learn Shortcuts to Automata"},{"paperId":"663a41c866d49ce052801fbc88947d39764cad29","title":"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"},{"paperId":"825333b7efe2cade106eaf36c7e731f757974806","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot"},{"paperId":"045a6f92b58f08d5b305dd5d661316a506ee4d43","title":"Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation"},{"paperId":"0b8772b7790c69f40897b5eb7f8fd57f24138f3d","title":"ContraGen: Effective Contrastive Learning For Causal Language Model"},{"paperId":"8a854331c593c6a766fa3b8037fb2ad1b95a6f06","title":"An Empirical Study of Code Smells in Transformer-based Code Generation Techniques"},{"paperId":"558b7245a54575e16143324df98129254d5a244c","title":"Enhancing Code Similarity with Augmented Data Filtering and Ensemble Strategies"},{"paperId":"95fa2b27ab7eb84738441ee16da97323538938f9","title":"I Speak, You Verify: Toward Trustworthy Neural Program Synthesis"},{"paperId":"2a0456b0408cd4c33f2ff4400374e7be2497a362","title":"Repairing Bugs in Python Assignments Using Large Language Models"},{"paperId":"cd6496bc404e18a24f634e3dded2ed1cdca03e0f","title":"Learning to Learn with Generative Models of Neural Network Checkpoints"},{"paperId":"971e875e28f26240987d2c9470d1ee74ad204205","title":"Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction"},{"paperId":"a7435722d8ab595da5a9c70ac9160f57d0dcd75a","title":"Enabling Transformers to Understand Low-Level Programs"},{"paperId":"ef29fb8cc6bdda4b011288f51da521d3c25fc53d","title":"Learning to Prevent Profitless Neural Code Completion"},{"paperId":"45875d5f55c72c1bdfa6d7c312eead7dcc93123d","title":"Borch: A Deep Universal Probabilistic Programming Language"},{"paperId":"e5993b3afe6384b5e6f90093989773ad1f868f71","title":"Towards Top-Down Deep Code Generation in Limited Scopes"},{"paperId":"0509c25103939d59ed4e27b1393e74ad5734c453","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot"},{"paperId":"befde3e07ce97f02f12fe92ab27e99f23ccd17aa","title":"Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation"},{"paperId":"6032212d5790b6a580d68d469a9895aad6238c89","title":"Diverse Title Generation for Stack Overflow Posts with Multiple Sampling Enhanced Transformer"},{"paperId":"9b61de7038290751377b64293baaf42f3e7cf441","title":"An Empirical Evaluation of Competitive Programming AI: A Case Study of AlphaCode"},{"paperId":"99f85119f113b5498517928eff74a904b69e37b7","title":"CCTEST: Testing and Repairing Code Completion Systems"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?"},{"paperId":"1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization"},{"paperId":"8c2d9d2aa891e3b53bbd9166c1b804a0741fc44a","title":"Finding Reusable Machine Learning Components to Build Programming Language Processing Pipelines"},{"paperId":"654cd297b307c0c6fa08732511ba852b8dce1977","title":"Out of the BLEU: how should we assess quality of the Code Generation models?"},{"paperId":"63c670ba8018da0a7e33c34ffd84c2a3ca54b894","title":"Language Models Can Teach Themselves to Program Better"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"746b6108a72b2b1bd78f70d4b1a211cdebfd8f49","title":"Multi-objective Grammar-guided Genetic Programming with Code Similarity Measurement for Program Synthesis"},{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"114eb5a35a3cd802cd1f46fff35c284e32ef6c54","title":"GitHub Copilot AI pair programmer: Asset or Liability?"},{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code"},{"paperId":"3a2aa950971a46167b6da9431098b02facffe342","title":"Questions Are All You Need to Train a Dense Passage Retriever"},{"paperId":"a82d6acc26d34f25d572da5dace6c29f4acfddfc","title":"Actionable Guidance for High-Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic Risks"},{"paperId":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation"},{"paperId":"29acc890e521f7a6415666ab9eb3432c49b4587a","title":"Self-critiquing models for assisting human evaluators"},{"paperId":"41f7bcca48c321071cbba7f9e7d735f65698dcce","title":"A Dataset and Benchmark for Automatically Answering and Generating Machine Learning Final Exams"},{"paperId":"a64871352f8ac7f8aa226fda5cce70251a18a4fc","title":"Assessing Project-Level Fine-Tuning of ML4SE Models"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"40edfa97cd02268fccff75eb9c693b11c1a968b2","title":"Formal Specifications from Natural Language"},{"paperId":"0d08ffccc982781e310bb184397bbe64b9aef157","title":"Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models"},{"paperId":"1100dee3fd78655cddc4b7bfaef1161351d4fab5","title":"Automated Feedback Generation for Competition-Level Code"},{"paperId":"35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs"},{"paperId":"e7e1feff05edf89cac6c2e6de46815a3f89144ef","title":"Tensor Program Optimization with Probabilistic Programs"},{"paperId":"0efa0441da820b1905572666ba1974a06a9663fb","title":"NaturalProver: Grounded Mathematical Proof Generation with Language Models"},{"paperId":"ea4749c6ed5f08b16bab31b1ca4fd3fdc259ae8f","title":"Improving automatically generated code from Codex via Automated Program Repair"},{"paperId":"0bcd59da541fdae66884afba8d25475a54a9da1a","title":"Automated Repair of Programs from Large Language Models"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"5922f437512158970c417f4413bface021df5f78","title":"A Generalist Agent"},{"paperId":"74600cebaec0ecd6a9bde7e3830d813899bf8a91","title":"From {Solution Synthesis} to {Student Attempt Synthesis} for Block-Based Visual Programming Tasks"},{"paperId":"abab9ae27efb40bbe6ffb9f6d27d56001412d856","title":"Scaling Genetic Improvement and Automated Program Repair"},{"paperId":"f8aa0cab09bc0668276e2cb5690bae47fa50d350","title":"An Initial Look at Self-Reprogramming Artificial Intelligence"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"1f87dc41bdf2c4c78e2dce9c5c8adfef5e25a70c","title":"Passport: Improving Automated Formal Verification Using Identifiers"},{"paperId":"bb7e46f316d319f9819c3554c99995ef8361ae9c","title":"CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex"},{"paperId":"905cbe787b20fca3917d2afd6a7a5f073a50386e","title":"MP-CodeCheck: Evolving Logical Expression Code Anomaly Learning with Iterative Self-Supervision"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"3eda53506586216acc96f4f34446f697874f360c","title":"Learning to Induce Causal Structure"},{"paperId":"6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis"},{"paperId":"608df5bd7a0ad0f4d335ae4d071b8cfe60e2f3c5","title":"On the Effectiveness of Pretrained Models for API Learning"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"590f6817b42407f96b079e82c935fae298196359","title":"Less is More: Summary of Long Instructions is Better for Program Synthesis"},{"paperId":"7b5aa186ca8abc585607c5ec91562e127a398601","title":"Open-Ended Knowledge Tracing"},{"paperId":"29ed68d701f8450853938827b5124c9613c56aff","title":"ET-BERT: A Contextualized Datagram Representation with Pre-training Transformers for Encrypted Traffic Classification"},{"paperId":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective"},{"paperId":"b2c4fdb49bdb23e6ad0ac3272029324157046ea7","title":"Automated Repair of Code from Language Models"},{"paperId":"407b9e9478ba6bff43ce4b20e8b6cb2b303477d2","title":"P LANNING WITH L ARGE L ANGUAGE M ODELS FOR C ODE G ENERATION"},{"paperId":"18bda734a1546eae13f6b13600023ff73f95b6e3","title":"Program Synthesis Through Learning the Input-Output Behavior of Commands"},{"paperId":"239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning"},{"paperId":"78863000eb70945cc8d791d45d4a3fe8a6521cb6","title":"Open-Ended Knowledge Tracing for Computer Science Education"},{"paperId":"91260f73dd179487fb16713deb8267634ae14716","title":"CodexDB: Synthesizing Code for Query Processing from Natural Language Instructions using GPT-3 Codex"},{"paperId":"3b0cf543a730e674d4213d344ebc857fada76ead","title":"Understanding High-Level Properties of Low-Level Programs Through Transformers"},{"paperId":null,"title":"Materials ExerciseTeacher Student Attempt Feedback"},{"paperId":"6df98ac2300c6e9c232440147ba976b4f501ca67","title":"C ODEX HACKS H ACKER R ANK : B ENEFITS AND R ISKS OF L ARGE -S CALE S OURCE C ODE M ODELS"},{"paperId":"15ef2d1b88f54fa32a32927463a7116219b89529","title":"SUPEROPTIMIZE REAL-WORLD PROGRAMS"},{"paperId":"36589346063ff26506330451976280011273b935","title":"Towards Teachable Reasoning Systems"},{"paperId":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"DECOMPOSITION IN NEURAL PROGRAM SYNTHESIS"},{"paperId":"27d16a7f2ce8f2b787b34ff1f9b4fece079700c3","title":"Figuring out Figures: Using Textual References to Caption Scientific Figures"},{"paperId":"6c2d43e71e240e354b5790a38da78a291ceffe7c","title":"Learning to Superoptimize Real-world Programs"},{"paperId":"4f278ab5ad629267e06196e273252262854c1c57","title":"BF++: a language for general-purpose program synthesis"}],"references":[{"paperId":"002c256d30d6be4b23d365a8de8ae0e67e4c9641","title":"Improving language models by retrieving from trillions of tokens"},{"paperId":"68f141724814839d556a989646194be88641b143","title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"},{"paperId":"fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf","title":"Ethical and social risks of harm from Language Models"},{"paperId":"f7987fa2aadc0b368c185dc4d2fdb1337a202c32","title":"Solving Probability and Statistics Problems by Program Synthesis"},{"paperId":"4e40595d6ecba027cebb4f2e3b43ae44bcf51daf","title":"Solving Linear Algebra by Program Synthesis"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"a176b0de62840f7118006277d94bbc1547162a4d","title":"Learning to Synthesize Programs as Interpretable and Generalizable Policies"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"309fb5c6ed6d7ec85281ee315760df342e6c4fcc","title":"Ten Lessons From Three Generations Shaped Google’s TPUv4i : Industrial Product"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"62d1a3137b01a69443bebf4d92c1990ec512a6a1","title":"Extracting Training Data from Large Language Models"},{"paperId":"87fe4a67e4cb5b8cdbc5da4d98888c037fb7d043","title":"Learning Autocompletion from Real-World Datasets"},{"paperId":"4dee6b82c7e59973ccd1520ff83f6b66f4d4bed4","title":"Investigating Softmax Tempering for Training Neural Machine Translation Models"},{"paperId":"51c62d63c6204deecb24a1d3f9ea8e0a42d23817","title":"Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces"},{"paperId":"3f97c2067cde9377e50b3160bbd7982c94abd88a","title":"An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions"},{"paperId":"f82f1c722831bc3def1853ac65497d6a4fea01b9","title":"Learning to Generate Code Sketches"},{"paperId":null,"title":"Research recitation: A first look at rote learning in GitHub Copilot suggestions"},{"paperId":null,"title":"How to interpret contest ratings"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"c94e49617f569204f989643e5462691b9b3a482b","title":"Estimating Training Data Influence by Tracking Gradient Descent"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":null,"title":"Codeforces: Results of 2020"},{"paperId":null,"title":"Haiku: Sonnet for JAX"},{"paperId":"dc52b09089704ebd6f471177474bc29741c50023","title":"Fast Transformer Decoding: One Write-Head is All You Need"},{"paperId":"361c00b22e29d0816ca896513d2c165e26399821","title":"Grandmaster level in StarCraft II using multi-agent reinforcement learning"},{"paperId":"07398e448180ad75c44d30f23a65289d40ff6f52","title":"Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"807370352540f171685998aec7a5701f7110f147","title":"Understanding Back-Translation at Scale"},{"paperId":"29de7c0fb3c09eaf55b20619bceaeafe72fd87a6","title":"Hierarchical Neural Story Generation"},{"paperId":"d11777ca327c6d91de34d1d2ac50316b905578b2","title":"Neural Sketch Learning for Conditional Program Generation"},{"paperId":null,"title":"JAX: composable transformations of Python+NumPy programs, 2018"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"3ff0af64279929a952ee340e645256b7e0580f65","title":"RobustFill: Neural Program Learning under Noisy I/O"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":null,"title":"Description2Code Dataset"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"50eba68089cf51323d95631c2f59ff916848863f","title":"The rust language"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"5f00233321d84bf28910752c3bd620cbff06a59d","title":"Code completion with statistical language models"},{"paperId":"ef12383f516840ec1ec998cd5921dfc6e197c9b2","title":"PPDB: The Paraphrase Database"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":null,"title":"Falsehoods programmers believe about names"},{"paperId":"4edaded63d9d7ea27f5c4819f35d7168e1ae7974","title":"Scratch: programming for all"},{"paperId":"af5a7be7bfc592e400e2f12a24ad3e6a5a44b58f","title":"Learning from examples to improve code completion systems"},{"paperId":"eb537c6403932af7d1c4d819d2f0179df23baaee","title":"How Program History Can Improve Code Completion"},{"paperId":"ae74531e6e73afaffb264b4e536b8fde95d03202","title":"Program synthesis by sketching"},{"paperId":"2fb9729c0a3b046c390ad6748773527eae6cfc84","title":"What Is Program Synthesis?"},{"paperId":"94d9cbfc474cb6ce961de45a06233b2853ca6724","title":"Toward automatic program synthesis"},{"paperId":"1beae9ef432a57bb5ec0c43944a07182814ab443","title":"Application of Theorem Proving to Problem Solving"},{"paperId":null,"title":"Application of recursive arithmetic to the problem of circuit synthesis."},{"paperId":null,"title":"Heiber for helping connect us with Codeforces"},{"paperId":null,"title":"Dathathri for analyzing our model"},{"paperId":null,"title":"Caballero for granting permission to use Description2Code data"},{"paperId":null,"title":"Stanway for logistically making the project possible"},{"paperId":null,"title":"GitHub's automatic coding tool rests on untested legal ground"},{"paperId":null,"title":"Removed problems that are duplicates of each other, ignoring whitespace. Submissions for duplicate problems were merged"},{"paperId":null,"title":"Irving for developing tools that we use to train large language models and for lending their expertise in model training"},{"paperId":null,"title":"Foley for project management in early stages"},{"paperId":null,"title":"AlphaCode examples, explanations, and visualizations can be"},{"paperId":null,"title":"AlphaCode data materials"},{"paperId":null,"title":"Yogatama for reviewing the paper"},{"paperId":null,"title":"Meeting our match: Buying 100 percent renewable energy"},{"paperId":null,"title":"worked on infrastructure for running and evaluating experiments"},{"paperId":null,"title":"Removed submissions that are duplicates of others"},{"paperId":null,"title":"Electric Sales, Revenue, and Average Price: Summary Table T5.a: 2021 Residential Average Monthly Bill by Census Division, and State"},{"paperId":null,"title":"Facebook hacker cup"},{"paperId":null,"title":"Mitrichev for helping connect us with Codeforces, and lending competitive programming expertise as the paper was being written"}],"id":"5cbe278b65a81602a864184bbca37de91448a5f5","summary":"AlphaCode is introduced, a system for code generation that achieved an average ranking in the top 54.3% in simulated evaluations on recent programming competitions on the Codeforces platform, marking the first time an artificial intelligence system has performed competitively in programming competitions."},{"url":"https://www.semanticscholar.org/paper/76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","title":"Investigating Explainability of Generative AI for Code through Scenario-based Design","venue":"International Conference on Intelligent User Interfaces","year":2022,"referenceCount":102,"citationCount":8,"influentialCitationCount":0,"publicationDate":"10/02/2022","authors":"Jiao Sun,Q. Liao,Michael J. Muller,Mayank Agarwal,Stephanie Houde,Kartik Talamadupula,Justin D. Weisz","citations":[{"paperId":"24c5450d8fa785e5f85d9427d2d65cf66476ac3a","title":"Toward General Design Principles for Generative AI Applications"},{"paperId":"edb6c93255bbaa879f6d4af173a947a2026ed4c6","title":"A nascent design theory for explainable intelligent systems"},{"paperId":"be747953df4b3269534c54addddc889986550343","title":"Psychological Impact and Influence of Animation on Viewer's Visual Attention and Cognition: A Systematic Literature Review, Open Challenges, and Future Research Directions"},{"paperId":"ba77991d19cf8c50ae2d2efcc9b5fb141acaa7b4","title":"Requirements Engineering for Machine Learning: A Review and Reflection"},{"paperId":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code"},{"paperId":"3fbea6c84b8c78c59392d7bf864dbe681924015f","title":"How Can We Develop Explainable Systems? Insights from a Literature Review and an Interview Study"},{"paperId":"a1ef81e17a9ca41e09aba802040a2eca2744716f","title":"Generation Probabilities are Not Enough: Improving Error Highlighting for AI Code Suggestions"},{"paperId":"a81d05e8812cd4adbd76bf408efdcab05d6bb8d7","title":"Creative Use of XAI In Socio-Technical Systems: A Case Study"}],"references":[{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"59641c10ed7431a3cf841f308367dc2dc0281b74","title":"What Makes Good In-Context Examples for GPT-3?"},{"paperId":"5e1746995debd1f17c24af01514c727598cc5613","title":"Human-Centered Explainable AI (XAI): From Algorithms to User Experiences"},{"paperId":"4abb90edf2ec4045ae62cf6e25725043209bf57b","title":"Explainability Pitfalls: Beyond Dark Patterns in Explainable AI"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"bc89a6fbf43cf911f71e5428d0b4a70fa5a40be9","title":"A Human-Centered Agenda for Intelligible Machine Learning"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"c32b72c3ed3de87aa0b28b2361ee344e18721b9e","title":"Applied AI matters: AI4Code"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"6648728ea8257eec4740f9602d18e75329779201","title":"Who needs to know what, when?: Broadening the Explainable AI (XAI) Design Space by Looking at Explanations Across the AI Lifecycle"},{"paperId":"93db03584d742ee2989016d83f571773ee79afcd","title":"Uncertainty Quantification 360: A Holistic Toolkit for Quantifying and Communicating the Uncertainty of AI"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"310bec02ca79f537f6854f76b991f94ebde70f3a","title":"Operationalizing Human-Centered Perspectives in Explainable AI"},{"paperId":"02d4f8a782da9cb195aeb1efbc79a66b5ed0e32e","title":"Introduction to Explainable AI"},{"paperId":"1c9b419b8d28aaca4f2923f673a0286ab0140ec7","title":"Method for Exploring Generative Adversarial Networks (GANs) via Automatically Generated Image Galleries"},{"paperId":"a56601de82b4f19ae1afab6e6edf18b9b9103f22","title":"Trade-offs for Substituting a Human with an Agent in a Pair Programming Context: The Good, the Bad, and the Ugly"},{"paperId":"ba741d13c7fe68bafdc7268fccac112558a37db9","title":"Designing Ground Truth and the Social Life of Labels"},{"paperId":"6e2f3594eb097c03ec0b046ae3443078b4245f24","title":"Towards A Process Model for Co-Creating AI Experiences"},{"paperId":"82b4a1dde07fa277333d80546501268e8471e7ff","title":"HAI-GEN 2021: 2nd Workshop on Human-AI Co-Creation with Generative Models"},{"paperId":"72ee8a42b05fa5d748f6e9160147b43c4a417acf","title":"Question-Driven Design Process for Explainable AI User Experiences"},{"paperId":"2a36402451c4fcab33c9e7f59fc027167c070aa6","title":"Perfection Not Required? Human-AI Partnerships in Code Translation"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"b0fd54759bc37461bf91b13f7a15c63cc24051b5","title":"Evaluating the Interpretability of Generative Models by Interactive Reconstruction"},{"paperId":"349e9e9c332465f02feba784bd6cb247aa741cb2","title":"The Sanction of Authority: Promoting Public Trust in AI"},{"paperId":"aa6b9d6081646a5461d953dc2412b6e0344cc2d9","title":"How AI Developers Overcome Communication Challenges in a Multidisciplinary Team"},{"paperId":"c8965761083d80ff762ce76c08df92d66e01f37d","title":"Expanding Explainability: Towards Social Transparency in AI systems"},{"paperId":"f156ecbbb9243522275490d698c6825f4d2e01af","title":"Explainable AI: A Review of Machine Learning Interpretability Methods"},{"paperId":"3973e0fab69a00f5ed6a81ca408f60c420fa6e61","title":"Uncertainty as a Form of Transparency: Measuring, Communicating, and Using Uncertainty"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"1311adebe439ccd9e373d594a8df1d720a685190","title":"Code Prediction by Feeding Trees to Transformers"},{"paperId":"c1f6777acbb7d7b22d683ce9eb19deda740c56b0","title":"How Data Scientists Improve Generated Code Documentation in Jupyter Notebooks"},{"paperId":"2a127ba560b74a86bf281fa1960e9e0100acad21","title":"Applied AI matters: AI4Code: applying artificial intelligence to source code"},{"paperId":null,"title":"HCAI@NeurIPS2021: Human Centered AI workshop at NeurIPS 2021"},{"paperId":"61d93a88b4db300f963e6e1dfd86d0cb8b5d231f","title":"Machine Learning Model Cards Transparency Review : Using model card toolkit"},{"paperId":"3a76c06f55f50d4c4d432f6036228e5ef2b841de","title":"Quality Estimation & Interpretability for Code Translation"},{"paperId":"0e1fce327f17d42c42997a67a9f4ad7944e5789a","title":"Participatory Machine Learning Using Community-Based System Dynamics"},{"paperId":"58eaf1cb6dab101551af6256c891f2f27b0d8e60","title":"Towards evaluating and eliciting high-quality documentation for intelligent systems"},{"paperId":"5a68df7665071a69355adffde6db71d263de8ff4","title":"Generative adversarial networks"},{"paperId":"05ceac747bec286129818e9e2fbd37ef034f5ada","title":"Interrogating Data Science"},{"paperId":"684f4cba51f717953ccef410094eb159fea58f33","title":"Bridging the Gap Between Ethics and Practice"},{"paperId":"b82287ae96f67ff12daebb59d953b4b08a9c1e02","title":"Unit Test Case Generation with Transformers"},{"paperId":"eb522cb3651416d8499a2c8ba8d9f95aa33be7a8","title":"Towards Designing Conversational Agents for Pair Programming: Accounting for Creativity Strategies and Conversational Styles"},{"paperId":"8befc59280bca3333d5b3075007b44d37024cfb2","title":"A Methodology for Creating AI FactSheets"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"7e02e25869b60f23c15bff7ffd8d406ff3321fca","title":"Human-Centered Approaches to Fair and Responsible AI"},{"paperId":"77a5e08f361b6f91cac8a24b380a14c12bb93383","title":"Novice-AI Music Co-Creation via AI-Steering Tools for Deep Generative Models"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"2c9d71966e1e8a527a392bbe28aa53f8e0918755","title":"Human-centered Explainable AI: Towards a Reflective Sociotechnical Approach"},{"paperId":"7d089d4cc4aff5c10c1704f02119e2487fc898c9","title":"Questioning the AI: Informing Design Practices for Explainable AI User Experiences"},{"paperId":"a3c86ee77f9622cbdfddf2f335de2ae858fe400a","title":"Mapping Out Human-Centered Data Science: Methods, Approaches, and Best Practices"},{"paperId":"4678aff07f985b72950645a119aa874865ece789","title":"Experiences with Improving the Transparency of AI Models and Services"},{"paperId":"1b0f4bd3872bb590d457990ac2b26b29f770fc44","title":"Explainable machine learning in deployment"},{"paperId":"aac29b1b8627f56a258c52e95092a3fdd863b137","title":"ORES: Lowering Barriers with Participatory Machine Learning in Wikipedia"},{"paperId":"5998dc629e57ad944c3f910a6b43da7780ec6997","title":"Cococo: AI-Steering Tools for Music Novices Co-Creating with Generative Models"},{"paperId":"2695145c36541c70f7f3c01711d3dc885cfe4e0b","title":"Enabling Value Sensitive AI Systems through Participatory Design Fictions"},{"paperId":"9a81b5aed97d222b2506357fce41b3a9651e0ab5","title":"ABOUT ML: Annotation and Benchmarking on Understanding and Transparency of Machine Learning Lifecycles"},{"paperId":"6d1db22a4a5995323c9911058d694c231d4a15f0","title":"WeBuildAI: Participatory Framework for Algorithmic Governance"},{"paperId":"ce177672b00ddf46e4906157a7e997ca9338b8b9","title":"Attention is not not Explanation"},{"paperId":"0de0a44b859a3719d11834479112314b4caba669","title":"A Multiscale Visualization of Attention in the Transformer Model"},{"paperId":"a039ea239e37f53a2cb60c68e0a1967994353166","title":"Analyzing the Structure of Attention in a Transformer Language Model"},{"paperId":"96bbcc5ba6cc1d196c533bc94e98bbdb4df93edc","title":"The Pragmatic Turn in Explainable Artificial Intelligence (XAI)"},{"paperId":"2081ed6854290a479f796f2432c7951ff24232fe","title":"Human-Centered Study of Data Science Work Practices"},{"paperId":"d2f18e0675d86e64fdc6e8beaa83c2b74f6a81b6","title":"Visualizing Uncertainty and Alternatives in Event Sequence Predictions"},{"paperId":"55640ff387069d205fa59da549680892db407a0a","title":"Explainability scenarios: towards scenario-based XAI design"},{"paperId":"1fccba11583dc9e1030713d61bd65e9e9990e39f","title":"Human-Centered Artificial Intelligence and Machine Learning"},{"paperId":"7365f887c938ca21a6adbef08b5a520ebbd4638f","title":"Model Cards for Model Reporting"},{"paperId":"4bb18e9211ee79190bd0c455837a5d89ddf239c4","title":"Increasing Trust in AI Services through Supplier's Declarations of Conformity"},{"paperId":"f7325d232c7ac7d2daaf6605377058db5b5b83cc","title":"A Survey of Methods for Explaining Black Box Models"},{"paperId":"9fb11647537cd9030e966481b77063cb19dc8cd4","title":"Value-Sensitive Algorithm Design"},{"paperId":"21dff47a4142445f83016da0819ffe6dd2947f66","title":"Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)"},{"paperId":"6fe99c4969c2f2d3dde8ada84e7388d74eaf0528","title":"Isolating Sources of Disentanglement in Variational Autoencoders"},{"paperId":"8deee44e1473a441b97b6407e52ca5304f9b15ba","title":"Learning Deep Disentangled Embeddings with the F-Statistic Loss"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"d516daff247f7157fccde6649ace91d969cd1973","title":"The Mythos of Model Interpretability"},{"paperId":"9a22c177e8e6bb0530141c3b7db3f9f40e04ae1a","title":"A detailed study of Software Development Life Cycle (SDLC) Models"},{"paperId":"442e10a3c6640ded9408622005e3c2a8906ce4c2","title":"A Unified Approach to Interpreting Model Predictions"},{"paperId":"9777caede85b213aa567d5257a456942580ede3b","title":"A Survey of Inductive Biases for Factorial Representation-Learning"},{"paperId":"b710f0c9ca89f997371ce06c142ad25be1b35cca","title":"Interpretable Decision Sets: A Joint Framework for Description and Prediction"},{"paperId":"467d5d8fc766e73bfd3e9415f75479823f92c2f7","title":"Rationalizing Neural Predictions"},{"paperId":"022788ecf8685ed30e3f69f9121b5a3d242a22d6","title":"On the naturalness of software"},{"paperId":"c04aaf36c8587e40747212e316d9bf44186ef64a","title":"Developing a Research Agenda for Human-Centered Data Science"},{"paperId":"5091316bb1c6db6c6a813f4391911a5c311fdfe0","title":"“Why Should I Trust You?”: Explaining the Predictions of Any Classifier"},{"paperId":"31f9eb39d840821979e5df9f34a6e92dd9c879f2","title":"Learning Deep Features for Discriminative Localization"},{"paperId":"27e1dbe9f7c71cd6cc1b0357f49aef497e572d09","title":"Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"},{"paperId":"cb030975a3dbcdf52a01cbd1c140711332313e13","title":"Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission"},{"paperId":"1f713ea97c87166875daf650fbdc3950eed8973a","title":"New Initiative: The Naturalness of Software"},{"paperId":"825ca26af5a2a510dbc1a7b97587212bc98ae968","title":"Power to the People: The Role of Humans in Interactive Machine Learning"},{"paperId":"3b6f2f5a97ff34a94c66e19fbed9858e714db113","title":"Migrating code with statistical machine translation"},{"paperId":"2b6382bef79d846cb1492bd0ce24a97432d8370a","title":"Social transparency in networked information exchange: a theoretical framework"},{"paperId":"2c154458fdbed11e77715ba26d60fb527fab90ba","title":"Toolkit to support intelligibility in context-aware applications"},{"paperId":"b062423d2c021274300e5a9f58fc66eea39d1c55","title":"Why and why not explanations improve the intelligibility of context-aware intelligent systems"},{"paperId":"83bc768a6e3d9049e68c64eb50e8f6b2c55b4cce","title":"How HCI interprets the probes"},{"paperId":null,"title":"Phoebe Sengers, and Paul Dourish"},{"paperId":"57c790c90f20a9015ce9aca20e7c0a218bb5cc8a","title":"Scenario-based design"},{"paperId":"14ae6f2231e09e226b99002aa04b5c70f3c59f2b","title":"A model for types and levels of human interaction with automation"},{"paperId":"5df85ae89af55c6d82a1a14836ea6bcfbfc2c0ec","title":"Principles of mixed-initiative user interfaces"},{"paperId":"6ae8b855704dd60df8b186037dd38b43d92c40cd","title":"Design: Cultural probes"},{"paperId":"50934979694fb48e55d0cf38888f67b84ad6601b","title":"Conversational Processes and Causal Explanation"},{"paperId":null,"title":"Can Now Write Its Own Computer Code. That's Good News for Humans"}],"id":"76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","summary":"This work explores explainability needs for GenAI for code and demonstrates how human-centered approaches can drive the technical development of XAI in novel domains."},{"url":"https://www.semanticscholar.org/paper/7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective","venue":"ArXiv","year":2022,"referenceCount":265,"citationCount":3,"influentialCitationCount":0,"publicationDate":"10/02/2022","authors":"Erfan Al-Hossami,Samira Shaikh","citations":[{"paperId":"13ea8689a4fe12e7443f5a3a0a25c5337a2f4cd8","title":"MarianCG: a code generation transformer model inspired by machine translation"},{"paperId":"e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models"},{"paperId":"862c0b672c9defded3111924310a07760cfa27ff","title":"Can NMT Understand Me? Towards Perturbation-based Evaluation of NMT Models for Code Generation"}],"references":[{"paperId":"2236b6036cd1ecba1792d5e1fedbae7cefc3d43b","title":"Can we generate shellcodes via natural language? An empirical study"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"7ad2b12526e77badd629b10880db147afce4864a","title":"Lyra: A Benchmark for Turducken-Style Code Generation"},{"paperId":"c8559021289f08eaf8cf2294e406bc1c6b506d19","title":"Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey"},{"paperId":"4a160efbe80c38cd5eb2f92c7c095b49b113397d","title":"In-IDE Code Generation from Natural Language: Promise and Challenges"},{"paperId":"40c25232bc3a3f36ac856ff517d5c70704f14965","title":"TF-Coder: Program Synthesis for Tensor Manipulations"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"dace03e57056d736f9e24937bdf486e894f8e866","title":"Is neural machine translation approach accurate enough for coding assistance?"},{"paperId":"46104f79702e959798ebfa7e07bfe22f22c1096b","title":"PyTorrent: A Python Library Corpus for Large-scale Language Models"},{"paperId":"5fbcfccd3736969d95ed660d8e6962c86b7a9113","title":"PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"69acc5f67ea7dba13c58d7281b8f0a25ed64f0e8","title":"EVIL: Exploiting Software via Natural Language"},{"paperId":"c56aced0f0c5cfebefadb530cb08d736c3ac5c05","title":"Retrieval Augmented Code Generation and Summarization"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"ebfcbe0a8b238d5a52286fdfaab7be0170dbc91b","title":"FAPR: Fast and Accurate Program Repair for Introductory Programming Courses"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"fd963ea665e7a6e49753652dc7efe5affba5feb0","title":"DreamCoder: bootstrapping inductive program synthesis with wake-sleep library learning"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"67e8e2d3b276c339588b9551e6b20cd62ebdda7c","title":"Text-to-SQL in the Wild: A Naturally-Occurring Dataset Based on Stack Exchange Data"},{"paperId":"cc9ad384ec0d0176ce05865d3866b44d2519bd68","title":"Reading StackOverflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation"},{"paperId":"0c21334be0228431d619a180c809b43be0065bdd","title":"CoSQA: 20,000+ Web Queries for Code Search and Question Answering"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"0b077c9577f4297dcf3da835e253d21965bbc6e0","title":"CoTexT: Multi-task Learning with Code-Text Transformer"},{"paperId":"b15fa9e57fb791899154a0f6c321eb703f1c0b09","title":"Shellcode_IA32: A Dataset for Automatic Shellcode Generation"},{"paperId":"a48743b889db1faf0f04d4b29382634d19975f3f","title":"Toward Code Generation: A Survey and Lessons from Semantic Parsing"},{"paperId":"3e5e2e6825596e2cfae5fdb7f201d4bd945a267f","title":"Text2App: A Framework for Creating Android Apps from Text Descriptions"},{"paperId":"26e3d58181724f9ef77973ff0f65bac06e499fec","title":"ProphetNet-X: Large-Scale Pre-training Models for English, Chinese, Multi-lingual, Dialog, and Code Generation"},{"paperId":"5bfb0cc16b871c75e32a6a9d54dd7db225260e04","title":"CodeTrans: Towards Cracking the Language of Silicone's Code Through Self-Supervised Deep Learning and High Performance Computing"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"f389c09ad85263bdd1bb2518d61c90a8a558441d","title":"MulCode: A Multi-task Learning Approach for Source Code Understanding"},{"paperId":"171440398a1c0f43063a7689e3b385280336fb68","title":"CURE: Code-Aware Neural Machine Translation for Automatic Program Repair"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"c2c7233293d55f201fe5b496234bed1914eea70e","title":"Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"2ad6b59ff14d6bec3f14d8b6480025bbebc50e46","title":"Optimal Neural Program Synthesis from Multimodal Specifications"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"ed2a639dd4a6b3abdcc0040733a264cb1dbcd7d4","title":"PlotCoder: Hierarchical Decoding for Synthesizing Visualization Code in Programmatic Context"},{"paperId":null,"title":"2021b. Codesc: A large code-description"},{"paperId":"8e5b4aad131263457a38adbbffebe1b252802f1e","title":"Text2PyCode: Machine Translation of Natural Language Intent to Python Source Code"},{"paperId":"e13d317fe0178a8b8b67f4af995e7fac12c35014","title":"Analysis of Tree-Structured Architectures for Code Generation"},{"paperId":"253710d510012727fd3662574cc71d024eccd197","title":"HIJaX: Human Intent JavaScript XSS Generator"},{"paperId":"e3d72c8589301c9254561b85d7afb1eafe2afe0a","title":"A Survey of Automatic Code Generation fromNatural Language"},{"paperId":"b029346bfa85fa2fc7a12498ae5f018922ce55f9","title":"Code Generation from Natural Language with Less Prior Knowledge and More Monolingual Data"},{"paperId":null,"title":"Code Clippy Data: A large dataset of code data from Github for research into code language models"},{"paperId":null,"title":"Natural languageguided programming"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"c5ad46ae286e1c8ad3ffa52d7c5481f7dba83e88","title":"Conversational Semantic Parsing"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"d944bf7942297f5670192c5cd33191c26a87973e","title":"Code to Comment “Translation”: Data, Metrics, Baselining & Evaluation"},{"paperId":"f430773cdc0fdd62192a48a372601d6aa40f3d7b","title":"Task-Oriented Dialogue as Dataflow Synthesis"},{"paperId":"da97bd6d2d0a2f11bb011b9925585e086010cff0","title":"A Promising Path Towards Autoformalization and General Artificial Intelligence"},{"paperId":"e0d4587181a8848e73612e8a32b02bd9cc82b595","title":"CoCoNuT: combining context-aware neural translation models using ensemble for program repair"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"1defaab75eac2d4684fb727ace2960b1ea879e4e","title":"Transition-based Semantic Dependency Parsing with Pointer Networks"},{"paperId":"0071ee55f1749e6be586645bab08f0385ee856b4","title":"Graph-based, Self-Supervised Program Repair from Diagnostic Feedback"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"c9aac0037e8abe2da081490cc9d10610aa8fdb3f","title":"Semantic code search using Code2Vec: A bag-of-paths model"},{"paperId":"5461c2b704a84ed2eb77121bc2e4a4fe81ad8b9c","title":"Generating Question Titles for Stack Overflow from Mined Code Snippets"},{"paperId":"01ec011977fc6cda03e8b447e1b5eb0551f1c961","title":"A Simple Language Model for Task-Oriented Dialogue"},{"paperId":"b01ac6b990770092c6784f6eda8f3e94e2feb5a8","title":"Wrex: A Unified Programming-by-Example Interaction for Synthesizing Readable Code for Data Scientists"},{"paperId":"77910e51a40d17157fc798325d06edfa6cff18d6","title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation"},{"paperId":"5e9b611a476e993f03c90424847311cd84e36a06","title":"Optimising the fit of stack overflow code snippets into existing code"},{"paperId":"9c49cdf0ac4665b320262156eb19bf2e39cb1bb4","title":"End-to-End Slot Alignment and Recognition for Cross-Lingual NLU"},{"paperId":"21d9d58ab9537d10927b749eaa1c8b8c5a970579","title":"Leveraging Code Generation to Improve Code Retrieval and Summarization via Dual Learning"},{"paperId":"4ecf4356c3b451b16780788a3f94e422d4deeda5","title":"Tree-structured Attention with Hierarchical Accumulation"},{"paperId":"f1288516ea2f8c0174d27f4e2cc28efb1826193d","title":"Exploring Neural Models for Parsing Natural Language into First-Order Logic"},{"paperId":"e41452747ac0674a7b6534e78be33134fe8ef650","title":"Learning to Represent Programs with Property Signatures"},{"paperId":"92515b7ed018194e340f9edefeb52d9b19f679ef","title":"Detecting Code Clones with Graph Neural Network and Flow-Augmented Abstract Syntax Tree"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"a470dfc3dd30e4a27c8480d6fb817ece6a11d813","title":"Associating Natural Language Comment and Source Code Entities"},{"paperId":"a3afb3795f7054fc5b334d2d6feb92f0999942c7","title":"Exploration of neural machine translation in autoformalization of mathematics in Mizar"},{"paperId":"51a920c3d201eec57bcc2e97e0268304f53b5161","title":"TreeGen: A Tree-Based Transformer Architecture for Code Generation"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"c20c68c45127439139a08adb0b1f2b8354a94d6c","title":"CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"},{"paperId":"388e2fcdcefbe0834e153ab2a0be127092f9674d","title":"DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":null,"title":"CodeBERT: A PreTrained Model for Programming and Natural"},{"paperId":null,"title":"There are various works in code translation (Lachaux et al., 2020"},{"paperId":"0d1f94b7698b8b3e12bb89c2ec571b1ecdd54d20","title":"Custom \"Caring IDE\" for Online Offering of CS1"},{"paperId":"3efa0276409aa6b4bb55cd5cbefd3500cb67119b","title":"Transformer Semantic Parsing"},{"paperId":null,"title":"2020) propose a custom IDE prototype that can be integrated with a dialogue system to deliver personalized learning interventions"},{"paperId":null,"title":"Gluecode: A benchmark for source code machine learning models"},{"paperId":"788d28e234fc69fb07b4a4da7fb1bcf05e5160b5","title":"Multi-Task Learning for Conversational Question Answering over a Large-Scale Knowledge Base"},{"paperId":"d2d44be771d01e277a9912249f2f7c211c393fee","title":"On the fly synthesis of edit suggestions"},{"paperId":"b7af363ff612f41e0c1071ae24ce68a836aaa678","title":"Code Generation as a Dual Task of Code Summarization"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"d530a007ae0493ef6a8167c25bd007104623c504","title":"DIRE: A Neural Approach to Decompiled Identifier Naming"},{"paperId":"545e60873b0fad25407bb6d647f92c905bd2483d","title":"CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases"},{"paperId":"00549af4bc3270e0f688acbf694f912d7ee39cad","title":"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks"},{"paperId":"27e8b827f29a3e674b44032fcba886e21ac9bead","title":"Survey of conversational agents in health"},{"paperId":"9656eeb6bb9fc1f35418791cf0c7310e378f2435","title":"Neural Code Search Evaluation Dataset"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"79b7a61abd4a4b4cf62c26cec815b55b0778f48d","title":"Pythia: AI-assisted Code Completion System"},{"paperId":"3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c","title":"Reranking for Neural Semantic Parsing"},{"paperId":"68c010953e7e485dd6154f14a80637149f07f918","title":"The Language of Programming: A Cognitive Perspective"},{"paperId":"b7f6f4ced1901c5ce78aa2d09ca16d5088220a1f","title":"SampleFix: Learning to Correct Programs by Sampling Diverse Fixes"},{"paperId":"84f7b32fa871a105b39e457fce1b74f57f72e60b","title":"Deep code comment generation with hybrid lexical and syntactical information"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"6b94dcac41325a03956402ff7862fa80936f9ddb","title":"A Survey of Natural Language Generation Techniques with a Focus on Dialogue Systems - Past, Present and Future Directions"},{"paperId":"f79af5e9e96f9c777e8759d791e33e9da83ffa65","title":"SParC: Cross-Domain Semantic Parsing in Context"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"317124ee4c11b0e1f25482395f23013d8e9bdbb4","title":"A Chatbot for Conflict Detection and Resolution"},{"paperId":"8d257e975b390b0507008bc2efc846be2e4c64f9","title":"A Neural Semantic Parser for Math Problems Incorporating Multi-Sentence Information"},{"paperId":"4fcd069322b2249fbf6db44d1e54f36c0b095212","title":"Context Dependent Semantic Parsing over Temporally Structured Data"},{"paperId":"3092325d55f6aa9ba28b0841bdcfd61991a38d48","title":"A Neural Model for Generating Natural Language Summaries of Program Subroutines"},{"paperId":"3838f0e9a0985e01d68afc731b04e71a274dfffd","title":"Learning-Based Recursive Aggregation of Abstract Syntax Trees for Code Clone Detection"},{"paperId":"516497e7fc39ebb126bed28dcc5c6129e500acb8","title":"An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"98d1307bed619b58b4a44acd8e65ac58495776c2","title":"code2seq: Generating Sequences from Structured Representations of Code"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"09c4ff0031ce0d2ad5c4e27203d3eb4a044cc4a0","title":"Say Hello to 'Coding Tutor'! Design and Evaluation of a Chatbot-based Learning System Supporting Students to Learn to Program"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"Hands-on transfer learning with Python: implement advanced deep learning and neural network models using TensorFlow and Keras"},{"paperId":"e92de0c4ef62a84201fac284eb66c37330b5fe1c","title":"Learning to Generate Corrective Patches using Neural Machine Translation"},{"paperId":"472a5227279b45f25508017816af34e3cb3ac0d7","title":"Semantic Parsing for Task Oriented Dialog using Hierarchical Representations"},{"paperId":"649c1148439a4e875dab414ba67bf8c80350af4a","title":"TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"d6b7abd5c17f9e7d83c5b709c27da3a82a9788f2","title":"NAPS: Natural Program Synthesis Dataset"},{"paperId":"4d1c856275744c0284312a3a50efb6ca9dc4cd4c","title":"Know What You Don’t Know: Unanswerable Questions for SQuAD"},{"paperId":"2f541b24f69798e255e04229e8dc78f4d1873fd7","title":"Improving Text-to-SQL Evaluation Methodology"},{"paperId":"e033a0b29af2939fd44e5765d03380c08897a9e8","title":"Deep Code Search"},{"paperId":"c5ecabddd4cdda1b2e97ce663ec34a46c2eb5588","title":"Context-Aware Conversational Developer Assistants"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"fd04e31c25451f9103a0ac2220ac8d7e7884c343","title":"Coarse-to-Fine Decoding for Neural Semantic Parsing"},{"paperId":"669e6be7cd92ba6bda39d9e3a030e72fde07a418","title":"Improving a Neural Semantic Parser by Counterfactual Learning from Human Bandit Feedback"},{"paperId":"15f16252af84a630f1f9442d4e0367b6fba089cf","title":"Deep Code Comment Generation"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"01d9579b81dc85ea2339ef309423296fef29aaf8","title":"(Almost) Zero-Shot Cross-Lingual Spoken Language Understanding"},{"paperId":"dc030c2e55b266c029356a54bb444b7d9b1f2abc","title":"StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow"},{"paperId":"429b65937d4922578a81e1f0ef5aeab7361ae36b","title":"NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System"},{"paperId":"89365ef9f61a62ea610f210083d14f70b8ee2972","title":"VulDeePecker: A Deep Learning-Based System for Vulnerability Detection"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"9858b40d23d329151f202b76aac7ca515dee5913","title":"pix2code: Generating Code from a Graphical User Interface Screenshot"},{"paperId":"be9350a3e8e3dfbce550d78581e4bdb08fae22a4","title":"Generating Regular Expressions from Natural Language Specifications: Are We There Yet?"},{"paperId":"271b0be2b6dfa6b76421206a4b44a7ea5edf8b69","title":"Measuring Transfer of Data-Driven Code Features Across Tasks in Alice"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":null,"title":"2018. A Survey"},{"paperId":null,"title":"Ashish Nagar, and others"},{"paperId":"116a877c774969b53399b1ccaa34d51bfb492ee4","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"759be48bcb76780957593bc4c7cd3756a61defa3","title":"Retrieving and classifying instances of source code plagiarism"},{"paperId":"3ade4d3be53981a1678b1e3a736d01547f7d3b9e","title":"Dialog for Language to Code"},{"paperId":"b0fd7a0f70b64c06031bb915d9aedd44b6550b16","title":"SemFuzz: Semantics-based Automatic Generation of Proof-of-Concept Exploits"},{"paperId":"7456179fa71bc237e051ecb3c02c043a50549029","title":"SQLizer: query synthesis from natural language"},{"paperId":"a26b00c607b6d6a2697f5444f14a9afc37701444","title":"DéjàVu: a map of code duplicates on GitHub"},{"paperId":"dffd889cc95805a9efd0fe2a8d86c1e1db470e78","title":"IDE-Based Learning Analytics for Computing Education"},{"paperId":"29d2ec9b25e5a0844d156a0e7bc19df24a2c3a48","title":"Supervised Deep Features for Software Functional Clone Detection by Exploiting Lexical and Syntactical Information in Source Code"},{"paperId":"8f0642cad60b338d493f4ea414167b08db6f3e0d","title":"Automated Data-Driven Hints for Computer Programming Students"},{"paperId":"3807a517403cfc7ed67fdfea64baf62b9948bcfe","title":"A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"8ff54aa8045b1e30c348cf2ca42259c946cd7a9e","title":"Search-based Neural Structured Learning for Sequential Question Answering"},{"paperId":"32ce5467ff884d2f90a233f4d9606c6e18b1a9d6","title":"Learning a Neural Semantic Parser from User Feedback"},{"paperId":"2c1e874c3b67510a3215e535f5646b362de5bc89","title":"Abstract Syntax Networks for Code Generation and Semantic Parsing"},{"paperId":"668db48c6a79826456341680ee1175dfc4cced71","title":"Get To The Point: Summarization with Pointer-Generator Networks"},{"paperId":"3ff0af64279929a952ee340e645256b7e0580f65","title":"RobustFill: Neural Program Learning under Noisy I/O"},{"paperId":"554c9b3aa8c3b7504810270b18b942a0d22156e7","title":"DeepFix: Fixing Common C Language Errors by Deep Learning"},{"paperId":"e4bcb45990a928ac06df4dc24dc07e16fcac9141","title":"Synthesizing benchmarks for predictive modeling"},{"paperId":"a486e2839291111bb44fa1f07731ada123539f75","title":"Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":"0d24a0695c9fc669e643bad51d4e14f056329dec","title":"An Actor-Critic Algorithm for Sequence Prediction"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":null,"title":"Artificial intelligenceassisted online social therapy for youth"},{"paperId":null,"title":"2017) proposed a fourphase process model for IDE-based data analytics consisting of: (1) data collection, (2) data analysis, (3) intervention design, and (4) intervention"},{"paperId":"7bd07765ed544a784bb1b93aaed36df6dd3cf6ee","title":"NLmaps: A Natural Language Interface to Query OpenStreetMap"},{"paperId":"3abce2e6f85817a1f66398d098115bca0edcfea2","title":"Learning to Fuzz: Application-Independent Fuzz Testing with Probabilistic, Generative Models of Input Data"},{"paperId":"74157ae408173bf713f1e94f15aca1475c43bd74","title":"Neural Generation of Regular Expressions from Natural Language with Minimal Domain Knowledge"},{"paperId":"c7fcaa13db8c89ff1f39b5687ba47f8beee107c4","title":"The Value of Semantic Parse Labeling for Knowledge Base Question Answering"},{"paperId":"32aa2517b03c871c11e521c2a3406f457833e2c3","title":"Summarizing Source Code using a Neural Attention Model"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"aad646a4d164c586b996fae852679a80c36ad43d","title":"Effective compiler error message enhancement for novice programming students"},{"paperId":"15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7","title":"Gaussian Error Linear Units (GELUs)"},{"paperId":"7306437b2145677fe7bf3b7711ac8aa25989f1e3","title":"Simpler Context-Dependent Logical Forms via Model Projections"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"a418437e3f75865bcad2da65ebf422c49a36eacd","title":"A Corpus and Semantic Parser for Multilingual Natural Language Querying of OpenStreetMap"},{"paperId":"b7eac64a8410976759445cce235469163d23ee65","title":"Data Recombination for Neural Semantic Parsing"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"ba30df190664193514d1d309cb673728ed48f449","title":"Incorporating Copying Mechanism in Sequence-to-Sequence Learning"},{"paperId":"b1b59ab6063a856b1e608bdfe640263e4c0ced22","title":"Automated Correction for Syntax Errors in Programming Assignments using Recurrent Neural Networks"},{"paperId":"eda2880f9ddd31aa17cfccfac75fffeb02069562","title":"Students' Syntactic Mistakes in Writing Seven Different Types of SQL Queries and its Application to Predicting Students' Success"},{"paperId":"484ed558a5863060b373e8a2cb7cb302b8c36116","title":"A Convolutional Attention Network for Extreme Summarization of Source Code"},{"paperId":"558ac446dc26bee9789d660a251b75728cb6eeb2","title":"Language to Logical Form with Neural Attention"},{"paperId":"9f2a8e923965b23c11066a2ead79658208f1fae1","title":"Minimum Risk Training for Neural Machine Translation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"1d5972b32a9b5a455a6eef389de5b7fca25771ad","title":"Domain-Adversarial Training of Neural Networks"},{"paperId":"49512270b39636375880d611d7b2192d324f4ba6","title":"Convolutional Neural Networks over Tree Structures for Programming Language Processing"},{"paperId":"89f240184b1260fbdd7307f5ca5284b9dea192e1","title":"Data-Driven Hint Generation in Vast Solution Spaces: a Self-Improving Python Programming Tutor"},{"paperId":"27e1dbe9f7c71cd6cc1b0357f49aef497e572d09","title":"Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"},{"paperId":"9f832bdcbc9d9566f7ab07b7455364bee62086fb","title":"Pseudogen: A Tool to Automatically Generate Pseudo-Code from Source Code"},{"paperId":"bf1262a77e79538ecff1338a71245530fb43e106","title":"Learning a strategy for adapting a program analysis via bayesian optimisation"},{"paperId":"bd06a0d166957b169be53434f1192cc985a587cc","title":"A user-guided approach to program analysis"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"b41e95c8c97846d5ca4c11ef79d7814499cc9663","title":"Compositional Semantic Parsing on Semi-Structured Tables"},{"paperId":"5b85caebb78ccbbd068d3cc8c8d2bc82d32ed4eb","title":"The Normalized Programming State Model: Predicting Student Performance in Computing Courses Based on Programming Behavior"},{"paperId":"25369f56a933e3bfb1d8e1588cdc6c50df93ecae","title":"Building a Semantic Parser Overnight"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"9653d5c2c7844347343d073bbedd96e05d52f69b","title":"Pointer Networks"},{"paperId":"d6f2f611da110b5b5061731be3fc4c7f45d8ee23","title":"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"},{"paperId":"be6665462fd6c56e8756dadf23373bb0c90b2b19","title":"Predicting Program Properties from \"Big Code\""},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"d4ab3e01c4d1308371c76fbc9665701100461e88","title":"Language to Code: Learning Semantic Parsers for If-This-Then-That Recipes"},{"paperId":"d8c9cab98a5fe33cb75aae9cc79a9bafdc4b0fa3","title":"SemEval 2015 Task 18: Broad-Coverage Semantic Dependency Parsing"},{"paperId":null,"title":"enhanced error messages (Becker et al., 2016), and generating hints to programmers (Chow et al., 2017; Rivers and Koedinger, 2017)"},{"paperId":"9277dc7dd5b348645e8fcf4a987e5b3fc9132c7c","title":"Towards a Big Data Curated Benchmark of Inter-project Code Clones"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"cb9588146a4335b96e3bd8c51e4b4e02fc10b7cd","title":"Constructing an Interactive Natural Language Interface for Relational Databases"},{"paperId":"baa919534218b7dfad833f3bd47314be7044c84b","title":"NLyze: interactive programming by natural language for spreadsheet data analysis and manipulation"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"3d1d42c9435b419ac928ebf7bcf4c86a460d6ef4","title":"Semantic Parsing via Paraphrasing"},{"paperId":"75963cf489c5935f27c280906772c2d48999f1c3","title":"Broad-Coverage Semantic Dependency Parsing"},{"paperId":"b29447ba499507a259ae9d8f685d60cc1597d7d3","title":"Semantic Parsing on Freebase from Question-Answer Pairs"},{"paperId":"393f96be13a96a161f0e99f3d90bae945031a57a","title":"An Intelligence-Based Optimization Model of Passenger Flow in a Transportation Station"},{"paperId":"e72e5ee5de14fd463ab58ce830474157258e3578","title":"Abstract Meaning Representation for Sembanking"},{"paperId":"6d7645d6a64b197083cf09eefb87e6e40a91eca8","title":"Universal Conceptual Cognitive Annotation (UCCA)"},{"paperId":"2335e7ff062b7be074e4c4eb1bee025fdb16d1ff","title":"Semantic Parsing with Combinatory Categorial Grammars"},{"paperId":"f7c6b122ee4e928fb88e287c26b9b50d2783415a","title":"Predicting Performance in an Introductory Programming Course by Logging and Analyzing Student Programming Behavior"},{"paperId":"86dd5d1493bbc0c72a739fba5a79f2582d8a497c","title":"Semantic Parsing Freebase: Towards Open-domain Semantic Parsing"},{"paperId":"e510bae5437936c24c18dfb81e5659f9f08a9531","title":"Using Semantic Unification to Generate Regular Expressions from Natural Language"},{"paperId":"cde902f11b0870c695428d865a35eb819b1d24b7","title":"Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions"},{"paperId":"0cdfa97e66ceb76144d4783d25e52c61fd0c4786","title":"Automatic Generation and Reranking of SQL-Derived Answers to NL Questions"},{"paperId":"304f7fd4af9cc82355eaf912a8f120f7429362de","title":"Spreadsheet data manipulation using examples"},{"paperId":"3954a3f80cf1b1f76430d80e924d85f2f1ba6799","title":"Learning to Parse Natural Language Commands to a Robot Control System"},{"paperId":"36d69fec4884389c1709d3ca74394cac814ce4a4","title":"Lexical Generalization in CCG Grammar Induction for Semantic Parsing"},{"paperId":"508e5b724c8b841aecfae864e5d6dcd02eb28772","title":"Bootstrapping Semantic Parsers from Conversations"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"03c85aa6d213f56cbd3602d9eaf2cc72de9f9a7e","title":"AEG: Automatic Exploit Generation"},{"paperId":"a25fbcbbae1e8f79c4360d26aa11a3abf1a11972","title":"A Survey on Transfer Learning"},{"paperId":"fbc6562814e08e416e28a268ce7beeaa3d0708c8","title":"Large-Scale Machine Learning with Stochastic Gradient Descent"},{"paperId":null,"title":"UCI source code data sets"},{"paperId":"81e982e91ab1c307e001301948caeda47cad43e0","title":"Type-logical semantics"},{"paperId":"0ac7b7eb0c9f75154d6faaa7585578c993072956","title":"Introduction to Information Retrieval Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze, Cambridge University Press, 2008"},{"paperId":null,"title":", and Eran Yahav . 2019 . code 2 vec : Learning distributed representations of code"},{"paperId":"774113732db34ce0b797fc3dcceded811fb6edbc","title":"Online Learning of Relaxed CCG Grammars for Parsing to Logical Form"},{"paperId":"812355cec91fa30bb50e9e992a3549af39e4f6eb","title":"One-shot learning of object categories"},{"paperId":"74fe7ec751cd50295b15cfd46389a8fefb37c414","title":"Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars"},{"paperId":"dd2d5ddc0399e0b87c339ebea4042ef2ad6f0317","title":"Guiding a Reinforcement Learner with Natural Language Advice: Initial Results in RoboCup Soccer"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"b50f78c9534182a09c060580811274928702b38d","title":"Towards a theory of natural language interfaces to databases"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"f60d8dd8ca3a7dfa7d0a14988af73084ad93619d","title":"Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing"},{"paperId":"151411a7b32e40dca8a51503135c6e9e3cdbc70c","title":"Automated Construction of Database Interfaces: Intergrating Statistical and Relational Learning for Semantic Parsing"},{"paperId":null,"title":"Chapter 14 - the role of goal orientation in self-regulated learning"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"b7c0e47f8b768258b7d536c21b218e6c46ab8791","title":"Learning to Parse Database Queries Using Inductive Logic Programming"},{"paperId":"c0a48ed7577a7b48288dfb2711cbd86e30636b5f","title":"Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval"},{"paperId":"6dae782f8be8fe5c25dbf2d0d681b3f708bf1a32","title":"Expanding the Scope of the ATIS Task: The ATIS-3 Corpus"},{"paperId":"f3d06b7a298e17e7fe91a902423b66e91f1a20cc","title":"Learning Semantic Grammars with Constructive Inductive Logic Programming"},{"paperId":"880cbe45abf7a3f92c9c52b391050814f8b0031b","title":"Pragmatics of Word Order Flexibility"},{"paperId":"8df509919b31397e225280962c59384fbe83144e","title":"Evaluation of Spoken Language Systems: the ATIS Domain"},{"paperId":"052b1d8ce63b07fec3de9dbb583772d860b7c769","title":"Learning representations by back-propagating errors"},{"paperId":"cd99e520e299e71c67ec2064f48e2394405e85c4","title":"The Commercial Application of Expert Systems Technology"},{"paperId":"ceb3163c56465fda5fef591d0ff0a6c7f434a04d","title":"A Deductive Approach to Program Synthesis"},{"paperId":"ce8cca19455e8d3055c57a9bafe882984c95a201","title":"Syntactic Process"},{"paperId":"6f0aa57820d5f1700461b317faabad9b98d0f70d","title":"Developing a natural language interface to complex data"},{"paperId":"d163709460265aa4901ac41a2b903793ad24b3c2","title":"Knowledge and Reasoning in Program Synthesis"},{"paperId":"e9533991eb9569e4cf515f8520bd9a3e43746fc7","title":"Progress in natural language understanding: an application to lunar geology"},{"paperId":"94d9cbfc474cb6ce961de45a06233b2853ca6724","title":"Toward automatic program synthesis"},{"paperId":"577e96521d62b9ebb5fd67412a21b02e9cd67b90","title":"PROW: A Step Toward Automatic Program Writing"},{"paperId":"a798bca71c8833e49ad9bac22da4b5c3503f1e6a","title":"ELIZA—a computer program for the study of natural language communication between man and machine"},{"paperId":"9b486c647916df9f8be0f8d4fc5c94c493bfaa80","title":"PRINCIPLES OF NEURODYNAMICS. PERCEPTRONS AND THE THEORY OF BRAIN MECHANISMS"},{"paperId":"cccc0a4817fd5f6d8758c66b4065a23897d49f1d","title":"Principles of neurodynamics"}],"id":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","summary":"This survey paper overviews major deep learning methods used in Natural Language Processing (NLP) and source code over the last 35 years and presents a software-engineering centered taxonomy for CI placing each of the works into one category describing how it best assists the software development cycle."},{"url":"https://www.semanticscholar.org/paper/590f6817b42407f96b079e82c935fae298196359","title":"Less is More: Summary of Long Instructions is Better for Program Synthesis","venue":"ArXiv","year":2022,"referenceCount":28,"citationCount":4,"influentialCitationCount":0,"publicationDate":"16/03/2022","authors":"Kirby Kuznia,Swaroop Mishra,Mihir Parmar,Chitta Baral","citations":[{"paperId":"99752e255a866484291866a5ff5cf94e96d6bdc4","title":"Is a Question Decomposition Unit All We Need?"}],"references":[{"paperId":"6cf8a4d05e66266233380f989edaf647eba7e1a5","title":"BioTABQA: Instruction Learning for Biomedical Table Question Answering"},{"paperId":"8abe4f7bfe89db1cf8f74831ca7e3af8c20d7808","title":"Is a Question Decomposition Unit All We Need?"},{"paperId":"05f0d34eb777d74e85b525e86e84d68a21d04a64","title":"Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions"},{"paperId":"31e396eab8edb44f79e3158eeefc3280afb404f4","title":"How Many Data Samples is an Additional Instruction Worth?"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"5d0db797a45ce2453f821f7ded0b547d3fdab054","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"17dd3555fd1ccf1141cf984347fa1b3fd6b009ca","title":"Multitask Prompted Training Enables Zero-Shot Task Generalization"},{"paperId":"3ac5aa6ac59253611ef3cb72a95cbe21ef5dda1b","title":"Reframing Instructional Prompts to GPTk’s Language"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"cbdb45fc16b0885905b91d84281c310e6cb49e9c","title":"Cross-Task Generalization via Natural Language Crowdsourcing Instructions"},{"paperId":null,"title":"InBoXBART: Get instructions into biomedical multi-task learning"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":null,"title":"The Codex model (Chen et al., 2021) is an advanced code generation model that powers GitHub’s Copilot. The state of the art model for program synthesis was introduced by Deepmind called AlphaCode"},{"paperId":null,"title":"2021) introduced the APPS dataset for testing the accuracy of large LMs on program synthesis"},{"paperId":null,"title":"2021a. Program synthesis with large language models. arXiv preprint arXiv:2108.07732"},{"paperId":null,"title":"Jurassic-1: Technical details and evaluation"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"f4061bd225b3be5b3f5b18eb1a229ce991efefeb","title":"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"},{"paperId":"0abcbdf40f872e6baf1c082811d4ae93df787698","title":"Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets"},{"paperId":"3ff0af64279929a952ee340e645256b7e0580f65","title":"RobustFill: Neural Program Learning under Noisy I/O"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":"7ae207cfaf01dc2b6799da67f454190b34994870","title":"A Statistical Semantic Parser that Integrates Syntax and Semantics"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"590f6817b42407f96b079e82c935fae298196359","summary":"A meta-dataset consisting of human and synthesized summaries of the long and complicated programming questions shows that summaries improve performance for introductory and interview programming questions and shows improvement by a small margin for competitive programming questions, implying scope for future research in this direction."},{"url":"https://www.semanticscholar.org/paper/1a903282f7c19dbdb2714b852fb42dbb4675422b","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis","venue":"","year":2022,"referenceCount":47,"citationCount":7,"influentialCitationCount":2,"publicationDate":"25/03/2022","authors":"Erik Nijkamp,Bo Pang,Hiroaki Hayashi,Lifu Tu,Haiquan Wang,Yingbo Zhou,S. Savarese,Caiming Xiong","citations":[{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"},{"paperId":"433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context"},{"paperId":"0f38267a8ba32789f5d3b1b19820f86940fea052","title":"Generation-Augmented Query Expansion For Code Retrieval"},{"paperId":"bf5fbd690f24f873df86d1b0a06579cf42f7dc36","title":"Dialog2API: Task-Oriented Dialogue with API Description and Example Programs"},{"paperId":"8210cef990b8e5cddbc95000e46309bdd25337f7","title":"Asking Clarification Questions for Code Generation in General-Purpose Programming Language"},{"paperId":"27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"}],"references":[{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"5d0db797a45ce2453f821f7ded0b547d3fdab054","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"dc32a984b651256a8ec282be52310e6bd33d9815","title":"Highly accurate protein structure prediction with AlphaFold"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"66c10bf1f11bc1b2d92204d8f8391d087f6de1c4","title":"RoFormer: Enhanced Transformer with Rotary Position Embedding"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f430773cdc0fdd62192a48a372601d6aa40f3d7b","title":"Task-Oriented Dialogue as Dataflow Synthesis"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"00c957711b12468cb38424caccdf5291bb354033","title":"ZeRO: Memory optimizations Toward Training Trillion Parameter Models"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"8323c591e119eb09b28b29fd6c7bc76bd889df7a","title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"},{"paperId":"545e60873b0fad25407bb6d647f92c905bd2483d","title":"CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"f79af5e9e96f9c777e8759d791e33e9da83ffa65","title":"SParC: Cross-Domain Semantic Parsing in Context"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"b227f3e4c0dc96e5ac5426b85485a70f2175a205","title":"Representation Learning with Contrastive Predictive Coding"},{"paperId":null,"title":"JAX: composable transformations of Python+NumPy programs"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"644ca74f80463415613847ab01cff067fb58f0ad","title":"Neuro-Symbolic Program Synthesis"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"7f6245d527e84092111a6adb0f1939d0209d3360","title":"FlashMeta: a framework for inductive program synthesis"},{"paperId":"307fb6c6cfc456ab3e510c08fde39e6e3574fe5d","title":"Automatically improving accuracy for floating point expressions"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"a72055d6b6581f880837c8981300fc93641ef25e","title":"Optimizing database-backed applications with query synthesis"},{"paperId":"84069287da0a6b488b8c933f3cb5be759cb6237e","title":"On the difficulty of training recurrent neural networks"},{"paperId":"308388616c12158423fbf8bd8c441d11d1f432a2","title":"Stochastic superoptimization"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"5090ed315d3ab9f0135c83f287c5021d61929760","title":"Denali: a goal-directed superoptimizer"},{"paperId":"2fb9729c0a3b046c390ad6748773527eae6cfc84","title":"What Is Program Synthesis?"},{"paperId":"94d9cbfc474cb6ce961de45a06233b2853ca6724","title":"Toward automatic program synthesis"},{"paperId":null,"title":"If you use this software, please cite it using these metadata"}],"id":"1a903282f7c19dbdb2714b852fb42dbb4675422b","summary":"The utility of the trained model is shown by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval, and experiments show that the multi-step program synthesis capacity scales as a function of the model size and data size."},{"url":"https://www.semanticscholar.org/paper/a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis","venue":"ArXiv","year":2022,"referenceCount":77,"citationCount":50,"influentialCitationCount":17,"publicationDate":"12/04/2022","authors":"Daniel Fried,Armen Aghajanyan,Jessy Lin,Sida I. Wang,Eric Wallace,Freda Shi,Ruiqi Zhong,Wen-tau Yih,Luke Zettlemoyer,M. Lewis","citations":[{"paperId":"0192638593c430e15e2804a74a3e8a7ecb52d435","title":"On the Design of AI-powered Code Assistants for Notebooks"},{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"},{"paperId":"71acdfd16b9de60ebcc1e3f7af0e1f380f97da29","title":"Transfer Knowledge from Natural Language to Electrocardiography: Can We Detect Cardiovascular Disease Through Language Models?"},{"paperId":"468992bf970c37bd1fef58b78a6c2fcd8c018868","title":"Scaling Laws for Generative Mixed-Modal Language Models"},{"paperId":"b2f256a1ec54325102119a57cb2359840fe68586","title":"SantaCoder: don't reach for the stars!"},{"paperId":"dae74645479f7c1fa3671066f9e24ec6c20c17ec","title":"TrojanPuzzle: Covertly Poisoning Code-Suggestion Models"},{"paperId":"ab8a53be2ff8a756776aa0245ad8da41189d60d6","title":"Fuzzing Deep-Learning Libraries via Large Language Models"},{"paperId":"269df328eec08b56b7b1f38a7555797fe2b999b6","title":"ReCode: Robustness Evaluation of Code Generation Models"},{"paperId":"433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming"},{"paperId":"32b58766a1bfcef7ebba07070a272687aa518206","title":"Explicit Knowledge Transfer for Weakly-Supervised Code Generation"},{"paperId":"27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation"},{"paperId":"ef58e99dbb90bebbdc6187b5cba94dd5973dbb16","title":"Retrieval-Augmented Multimodal Language Modeling"},{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"78f1ca609cd6f789749365c2870e2c2efd8f1fdf","title":"UniMASK: Unified Inference in Sequential Decision Problems"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","title":"Evaluating How Fine-tuning on Bimodal Data Effects Code Generation"},{"paperId":"b1eebb2df3b9ff7ff2b00fb1a786f6ada2caebce","title":"Towards a Mathematics Formalisation Assistant using Large Language Models"},{"paperId":"327f1561b544b0a3b9d8d5d0e6d82c2a5911fca9","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"71280dba5bda65c162f9deaffed7d3d20692ca0a","title":"SecurityEval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques"},{"paperId":"3cba16fc46ac5b35c1cc72a822208aa0097384cc","title":"CodePAD: Sequence-based Code Generation with Pushdown Automaton"},{"paperId":"ca1df79eeb278aacaa20c11451e7c5f47c932ff6","title":"CODEP: Grammatical Seq2Seq Model for General-Purpose Code Generation"},{"paperId":"20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers"},{"paperId":"f031ba42cf82f106200bb03fbb91dd5671a59b9c","title":"Practical Program Repair in the Era of Large Pre-trained Language Models"},{"paperId":"3994eb8e237a94dae1efc6e767a09044b8550ace","title":"FCM: Forgetful Causal Masking Makes Causal Language Models Better Zero-Shot Learners"},{"paperId":"39e40821b7207125e54e6ed7112e55cd38c6f0c3","title":"Language Models of Code are Few-Shot Commonsense Learners"},{"paperId":"259b7a01700c39d5669e88d1434873ea38a13528","title":"In-Context Policy Iteration"},{"paperId":"8fbd7ddf1ea30c991f3b1152a245df77caa18e16","title":"Learning by Distilling Context"},{"paperId":"6032212d5790b6a580d68d469a9895aad6238c89","title":"Diverse Title Generation for Stack Overflow Posts with Multiple Sampling Enhanced Transformer"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"05ccfb058dc6788254742722278b0af2fc87f083","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"},{"paperId":"1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization"},{"paperId":"ec4c8d99eb1c028c43af6d8bbf727392d351cb59","title":"Efficient Training of Language Models to Fill in the Middle"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"96f0f08e2dbeacc89a30d419a9cfb24312bd8da7","title":"BigIssue: A Realistic Bug Localization Benchmark"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs"},{"paperId":"b3a54332a0791751fcf234f6264f242c42ac00d2","title":"DocPrompting: Generating Code by Retrieving the Docs"},{"paperId":"53661ff6fdbfb8557c5b19895fad151792c62da7","title":"Few-shot training LLMs for project-specific code-summarization"},{"paperId":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs"},{"paperId":"1a903282f7c19dbdb2714b852fb42dbb4675422b","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"3d5463a16d85d9d1d0d8ebb4117a31aca3c240ff","title":"Fooling MOSS Detection with Pretrained Language Models"},{"paperId":"09e14c4c80e20e80c052e0adb0d49df51aff718d","title":"Yes, You Can Make an App Too: A Systematic Study of Prompt Engineering in the Automatic Generation of Mobile Applications from User Queries"}],"references":[{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"f07570ed7847a7c95f35f4315afe543745f71e29","title":"Deduplicating Training Data Mitigates Privacy Risks in Language Models"},{"paperId":"d407aa1cc3f9ce8478d3052344947fb49baa04d4","title":"CodeFill: Multi-token Code Completion by Jointly learning from Structure and Naming Sequences"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"c783e1fb3ce8514f981925ee590c00884660ee4e","title":"CM3: A Causal Masked Multimodal Model of the Internet"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"4566c0d22ebf3c31180066ab23b6c445aeec78d5","title":"Deduplicating Training Data Makes Language Models Better"},{"paperId":null,"title":"Natural Language Processing with Transformers"},{"paperId":"fc5d310521668ea4084c4d3b05c617c53c8f7a5b","title":"Efficient Large Scale Language Modeling with Mixtures of Experts"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"1aaddc7a5bfa9fbfad540f682f45ecd1ededcfd8","title":"Learning type annotation: is big data enough?"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"e596b8adbffa546dbc163e817fb3de72744ec4f6","title":"HTLM: Hyper-Text Pre-Training and Prompting of Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"83a86fdf5d42fc70a07a2badd4fc9d42863f9b64","title":"SpreadsheetCoder: Formula Prediction from Semi-structured Context"},{"paperId":"a989f5daa7c438d6b8433efbb1461f1b0af1aa8e","title":"Break-It-Fix-It: Unsupervised Learning for Program Repair"},{"paperId":"4b0ec90dc10e51c1fc983edcd57bb86636d7b3ca","title":"Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"3a1efac4a8aa2f40a8128e034c01f18660652dd5","title":"DOBF: A Deobfuscation Pre-Training Objective for Programming Languages"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"1dc513a688ca92809e504144c3d1e361d1df9927","title":"Reflective Decoding: Beyond Unidirectional Generation with Off-the-Shelf Language Models"},{"paperId":"59cc0871534a20ac98db0663539c6675ab63566f","title":"SequenceR: Sequence-to-Sequence Learning for End-to-End Program Repair"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax"},{"paperId":null,"title":"FairScale: A general purpose modular PyTorch library for high performance and large scale training"},{"paperId":"3efbcfeeb0ea1051a71101d3318da4411081f0b8","title":"Scaling Laws for Autoregressive Generative Modeling"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0071ee55f1749e6be586645bab08f0385ee856b4","title":"Graph-based, Self-Supervised Program Repair from Diagnostic Feedback"},{"paperId":"94551d326be51a57434659093904524c39b877cd","title":"Enabling Language Models to Fill in the Blanks"},{"paperId":"29fc7ff02e68537a86d32978bfd2e8e45258e77d","title":"LambdaNet: Probabilistic Type Inference using Graph Neural Networks"},{"paperId":"1e655fa69c62b430b051224153f701f1b607fd9c","title":"Typilus: neural type hints"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"d79875a95f03e4e3bc2c71ae76cdf05dc95548b9","title":"TypeWriter: neural type prediction with search-based validation"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"db392858262b17aa9c8ff8659738f68fbf832ebe","title":"Structural Language Models of Code"},{"paperId":"eaddaa76aa4c5b1804802d512c7cc1f854b0bda9","title":"CLN2INV: Learning Loop Invariants with Continuous Logic Networks"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"1e411c5e2fe44529a4033a2121adeecc60a7b409","title":"XL-Editor: Post-editing Sentences with XLNet"},{"paperId":"eb4c28a41db510d33bdfe56c4689289f534c9ba0","title":"Restoring ancient text using deep learning: a case study on Greek epigraphy"},{"paperId":"26d9141ed3f021af7533e1d84fc83111d20df925","title":"AutoPandas: neural-backed generators for program synthesis"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"75acc731bdd2b626edc74672a30da3bc51010ae8","title":"CTRL: A Conditional Transformer Language Model for Controllable Generation"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"130277ff64c7171c90d98d7e73f4bda8a0b0c1f9","title":"KERMIT: Generative Insertion-Based Modeling for Sequences"},{"paperId":"ad7129af0644dbcafa9aa2f111cb76526ea444a1","title":"Defending Against Neural Fake News"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"5efadc9019ce3378a0eb6c8f939cdde6c8918b1e","title":"Mask-Predict: Parallel Decoding of Conditional Masked Language Models"},{"paperId":"58d34a4fb936ffe95917d8fb4016ff5e3520429a","title":"Insertion Transformer: Flexible Sequence Generation via Insertion Operations"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"ea1b62a03ab16c4acca6f7041c6f096bedec244b","title":"Deep learning type inference"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"554c9b3aa8c3b7504810270b18b942a0d22156e7","title":"DeepFix: Fixing Common C Language Errors by Deep Learning"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":"f801f962c895d5c606cb52db85f099a4ed8c34e2","title":"Python probabilistic type inference with natural language support"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"216b98bb3d9221d5f5d261864975612e4d0faaa6","title":"EvoSuite: automatic test suite generation for object-oriented software"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"1ef301c1b275091b6a50d620b41df4722f2108f0","title":"Combinatorial sketching for finite programs"},{"paperId":"b7c0e47f8b768258b7d536c21b218e6c46ab8791","title":"Learning to Parse Database Queries Using Inductive Logic Programming"},{"paperId":null,"title":"RFC1321: The MD5 message-digest algorithm"},{"paperId":null,"title":"RFC1321: The MD5"},{"paperId":"2fb9729c0a3b046c390ad6748773527eae6cfc84","title":"What Is Program Synthesis?"},{"paperId":"f39a2c11983b21fd5054d5393614959bfbc4e50f","title":"Space/time trade-offs in hash coding with allowable errors"}],"id":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","summary":"INCODER is introduced, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling) and the ability to condition on bidirectional context substantially improves performance on challenging tasks such as type inference, comment generation, and variable re-naming."},{"url":"https://www.semanticscholar.org/paper/47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":13,"influentialCitationCount":4,"publicationDate":"25/04/2022","authors":"Freda Shi,Daniel Fried,Marjan Ghazvininejad,Luke Zettlemoyer,Sida I. Wang","citations":[{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"},{"paperId":"27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"a4bdc300db297756f36bedee2859b62df8e268c2","title":"Follow the Wisdom of the Crowd: Effective Text Generation via Minimum Bayes Risk Decoding"},{"paperId":"20e4ca6368a0731d3cbbd2e68aa90cef7384dc72","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"}],"references":[{"paperId":"79b88230fabb59a1d368641bbc822af0f09bf262","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"5d0db797a45ce2453f821f7ded0b547d3fdab054","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"53c0abe83fe9b4fdaf2208295d8504fcf5241694","title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models"},{"paperId":"eea7bca03bda3ee2448cd012bbcb2b33822861d8","title":"Noisy Channel Language Model Prompting for Few-Shot Text Classification"},{"paperId":"5fbcfccd3736969d95ed660d8e6962c86b7a9113","title":"PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"a6a7724763d8adba466519489b0b9d209e7f2d15","title":"BARTScore: Evaluating Generated Text as Text Generation"},{"paperId":"ac879df2cc36f3f824fa24149517622b6bc7bd09","title":"Implicit Representations of Meaning in Neural Language Models"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"a847237e36b954c60e1959152468ebed0118f286","title":"Factual Probing Is [MASK]: Learning vs. Learning to Recall"},{"paperId":"3a1efac4a8aa2f40a8128e034c01f18660652dd5","title":"DOBF: A Deobfuscation Pre-Training Objective for Programming Languages"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"85e7d63f75c0916bd350a229e040c5fbb1472e7a","title":"Making Pre-trained Language Models Better Few-shot Learners"},{"paperId":"994dbb06b970eb842b7782e4df75ca1633f83e4e","title":"Sampling-Based Minimum Bayes Risk Decoding for Neural Machine Translation"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":null,"title":"Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing"},{"paperId":"e6e8a2c56243847b77b604259cde9e10a2daccb8","title":"Semantic Evaluation for Text-to-SQL with Distilled Test Suite"},{"paperId":"02eaaf87f9cae34cca398fed146079e6eeb1f868","title":"Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"8b6c9adf85a9d6391e3ccd503c2e5af929a36735","title":"Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation"},{"paperId":"ce18780963b067a1295fc847e7ab33f2fcbfaca1","title":"Efficient Second-Order TreeCRF for Neural Dependency Parsing"},{"paperId":"ad5970584754cc7a1d91c95ab84a1e210258183a","title":"UnifiedQA: Crossing Format Boundaries With a Single QA System"},{"paperId":"77910e51a40d17157fc798325d06edfa6cff18d6","title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation"},{"paperId":"53a77e8f73f2ca422d6e38fa9ecc490231ac044c","title":"Neural Text Generation with Unlikelihood Training"},{"paperId":null,"title":"AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts"},{"paperId":null,"title":"How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423–438"},{"paperId":"708f8c0eb5032edd6f31663a27febbb0529cbcf3","title":"Visually Grounded Neural Syntax Acquisition"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"fd04e31c25451f9103a0ac2220ac8d7e7884c343","title":"Coarse-to-Fine Decoding for Neural Semantic Parsing"},{"paperId":"2012c176d7b003eb57a282bfd8681190704fb965","title":"Learning to Map Context-Dependent Sentences to Executable Formal Queries"},{"paperId":"429b65937d4922578a81e1f0ef5aeab7361ae36b","title":"NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System"},{"paperId":"3807a517403cfc7ed67fdfea64baf62b9948bcfe","title":"A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2c1e874c3b67510a3215e535f5646b362de5bc89","title":"Abstract Syntax Networks for Code Generation and Semantic Parsing"},{"paperId":"bcf16c08a41009d9f9174c6f72b2ff534232c147","title":"Sequence-based Structured Prediction for Semantic Parsing"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"67d5f03c4921e75a9c0dd5bd699ad74512e52eb8","title":"Mathematical Statistics : Basic Ideas and Selected Topics, Volumes I-II Package"},{"paperId":null,"title":"Efficient multipass decoding for synchronous context free grammars"},{"paperId":"779b9d2437e68ea12d2f271b01f913460e16b91d","title":"Bayes Risk Minimization in Natural Language Parsing"},{"paperId":"e2a68774f92d1e894cbbbef2c819e4592990eb4b","title":"Minimum Bayes-Risk Decoding for Statistical Machine Translation"},{"paperId":"cd5a169879504ea91660a443b9151753cc29c42f","title":"Minimum Bayes-risk automatic speech recognition"}],"id":"47e15941c8b157873c8264e4bf50318d1ba5cd18","summary":"This work introduces execution result– based minimum Bayes risk decoding (MBR-EXEC) for program selection and shows that it improves the few-shot performance of pretrained code models on natural-language-to-code tasks, suggesting it as an effective approach for natural language to code translation."},{"url":"https://www.semanticscholar.org/paper/6074c7b75f27ca9adb6d74b080c07d5d079c3ea0","title":"Improving automatically generated code from Codex via Automated Program Repair","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":4,"influentialCitationCount":0,"publicationDate":2022,"authors":"Zhiyu Fan,Xiang Gao,Abhik Roychoudhury,Shin Hwei Tan","citations":[{"paperId":"a3e355b5de868f34fdfa2500415c5f74c69d2091","title":"Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models"},{"paperId":"e9a64e58855dbbc725203d0202ceb9e7f8b7bb36","title":"A Survey of Learning-based Automated Program Repair"},{"paperId":"459176532c85ae72f8b5cb35589b72468401d844","title":"SelfAPR: Self-supervised Program Repair with Test Execution Diagnostics"}],"references":[],"id":"6074c7b75f27ca9adb6d74b080c07d5d079c3ea0","summary":"This study systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests, revealing that automatically generated codes share some common programming mistakes with human-crafted solutions, indicating existing APR tools have the potential to fix auto-generated code."},{"url":"https://www.semanticscholar.org/paper/6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":77,"citationCount":9,"influentialCitationCount":1,"publicationDate":"05/07/2022","authors":"Hung Le,Yue Wang,Akhilesh Deepak Gotmare,S. Savarese,S. Hoi","citations":[{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming"},{"paperId":"045a6f92b58f08d5b305dd5d661316a506ee4d43","title":"Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"407b9e9478ba6bff43ce4b20e8b6cb2b303477d2","title":"P LANNING WITH L ARGE L ANGUAGE M ODELS FOR C ODE G ENERATION"},{"paperId":"8d282bf295d655e38b63ddbc7cdc94b188df8bb2","title":"G ENERATING S EQUENCES BY L EARNING TO [S ELF -]C ORRECT"}],"references":[{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"47c740e858d3dfec0bf95104600851a8a2bec9ff","title":"VarCLR: Variable Semantic Representation Pre-training via Contrastive Learning"},{"paperId":"3f791ea6584b78fdf0ed80560d09d9496cf5a353","title":"Learning to Complete Code with Sketches"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a176b0de62840f7118006277d94bbc1547162a4d","title":"Learning to Synthesize Programs as Interpretable and Generalizable Policies"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"87fe4a67e4cb5b8cdbc5da4d98888c037fb7d043","title":"Learning Autocompletion from Real-World Datasets"},{"paperId":"07bcda1dff9bb696ea9cbc69303eee8bd3d85bd6","title":"GeDi: Generative Discriminator Guided Sequence Generation"},{"paperId":"04413d13c76c4eadf1adcbbe88c3a72e6462f166","title":"Fast and Memory-Efficient Neural Code Completion"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax"},{"paperId":"7093ad29f2de2781cf5ed6e6d212ed31ebaa2c8b","title":"Latent Execution for Neural Program Synthesis Beyond Domain-Specific Languages"},{"paperId":"3f97c2067cde9377e50b3160bbd7982c94abd88a","title":"An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions"},{"paperId":"4fa24cc5b17e8ff1eb5a01fd37a9d267a57ac563","title":"Recipes for Safety in Open-domain Chatbots"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"f669b5406320a83e94b4dd439806211c8b1e95fc","title":"Write, Execute, Assess: Program Synthesis with a REPL"},{"paperId":"7e2f5eca9465cf114043ed6c95ea59d9dbea45a1","title":"Learning to Infer and Execute 3D Shape Programs"},{"paperId":"58dcc24c63ed179d9a9b458d5f1284a7a297c5d6","title":"Learning to Describe Scenes with Programs"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"c35f8f48f748c040c33bbe1f21c794e209341987","title":"Neural Program Synthesis from Diverse Demonstration Videos"},{"paperId":"2fc1cfc75d6ba80846d64fbec424f6c35682f5d8","title":"Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing"},{"paperId":"5218e7da3b13d097e8a9fdc9fdb5d68ff619edc0","title":"Synthesizing Programs for Images using Reinforced Adversarial Learning"},{"paperId":"dea6aeb514b1969ab879c793d46a0d2eceaa2cbf","title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis"},{"paperId":"74b284a66e75b65f5970d05bac000fe91243ee49","title":"Video Captioning via Hierarchical Reinforcement Learning"},{"paperId":"9f0b217cb21ec8c19610e28bd0805d7871456e4b","title":"Learning to Infer Graphics Programs from Hand-Drawn Images"},{"paperId":"116a877c774969b53399b1ccaa34d51bfb492ee4","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"47f65c165f7ccedd4c18189d4690eec5369dd9c5","title":"SQLNet: Generating Structured Queries From Natural Language Without Reinforcement Learning"},{"paperId":"e60f6be86bc88fc2309b17d3df2885886448d16e","title":"Neural Scene De-rendering"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2e17cf6a339fd071ad222062f868e882ef4120a4","title":"Inferring and Executing Programs for Visual Reasoning"},{"paperId":"1ae84940e212e2e3ca132c7aa0878baf0fda06cd","title":"From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood"},{"paperId":"c689f73f8ea65c6e81c628f2b37feae09b29e46b","title":"Deep Reinforcement Learning-Based Image Captioning with Embedding Reward"},{"paperId":"3ff0af64279929a952ee340e645256b7e0580f65","title":"RobustFill: Neural Program Learning under Noisy I/O"},{"paperId":"6c8353697cdbb98dfba4f493875778c4286d3e3a","title":"Self-Critical Sequence Training for Image Captioning"},{"paperId":"644ca74f80463415613847ab01cff067fb58f0ad","title":"Neuro-Symbolic Program Synthesis"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":"0d24a0695c9fc669e643bad51d4e14f056329dec","title":"An Actor-Critic Algorithm for Sequence Prediction"},{"paperId":"35c1668dc64d24a28c6041978e5fcca754eb2f4b","title":"Sequence Level Training with Recurrent Neural Networks"},{"paperId":"a2499dd426c46c645ee805d7594b6687547c72d4","title":"Neural Random Access Machines"},{"paperId":"024006d4c2a89f7acacc6e4438d156525b60a98f","title":"Continuous control with deep reinforcement learning"},{"paperId":"3411535f7888a943853895e8eef2bb0b6d328c2a","title":"Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis"},{"paperId":"df137487e20ba7c6e1e2b9a1e749f2a578b5ad99","title":"Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks"},{"paperId":"370a5f4b305540f35821d2179c384f95f5a63eee","title":"Toward Deep Learning Software Repositories"},{"paperId":"687e80eb70c7bbad6001006d9269b202650a3354","title":"Deep Convolutional Inverse Graphics Network"},{"paperId":"d38e8631bba0720becdaf7b89f79d9f9dca45d82","title":"Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets"},{"paperId":"258986132bf17755fe8263e42429fe73218c1534","title":"CIDEr: Consensus-based image description evaluation"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"304f7fd4af9cc82355eaf912a8f120f7429362de","title":"Spreadsheet data manipulation using examples"},{"paperId":"c26770f29afafe22f2a507506e3f43c413f6a619","title":"Learning Programs: A Hierarchical Bayesian Approach"},{"paperId":"af5a7be7bfc592e400e2f12a24ad3e6a5a44b58f","title":"Learning from examples to improve code completion systems"},{"paperId":"eb537c6403932af7d1c4d819d2f0179df23baaee","title":"How Program History Can Improve Code Completion"},{"paperId":"97efafdb4a3942ab3efba53ded7413199f79c054","title":"Reinforcement Learning: An Introduction"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"4c915c1eecb217c123a36dc6d3ce52d12c742614","title":"Simple statistical gradient-following algorithms for connectionist reinforcement learning"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"a20f0ce0616def7cc9a87446c228906cd5da093b","title":"Policy Gradient Methods for Reinforcement Learning with Function Approximation"},{"paperId":"ac4af1df88e178386d782705acc159eaa0c3904a","title":"Actor-Critic Algorithms"},{"paperId":"69d7086300e7f5322c06f2f242a565b3a182efb5","title":"In Advances in Neural Information Processing Systems"},{"paperId":"22069cd4504656d3bb85748a4d43be7a4d7d5545","title":"Temporal credit assignment in reinforcement learning"},{"paperId":"dd7abc005846b18c9a78ab80467cbbaedb643456","title":"A Methodology for LISP Program Construction from Examples"},{"paperId":"94d9cbfc474cb6ce961de45a06233b2853ca6724","title":"Toward automatic program synthesis"},{"paperId":"577e96521d62b9ebb5fd67412a21b02e9cd67b90","title":"PROW: A Step Toward Automatic Program Writing"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"}],"id":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","summary":"This work proposes “CodeRL”, a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL), and treats the code-generating LM as an actor network, and introduces a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor."},{"url":"https://www.semanticscholar.org/paper/b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs","venue":"ArXiv","year":2022,"referenceCount":33,"citationCount":3,"influentialCitationCount":1,"publicationDate":2022,"authors":"Shuyan Zhou,Uri Alon,Frank F. Xu,Zhengbao Jiang,Graham Neubig","citations":[{"paperId":"433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"}],"references":[{"paperId":"bcd4c46e4d75ddedb6138cfd77600c6d964a9aa8","title":"Natural Language to Code Translation with Execution"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"1f11601f9eb8c0bee1d1d2cc7f4fa187e8c5e525","title":"The impact of lexical and grammatical processing on generating code from natural language"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"a0e92f6e9564b8c38b6649ae71b892ddfb988faa","title":"Controllable Semantic Parsing via Retrieval Augmentation"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"c56aced0f0c5cfebefadb530cb08d736c3ac5c05","title":"Retrieval Augmented Code Generation and Summarization"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"c26759e6c701201af2f62f7ee4eb68742b5bf085","title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"bde0c85ed3d61de2a8874ddad70497b3d68bc8ad","title":"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"},{"paperId":"45793225c6593db60e9efd95bce1d70bf4844198","title":"Evaluation Paradigms in Question Answering"},{"paperId":"58ed1fbaabe027345f7bb3a6312d41c5aac63e22","title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"paperId":"77910e51a40d17157fc798325d06edfa6cff18d6","title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation"},{"paperId":"79cd9f77e5258f62c0e15d11534aea6393ef73fe","title":"Dense Passage Retrieval for Open-Domain Question Answering"},{"paperId":"34733eaf66007516347a40ad5d9bbe1cc9dacb6b","title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"832fff14d2ed50eb7969c4c4b976c35776548f56","title":"REALM: Retrieval-Augmented Language Model Pre-Training"},{"paperId":"3c9fa66674e1053eb9de12076c48f13e46422c29","title":"CLAI: A Platform for AI Skills on the Command Line"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2b88aefb70cd52a4d6899020f4be97c669a5edcb","title":"RTFM: Generalising to Novel Environment Dynamics via Reading"},{"paperId":"d3e13d2514edaf74b863bfbe45a739c32a7689e1","title":"Retrieval-Based Neural Code Generation"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"429b65937d4922578a81e1f0ef5aeab7361ae36b","title":"NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"2c1e874c3b67510a3215e535f5646b362de5bc89","title":"Abstract Syntax Networks for Code Generation and Semantic Parsing"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"b5defb8729294614d40e0247eabfdc483bf1a992","title":"Bimodal Modelling of Source Code and Natural Language"},{"paperId":"99f1921a8ba5ceecb18e5ca7ff19b7f95c4e250d","title":"Learning to Win by Reading Manuals in a Monte-Carlo Framework"},{"paperId":"f6e3e57567e9803718623ec088cd7fea65cfbc9d","title":"Relevance weighting of search terms"}],"id":"b31b21d0750e849badfe76000e8170482f32b9be","summary":"DocCoder is introduced : an approach that explicitly leverages code manuals and documentation by retrieving the relevant documentation given the natural language intent, and generating the code based on the NL intent and the retrieved documentation."},{"url":"https://www.semanticscholar.org/paper/63c670ba8018da0a7e33c34ffd84c2a3ca54b894","title":"Language Models Can Teach Themselves to Program Better","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":2,"influentialCitationCount":1,"publicationDate":"29/07/2022","authors":"Patrick M. Haluptzok,Matthew Bowers,A. Kalai","citations":[{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"66eae7128c34dd7967d79224eb9dbc978773c3d0","title":"I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation"}],"references":[{"paperId":"41498583394cfd957986dd5553b497e59e3677c0","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"0d08ffccc982781e310bb184397bbe64b9aef157","title":"Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"011a4019aa0d0ce3edfa56bb2ca1e7586eb43fb2","title":"Training Compute-Optimal Large Language Models"},{"paperId":"23c265ba884b92ecbd9d18641078d964697e4590","title":"Generating Training Data with Language Models: Towards Zero-Shot Language Understanding"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"21d45b4923ad165fbb6612e08d06f9d786f9b4cc","title":"Symbolic Knowledge Distillation: from General Language Models to Commonsense Models"},{"paperId":"1aed58bd07026492194672adec494dc37c894a28","title":"Leveraging Automated Unit Tests for Unsupervised Code Translation"},{"paperId":"be7cb8f79bc018e57467168fc0c7f8ad59bba04f","title":"Adaptive Testing and Debugging of NLP Models"},{"paperId":null,"title":"2022] use a human-in-the-loop approach to NLP"},{"paperId":null,"title":"2022] has enabled synthesis in general-purpose programming languages"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"fd963ea665e7a6e49753652dc7efe5affba5feb0","title":"DreamCoder: bootstrapping inductive program synthesis with wake-sleep library learning"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"b769b629c8de35b16735214251d6b4e99cb55762","title":"Generating Datasets with Pretrained Language Models"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"58c74cec28f3416b9a1d308bb2d6519d21d53ab0","title":"LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"1728cb805a9573b59330890ba9723e73d6c3c974","title":"Knowledge Distillation: A Survey"},{"paperId":"8990154f515dadf6d1bfda745e62a67dc3b0e709","title":"A large-scale benchmark for few-shot program induction and synthesis"},{"paperId":null,"title":"2021]. However, these works do not consider the AI system"},{"paperId":null,"title":"Recent work in problem solving Cobbe et al"},{"paperId":null,"title":"Temperature controls the amount of diversity in the code solutions generated by Neo. All experiments in our paper were done with a fixed temperature of 0.8, based on the recommendation"},{"paperId":"5fe0a4af3bd1479d5e39fbda2215c86bce54722b","title":"Generative Language Modeling for Automated Theorem Proving"},{"paperId":"f3313f5c49b756243191c3f39c5f93692c6e4a69","title":"Learning to Prove from Synthetic Theorems"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":null,"title":"GitHub - lmcinnes/umap: Uniform Manifold Approximation and Projection (UMAP)"},{"paperId":null,"title":"2020] show potential value in learning to prove theorems or solve problems"},{"paperId":"a15763582df784b43548c6d53edfd55568c35168","title":"Synthetic Datasets for Neural Program Synthesis"},{"paperId":"f669b5406320a83e94b4dd439806211c8b1e95fc","title":"Write, Execute, Assess: Program Synthesis with a REPL"},{"paperId":"f9717d29840f4d8f1cc19d1b1e80c5d12ec40608","title":"A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play"},{"paperId":"d6b7abd5c17f9e7d83c5b709c27da3a82a9788f2","title":"NAPS: Natural Program Synthesis Dataset"},{"paperId":"3a288c63576fc385910cb5bc44eaea75b442e62e","title":"UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction"},{"paperId":"45dfef0cc1ed96558c1c650432ce39d6a1050b6a","title":"Fixing Weight Decay Regularization in Adam"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":null,"title":"Like English descriptions, PBE is inherently ambiguous"},{"paperId":"0c908739fbff75f03469d13d4a1a07de3414ee19","title":"Distilling the Knowledge in a Neural Network"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"43dcda631f8a0d39949ba3f9e3e22101db4daba0","title":"A Machine Learning Framework for Programming by Example"},{"paperId":"168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74","title":"Scikit-learn: Machine Learning in Python"},{"paperId":"1c46943103bd7b7a2c7be86859995a4144d1938b","title":"Visualizing Data using t-SNE"},{"paperId":"2fb9729c0a3b046c390ad6748773527eae6cfc84","title":"What Is Program Synthesis?"},{"paperId":null,"title":"(set(days)) <= k and (n -len(set(days))) * n >= n * (1 + (n -1) // k) and numx <= n // 2 and numx != nums"}],"id":"63c670ba8018da0a7e33c34ffd84c2a3ca54b894","summary":"This work shows how generating synthetic programming puzzles and solutions, veriﬁed for correctness by a Python interpreter, can be used to improve performance in solving test puzzles from P3, a public benchmark set of Python Programming Puzzles."},{"url":"https://www.semanticscholar.org/paper/453a8fac3be9282be53908f0735160d0d21e0f48","title":"Repair Is Nearly Generation: Multilingual Program Repair with LLMs","venue":"ArXiv","year":2022,"referenceCount":48,"citationCount":5,"influentialCitationCount":0,"publicationDate":"24/08/2022","authors":"Harshit Joshi,J. Cambronero,Sumit Gulwani,Vu Le,Ivan Radicek,Gust Verbruggen","citations":[{"paperId":"0885556b71b24a641b4ffe139afd4d2712228cff","title":"Beware of the Unexpected: Bimodal Taint Analysis"},{"paperId":"a3e355b5de868f34fdfa2500415c5f74c69d2091","title":"Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models"},{"paperId":"e9a64e58855dbbc725203d0202ceb9e7f8b7bb36","title":"A Survey of Learning-based Automated Program Repair"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"}],"references":[{"paperId":"1e3ec5709b2ca43233c566cd77dbabbd6892bfa6","title":"Neurosymbolic repair for low-code formula languages"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code"},{"paperId":"23e8ed7568454e11d9a6fecb8242e1d16b1828d5","title":"Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"166b64f2ae8e52f5779682fab756cbd617a6e74b","title":"A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level"},{"paperId":"3f791ea6584b78fdf0ed80560d09d9496cf5a353","title":"Learning to Complete Code with Sketches"},{"paperId":null,"title":"StackOverflow"},{"paperId":null,"title":"ANNOY library"},{"paperId":"1444536496d8064f33e10b38b5820fecfab5b367","title":"Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs"},{"paperId":"4f68e07c6c3173480053fd52391851d6f80d651b","title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"a989f5daa7c438d6b8433efbb1461f1b0af1aa8e","title":"Break-It-Fix-It: Unsupervised Learning for Program Repair"},{"paperId":"5d47236f0e81e03484752908d476cd988af6c5b1","title":"SYNFIX: Automatically Fixing Syntax Errors using Compiler Diagnostics"},{"paperId":"1a26fac55bbcc6d40070742e7dc8b9752281f435","title":"A critical review on the evaluation of automated program repair systems"},{"paperId":"2cbb8de53759e75411bc528518947a3094fbce3a","title":"Billion-Scale Similarity Search with GPUs"},{"paperId":"0505f17c4052366cbc4fad99150d3542edf85faa","title":"TFix: Learning to Fix Coding Errors with a Text-to-Text Transformer"},{"paperId":null,"title":"Software developers, Quality Assurance Analysts, and testers : Occupational outlook handbook"},{"paperId":"5eba2afc862d642c9ebec3946875130fd43823ab","title":"The Adoption of JavaScript Linters in Practice: A Case Study on ESLint"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0071ee55f1749e6be586645bab08f0385ee856b4","title":"Graph-based, Self-Supervised Program Repair from Diagnostic Feedback"},{"paperId":"e662ce9f731ceb0ba4d19b3baf004bc4878fe210","title":"Calibration of probability predictions from machine‐learning and statistical models"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"246063d33bf9fd2d95a2cd26096c7ce6ecde0ab7","title":"Don't Panic! Better, Fewer, Syntax Errors for LR Parsers"},{"paperId":"9b1d319e06e3d224ebdb59c23a32a7883027cbd6","title":"SampleFix: Learning to Correct Programs by Efficient Sampling of Diverse Fixes"},{"paperId":"b5abecf78c1bd975449e34350289a904e6ef35cc","title":"Automated program repair"},{"paperId":"3b656bba99dbcf6c6d8fd764d53ea64cd38c7050","title":"Measuring Calibration in Deep Learning"},{"paperId":"c99179ca3784e3465fd9ed049d7f34b50d39393e","title":"Ensemble learning: A survey"},{"paperId":"e1531c72052c009822f13590964715e1ef027e14","title":"Automatic Software Repair: A Survey"},{"paperId":"b599ce0f548a73c156e94c7bcca8aed5b0a9678b","title":"Compilation Error Repair: For the Student Programs, From the Student Programs"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"a948e30cb47360d23d22f73008880c9c27d034c6","title":"HappyFace: Identifying and predicting frustrating obstacles for learning programming at scale"},{"paperId":"d6d0080e4ef0897e5d17b33f55f44f2b7af40454","title":"Automatic Grading and Feedback using Program Repair for Introductory Programming Courses"},{"paperId":"554c9b3aa8c3b7504810270b18b942a0d22156e7","title":"DeepFix: Fixing Common C Language Errors by Deep Learning"},{"paperId":"5b0602f9d0f1384dc95335c6ed220fef40a7e186","title":"sk_p: a neural program corrector for MOOCs"},{"paperId":"7f6245d527e84092111a6adb0f1939d0209d3360","title":"FlashMeta: a framework for inductive program synthesis"},{"paperId":"2e4e8bdd65e66ac24a85589ee4a46ff0e26c155f","title":"An Empirical Study on Real Bug Fixes"},{"paperId":"010d163129deb9a5fdb611a565ed1fbb4e62a114","title":"37 Million Compilations: Investigating Novice Programming Mistakes in Large-Scale Student Data"},{"paperId":"90441b975380c806320420724fc9d0ec77dbefdb","title":"The strength of random search on automated program repair"},{"paperId":"05607111cf79330d56164a10d351dbf94e2cfa44","title":"SemFix: Program repair via semantic analysis"},{"paperId":"3284cce500a3138fac2a0b9802325589a6a75984","title":"Calibration of Machine Learning Models"},{"paperId":"f69412d8c00780b66ec14b09c5045a5c2ec8250e","title":"Using Mutation to Automatically Suggest Fixes for Faulty Programs"},{"paperId":"ab2648283e6bf167d6b6ff843fe80ed4b428ceed","title":"On the automation of fixing software bugs"},{"paperId":"46ff473758bc14f31bfecda49f7339b91cfb5228","title":"Debugging: the good, the bad, and the quirky -- a qualitative analysis of novices' strategies"},{"paperId":"6d12a1d23b21a9b170118a56386552bc5d4727de","title":"A Mathematical Theory of Communication"},{"paperId":"a49ae26b7cc9017e9578e32d73586164662f23b8","title":"Maxims for malfeasant designers, or how to design languages to make programming as difficult as possible"},{"paperId":"b2f8876482c97e804bb50a5e2433881ae31d0cdd","title":"Binary codes capable of correcting deletions, insertions, and reversals"}],"id":"453a8fac3be9282be53908f0735160d0d21e0f48","summary":"This work introduces RING, a multilingual repair engine powered by a large language model trained on code (LLMC) such as Codex that enables a ﬂipped model for programming assistance, one where the programmer writes code and the AI assistance suggests code, compared to traditional code suggestion technology."},{"url":"https://www.semanticscholar.org/paper/41f5e1ad7793593befc0b9c38f756836e8b07c98","title":"Code4ML: a Large-scale Dataset of annotated Machine Learning Code","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/10/2022","authors":"Anastasia Drozdova,P. Guseva,E. Trofimova,Anna Scherbakova,Andrey Ustyuzhanin","citations":[],"references":[{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"bea6af010fc02f5ac29edfc17096be6078edab46","title":"A Survey on Deep Learning for Software Engineering"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"24dbf2b111ad3e5f9984f127c694e9298a92a7f8","title":"KGTorrent: A Dataset of Python Jupyter Notebooks from Kaggle"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"3a7fa673ff8ec4ec2f322473de005f3cd09ea820","title":"AutoML: A Survey of the State-of-the-Art"},{"paperId":null,"title":"Artificial Intelligence (AI) -Assessment of the robustness of neural networks -Part 1: Overview. Standard, International Organization for Standardization"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"d08483307335b3571cb29c3b1903741df19148a9","title":"A systematic literature review of machine learning techniques for software maintainability prediction"},{"paperId":"20ba55ee3229db5cb190a00e788c59f08d2a767d","title":"Self-Training With Noisy Student Improves ImageNet Classification"},{"paperId":"40df572b0fbeae0f3db9b364be838c6467d189f2","title":"A Self-Attentional Neural Architecture for Code Completion with Multi-Task Learning"},{"paperId":"f2e5702330e52b730378ca1e7f1a041fc4f81c07","title":"Vulnerability Prediction From Source Code Using Machine Learning"},{"paperId":null,"title":"Six levels of auto ml"},{"paperId":"b5abecf78c1bd975449e34350289a904e6ef35cc","title":"Automated program repair"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"4cdf2fad22afc865999747336c7399fe422e6e8e","title":"Optuna: A Next-generation Hyperparameter Optimization Framework"},{"paperId":"df3f507f3d46dece98a527999676b978af4ae987","title":"Boa Meets Python: A Boa Dataset of Data Science Software in Python Language"},{"paperId":"d02285385fd2b53a6100b8d62421bba6693b788c","title":"When Code Completion Fails: A Case Study on Real-World Completions"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"273dfbcb68080251f5e9ff38b4413d7bd84b10a1","title":"LIBSVM: A library for support vector machines"},{"paperId":"9bca4d7b932e0854c3325f1578cfd17341dd8ea8","title":"A Kernel Method for the Two-Sample-Problem"},{"paperId":"6dd38e533bf912364f8d5af73f66c67263933dd5","title":"How are Java software developers using the Elipse IDE?"},{"paperId":"0fe16f0424e32a849b48ca87e37e5bb2817aec6d","title":"Why Inverse Document Frequency?"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"},{"paperId":null,"title":"The crisp-dm model: the new blueprint for data mining"},{"paperId":"2599131a4bc2fa957338732a37c744cfe3e17b24","title":"A training algorithm for optimal margin classifiers"}],"id":"41f5e1ad7793593befc0b9c38f756836e8b07c98","summary":"The Code4ML corpus, which contains code snippets, task summaries, competitions and dataset descriptions publicly available from Kaggle, can potentially help address a number of software engineering or data science challenges through a data-driven approach."},{"url":"https://www.semanticscholar.org/paper/4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":1,"influentialCitationCount":0,"publicationDate":"31/10/2022","authors":"Daoguang Zan,Bei Chen,Zeqi Lin,Bei Guan,Yongji Wang,Jian-Guang Lou","citations":[{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"}],"references":[{"paperId":"7d180ff2a86d259794aa85466d87c215d86b5c92","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation"},{"paperId":"34503c0b6a615124eaf82cb0e4a1dab2866e8980","title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"},{"paperId":"2e12b5f42d3df4b3c42fff7f4ade444bb2c67811","title":"From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"2b556fcf2ac634f03c1fb0ace5e602e829418e65","title":"ReACC: A Retrieval-Augmented Code Completion Framework"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis"},{"paperId":"590432f953b6ce1b4b36bf66a2ac65eeee567515","title":"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction"},{"paperId":"737daa49c4234a8897b1f5b466c004db56241d83","title":"S2QL: Retrieval Augmented Zero-Shot Question Answering over Knowledge Graph"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"c56aced0f0c5cfebefadb530cb08d736c3ac5c05","title":"Retrieval Augmented Code Generation and Summarization"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"3416e5e5694855f7175125b5fe2e0b659c3cdbfa","title":"RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"bde0c85ed3d61de2a8874ddad70497b3d68bc8ad","title":"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"},{"paperId":"c9b8593db099869fe7254aa1fa53f3c9073b0176","title":"Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval"},{"paperId":"2cbb8de53759e75411bc528518947a3094fbce3a","title":"Billion-Scale Similarity Search with GPUs"},{"paperId":"375b9b36ef68678185f2b6e4dbbbe7bbfad6535a","title":"Large-Scale Relation Learning for Question Answering over Knowledge Bases with Pre-trained Language Models"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"79cd9f77e5258f62c0e15d11534aea6393ef73fe","title":"Dense Passage Retrieval for Open-Domain Question Answering"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"1fa9ed2bea208511ae698a967875e943049f16b6","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"072c23038332c34473e0a511099f064930c368d1","title":"API practices and paradigms: Exploring the protocological parameters of APIs as key facilitators of sociotechnical forms of exchange"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"e4a323a3a1ca5ff4cc2dd74ede65f48fd62b897b","title":"Deep API learning"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"}],"id":"4fbe0cb0777b228e39243692bf29e2829060b8de","summary":"This paper investigates how to equip pre-trained language models with the ability of code generation for private libraries, and proposes a novel framework with two modules: the APIRetriever and the APICoder, which generates code using these APIs."},{"url":"https://www.semanticscholar.org/paper/20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":1,"influentialCitationCount":0,"publicationDate":"31/10/2022","authors":"Spyridon Mouselinos,Mateusz Malinowski,H. Michalewski","citations":[{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"}],"references":[{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"5e3a7235c730b512f0bb3004a6deb0c88ec9fc8e","title":"On the Paradox of Learning to Reason from Data"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"6847e2ba6796b8a786b3b6d8d8a2d922a6c7c31d","title":"Measuring CLEVRness: Blackbox testing of Visual Reasoning Models"},{"paperId":"833a2f1817cb9aeb292620454889cae78e26dda4","title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"59641c10ed7431a3cf841f308367dc2dc0281b74","title":"What Makes Good In-Context Examples for GPT-3?"},{"paperId":null,"title":"Natural language processing with transformers"},{"paperId":"9cac09098aa611bd9a94d080d2401840632ab16f","title":"LM-Critic: Language Models for Unsupervised Grammatical Error Correction"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"114aa720872462b0ca1b97bfdec0ebd56c36fd0a","title":"Towards Understanding and Mitigating Social Biases in Language Models"},{"paperId":"d65a064eb837f838faf6ff67781b62450b92b159","title":"CommonsenseQA 2.0: Exposing the Limits of AI through Gamification"},{"paperId":"57a1258571a21817d89197dc84c986861fb6e580","title":"Measuring and Improving BERT’s Mathematical Abilities by Predicting the Order of Reasoning."},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"56fa0b9cba4d9aee5ccc327365b3b3a721031c69","title":"Calibrate Before Use: Improving Few-Shot Performance of Language Models"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":null,"title":"Gpt-j-6b: A 6 billion parameter autoregressive language model"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"bf9a8fb50aca26774ebf4815db2d8712e2c5830c","title":"TextAttack: A Framework for Adversarial Attacks in Natural Language Processing"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"463fefdbd81a4a0a32cf59bc58a9545757c8cf2e","title":"Pre-trained Contextual Embedding of Source Code"},{"paperId":null,"title":"Keybert: Minimal keyword extraction with bert"},{"paperId":null,"title":"Codeforces: Results of 2020"},{"paperId":"18a1c21f35153c45d0ef30c564bffb7d70a13ccc","title":"Universal Adversarial Triggers for Attacking and Analyzing NLP"},{"paperId":"735ce0447e459e13f89ae751b19323d76a2af786","title":"Program Synthesis and Semantic Parsing with Learned Code Idioms"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"413a03a146e6f7b16c11e73243d83e6f1a6627a3","title":"Breaking NLI Systems with Sentences that Require Simple Lexical Inferences"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":null,"title":"Description2code dataset"},{"paperId":"f48eed915cbb9c6592cdb9df80c1edaeb46959af","title":"A measure of intelligence"},{"paperId":"7e0006dc1972e276805d1bd6c5dc2813c7a6d824","title":"Automatic Keyword Extraction from Individual Documents"},{"paperId":"0d175746355f293187ed491665563018ee690cfa","title":"Machine super intelligence"},{"paperId":null,"title":"The world's largest open multilingual language model"}],"id":"20fae749e3d469c331731ffa2f811079db792fdc","summary":"This work shows that current code generation systems exhibit biases inherited from large language model backbones, which might leak into generated code under specific circumstances, and proposes a framework that automatically removes hints and exposes various biases that these code generation models use."},{"url":"https://www.semanticscholar.org/paper/ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","title":"Evaluating How Fine-tuning on Bimodal Data Effects Code Generation","venue":"ArXiv","year":2022,"referenceCount":31,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Gabriel Orlanski,Seonhye Yang,Michael Healy","citations":[{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"}],"references":[{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"3f791ea6584b78fdf0ed80560d09d9496cf5a353","title":"Learning to Complete Code with Sketches"},{"paperId":null,"title":"Natural Language Processing with Transformers: Building Language Applications with Hugging Face"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"cc9ad384ec0d0176ce05865d3866b44d2519bd68","title":"Reading StackOverflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"3a1efac4a8aa2f40a8128e034c01f18660652dd5","title":"DOBF: A Deobfuscation Pre-Training Objective for Programming Languages"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"6faf62ac9e69e66cb92c923b749fa071554d23ca","title":"Learning Programmatic Idioms for Scalable Semantic Parsing"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"649c1148439a4e875dab414ba67bf8c80350af4a","title":"TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"558ac446dc26bee9789d660a251b75728cb6eeb2","title":"Language to Logical Form with Neural Attention"},{"paperId":"b7c0e47f8b768258b7d536c21b218e6c46ab8791","title":"Learning to Parse Database Queries Using Inductive Logic Programming"}],"id":"ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","summary":"It is found that at higher temperatures, there are decreases to the model’s ability to generate runnable programs despite higher pass @ k scores, underscoring the need for better methods of incorporating such data that mitigate these side effects."},{"url":"https://www.semanticscholar.org/paper/e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models","venue":"ArXiv","year":2022,"referenceCount":35,"citationCount":2,"influentialCitationCount":2,"publicationDate":"17/11/2022","authors":"Junjie Huang,Chenglong Wang,Jipeng Zhang,Cong Yan,Haotian Cui,J. Inala,Colin B. Clement,Nan Duan,Jianfeng Gao","citations":[{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"}],"references":[{"paperId":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":null,"title":"Natural Language Processing with Transformers: Building Language Applications with Hugging Face"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"6e2b1038682cd116b2e38bec19b5721196c41eea","title":"HAConvGNN: Hierarchical Attention Based Convolutional Graph Neural Network for Code Documentation Generation in Jupyter Notebooks"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"010c9f63c51ccc504de36d2d1693e0ea9d525da1","title":"Deep Learning for Source Code Modeling and Generation"},{"paperId":"ed2a639dd4a6b3abdcc0040733a264cb1dbcd7d4","title":"PlotCoder: Hierarchical Decoding for Synthesizing Visualization Code in Programmatic Context"},{"paperId":"21d6bb1a79a69c207a5d1187ebfce5150b58e441","title":"Graph-Augmented Code Summarization in Computational Notebooks"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"ed3273b49e9b2fa17ea35f4721689927caa4263b","title":"Auto-Suggest: Learning-to-Recommend Data Preparation Steps Using Data Science Notebooks"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"c7462e0ee928f095a7fc40b91f1e7557d283ae8e","title":"Release Strategies and the Social Impacts of Language Models"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"d6b7abd5c17f9e7d83c5b709c27da3a82a9788f2","title":"NAPS: Natural Program Synthesis Dataset"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"d4c85ef35c5224792186100c29df141761d6abd8","title":"Neural Program Search: Solving Data Processing Tasks from Description and Examples"},{"paperId":"429b65937d4922578a81e1f0ef5aeab7361ae36b","title":"NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System"},{"paperId":null,"title":"Spider: A largescale human-labeled dataset for complex and cross"},{"paperId":"116a877c774969b53399b1ccaa34d51bfb492ee4","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"f56425ec56586dcfd2694ab83643e9e76f314e91","title":"50 Years of Data Science"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"e47868841d87efe261451a43b00d6c81cf7fb7a3","title":"Jupyter Notebooks - a publishing format for reproducible computational workflows"},{"paperId":"27e1dbe9f7c71cd6cc1b0357f49aef497e572d09","title":"Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"},{"paperId":"25369f56a933e3bfb1d8e1588cdc6c50df93ecae","title":"Building a Semantic Parser Overnight"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"e402dd77eba504ea93bc38e2a052398bb95db351","summary":"ExeDS is introduced, an evaluation dataset for execution evaluation for data science code generation tasks that contains a set of 534 problems from Jupyter Notebooks, each consisting of code context, task description, reference program, and the desired execution output."},{"url":"https://www.semanticscholar.org/paper/f3a6115e5fb2237df938976e005468f0b18da797","title":"The Stack: 3 TB of permissively licensed source code","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":2,"influentialCitationCount":0,"publicationDate":"20/11/2022","authors":"Denis Kocetkov,Raymond Li,Loubna Ben Allal,Jia Li,Chenghao Mou,Carlos Muñoz Ferrandis,Yacine Jernite,Margaret Mitchell,Sean Hughes,Thomas Wolf,Dzmitry Bahdanau,Leandro von Werra,Harm de Vries","citations":[{"paperId":"b2f256a1ec54325102119a57cb2359840fe68586","title":"SantaCoder: don't reach for the stars!"},{"paperId":"bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence"}],"references":[{"paperId":"ec4c8d99eb1c028c43af6d8bbf727392d351cb59","title":"Efficient Training of Language Models to Fill in the Middle"},{"paperId":"dd112d4dbd4656223770989778f39700de3052bc","title":"A Hazard Analysis Framework for Code Synthesis Large Language Models"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"aa4d9972af3264d032dbee58501ed4ac49477103","title":"Scaling Laws and Interpretability of Learning from Repeated Data"},{"paperId":"3ae3716f125d71e9daccac3dafa4fab7482fb16a","title":"Data Governance in the Age of Large-Scale Data-Driven Language Technology"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"f07570ed7847a7c95f35f4315afe543745f71e29","title":"Deduplicating Training Data Mitigates Privacy Risks in Language Models"},{"paperId":"d407aa1cc3f9ce8478d3052344947fb49baa04d4","title":"CodeFill: Multi-token Code Completion by Jointly learning from Structure and Naming Sequences"},{"paperId":"4566c0d22ebf3c31180066ab23b6c445aeec78d5","title":"Deduplicating Training Data Makes Language Models Better"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":null,"title":"The bigscience ROOTS corpus: A 1.6TB composite multilingual dataset"},{"paperId":null,"title":"Copyright Implications of the Use of Code Repositories to Train a Machine Learning Model"},{"paperId":null,"title":"If Software is My Copilot, Who Programmed My Software"},{"paperId":null,"title":"This CoPilot is stupid and wants to kill me"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"4f68e07c6c3173480053fd52391851d6f80d651b","title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"3a1efac4a8aa2f40a8128e034c01f18660652dd5","title":"DOBF: A Deobfuscation Pre-Training Objective for Programming Languages"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"62d1a3137b01a69443bebf4d92c1990ec512a6a1","title":"Extracting Training Data from Large Language Models"},{"paperId":"0df347f5e3118fac7c351917e3a497899b071d1e","title":"Datasheets for datasets"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"8323c591e119eb09b28b29fd6c7bc76bd889df7a","title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"97bfa89addc6e5d76361e4c1e296949cad887b86","title":"Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science"},{"paperId":"a26b00c607b6d6a2697f5444f14a9afc37701444","title":"DéjàVu: a map of code duplicates on GitHub"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"4bae4bd4bf541a2af9d0df8ac2487d26523e7f25","title":"Approximate Nearest Neighbor: Towards Removing the Curse of Dimensionality"},{"paperId":"cf6023368a683572a090203406a7e4285566e9db","title":"Identifying and Filtering Near-Duplicate Documents"},{"paperId":null,"title":"• BSD-2-Clause-NetBSD • zlib-acknowledgement • OLDAP-2"}],"id":"f3a6115e5fb2237df938976e005468f0b18da797","summary":"This work introduces The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages, and describes how to collect the full dataset, construct a permissically licensed subset, and present a data governance plan."},{"url":"https://www.semanticscholar.org/paper/20b60fb3993d2e9a5af04611f7bdf248e5a3a736","title":"Programming by Example and Text-to-Code Translation for Conversational Code Generation","venue":"ArXiv","year":2022,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Eli Whitehouse,William Gerard,Yauhen Klimovich,Marc Franco-Salvador","citations":[],"references":[],"id":"20b60fb3993d2e9a5af04611f7bdf248e5a3a736","summary":"Modular Programs for Text-guided Hierarchical Synthesis (MPaTHS), a method for integrating Programming by Example and text-to-code systems which uses an accessible natural language interface for synthesizing general programs, is proposed."},{"url":"https://www.semanticscholar.org/paper/27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation","venue":"ArXiv","year":2022,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/11/2022","authors":"Tianyi Zhang,Tao Yu,Tatsunori Hashimoto,M. Lewis,Wen-tau Yih,Daniel Fried,Sida I. Wang","citations":[],"references":[{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"07ec0d4cc6a2be39def51139d228292c6a0dc627","title":"Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners"},{"paperId":"4a4581003f56e8cb581ad6f383c037964765d3d5","title":"Active Programming by Example with a Natural Language Prior"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"1a903282f7c19dbdb2714b852fb42dbb4675422b","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"eea7bca03bda3ee2448cd012bbcb2b33822861d8","title":"Noisy Channel Language Model Prompting for Few-Shot Text Classification"},{"paperId":null,"title":"Scaling language modeling with pathways"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0071ee55f1749e6be586645bab08f0385ee856b4","title":"Graph-based, Self-Supervised Program Repair from Diagnostic Feedback"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"d3231772937a2182b2377d028417245c49868dd1","title":"On NMT Search Errors and Model Errors: Cat Got Your Tongue?"},{"paperId":"3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c","title":"Reranking for Neural Semantic Parsing"},{"paperId":"6182aed90596acd1573bd5ccbc2284b1e8a7291b","title":"Generative Question Answering: Learning to Answer the Whole Question"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"ede8ba65c4db10d357d9c3bf8e75b092f536fc84","title":"Speaker-Follower Models for Vision-and-Language Navigation"},{"paperId":"429b65937d4922578a81e1f0ef5aeab7361ae36b","title":"NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System"},{"paperId":null,"title":"A syntactic neural model for generalpurpose code generation"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"651e5bcc14f14605a879303e97572a27ea8c7956","title":"A Diversity-Promoting Objective Function for Neural Conversation Models"},{"paperId":"cd7ee037c029b25e691650a26cec7538d255405c","title":"Suggesting accurate method and class names"},{"paperId":"b5f09ce0dd760857e0d0e4879f6e2543f04c5d33","title":"Maximum mutual information estimation of hidden Markov model parameters for speech recognition"}],"id":"27961ae80ad008bd4006704b1b8fa82664137d69","summary":"Experimental results show that Coder-Reviewer reranking leads to consistent and signiﬁcant improvement (up to 17 % absolute accuracy gain) over reranking with the Coder model only, and when combined with executability ﬁltering,Coder- reviewer reranking can often outperform the minimum Bayes risk method."},{"url":"https://www.semanticscholar.org/paper/9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming","venue":"ArXiv","year":2022,"referenceCount":79,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/12/2022","authors":"Qingfu Zhu,Xianzhen Luo,Fang Liu,Cuiyun Gao,Wanxiang Che","citations":[],"references":[{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"62851a515ea0ee3a547d94e8a493d978c22d0be9","title":"Multilingual Code Snippets Training for Program Translation"},{"paperId":"2417ab25a53e97410f44a20af69b82fff077fd53","title":"Deep Learning Meets Software Engineering: A Survey on Pre-Trained Models of Source Code"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"4b27f18bff43d605805c92696a979714ced0b805","title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"a1e1297fb132d7769dda3f7917e57757e6e22605","title":"Impact of Evaluation Methodologies on Code Summarization"},{"paperId":null,"title":"A conversational"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"51654d8317478fe1497678b15e36ac99d73a65b2","title":"CAST: Enhancing Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"05e396e79a2f88f0b8f8d99f5ab36ab3efa95c14","title":"Language-Agnostic Representation Learning of Source Code from Structure and Context"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"ceaff192479db6faee58ae88e053b0b319cf1893","title":"Retrieval-Augmented Generation for Code Summarization via Hybrid GNN"},{"paperId":"1cedb352a10828b3ec263a4da907794bea052282","title":"PLUR: A Unifying, Graph-Based View of Program Learning, Understanding, and Repair"},{"paperId":"5a129c5aa13334c7ff10c40d2d33165656a628f0","title":"Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021)"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"7157d9e7a8dcd7e9900dcfd3d10d8a07d33468bc","title":"Synthesize, Execute and Debug: Learning to Repair for Neural Program Synthesis"},{"paperId":"89fdb0555f6643b18c0094bcce66eaea701bef85","title":"Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"805a6d1df9f460abfcea3d51d181cf1e80680be4","title":"A Transformer-based Approach for Source Code Summarization"},{"paperId":"a266b37f98928f27fddd863d11b38a5563043315","title":"Learning to Update Natural Language Comments Based on Code Changes"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"495da6f19baa09c6db3697d839e10432cdc25934","title":"Multilingual Denoising Pre-training for Neural Machine Translation"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"b7af363ff612f41e0c1071ae24ce68a836aaa678","title":"Code Generation as a Dual Task of Code Summarization"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"00549af4bc3270e0f688acbf694f912d7ee39cad","title":"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks"},{"paperId":"08218ed6095e407eba3f02ce141994241d03a5b9","title":"Automatic Source Code Summarization with Extended Tree-LSTM"},{"paperId":"516497e7fc39ebb126bed28dcc5c6129e500acb8","title":"An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation"},{"paperId":"98d1307bed619b58b4a44acd8e65ac58495776c2","title":"code2seq: Generating Sequences from Structured Representations of Code"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"1a2fb75103ea9dc9ff31711ba5aa357088c026bc","title":"Automated Vulnerability Detection in Source Code Using Deep Representation Learning"},{"paperId":"15f16252af84a630f1f9442d4e0367b6fba089cf","title":"Deep Code Comment Generation"},{"paperId":"be82528df1512ea65bc5edcaeefc1ec33cd27c82","title":"Automatic Generation of Text Descriptive Comments for Code Blocks"},{"paperId":"dea6aeb514b1969ab879c793d46a0d2eceaa2cbf","title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis"},{"paperId":"dd908c09eaf8f3a3c20d64fff798ec902433737d","title":"Code Completion with Neural Attention and Pointer Networks"},{"paperId":"d0f2d7236e43f129744e88130fb71f8f872d2b31","title":"Learning to Represent Programs with Graphs"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"6cf6dc8bb7995a1f529d51b11f1677e045337337","title":"SmartPaste: Learning to Adapt Source Code"},{"paperId":"45416ffd8fa572c23c8dbc43cc7b8b5095fcbcc2","title":"A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes"},{"paperId":"e235bc8ccbe85de40f406d1a1201d50aec893b2d","title":"A Simple, Fast Diverse Decoding Algorithm for Neural Generation"},{"paperId":"165f45f42a1418526c2fd5eeb0ebf6c147f4f507","title":"Latent Attention For If-Then Program Synthesis"},{"paperId":"32aa2517b03c871c11e521c2a3406f457833e2c3","title":"Summarizing Source Code using a Neural Attention Model"},{"paperId":"221ef0a2f185036c06f9fb089109ded5c888c4c6","title":"Sequence-to-Sequence RNNs for Text Summarization"},{"paperId":"49512270b39636375880d611d7b2192d324f4ba6","title":"Convolutional Neural Networks over Tree Structures for Programming Language Processing"},{"paperId":"7f6245d527e84092111a6adb0f1939d0209d3360","title":"FlashMeta: a framework for inductive program synthesis"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"9ef911fea38b43aedad131d5d08efa6a1833d747","title":"On the localness of software"},{"paperId":"9277dc7dd5b348645e8fcf4a987e5b3fc9132c7c","title":"Towards a Big Data Curated Benchmark of Inter-project Code Clones"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"07c4549be429a52274bc0ec083bf5598a3e5c365","title":"Modeling and Discovering Vulnerabilities with Code Property Graphs"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"37cb53b8cdb46ba1d7b933e3fc0e22dd7ee41bcb","title":"Software vulnerability prediction using text analysis techniques"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"9819b600a828a57e1cde047bbe710d3446b30da5","title":"Recurrent neural network based language model"},{"paperId":"0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7","title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"9bdac1f28f7d4c5f3b28c25ea5bbc6a3be0449b1","title":"Identifying similar code with program dependence graphs"},{"paperId":"1cc3f5cdd4204f8e55e46d9cbaef730d17ca647c","title":"Your Wish is My Command: Programming By Example"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"a8e8f3c8d4418c8d62e306538c9c1292635e9d27","title":"Backpropagation Applied to Handwritten Zip Code Recognition"},{"paperId":"ece80f049a527954af1c153d61cafee5789c2afe","title":"The program dependence graph and its use in optimization"},{"paperId":"ceb3163c56465fda5fef591d0ff0a6c7f434a04d","title":"A Deductive Approach to Program Synthesis"}],"id":"9b4055674cd9849f8595240695bed69cd02492bc","summary":"This paper comprehensively investigates existing work in natural language processing for programming, rang-ing from early deductive models to the latest competition-level models."},{"url":"https://www.semanticscholar.org/paper/690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation","venue":"ArXiv","year":2022,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Zhiruo Wang,Shuyan Zhou,Daniel Fried,Graham Neubig","citations":[],"references":[{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models"},{"paperId":"654cd297b307c0c6fa08732511ba852b8dce1977","title":"Out of the BLEU: how should we assess quality of the Code Generation models?"},{"paperId":"63c670ba8018da0a7e33c34ffd84c2a3ca54b894","title":"Language Models Can Teach Themselves to Program Better"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"c96363c42bc8c465902c22b8c33c8704233f519e","title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages"},{"paperId":"eadcf2f484b2d326f4d32ba4a897b009e4de1784","title":"Pynguin: Automated Unit Test Generation for Python"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"2222fe87201177339c89fbfe9ef3c9c8e67674a5","title":"Prefix-to-SQL: Text-to-SQL Generation from Incomplete User Questions"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"15a60130ea9cb4a00e0dac6d11a368894974cedf","title":"Unit Test Case Generation with Transformers and Focal Context"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"649c1148439a4e875dab414ba67bf8c80350af4a","title":"TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"116a877c774969b53399b1ccaa34d51bfb492ee4","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"558ac446dc26bee9789d660a251b75728cb6eeb2","title":"Language to Logical Form with Neural Attention"},{"paperId":"285c165c81fc9275955147a892b9a039ec8b1052","title":"chrF: character n-gram F-score for automatic MT evaluation"},{"paperId":"0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7","title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"9ca86842aad16797d0fe0323358f3beb1ac6a5c6","title":"Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"690c210564226c9307b3bab977cdc07a6a45863a","summary":"ODEX corroborates the mer-its of execution-based evaluation over metrics without execution but also unveils their complementary effects, and is released to facilitate research into open-domain problems for the code generation community."},{"url":"https://www.semanticscholar.org/paper/ab8a53be2ff8a756776aa0245ad8da41189d60d6","title":"Fuzzing Deep-Learning Libraries via Large Language Models","venue":"ArXiv","year":2022,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/12/2022","authors":"Yinlin Deng,Chun Xia,Haoran Peng,Chenyuan Yang,Lingming Zhang","citations":[],"references":[{"paperId":"f031ba42cf82f106200bb03fbb91dd5671a59b9c","title":"Practical Program Repair in the Era of Large Pre-trained Language Models"},{"paperId":"e37155d21818513bd40d64ee212099aac82bd6f8","title":"Less training, more repairing please: revisiting automated program repair via zero-shot learning"},{"paperId":"57de3be56d84cb0fc5795df93bc87c1c52721e56","title":"Fuzzing deep-learning libraries via automated relational API inference"},{"paperId":"d092e753c112f1e71cc9f51b94eddcee6568785f","title":"EAGLE: Creating Equivalent Graphs to Test Deep Learning Libraries"},{"paperId":"b2a2059105ded9dcd0be69f82d389c438a76e789","title":"Muffin: Testing Deep Learning Libraries via Neural Architecture Fuzzing"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"8a02e3747123a605174beac0d622dff386d2a8db","title":"Coverage-guided tensor compiler fuzzing with joint IR-pass mutation"},{"paperId":"5d0db797a45ce2453f821f7ded0b547d3fdab054","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"8c8d4698873fc56e0358cfd5c346ca8c669aaf3a","title":"Free Lunch for Testing: Fuzzing Deep-Learning Libraries from Open Source"},{"paperId":"818006c5f74c6198399638bf38f9837c0964a066","title":"DocTer: documentation-guided fuzzing for testing deep learning API functions"},{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"3f650c0d0a046c1fb6c068d636b138e14ded77b9","title":"Generative type-aware mutation for testing SMT solvers"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"dd63c76b40d937dc3a7af3de5ba42232b858bd6c","title":"Automated conformance testing for JavaScript engines via deep compiler fuzzing"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"ac3cdb50606f7770eef8e4cd951840a4f71287a0","title":"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"99ddf774164df29e733a99a56534b187f224e15c","title":"Fuzzing: Challenges and Reflections"},{"paperId":"30a156f17ca8f54aa14d01d32c2315c11fcbe723","title":"BUSTLE: Bottom-up program-Synthesis Through Learning-guided Exploration"},{"paperId":null,"title":"Hospitals turn to artificial intelligence to help with an age-old problem: Doctors’ poor bedside manners"},{"paperId":null,"title":"A Smarter App Is Watching Your Wallet"},{"paperId":"368f36c67c70d484901c3f303aa25312e864d875","title":"Deep learning library testing via effective model generation"},{"paperId":"e9e9a51b46b81feaa8475f116c17e93b03b24f02","title":"Audee: Automated Testing for Deep Learning Frameworks"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"f976a25fbbb86fc7c10008b1276940885cee41d0","title":"Montage: A Neural Network Language Model-Guided JavaScript Engine Fuzzer"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"ff4804de2b9db68a052d7113cae41ef2122a1c51","title":"AFL++ : Combining Incremental Steps of Fuzzing Research"},{"paperId":"b96c1b6446b35cf18ff4364b3104bcde1a7204a1","title":"Coverage guided, property based testing"},{"paperId":"87ba716837af374824ea825cdfc8ab0002dcdd03","title":"DeepFuzz: Automatic Generation of Syntax Valid C Programs for Fuzz Testing"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"c1f55baff1e8bce91de083d9741045342ddb0b29","title":"CRADLE: Cross-Backend Validation to Detect and Localize Bugs in Deep Learning Libraries"},{"paperId":"9898df29efc9016ebeb7098e56fd9f325aa89fc6","title":"SeqFuzzer: An Industrial Protocol Fuzzing Framework from a Deep Learning Perspective"},{"paperId":"a10a6495723ea8c928680ecdd61714f5750586c3","title":"Fine-tune BERT for Extractive Summarization"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"The fuzzing book"},{"paperId":"ab758c7d163dba039f1b1badaa9ea72064c887ba","title":"DeepRoad: GAN-Based Metamorphic Testing and Input Validation Framework for Autonomous Driving Systems"},{"paperId":"c897ad6ee09811644dd516038502501b102792d6","title":"FairFuzz: A Targeted Mutation Strategy for Increasing Greybox Fuzz Testing Coverage"},{"paperId":"76655792a9c3ebcbc64d2665ecf5bba81966e478","title":"Evaluating Fuzz Testing"},{"paperId":"0ab21244b20b42dd152f8ae367fba636d32d2049","title":"Compiler fuzzing through deep learning"},{"paperId":"358d45ed3157cbc9e741a292b94ce1173728c66d","title":"GANFuzz: a GAN-based industrial network protocol fuzzing framework"},{"paperId":"01f6488741f1527201a428c7b05df9649cb9a631","title":"A Tutorial on Thompson Sampling"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"5bd13e6313008e1555e530dda6d84c5004aa09ed","title":"Learn&Fuzz: Machine learning for input fuzzing"},{"paperId":null,"title":"American Fuzzy Lop -Whitepaper"},{"paperId":"12806c298e01083a79db77927530367d85939907","title":"An Empirical Evaluation of Deep Learning on Highway Driving"},{"paperId":null,"title":"libFuzzer a library for coverage-guided fuzz testing. https: //llvm.org/docs/LibFuzzer.html"},{"paperId":"54e325aee6b2d476bbbb88615ac15e251c6e8214","title":"Generative Adversarial Nets"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"ab867c140d2947511979c87e7ae580d9d3f0aeab","title":"An Empirical Evaluation of Thompson Sampling"},{"paperId":"ae74531e6e73afaffb264b4e536b8fde95d03202","title":"Program synthesis by sketching"},{"paperId":"4db2b87478c87c677135e802dbeccc8e5e384aed","title":"Fuzzing: Brute Force Vulnerability Discovery"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"15bd56e6081f4f8b6751744e3e2190fd278c1a3a","title":"The Gamma Function"},{"paperId":"94d9cbfc474cb6ce961de45a06233b2853ca6724","title":"Toward automatic program synthesis"},{"paperId":"ee2cd1d17f833d3c157a1016a778c7c22af555a2","title":"ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES"},{"paperId":"c9bc5651c1560ea5111c6246c3916d3c1905d9eb","title":"Elementary Principles in Statistical Mechanics: Developed with Especial Reference to the Rational Foundation of Thermodynamics"},{"paperId":null,"title":"TensorFlow 2020"},{"paperId":null,"title":"Keras 2020"}],"id":"ab8a53be2ff8a756776aa0245ad8da41189d60d6","summary":"LLMFuzz is the first automated approach to directly leveraging Large Pre-trained Language Models (LLMs) to generate input programs for fuzzing DL libraries, and is able to detect 65 bugs, with 41 already confirmed as previously unknown bugs."},{"url":"https://www.semanticscholar.org/paper/69e330037afd18e5546fdcacbc9a9f7deb69bba9","title":"Memorization and Generalization in Neural Code Intelligence Models","venue":"Information and Software Technology","year":2021,"referenceCount":93,"citationCount":8,"influentialCitationCount":0,"publicationDate":"16/06/2021","authors":"Md Rafiqul Islam Rabin,Aftab Hussain,V. Hellendoorn,Mohammad Amin Alipour","citations":[{"paperId":"81ce2664e892fc5f71fa4f8d61e7b42314dccb5e","title":"FeatureExtractor: A tool for extracting key input features of code intelligence models"},{"paperId":"22df866f9605d27d1e5cca9b3ab721f33673e158","title":"ProgramTransformer: A tool for generating semantically equivalent transformed programs"},{"paperId":"6042c51ccce53b94b84d1bdbcb33c3ab493323b4","title":"Syntax-guided program reduction for understanding neural code intelligence models"},{"paperId":"b070d2c844d5b18d0c94fb6b20bef3946d60abfd","title":"Readle: A Formal Framework for Designing AI-based Edge Systems"},{"paperId":"f0aacc7a0379883c4ab67d9a2d852c7bd99d9797","title":"Extracting Label-specific Key Input Features for Neural Code Intelligence Models"},{"paperId":"87bb5593d04450bbbd29afc5b2ef395127d1ba6a","title":"Testing the Robustness of a BiLSTM-based Structural Story Classifier"},{"paperId":"6a3b6512de2caa712311feb876f1be599a7c0b68","title":"Encoding Program as Image: Evaluating Visual Representation of Source Code"},{"paperId":"8e9d9bf41d69ca8643b67a280b96f886c64bd817","title":"Code2Snapshot: Using Code Snapshots for Learning Representations of Source Code"}],"references":[{"paperId":"6042c51ccce53b94b84d1bdbcb33c3ab493323b4","title":"Syntax-guided program reduction for understanding neural code intelligence models"},{"paperId":"a20e0f8bdacf59d98e234c3456a90ad1411cee4f","title":"Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"17c8d5d173d915d9662ad7b45c593d1ab3b742e1","title":"Semantic Robustness of Models of Source Code"},{"paperId":"218062f45c15f39bc8f4fb2c930ddf20b5809b11","title":"Machine Learning Testing: Survey, Landscapes and Horizons"},{"paperId":"8e9d9bf41d69ca8643b67a280b96f886c64bd817","title":"Code2Snapshot: Using Code Snapshots for Learning Representations of Source Code"},{"paperId":"416bb2238e92cc9c664bc932a5e968bf03bd2845","title":"A Survey on Machine Learning Techniques for Source Code Analysis"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"0a082e6dedf0f65e0e7c2010653d4d4768ecb4f4","title":"Multimodal Representation for Neural Code Search"},{"paperId":"98097b7499bffd76e89d6c28449346f383143553","title":"Understanding neural code intelligence through program simplification"},{"paperId":"a4f9e7e695bba1ffb90b30752a40d5ee907dcb36","title":"Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks"},{"paperId":"6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"9eea59c34f139f3d2153226c8cf026e975622074","title":"Memorization vs. Generalization : Quantifying Data Leakage in NLP Performance Evaluation"},{"paperId":"4b0e5209664ff0f5429d6d12a59921ad448db08b","title":"Probing model signal-awareness via prediction-preserving input minimization"},{"paperId":"be910753f93c24712942536c8dc69e320247c680","title":"On the generalizability of Neural Program Models with respect to semantic-preserving program transformations"},{"paperId":"010c9f63c51ccc504de36d2d1693e0ea9d525da1","title":"Deep Learning for Source Code Modeling and Generation"},{"paperId":"e190c23abd3cbc3dd23280076b97bd9c05da4d09","title":"Demystifying Code Summarization Models"},{"paperId":"2bc1d0c1465ddc402e97e74cea5aaf409ff800e2","title":"Towards demystifying dimensions of source code embeddings"},{"paperId":"aa92c4a7fdc93eadee1752b72812f43f8f037fb3","title":"Measuring Memorization Effect in Word-Level Neural Networks Probing"},{"paperId":"08066c80919620397e8e4e5372ff84caf401e675","title":"Global Relational Models of Source Code"},{"paperId":"6acf0d7b24a93953269a4d52c6b2f0c7d11f9ae4","title":"Hoppity: Learning Graph Transformations to Detect and Fix Bugs in Programs"},{"paperId":"9e35c6dd644fa8cbba878f3661b0c85dc538768c","title":"Evaluation of Generalizability of Neural Program Analyzers under Semantic-Preserving Transformations"},{"paperId":"145319588bb154948edbe736afd3561bb280da84","title":"Embedding Java Classes with code2vec: Improvements from Variable Obfuscation"},{"paperId":"c345b74be9f98cca9592cc376465118df5c9f2da","title":"Improved Code Summarization via a Graph Neural Network"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"60739c6a62f86d274d7e6a0565acc78e6f319ab1","title":"Adversarial Robustness for Code"},{"paperId":"ced24140297c67622a929bf577105650ffa5aea3","title":"Rethinking Generalization of Neural Models: A Named Entity Recognition Case Study"},{"paperId":"7be8c119dbe065c52125ee7716601751f3116844","title":"Generalization through Memorization: Nearest Neighbor Language Models"},{"paperId":"2ab44880e1763baf3d8753ccb43ad3bd5f122b70","title":"Adversarial examples for models of code"},{"paperId":"a14ef6eee732a0d66a6c534d4e696e36767332d7","title":"How Often Do Single-Statement Bugs Occur?: The ManySStuBs4J Dataset"},{"paperId":"45a7ce70b9a1c46f76a9eac22bcf7bc08e2befc9","title":"Let's Agree to Agree: Neural Networks Share Classification Order on Real Datasets"},{"paperId":"8f977a3831132ffaad2f13eea05c1fa46205b8ec","title":"Identity Crisis: Memorization and Generalization under Extreme Overparameterization"},{"paperId":"d47f1239d13c153af15a6bb62e67109bcb5beaad","title":"Assessing the Generalizability of Code2vec Token Embeddings"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"6966a83bbfb67f973e85cb761c83e41758305006","title":"Testing Neural Program Analyzers"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"fb3d9eca57e117c7a9f2d652c7a8c38de45e2d04","title":"COSET: A Benchmark for Evaluating Neural Program Embeddings"},{"paperId":"0073547de99221662cc1dd3ea4babab91a2512f6","title":"Learning to Spot and Refactor Inconsistent Method Names"},{"paperId":"b235c83564f8cd4b27343ff30faf744929d7b961","title":"Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels"},{"paperId":"564bce85c8ad9a50f4652a4d05e1ed0aaa22df49","title":"Neural Program Repair by Jointly Learning to Localize and Repair"},{"paperId":"4e0bb8c1c683b43357c5d5216f6b74ff2cb32434","title":"Do ImageNet Classifiers Generalize to ImageNet?"},{"paperId":"7c894129cac23ba881d5b4c2b4d032e3949e6d7e","title":"Bilateral Dependency Neural Networks for Cross-Language Algorithm Classification"},{"paperId":"ef5561ae5da38ef899333e0444276f5d97923372","title":"Studying the difference between natural and programming language corpora"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"2e4316e7c38373d068f8ff55f26ff83dfc4238b8","title":"Learning to Learn From Noisy Labeled Data"},{"paperId":"98d1307bed619b58b4a44acd8e65ac58495776c2","title":"code2seq: Generating Sequences from Structured Representations of Code"},{"paperId":"73bade9162bc4df6ca6c9ab8a7eb2d54d35acad2","title":"code2vec: learning distributed representations of code"},{"paperId":"520ec00dc35475e0554dbb72f27bd2eeb6f4191d","title":"The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"ea1b62a03ab16c4acca6f7041c6f096bedec244b","title":"Deep learning type inference"},{"paperId":"2d7cd527303f8f515cd282be0243c38e24d2e11d","title":"Prediction of relatedness in stack overflow: deep learning vs. SVM: a reproducibility study"},{"paperId":"077ea3a496bd76bd22d76c432b79b9fa40e136e8","title":"Improving Automatic Source Code Summarization via Deep Reinforcement Learning"},{"paperId":"4d1c856275744c0284312a3a50efb6ca9dc4cd4c","title":"Know What You Don’t Know: Unanswerable Questions for SQuAD"},{"paperId":"e5a95a679774e069e1e36d96f92bac6b93027118","title":"Insights on representational similarity in neural networks with canonical correlation"},{"paperId":"e033a0b29af2939fd44e5765d03380c08897a9e8","title":"Deep Code Search"},{"paperId":"a8b2c73f7c19f4e6e3783a5c19304025d9b7025f","title":"Learning to Attack: Adversarial Transformation Networks"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"804fb9542f4f56e264dd2df57c255a9a2011c00f","title":"Adversarially Robust Generalization Requires More Data"},{"paperId":"d0f2d7236e43f129744e88130fb71f8f872d2b31","title":"Learning to Represent Programs with Graphs"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"1f10df986e3235f1deda73e1200155f4f4b88713","title":"Curating GitHub for engineered software projects"},{"paperId":"a26b00c607b6d6a2697f5444f14a9afc37701444","title":"DéjàVu: a map of code duplicates on GitHub"},{"paperId":"5ddd38a5df945e4afee68d96ed51fd6ca1f7d4cf","title":"A Closer Look at Memorization in Deep Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"82364428995c29b3dcb60c1835548eeff4adcd20","title":"What do Neural Machine Translation Models Learn about Morphology?"},{"paperId":"54ddb00fa691728944fd8becea90a373d21597cf","title":"Understanding deep learning requires rethinking generalization"},{"paperId":"d1cbd204ab108ac508cd773ffc9fad3f7e06f8d7","title":"Memorization in Recurrent Neural Networks"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"32aa2517b03c871c11e521c2a3406f457833e2c3","title":"Summarizing Source Code using a Neural Attention Model"},{"paperId":"0a93c60ed946c31edeed419fd564a405487161f8","title":"Learning Unified Features from Natural and Programming Languages for Locating Buggy Source Code"},{"paperId":"e4a323a3a1ca5ff4cc2dd74ede65f48fd62b897b","title":"Deep API learning"},{"paperId":"77f0a39b8e02686fd85b01971f8feb7f60971f80","title":"Identity Mappings in Deep Residual Networks"},{"paperId":"484ed558a5863060b373e8a2cb7cb302b8c36116","title":"A Convolutional Attention Network for Extreme Summarization of Source Code"},{"paperId":"8ab441ed65209e807c6ffe47a7ca8ab7d02f3142","title":"Google AI algorithm masters ancient game of Go"},{"paperId":"e2db78c001e4644f4841da5a8d13548157175161","title":"Learning programs from noisy data"},{"paperId":"492f57ee9ceb61fb5a47ad7aebfec1121887a175","title":"Gated Graph Sequence Neural Networks"},{"paperId":"cd7ee037c029b25e691650a26cec7538d255405c","title":"Suggesting accurate method and class names"},{"paperId":"370a5f4b305540f35821d2179c384f95f5a63eee","title":"Toward Deep Learning Software Repositories"},{"paperId":null,"title":"Mario Linares-Vásquez, and Denys Poshyvanyk"},{"paperId":"6081ceb60d07fa0a2f0037ece6e540228e4edf73","title":"Automatic documentation generation via source code summarization of method context"},{"paperId":"34f25a8704614163c4095b3ee2fc969b60de4698","title":"Dropout: a simple way to prevent neural networks from overfitting"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"9eaae704216b322f5a17400377fce443853d7b86","title":"Dealing with noise in defect prediction"},{"paperId":"d664cc7336b6874f4e9a955659920bcd41264734","title":"A study of the uniqueness of source code"},{"paperId":"443516aeb2819d4d362ffe7d5418a54e5427a016","title":"ORANGE: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"f42b865e20e61a954239f421b42007236e671f19","title":"GradientBased Learning Applied to Document Recognition"},{"paperId":"162d958ff885f1462aeda91cd72582323fd6a1f4","title":"Gradient-based learning applied to document recognition"},{"paperId":"95c4ebb6df40abc74c9cf36994c0f914be3b04bd","title":"The Association for Computing Machinery"},{"paperId":null,"title":"On the measure of concentration with special reference to income and statistics"}],"id":"69e330037afd18e5546fdcacbc9a9f7deb69bba9","summary":"This work evaluates the memorization and generalization tendencies in neural code intelligence models through a case study across several benchmarks and model families by leveraging established approaches from other fields that use DNNs, such as introducing targeted noise into the training dataset."},{"url":"https://www.semanticscholar.org/paper/642e280df732665249315d6c144871f0e2ceeae6","title":"NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash Commands","venue":"NeurIPS","year":2021,"referenceCount":41,"citationCount":7,"influentialCitationCount":3,"publicationDate":"03/03/2021","authors":"Mayank Agarwal,T. Chakraborti,Quchen Fu,David Gros,Xi Victoria Lin,Jaron Maene,Kartik Talamadupula,Zhongwei Teng,Jules White","citations":[{"paperId":"988cb68d6510f3c4477b8c8ffe9cbdbea7971474","title":"Towards NLP-based Processing of Honeypot Logs"},{"paperId":"9a7d4c2f4309a2a38ea8e5b23c6d616fa0952d44","title":"Neural language models for network configuration: Opportunities and reality check"},{"paperId":"94ac02f13ac3252a55b8740b7b310383cdf53445","title":"ShellFusion: Answer Generation for Shell Programming Tasks via Knowledge Fusion"},{"paperId":"012d5d4346e84b6e158b252de9c87589dd62b16e","title":"Efficient Constituency Tree based Encoding for Natural Language to Bash Translation"},{"paperId":"7497360b0f411a44aa6afbd8b830050c40ec8aed","title":"Dataset of Student Solutions to Algorithm and Data Structure Programming Assignments"},{"paperId":"bd5d3022dc395ca85f72e346022ed6175e13a278","title":"A Transformer-based Approach for Translating Natural Language to Bash Commands"},{"paperId":"6fe61d77b8a4a090899867b79e32efd658f848e7","title":"Explainable Natural Language to Bash Translation using Abstract Syntax Tree"}],"references":[{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"62d1a3137b01a69443bebf4d92c1990ec512a6a1","title":"Extracting Training Data from Large Language Models"},{"paperId":"e6e8a2c56243847b77b604259cde9e10a2daccb8","title":"Semantic Evaluation for Text-to-SQL with Distilled Test Suite"},{"paperId":"0544af8d6b6b6e9ce14be9c6425e520a638be380","title":"Photon: A Robust Cross-Domain Text-to-SQL System"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"13f25c69973373e616c48688d06a6b6ae2736ef0","title":"Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning"},{"paperId":"3c9fa66674e1053eb9de12076c48f13e46422c29","title":"CLAI: A Platform for AI Skills on the Command Line"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":null,"title":"Bash shell scripting"},{"paperId":null,"title":"NLC2CMD Report from JB Team"},{"paperId":null,"title":"Hierarchical Decoding of Bash Commands"},{"paperId":null,"title":"Shellshock -High Voltage"},{"paperId":"1d2d3ba8511767d0311413d9081284f6d550b559","title":"Energy Usage Reports: Environmental awareness as part of algorithmic accountability"},{"paperId":"b3ea2d9c8e5ea3b87ace121f0bece71565abc187","title":"Quantifying the Carbon Emissions of Machine Learning"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"fc6e36c9df6521aa805bc622d066e2964d2e471d","title":"Semantic program alignment for equivalence checking"},{"paperId":"d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","title":"Energy and Policy Considerations for Deep Learning in NLP"},{"paperId":"9e955da40c62b2543b963454b90928ad6cbd5f8a","title":"Does BLEU Score Work for Code Migration?"},{"paperId":"0d96ac48e92b6b42737276a319f48d9d27080fce","title":"EvalAI: Towards Better Evaluation Systems for AI Agents"},{"paperId":"444dcd6b84b1ce1d4aa02aaab71812974735cabb","title":"AInix: An open platform for natural language interfaces to shell commands"},{"paperId":"a46bd6fadfb95a562252c2a2c0d184882a30c579","title":"Coda: An End-to-End Neural Program Decompiler"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"e3ee61f49cd2639c15c8662a45f1d0c2b83a60c1","title":"Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures"},{"paperId":"807370352540f171685998aec7a5701f7110f147","title":"Understanding Back-Translation at Scale"},{"paperId":"6c6170ffb39cdc8cfffbeda9c7a2259eda5875f2","title":"Tree-to-tree Neural Networks for Program Translation"},{"paperId":"429b65937d4922578a81e1f0ef5aeab7361ae36b","title":"NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"d13bb317e87f3f6da10da11059ebf4350b754814","title":"Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation"},{"paperId":"116a877c774969b53399b1ccaa34d51bfb492ee4","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"1ae84940e212e2e3ca132c7aa0878baf0fda06cd","title":"From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood"},{"paperId":"ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921","title":"StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks"},{"paperId":"c3ab52cc41f402650f250a90fc3ced1a22afe082","title":"Word Embeddings for Practical Information Retrieval"},{"paperId":"8ec6abfdc5009b4e490e975991c871dfeec05434","title":"Program Synthesis from Natural Language Using Recurrent Neural Networks"},{"paperId":"cbd569036fc72ae7ff747350b91816440282596b","title":"Seq 2 SQL : Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"259bd09bc382763f864986498e46ab0178714f58","title":"Lifelong Machine Learning"},{"paperId":"6cd61b00eb6248ead7ea029d2daa9d79e68572cb","title":"Evaluating the Energy Efficiency of Deep Convolutional Neural Networks on CPUs and GPUs"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":null,"title":"Command Injection"}],"id":"642e280df732665249315d6c144871f0e2ceeae6","summary":"The NLC2CMD Competition hosted at NeurIPS 2020 aimed to bring the power of natural language processing to the command line by building models that can transform descriptions of command line tasks in English to their Bash syntax."},{"url":"https://www.semanticscholar.org/paper/205ac1373eb7981aca2d08f2ab651871a001271e","title":"CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code","venue":"2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)","year":2022,"referenceCount":55,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/05/2022","authors":"A. Eghbali,Michael Pradel","citations":[{"paperId":"ef29fb8cc6bdda4b011288f51da521d3c25fc53d","title":"Learning to Prevent Profitless Neural Code Completion"}],"references":[{"paperId":"78fff371e2b512360fa6f6494eca9354a52b0932","title":"Generating realistic vulnerabilities via neural code editing: an empirical study"},{"paperId":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code"},{"paperId":"26f1c97c9e38ed0da329e3197ed554553b305d18","title":"Neural software analysis"},{"paperId":"c317f4d5ddeb11da9a5c8fea1e3cdec2f7de485d","title":"Deep Learning Based Program Generation From Requirements Text: Are We There Yet?"},{"paperId":"6162b14349877749b280baa9060daaf7f7faeede","title":"Semantic bug seeding: a learning-based approach for creating realistic bugs"},{"paperId":"1311adebe439ccd9e373d594a8df1d720a685190","title":"Code Prediction by Feeding Trees to Transformers"},{"paperId":"59cc0871534a20ac98db0663539c6675ab63566f","title":"SequenceR: Sequence-to-Sequence Learning for End-to-End Program Repair"},{"paperId":"708932f350dd6a28a8aaabac9600e2181ac8b70f","title":"On the naturalness of hardware descriptions"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"fcb1063cf60e48ee694be9dccdcb37ae066b9259","title":"Learning to Handle Exceptions"},{"paperId":"ee25620eea19f793d598dbbf6241cf7520a60ccd","title":"Patching as Translation: the Data and the Metaphor"},{"paperId":"bd47bb8cdd749a3356149da6155d2dcd7458779f","title":"Retrieval-based Neural Source Code Summarization"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"6acf0d7b24a93953269a4d52c6b2f0c7d11f9ae4","title":"Hoppity: Learning Graph Transformations to Detect and Fix Bugs in Programs"},{"paperId":"4ddb9135fc6fdf3f9e01fdfcd115fdbf6f7486b4","title":"Sequence Model Design for Code Completion in the Modern IDE"},{"paperId":"3944354c42ddfff7414ad06022f96c72858d5fa6","title":"Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code"},{"paperId":"0dfe706526e5234338411489e1826b4060acc4e8","title":"CC2Vec: Distributed Representations of Code Changes"},{"paperId":"fd12fc2b3e825885f3b921c8f796abf45f5a5b91","title":"On Learning Meaningful Assert Statements for Unit Test Cases"},{"paperId":"92515b7ed018194e340f9edefeb52d9b19f679ef","title":"Detecting Code Clones with Graph Neural Network and Flow-Augmented Abstract Syntax Tree"},{"paperId":"88e6da7240d1b524b6f8dada89e0ab2bca34039e","title":"Bridging Semantic Gaps between Natural Languages and APIs with Word Embedding"},{"paperId":"b5abecf78c1bd975449e34350289a904e6ef35cc","title":"Automated program repair"},{"paperId":"558b72b9e7a1e4d6633c2836aba5896354d37d24","title":"DeepDelta: learning to repair compilation errors"},{"paperId":"9e955da40c62b2543b963454b90928ad6cbd5f8a","title":"Does BLEU Score Work for Code Migration?"},{"paperId":"b3a6e5a38bb5984a27823ecc040526cde10eb730","title":"On Learning Meaningful Code Changes Via Neural Machine Translation"},{"paperId":"9c2ab4707e902f6174910847e3f94f3b85017ec3","title":"A Grammar-Based Structural CNN Decoder for Code Generation"},{"paperId":"98d1307bed619b58b4a44acd8e65ac58495776c2","title":"code2seq: Generating Sequences from Structured Representations of Code"},{"paperId":"28eff8fbf69faa452ac8a5317cc501a04845687b","title":"Automatically Generating API Usage Patterns from Natural Language Queries"},{"paperId":"643de4fe3fcd960bea7b2491831308bc0febcef9","title":"Tree2Tree Neural Translation Model for Learning Source Code Changes"},{"paperId":"077ea3a496bd76bd22d76c432b79b9fa40e136e8","title":"Improving Automatic Source Code Summarization via Deep Reinforcement Learning"},{"paperId":"814db99ccf6b88d6af5b406b0c344b64c0a710b7","title":"A Structured Review of the Validity of BLEU"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"3646885c6ffec41d5ac0c3688b99bd2cd913fe91","title":"TreeGAN: Syntax-Aware Sequence Generation with Generative Adversarial Networks"},{"paperId":"9bab309fa2cc8e6da01378bd693b0333724f7e63","title":"From UI Design Image to GUI Skeleton: A Neural Machine Translator to Bootstrap Mobile GUI Implementation"},{"paperId":"2b80df13ad4ce21d85b36355403d5aded39e3f6e","title":"Learning to Repair Software Vulnerabilities with Generative Adversarial Networks"},{"paperId":"15f16252af84a630f1f9442d4e0367b6fba089cf","title":"Deep Code Comment Generation"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"02b92dcc30865950517894d8adafd3bd4474b018","title":"A general path-based representation for predicting program properties"},{"paperId":"1c159db22f8a6e7c177d9c9fdb5c3972514a12f9","title":"A deep neural network language model with contexts for source code"},{"paperId":"dd908c09eaf8f3a3c20d64fff798ec902433737d","title":"Code Completion with Neural Attention and Pointer Networks"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"554c9b3aa8c3b7504810270b18b942a0d22156e7","title":"DeepFix: Fixing Common C Language Errors by Deep Learning"},{"paperId":"e4a323a3a1ca5ff4cc2dd74ede65f48fd62b897b","title":"Deep API learning"},{"paperId":"927aaa20318d18564bd331ed37428e05820d647e","title":"Divide-and-Conquer Approach for Multi-phase Statistical Migration for Source Code (T)"},{"paperId":"27e1dbe9f7c71cd6cc1b0357f49aef497e572d09","title":"Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"},{"paperId":"333f98412ff246cd646551b4ca6f4b059dc1ea81","title":"Re-evaluating Automatic Summarization with BLEU and 192 Shades of ROUGE"},{"paperId":"9277dc7dd5b348645e8fcf4a987e5b3fc9132c7c","title":"Towards a Big Data Curated Benchmark of Inter-project Code Clones"},{"paperId":"8bc150dc49fc81c7c4dacd35a2b8b1afe1a1692a","title":"A Systematic Comparison of Smoothing Techniques for Sentence-Level BLEU"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":null,"title":"Re-evaluation the Role of Bleu inMachine Translation Research"},{"paperId":"ea5cf5569eef0a99df9b6d92b628a33fc82ca2e7","title":"On Some Pitfalls in Automatic Evaluation and Significance Testing for MT"},{"paperId":"4774432f02ef4c5285952dd8c7daff0852c3a601","title":"Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization"},{"paperId":"4fb5c3f5dcb865134e99402a39a3fb2eff0ab628","title":"Extending the BLEU MT Evaluation Method with Frequency Weightings"},{"paperId":"c63bb976dc0d3a897f3b0920170a4c573ef904c6","title":"Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"205ac1373eb7981aca2d08f2ab651871a001271e","summary":"The results show that CrystalBLEU differentiates similar and unrelated programs better than the original BLEU score and also a variant designed specifically for source code, CodeBLEU."},{"url":"https://www.semanticscholar.org/paper/09e14c4c80e20e80c052e0adb0d49df51aff718d","title":"Yes, You Can Make an App Too: A Systematic Study of Prompt Engineering in the Automatic Generation of Mobile Applications from User Queries","venue":"","year":2022,"referenceCount":16,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Jasmine Shone","citations":[],"references":[{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"6d7d4fca9840504f630e9bea6acaa07322a6e889","title":"Text and Code Embeddings by Contrastive Pre-Training"},{"paperId":"166b64f2ae8e52f5779682fab756cbd617a6e74b","title":"A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level"},{"paperId":"a2aa222de671fafa94b75ddc001388f693c21e14","title":"The Impacts of Low/No-Code Development on Digital Transformation and Software Development"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"56fa0b9cba4d9aee5ccc327365b3b3a721031c69","title":"Calibrate Before Use: Improving Few-Shot Performance of Language Models"},{"paperId":"ac3cdb50606f7770eef8e4cd951840a4f71287a0","title":"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"},{"paperId":"1fa9ed2bea208511ae698a967875e943049f16b6","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"c5e921a67842dcc2b508d49d08c90a3a1ed7c459","title":"Maximum Relevance and Minimum Redundancy Feature Selection Methods for a Marketing Machine Learning Platform"},{"paperId":"3b5505eaec8583a2dd98d72a84d95b9eff475a81","title":"Few-shot Learning: A Survey"},{"paperId":"7a65f23d990231d461418067c808b09d84c19b2c","title":"Natural Language Processing with Python"},{"paperId":"86d2df70fa9caddad48f479a902b8e052852ac51","title":"Minimum redundancy feature selection from microarray gene expression data"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":null,"title":"Structured testing"},{"paperId":null,"title":"Few-shot learning: A survey. CoRR, abs"},{"paperId":null,"title":"The future of it lies in democratising application development"}],"id":"09e14c4c80e20e80c052e0adb0d49df51aff718d","summary":"One of the first systematic studies of prompt engineering for the Codex model, a LLM that produces code from a natural-language input, is embarked on, improving the pipeline’s performance from baseline for complex apps using example selection mechanisms and 43% for simple apps."},{"url":"https://www.semanticscholar.org/paper/91260f73dd179487fb16713deb8267634ae14716","title":"CodexDB: Synthesizing Code for Query Processing from Natural Language Instructions using GPT-3 Codex","venue":"Proceedings of the VLDB Endowment","year":2022,"referenceCount":39,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"Immanuel Trummer","citations":[{"paperId":"270b015093073d3ba254928b6d736a59870d3fb1","title":"Chatbots As Fluent Polyglots: Revisiting Breakthrough Code Snippets"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"9bf75110ea0923bbed49256b5491f1ec284019ec","title":"From BERT to GPT-3 Codex: Harnessing the Potential of Very Large Language Models for Data Management"}],"references":[{"paperId":"51000d9f79be0eefd7972fe94e3c71dddc90d2c6","title":"Evaluating the Text-to-SQL Capabilities of Large Language Models"},{"paperId":"2236b6036cd1ecba1792d5e1fedbae7cefc3d43b","title":"Can we generate shellcodes via natural language? An empirical study"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis"},{"paperId":"6fa43591e6da321e43722dac609f6b8ef8204768","title":"SeaD: End-to-end Text-to-SQL Generation with Schema-aware Denoising"},{"paperId":"59641c10ed7431a3cf841f308367dc2dc0281b74","title":"What Makes Good In-Context Examples for GPT-3?"},{"paperId":null,"title":"A first look at rote learning in GitHub Copilot suggestions"},{"paperId":"5fbcfccd3736969d95ed660d8e6962c86b7a9113","title":"PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models"},{"paperId":"4f68e07c6c3173480053fd52391851d6f80d651b","title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"fe935caed47ef090a306d6d09240f76adc43a420","title":"Ember: No-Code Context Enrichment via Similarity-Based Keyless Joins"},{"paperId":"6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"bfe6cce534d76df1aa7ebd629c09e30d6b8abdd4","title":"RPT: Relational Pre-trained Transformer Is Almost All You Need towards Democratizing Data Preparation"},{"paperId":null,"title":"Introducing GitHub Copilot: your AI pair programmer"},{"paperId":null,"title":"GitHub Copilot: Copyright, Fair Use, Creativity, Transformativity, and Algorithms "},{"paperId":"232b40980acb55afa89ec50dd9806a5e551f699b","title":"Bridging Textual and Tabular Data for Cross-Domain Text-to-SQL Semantic Parsing"},{"paperId":"43a712e8b9d2db596abd6dc2ca0ffedb8878cde3","title":"ATHENA++: Natural Language Querying for Complex Nested SQL Queries"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":null,"title":"AUTOPROMPT: Eliciting knowledge from language models with automatically generated prompts"},{"paperId":"5452cd8a1ebb941e473f0e5c8ba6cc7359e40b24","title":"DBPal: Weak Supervision for Learning a Natural Language Interface to Databases"},{"paperId":"157a7ae44613a1fcf34e2be8c1e19a4f6e3c50e3","title":"Transfer Learning in Natural Language Processing"},{"paperId":"5ba52bbe1101939c490a06cc0cf316a09000834e","title":"Neo: A Learned Query Optimizer"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"287050dc91b146768c9d4435e5582fc9975ba84c","title":"Learned Cardinalities: Estimating Correlated Joins with Deep Learning"},{"paperId":null,"title":"Request for Comments on Intellectual Property Protection for Artificial Intelligence Innovation"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"f6eee02a2f4c74c2b543bf419f76cef60d5752f8","title":"The Case for Learned Index Structures"},{"paperId":"116a877c774969b53399b1ccaa34d51bfb492ee4","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"ac67d5f9c89d8d72fbd074f94079608220348f3f","title":"ATHENA: An Ontology-Driven System for Natural Language Querying over Relational Data Stores"},{"paperId":"1c39f884bc53d7d5465d0f56c4b432a9352afbe7","title":"Understanding Natural Language Queries over Relational Databases"},{"paperId":"888764f05a60d770cfc0b49944308fd92ed45ee5","title":"How Good Are Query Optimizers, Really?"},{"paperId":"42414b70fc61def8adcf5c159604c72e4508e9c1","title":"NaLIR: an interactive natural language interface for querying relational databases"},{"paperId":"bac4169d6b6f713c76271b5ccf3d45293351f785","title":"Runtime Code Generation in Cloudera Impala"},{"paperId":"e89fb3dd3412fc45506bfd06ea338d31093215ab","title":"Polynomial heuristics for query optimization"},{"paperId":"56eeedcd45e384f1d73cd27b4aa426c154554bbb","title":"Generating code for holistic query evaluation"},{"paperId":"cf6657ee417fb38bed405793e06a3656e2bd1d23","title":"Access path selection in a relational database management system"},{"paperId":null,"title":"Training a causal language model from scratch"}],"id":"91260f73dd179487fb16713deb8267634ae14716","summary":"The CodexDB framework is a framework on top of GPT-3 Codex that decomposes complex SQL queries into a series of simple processing steps, described in natural language, that enables users to customize SQL query processing via natural language instructions."},{"url":"https://www.semanticscholar.org/paper/d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis","venue":"International Conference on Software Engineering","year":2021,"referenceCount":42,"citationCount":23,"influentialCitationCount":1,"publicationDate":"06/12/2021","authors":"Naman Jain,Skanda Vaidyanath,Arun Shankar Iyer,Nagarajan Natarajan,Suresh Parthasarathy,S. Rajamani,Rahul Sharma","citations":[{"paperId":"0192638593c430e15e2804a74a3e8a7ecb52d435","title":"On the Design of AI-powered Code Assistants for Notebooks"},{"paperId":"0885556b71b24a641b4ffe139afd4d2712228cff","title":"Beware of the Unexpected: Bimodal Taint Analysis"},{"paperId":"d8405996b4d08c304098636aedd9e1c1a1e262ee","title":"The Premature Obituary of Programming"},{"paperId":"a3e355b5de868f34fdfa2500415c5f74c69d2091","title":"Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models"},{"paperId":"9e8f0d9ba4e7af673c8b214b3764e020706dd1f3","title":"Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"30624a18720bf93a85dc3efe570df271a8c9f4c3","title":"Program Repair"},{"paperId":"6d56ab40eba28678c4f4d6b054f2ae4e7048c928","title":"CLAWSAT: Towards Both Robust and Accurate Code Models"},{"paperId":"b1eebb2df3b9ff7ff2b00fb1a786f6ada2caebce","title":"Towards a Mathematics Formalisation Assistant using Large Language Models"},{"paperId":"ec7324a15009a9bd0b676f6b17762f759cf5dd9a","title":"Large Language Models Are Human-Level Prompt Engineers"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"43112f25190a9e19dc84cc7a0851318fdd1d9f71","title":"INTENT: Interactive Tensor Transformation Synthesis"},{"paperId":"a6ef5bad716091fb1888bf365f6129628ab3a5ee","title":"ObSynth: An Interactive Synthesis System for Generating Object Models from Natural Language Specifications"},{"paperId":"045a6f92b58f08d5b305dd5d661316a506ee4d43","title":"Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation"},{"paperId":"55e3fe05598be7c3dd357d51166869f6571b824f","title":"Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors"},{"paperId":"1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization"},{"paperId":"2f2750b48a6f958ff12cba90e99695123d1e2f47","title":"Using pre-trained language models to resolve textual and semantic merge conflicts (experience paper)"},{"paperId":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code"},{"paperId":"ea4749c6ed5f08b16bab31b1ca4fd3fdc259ae8f","title":"Improving automatically generated code from Codex via Automated Program Repair"},{"paperId":"0bcd59da541fdae66884afba8d25475a54a9da1a","title":"Automated Repair of Programs from Large Language Models"},{"paperId":"bb7e46f316d319f9819c3554c99995ef8361ae9c","title":"CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex"},{"paperId":"b2c4fdb49bdb23e6ad0ac3272029324157046ea7","title":"Automated Repair of Code from Language Models"},{"paperId":"91260f73dd179487fb16713deb8267634ae14716","title":"CodexDB: Synthesizing Code for Query Processing from Natural Language Instructions using GPT-3 Codex"}],"references":[{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"40c25232bc3a3f36ac856ff517d5c70704f14965","title":"TF-Coder: Program Synthesis for Tensor Manipulations"},{"paperId":"4b2137280915ccc0e06e97b604778b05876a34ad","title":"Evaluating Large Language Models"},{"paperId":"a8863de15a5ee8eed98107f423138a1a8f5a2ba8","title":"Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"b58d8579ece27a60432e667bfbdb750590fa65d9","title":"True Few-Shot Learning with Language Models"},{"paperId":"93b9c18e5e4924f793241bb0fc359741032c9ff6","title":"Web question answering with neurosymbolic program synthesis"},{"paperId":"56fa0b9cba4d9aee5ccc327365b3b3a721031c69","title":"Calibrate Before Use: Improving Few-Shot Performance of Language Models"},{"paperId":"5868a7bfe6a4590d332ca66b8097dbe5490c8a73","title":"SmBoP: Semi-autoregressive Bottom-up Semantic Parsing"},{"paperId":"3f97c2067cde9377e50b3160bbd7982c94abd88a","title":"An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions"},{"paperId":"6099feb5eb536f3666614565b1bda5b701d40be2","title":"B2: Bridging Code and Interactive Visualization in Computational Notebooks"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0c5bc409e62e65f86838968a2a7cdae5fa0b288b","title":"RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers"},{"paperId":"1bb27a0180f0c5fdd66cf34864752dfb1d6d94d2","title":"Sketch-Driven Regular Expression Generation from Natural Language and Examples"},{"paperId":"9d2f40625123d7000ab044a5c38eb47168a4481c","title":"Multi-modal synthesis of regular expressions"},{"paperId":"26d9141ed3f021af7533e1d84fc83111d20df925","title":"AutoPandas: neural-backed generators for program synthesis"},{"paperId":"d2d44be771d01e277a9912249f2f7c211c393fee","title":"On the fly synthesis of edit suggestions"},{"paperId":"93d63ec754f29fa22572615320afe0521f7ec66d","title":"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"},{"paperId":"4e7fab8c0f695f50128045f0394a4d7a92be32b5","title":"Maximal multi-layer specification synthesis"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"62fb54f5736be4a55b2da17010a115cee701d72c","title":"Accelerating search-based program synthesis using learned probabilistic models"},{"paperId":"93096257f8418b0414cddad54d3f53bc78273355","title":"Transform-Data-by-Example (TDE): An Extensible Search Engine for Data Transformations"},{"paperId":"85bdb70962147cd3586d99f50328025c82c9ac8e","title":"Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples"},{"paperId":"bbe832982b47a6b39904d5abc608a8c2fc10c5ee","title":"Program synthesis using conflict-driven learning"},{"paperId":"3ff0af64279929a952ee340e645256b7e0580f65","title":"RobustFill: Neural Program Learning under Noisy I/O"},{"paperId":"644ca74f80463415613847ab01cff067fb58f0ad","title":"Neuro-Symbolic Program Synthesis"},{"paperId":"cf404026f1e66dc04417d699ef2b4731cbec842c","title":"Learning Syntactic Program Transformations from Examples"},{"paperId":"bb1f78612d80cf5e72345b21be1d129c6ec02629","title":"Programming by Examples"},{"paperId":null,"title":"DeepCoder: Learning toWrite Programs"},{"paperId":"7f6245d527e84092111a6adb0f1939d0209d3360","title":"FlashMeta: a framework for inductive program synthesis"},{"paperId":"e2d2099379af74d7be80a1964830f99edddaeb11","title":"Compositional Program Synthesis from Natural Language and Examples"},{"paperId":"e5412b187655613ce29ec0651a450fed3bac288b","title":"Predicting a Correct Program in Programming by Example"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"8ad378c449db1ac08e8b98238e5387b62c549020","title":"Integrating Programming by Example and Natural Language Programming"},{"paperId":"43dcda631f8a0d39949ba3f9e3e22101db4daba0","title":"A Machine Learning Framework for Programming by Example"},{"paperId":"49af3e80343eb80c61e727ae0c27541628c7c5e2","title":"Introduction to Modern Information Retrieval"},{"paperId":null,"title":"GitHub Copilot · Your AI pair programmer"},{"paperId":null,"title":"The pandas development team. 2020. pandas-dev/pandas: Pandas"},{"paperId":null,"title":"Spider 1.0: Yale Semantic Parsing and Text-to-SQL Challenge"}],"id":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","summary":"This paper presents an approach to augment these large language models with post-processing steps based on program analysis and synthesis techniques, that understand the syntax and semantics of programs, and presents the experiences from building and evaluating such a tool Jigsaw."},{"url":"https://www.semanticscholar.org/paper/075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":7,"influentialCitationCount":2,"publicationDate":"04/06/2022","authors":"J. Inala,Chenglong Wang,Mei Yang,Andrés Codas,Mark Encarnaci'on,Shuvendu K. Lahiri,M. Musuvathi,Jianfeng Gao","citations":[{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"},{"paperId":"045a6f92b58f08d5b305dd5d661316a506ee4d43","title":"Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation"},{"paperId":"453a8fac3be9282be53908f0735160d0d21e0f48","title":"Repair Is Nearly Generation: Multilingual Program Repair with LLMs"}],"references":[{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"bd02bcf2a437217eaac245abe443b2f672b3b36a","title":"Deep Learning Based Vulnerability Detection: Are We There Yet?"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"570a6a5b8ec2827c3f33bb1b1bd027190a0d3e07","title":"Neural Program Generation Modulo Static Analysis"},{"paperId":"4698fc4712f0212c8a3810fd67b41ee8b8896aba","title":"Generate & Rank: A Multi-task Framework for Math Word Problems"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"7093ad29f2de2781cf5ed6e6d212ed31ebaa2c8b","title":"Latent Execution for Neural Program Synthesis Beyond Domain-Specific Languages"},{"paperId":"482f1befc7574c282aa35ec242ee8dc14030725e","title":"DeepWukong: Statically Detecting Software Vulnerabilities Using Deep Graph Neural Network"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"f2e5702330e52b730378ca1e7f1a041fc4f81c07","title":"Vulnerability Prediction From Source Code Using Machine Learning"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"00549af4bc3270e0f688acbf694f912d7ee39cad","title":"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"f669b5406320a83e94b4dd439806211c8b1e95fc","title":"Write, Execute, Assess: Program Synthesis with a REPL"},{"paperId":"0e412349f4c5161b413b43e0fe27e00f4ef83043","title":"When deep learning met code search"},{"paperId":"6c41bedc4637f3fd504c68baa3b3d8881e056ac1","title":"Execution-Guided Neural Program Synthesis"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"e033a0b29af2939fd44e5765d03380c08897a9e8","title":"Deep Code Search"},{"paperId":"6c6170ffb39cdc8cfffbeda9c7a2259eda5875f2","title":"Tree-to-tree Neural Networks for Program Translation"},{"paperId":"3807a517403cfc7ed67fdfea64baf62b9948bcfe","title":"A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation"},{"paperId":"49512270b39636375880d611d7b2192d324f4ba6","title":"Convolutional Neural Networks over Tree Structures for Programming Language Processing"},{"paperId":"927aaa20318d18564bd331ed37428e05820d647e","title":"Divide-and-Conquer Approach for Multi-phase Statistical Migration for Source Code (T)"},{"paperId":"9277dc7dd5b348645e8fcf4a987e5b3fc9132c7c","title":"Towards a Big Data Curated Benchmark of Inter-project Code Clones"},{"paperId":null,"title":"Github copilot: Your ai pair programmer"},{"paperId":null,"title":"Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation"},{"paperId":null,"title":"Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?"}],"id":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","summary":"C ODE R ANKER is a neural ranker that can predict the correctness of a sampled program without executing it and can signiﬁcantly increase the pass@1 accuracy of various code generation models on APPS, HumanEval, and MBPP datasets."},{"url":"https://www.semanticscholar.org/paper/239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning","venue":"ArXiv","year":2022,"referenceCount":64,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"E. Zelikman,Qian Huang,Gabriel Poesia,Noah D. Goodman,N. Haber","citations":[],"references":[{"paperId":"fb03acf0a7d90f4bc2cc91ed8980c7ec055edbf9","title":"Top-Down Synthesis for Library Learning"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"ae9a0d7c98f8a74a538198291b0925609e5c26ac","title":"Neural Story Planning"},{"paperId":"a8afd12bcd51488ba69bd838ef6dbf2728d5121a","title":"Prompting Is Programming: A Query Language For Large Language Models"},{"paperId":"c90151f00b1ac4abf1cc353849b453aa21cc2df3","title":"Successive Prompting for Decomposing Complex Questions"},{"paperId":"6d7b8a478801bd9d21df82d5f33ae6eced90da5e","title":"Solving math word problems with process- and outcome-based feedback"},{"paperId":"5032c0946ee96ff11a292762f23e6377a6cf2731","title":"Holistic Evaluation of Language Models"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"ce3f027b68dad014a58aa35f52380932c8d0b209","title":"Do Users Write More Insecure Code with AI Assistants?"},{"paperId":"0566c1c3eeeef5c968fced6d80b77fe22d02bbd9","title":"Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language"},{"paperId":"20e4ca6368a0731d3cbbd2e68aa90cef7384dc72","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"d26f616699a122e5455a13189e276002ee4cf923","title":"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs"},{"paperId":"2d2ca2e54c54748557b8aac7d328ce32ebfe8944","title":"ReAct: Synergizing Reasoning and Acting in Language Models"},{"paperId":"c140fe515de2f20d0c85c813c7b3ec1defc41f9d","title":"Binding Language Models in Symbolic Languages"},{"paperId":"c03fa01fbb9c77fe3d10609ba5f1dee33a723867","title":"ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"},{"paperId":"29bc052f3c46bf1110cc02fd71d454ebe8b13b80","title":"A Generalist Neural Algorithmic Learner"},{"paperId":"1c8e15f15d67c5974445634bb971e2275e957aff","title":"Security Implications of Large Language Model Code Assistants: A User Study"},{"paperId":"e3a9af420cd2c0c8241856da92374027fefb87be","title":"Language Model Cascades"},{"paperId":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models"},{"paperId":"51c2a0835fc565ad1fc7a58559ede9cbe8f6551e","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"6d86f08a5d936780a4785acfad92f5f3e82004ad","title":"Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks"},{"paperId":"341bdbcfc3febef7691a97c216ad394653211095","title":"Can language models learn from explanations in context?"},{"paperId":"cb5e3f085caefd1f3d5e08637ab55d39e61234fc","title":"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"},{"paperId":"23dd78e424d32f6a48660dcd67ce994b8a7db8be","title":"STaR: Bootstrapping Reasoning With Reasoning"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"},{"paperId":"9a258f42e333ed5ff79037724eb01747ede0bb49","title":"Few-Shot Self-Rationalization with Natural Language Prompts"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":null,"title":"Formal Environment Multi-step Planning Also encouragingly, several existing works can be expressed in Parsel"},{"paperId":null,"title":"That is, it would be valuable to allow a model to determine which target language to use, possibly combining them. For example, for large parts of the Tensorflow and PyTorch"},{"paperId":null,"title":"Abstract Reasoning Corpus (Chollet, 2019), tended to provide step-by-step hierarchical descriptions with many verification"},{"paperId":null,"title":"Ml-enhanced code completion improves developer productivity, 2022"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"fd963ea665e7a6e49753652dc7efe5affba5feb0","title":"DreamCoder: bootstrapping inductive program synthesis with wake-sleep library learning"},{"paperId":"98485ce6532d69f34a8ec67de6b09a39532bd221","title":"Communicating Natural Programs to Humans and Machines"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"2bb1e1a5b9a16f6828fe94736cea5dab264533a6","title":"Provable Limitations of Acquiring Meaning from Ungrounded Form: What Will Future Language Models Understand?"},{"paperId":"30a156f17ca8f54aa14d01d32c2315c11fcbe723","title":"BUSTLE: Bottom-up program-Synthesis Through Learning-guided Exploration"},{"paperId":null,"title":"2021) showed that giving language models access to a calculator allowed them to solve more complex math word problems"},{"paperId":"5fe0a4af3bd1479d5e39fbda2215c86bce54722b","title":"Generative Language Modeling for Automated Theorem Proving"},{"paperId":"9808d59113029d96f48a0376b1578dbab5427bb4","title":"Unsupervised Commonsense Question Answering with Self-Talk"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"874e9318c09c711ecd48a903b3824a3a03e2cd62","title":"Explain Yourself! Leveraging Language Models for Commonsense Reasoning"},{"paperId":"7139a5f730652abbeabf9e140009907d2c7da3e5","title":"VirtualHome: Simulating Household Activities Via Programs"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"745c614dbd23bc1e3def79f600680b88cee28700","title":"Thinking Fast and Slow with Deep Learning and Tree Search"},{"paperId":"bb1f78612d80cf5e72345b21be1d129c6ec02629","title":"Programming by Examples"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"2579e826f22da988fbc1b5d545aa59ab68bde4f3","title":"Program Synthesis Using Natural Language"},{"paperId":"e2d2099379af74d7be80a1964830f99edddaeb11","title":"Compositional Program Synthesis from Natural Language and Examples"},{"paperId":null,"title":"TensorFlow: Largescale machine learning on heterogeneous systems"},{"paperId":"f193e68ce6f3200b1f801e64bf49e56f668fd3ef","title":"A Survey on Unit Testing Practices and Problems"},{"paperId":"f48eed915cbb9c6592cdb9df80c1edaeb46959af","title":"A measure of intelligence"},{"paperId":"1bc831136e380489a0a76ff07f501003866cb954","title":"The status of the P versus NP problem"},{"paperId":"2fb9729c0a3b046c390ad6748773527eae6cfc84","title":"What Is Program Synthesis?"},{"paperId":"84bdb440504732f38cf08a996c808156cd504e32","title":"An algorithm for reduction of operator strength"},{"paperId":"18ce82b07ac84aaf30b502c93076cec2accbfcaa","title":"Human problem solving: The state of the theory in 1970."},{"paperId":null,"title":"Unified Natural Language Framework for Algorithmic Reasoning Simon"},{"paperId":null,"title":"The fantastic combinations of john conway’s new solitaire game “life” by martin gardner"}],"id":"239b5649b12f28fd610de036afba41b9246db6c9","summary":"This work introduces Parsel 2, a framework enabling automatic implementation and validation of complex algorithms with code LLMs, based on hierarchical function descriptions in natural language, which can be used across domains requiring hierarchical reasoning, e.g. code synthesis, theorem proving, and robotic planning."},{"url":"https://www.semanticscholar.org/paper/35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs","venue":"ArXiv","year":2022,"referenceCount":28,"citationCount":4,"influentialCitationCount":3,"publicationDate":"28/05/2022","authors":"Ansong Ni,J. Inala,Chenglong Wang,Oleksandr Polozov,Christopher Meek,Dragomir R. Radev,Jianfeng Gao","citations":[{"paperId":"d3a7a4543d83f568f79d1febe8379465ff0140c9","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"32b58766a1bfcef7ebba07070a272687aa518206","title":"Explicit Knowledge Transfer for Weakly-Supervised Code Generation"},{"paperId":"538288d24bdad73d831dfed44b706958287ed318","title":"Generating Sequences by Learning to Self-Correct"},{"paperId":"8d282bf295d655e38b63ddbc7cdc94b188df8bb2","title":"G ENERATING S EQUENCES BY L EARNING TO [S ELF -]C ORRECT"}],"references":[{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"4116e65ef8a05c82c0fd739d98ca72e50802cf83","title":"Representing Partial Programs with Blended Abstract Semantics"},{"paperId":"7093ad29f2de2781cf5ed6e6d212ed31ebaa2c8b","title":"Latent Execution for Neural Program Synthesis Beyond Domain-Specific Languages"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"f669b5406320a83e94b4dd439806211c8b1e95fc","title":"Write, Execute, Assess: Program Synthesis with a REPL"},{"paperId":"84b888b66d598f4fd5909c524a1818e1004a3254","title":"Learning to Generalize from Sparse and Underspecified Rewards"},{"paperId":"6c41bedc4637f3fd504c68baa3b3d8881e056ac1","title":"Execution-Guided Neural Program Synthesis"},{"paperId":"2fc1cfc75d6ba80846d64fbec424f6c35682f5d8","title":"Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing"},{"paperId":"e3deff41f9d3eb1e141207b69d6c7a4c007bd63e","title":"Program Synthesis Through Reinforcement Learning Guided Tree Search"},{"paperId":"2f541b24f69798e255e04229e8dc78f4d1873fd7","title":"Improving Text-to-SQL Evaluation Methodology"},{"paperId":"dea6aeb514b1969ab879c793d46a0d2eceaa2cbf","title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis"},{"paperId":"1ae84940e212e2e3ca132c7aa0878baf0fda06cd","title":"From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood"},{"paperId":"4ca430d4640afa4a3838371a08f8f418284bdb7c","title":"Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"94d9cbfc474cb6ce961de45a06233b2853ca6724","title":"Toward automatic program synthesis"}],"id":"35afb74de9660962ebac2843d26de22a6fac2ef6","summary":"This work proposes to let the model perform sampling during training and learn from both self-sampled fully-Correct programs, which yield the gold execution results, as well as partially-correct programs, whose intermediate execution state matches another correct program."},{"url":"https://www.semanticscholar.org/paper/06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling","venue":"ArXiv","year":2022,"referenceCount":72,"citationCount":6,"influentialCitationCount":0,"publicationDate":"22/07/2022","authors":"Fenia Christopoulou,Gerasimos Lampouras,Milan Gritta,Guchun Zhang,Yinpeng Guo,Zhong-Yi Li,Qi Zhang,M. Xiao,Bo Shen,Lin Li,Hao Yu,Li-yu Yan,Pingyi Zhou,Xin Wang,Yu Ma,Ignacio Iacobacci,Yasheng Wang,Guangtai Liang,Jia Wei,Xin Jiang,Qianxiang Wang,Qun Liu","citations":[{"paperId":"b2f256a1ec54325102119a57cb2359840fe68586","title":"SantaCoder: don't reach for the stars!"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming"},{"paperId":"46d0a832fada6147bceb0bd4e39928e482733246","title":"How Important are Good Method Names in Neural Code Generation? A Model Robustness Perspective"},{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"045a6f92b58f08d5b305dd5d661316a506ee4d43","title":"Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation"}],"references":[{"paperId":"0121c151c96b32cd7851e4bcda2a468b279c1e6f","title":"CODE-MVP: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"456a70485aafc12dfed4fb7354668d72aae9b658","title":"SecureBERT: A Domain-Specific Language Model for Cybersecurity"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"4b27f18bff43d605805c92696a979714ced0b805","title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"6d7d4fca9840504f630e9bea6acaa07322a6e889","title":"Text and Code Embeddings by Contrastive Pre-Training"},{"paperId":"42a7015e48a1e00b70ebb442a82afb4b10017c0b","title":"When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations"},{"paperId":null,"title":"The 1st Intl. Workshop on Natural Language-based Software Engineering Co-located with ICSE 2022"},{"paperId":null,"title":"TS-BERT: A fusion model for pre-trainning time series-text representations"},{"paperId":null,"title":"Deep Learning For Code (DL4C) Workshop at ICLR 2022"},{"paperId":null,"title":"2nd International Workshop on Software Engineering Automation: A Natural Language Perspective (NLP-SEA 2021) at ASE 2021"},{"paperId":"a09395dc9312625d7e2dfcda0160b51344352289","title":"NLPaSE 2021 Organization"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"feba0c47bf12a02c3a725174bb53df78658a72a8","title":"Pre-Trained Models: Past, Present and Future"},{"paperId":"b5f9c1cc4c74d973986bc4b352b85a6ee2f475d6","title":"TreeBERT: A Tree-Based Pre-Trained Model for Programming Language"},{"paperId":"c07651110d3b98b63607557b57808d15d99013dd","title":"ProteinBERT: a universal deep-learning model of protein sequence and function"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"0b077c9577f4297dcf3da835e253d21965bbc6e0","title":"CoTexT: Multi-task Learning with Code-Text Transformer"},{"paperId":"78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f","title":"PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"ad7ddcc14984caae308c397f1a589aae75d4ab71","title":"Training data-efficient image transformers & distillation through attention"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"021bbcefc993c389bad6c1daefd8ff92d0fc2441","title":"Contrastive Code Representation Learning"},{"paperId":"036fda02d93139ad0ef37d892d41796a2a39fdcd","title":"CLSEBERT: Contrastive Learning for Syntax Enhanced Code Pre-Trained Model"},{"paperId":null,"title":"Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f6245b3e6270e4dc2e279c4b728030523dffcff4","title":"LEGAL-BERT: The Muppets straight out of Law School"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8","title":"Generative Pretraining From Pixels"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"962dc29fdc3fbdc5930a10aba114050b82fe5a3e","title":"End-to-End Object Detection with Transformers"},{"paperId":"75e924bd79d27a23f3f93d9b1ab62a779505c8d2","title":"Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"f64e1d6bc13aae99aab5449fc9ae742a9ba7761e","title":"UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"34733eaf66007516347a40ad5d9bbe1cc9dacb6b","title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"f5a28db512357b700b62fb655ef4a90864e2fe7e","title":"Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case"},{"paperId":"add2f205338d70e10ce5e686df4a690e2851bdfc","title":"Momentum Contrast for Unsupervised Visual Representation Learning"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"463fefdbd81a4a0a32cf59bc58a9545757c8cf2e","title":"Pre-trained Contextual Embedding of Source Code"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining"},{"paperId":null,"title":"CodeBERT: A pre-trained 19 model for programming and natural languages. In Findings of the Association for Computational Linguistics: EMNLP"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"7102bb3fe73bd057ff161d9db5214a267c1ef312","title":"FinBERT: Financial Sentiment Analysis with Pre-trained Language Models"},{"paperId":"5466ee5f16fc3c776fd1da667917592e5fd06720","title":"Selfie: Self-supervised Pretraining for Image Embedding"},{"paperId":"1c71771c701aadfd72c5866170a9f5d71464bb88","title":"Unified Language Model Pre-training for Natural Language Understanding and Generation"},{"paperId":"b3c2c9f53ab130f3eb76eaaab3afa481c5a405eb","title":"ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission"},{"paperId":"87078d95bee341a1767034d9432fb34937ecf65a","title":"SciBERT: Pretrained Contextualized Embeddings for Scientific Text"},{"paperId":"73bade9162bc4df6ca6c9ab8a7eb2d54d35acad2","title":"code2vec: learning distributed representations of code"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"2867eb64cc4bb1945363fdae12a168962ea823aa","title":"The GHTorent dataset and tool suite"},{"paperId":"330da625c15427c6e42ccfa3b747fb29e5835bf0","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"06ea568379211ffa07d9605f66f26f6f736ea5e0","summary":"A pretrained decoder-only language model adopting the P AN G U - α architecture for text-to-code generation, i.e. the synthesis of programming language solutions given a natural language problem description is presented."},{"url":"https://www.semanticscholar.org/paper/3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion","venue":"MAPS@PLDI","year":2022,"referenceCount":23,"citationCount":17,"influentialCitationCount":2,"publicationDate":"13/05/2022","authors":"Albert Ziegler,Eirini Kalliamvakou,Shawn Simister,Ganesh Sittampalam,X. A. Li,A. Rice,Devon Rifkin,E. Aftandilian","citations":[{"paperId":"0192638593c430e15e2804a74a3e8a7ecb52d435","title":"On the Design of AI-powered Code Assistants for Notebooks"},{"paperId":"5a05d7f6a2ee8fdb20f2e27baa95bd1e1a71c634","title":"Practitioners' Expectations on Code Completion"},{"paperId":"5e8bc5f84f3a550319b0d2b54cc0062b410d2328","title":"Taking Flight with Copilot"},{"paperId":"51d253814e85249a84bbe634b4a80d306b74fbd0","title":"\"It would work for me too\": How Online Communities Shape Software Developers' Trust in AI-Powered Code Generation Tools"},{"paperId":"ce3f027b68dad014a58aa35f52380932c8d0b209","title":"Do Users Write More Insecure Code with AI Assistants?"},{"paperId":"cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers"},{"paperId":"0b340dd78fd04bbde2807d5efedb796d319355e3","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?"},{"paperId":"5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","title":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming"},{"paperId":"825333b7efe2cade106eaf36c7e731f757974806","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot"},{"paperId":"ef29fb8cc6bdda4b011288f51da521d3c25fc53d","title":"Learning to Prevent Profitless Neural Code Completion"},{"paperId":"0509c25103939d59ed4e27b1393e74ad5734c453","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot"},{"paperId":"1c8e15f15d67c5974445634bb971e2275e957aff","title":"Security Implications of Large Language Model Code Assistants: A User Study"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?"},{"paperId":"1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization"},{"paperId":"114eb5a35a3cd802cd1f46fff35c284e32ef6c54","title":"GitHub Copilot AI pair programmer: Asset or Liability?"},{"paperId":"a1ef81e17a9ca41e09aba802040a2eca2744716f","title":"Generation Probabilities are Not Enough: Improving Error Highlighting for AI Code Suggestions"}],"references":[{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"57d8532ea99294a40696ae0f0a3fcd71440dc52a","title":"Improving Code Autocompletion with Transfer Learning"},{"paperId":"4a160efbe80c38cd5eb2f92c7c095b49b113397d","title":"In-IDE Code Generation from Natural Language: Promise and Challenges"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"2a36402451c4fcab33c9e7f59fc027167c070aa6","title":"Perfection Not Required? Human-AI Partnerships in Code Translation"},{"paperId":"6c107f38e218d967f5ad0618dfce2e9edf796063","title":"The SPACE of Developer Productivity"},{"paperId":"696aff001f8aa43a5ea0565841e9fc8842999027","title":"Mind the Gap: On the Relationship Between Automatically Measured and Self-Reported Productivity"},{"paperId":"87fe4a67e4cb5b8cdbc5da4d98888c037fb7d043","title":"Learning Autocompletion from Real-World Datasets"},{"paperId":"04413d13c76c4eadf1adcbbe88c3a72e6462f166","title":"Fast and Memory-Efficient Neural Code Completion"},{"paperId":"28797c288403828f29d3faca6ab9d64d134a4ea3","title":"The SPACE of Developer Productivity: There's more to it than you think"},{"paperId":null,"title":"2021. The SPACE of Developer Productivity: There's more to it than you think"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"660a2244efa8af0b77fd314a1c75dcc01aa677fe","title":"From Human-Human Collaboration to Human-AI Collaboration: Designing AI Systems That Can Work Together with People"},{"paperId":"d1aa325a5adefeba786d4c50a8ca9a8df9598f32","title":"Best practices for the human evaluation of automatically generated text"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"d02285385fd2b53a6100b8d62421bba6693b788c","title":"When Code Completion Fails: A Case Study on Real-World Completions"},{"paperId":"8059b85332572e60c8a1daa0ccb8ddc008513f00","title":"What makes a good conversation? How controllable attributes affect human judgments"},{"paperId":"71f85c2fab38e26e286609144a445d610f2a9ded","title":"A Study of Visual Studio Usage in Practice"},{"paperId":"adbe76a937270c76d50c1cd956be447271c3a8f0","title":"PLS-regression: a basic tool of chemometrics"},{"paperId":null,"title":", Brian Houck , and Jenna Butler . 2021 . The SPACE of Developer Productivity : There ’ s more to it than you think"}],"id":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","summary":"It is found that the rate with which shown suggestions are accepted, rather than more specific metrics regarding the persistence of completions in the code over time, drives developers’ perception of productivity."},{"url":"https://www.semanticscholar.org/paper/1100dee3fd78655cddc4b7bfaef1161351d4fab5","title":"Automated Feedback Generation for Competition-Level Code","venue":"International Conference on Automated Software Engineering","year":2022,"referenceCount":48,"citationCount":1,"influentialCitationCount":0,"publicationDate":"03/06/2022","authors":"Jialu Zhang,De Li,John C. Kolesar,Hanyuan Shi,R. Piskac","citations":[{"paperId":"2a0456b0408cd4c33f2ff4400374e7be2497a362","title":"Repairing Bugs in Python Assignments Using Large Language Models"}],"references":[{"paperId":"b78d0f729d44b54b8044afb0fbd426c2201cba02","title":"Generating Concise Patches for Newly Released Programming Assignments"},{"paperId":"1e3ec5709b2ca43233c566cd77dbabbd6892bfa6","title":"Neurosymbolic repair for low-code formula languages"},{"paperId":"2f2750b48a6f958ff12cba90e99695123d1e2f47","title":"Using pre-trained language models to resolve textual and semantic merge conflicts (experience paper)"},{"paperId":"cdc609428066914e1b5e17984d31894ad360aad4","title":"Learning CI Configuration Correctness for Early Build Feedback"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"c7684a8a4f24dc0fc9416e3cd0782cdbd467f27c","title":"Verifix: Verified Repair of Programming Assignments"},{"paperId":null,"title":"An example of repairing a faulty submission that need an algorithm-level redesign"},{"paperId":null,"title":"The Yandex Algorithm Cup"},{"paperId":null,"title":"The 10 Most Prestigious Programming Contests and Coding Challenges"},{"paperId":"46ca52b858c0ac1088cdcfb5bed7006b55cadf33","title":"Context-aware and data-driven feedback generation for programming assignments"},{"paperId":"ebfcbe0a8b238d5a52286fdfaab7be0170dbc91b","title":"FAPR: Fast and Accurate Program Repair for Introductory Programming Courses"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"a989f5daa7c438d6b8433efbb1461f1b0af1aa8e","title":"Break-It-Fix-It: Unsupervised Learning for Program Repair"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"305da9d093cde16cd568f16bf0cbbc0950d8d1f1","title":"Guide to Competitive Programming: Learning and Improving Algorithms Through Contests"},{"paperId":"b5abecf78c1bd975449e34350289a904e6ef35cc","title":"Automated program repair"},{"paperId":"325aecaa34bcf6e5f0e6683e526dc9946a6c1f4f","title":"Re-Factoring Based Program Repair Applied to Programming Assignments"},{"paperId":"0da0dcfa3642bf91ce53d88dee686279c65ab89b","title":"SemCluster: clustering of imperative programming assignments based on quantitative semantic features"},{"paperId":"f1bb2cd6b4455fa91784a90f3e114a7e445b80a3","title":"Automatic diagnosis and correction of logical errors for functional programming assignments"},{"paperId":"d6b7abd5c17f9e7d83c5b709c27da3a82a9788f2","title":"NAPS: Natural Program Synthesis Dataset"},{"paperId":"2a1392a112919229b2b5512e441dfe3a388ad2b5","title":"Automated clustering and program repair for introductory programming assignments"},{"paperId":"a1b9e544b74bb9e1b7ef20ee18d90644367bab17","title":"Semantic Program Repair Using a Reference Implementation"},{"paperId":"d591ed7f34cd0c169baf0ee75f82a11c338249ca","title":"Search, align, and repair: data-driven feedback generation for introductory programming exercises"},{"paperId":"c492e49c9eb3cfd51e77ab814201e81fe33cdcc5","title":"Dynamic Neural Program Embedding for Program Repair"},{"paperId":"2f17bcaa861dd6e6dff107e8ff39d92a24af5c74","title":"Evaluating and improving semistructured merge"},{"paperId":"380fdb98d66241e3769143d9c6564f23c053ec89","title":"The Continuous Hint Factory - Providing Hints in Vast and Sparsely Populated Edit Distance Spaces"},{"paperId":"6e5f3c4507aeb175da8712cff43cb8b2e60b5a12","title":"A feasibility study of using automated program repair for introductory programming assignments"},{"paperId":"bef6f21fccdcac24ebcd6b4287801ff1c9734966","title":"Automatic inference of code transforms for patch generation"},{"paperId":"cf404026f1e66dc04417d699ef2b4731cbec842c","title":"Learning Syntactic Program Transformations from Examples"},{"paperId":"33c65aace24e26979cc9ba6242310d06710ad8d2","title":"Qlose: Program Repair with Quantitative Objectives"},{"paperId":"5b0602f9d0f1384dc95335c6ed220fef40a7e186","title":"sk_p: a neural program corrector for MOOCs"},{"paperId":"3f215e83b39a0887257a03274002f353c5a57537","title":"Angelix: Scalable Multiline Program Patch Synthesis via Symbolic Analysis"},{"paperId":"d3e5f4ab52923337ff04cca46291204c583a3968","title":"Semi-supervised verified feedback generation"},{"paperId":"e8ec01a64c16f4e2fa40030b3e19fa6abe3d3b3a","title":"A Programming Contest Strategy Guide"},{"paperId":"50358e3f30ab8c99f9d383e5683b31c2311e5651","title":"Automatic patch generation by learning correct code"},{"paperId":"5e2ae818c7e387c0cd3ed7f09a85dbef3df3e296","title":"An Automated Framework for Recommending Program Elements to Novices (N)"},{"paperId":"0c1bf6da80dad0d272f3c87bc0e8ba365219b017","title":"CARAMEL: Detecting and Fixing Performance Problems That Have Non-Intrusive Fixes"},{"paperId":"90441b975380c806320420724fc9d0ec77dbefdb","title":"The strength of random search on automated program repair"},{"paperId":"b8afcb72c82c0f3ce8b85b211c99ee5b1591598b","title":"Current challenges in automatic software repair"},{"paperId":"d7aa22f4db5ca61d029b93db3770a2a05cb169a8","title":"Cachetor: detecting cacheable data to remove bloat"},{"paperId":"3136ad216d30bdff223e5c3f02e07f980a6a45a5","title":"Automatic patch generation learned from human-written patches"},{"paperId":"169a72730c870f30a4452bfc0648e35ac4561f42","title":"Automated feedback generation for introductory programming assignments"},{"paperId":"3aa35c213730e688fa191a1ce241db31087956f0","title":"GenProg: A Generic Method for Automatic Software Repair"},{"paperId":"d96d84083a1785bff755f3900e847604401e0590","title":"Semistructured merge: rethinking merge in revision control systems"},{"paperId":"aa6afb8ba5addd1b2cf47adc3d344d87a39a0b53","title":"ACM International Collegiate Programming Contest"},{"paperId":"428250f9387b2de004ac85c8000fd14c4e7580da","title":"MJRTY: A Fast Majority Vote Algorithm"},{"paperId":"277ff0c74cc72663d0aabbeae25a3e97b245457c","title":"Simple Fast Algorithms for the Editing Distance Between Trees and Related Problems"}],"id":"1100dee3fd78655cddc4b7bfaef1161351d4fab5","summary":"This work presents Clef, the first data-driven tool that can generate feedback on competition-level code automatically by repairing programmers’ incorrect submissions, and introduces a new data structure, merge trees, to capture the changes between submissions."},{"url":"https://www.semanticscholar.org/paper/618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?","venue":"ArXiv","year":2022,"referenceCount":97,"citationCount":5,"influentialCitationCount":2,"publicationDate":"12/08/2022","authors":"Advait Sarkar,A. Gordon,C. Negreanu,Christian Poelitz,Sruti Srinivasa Ragavan,B. Zorn","citations":[{"paperId":"0192638593c430e15e2804a74a3e8a7ecb52d435","title":"On the Design of AI-powered Code Assistants for Notebooks"},{"paperId":"51d253814e85249a84bbe634b4a80d306b74fbd0","title":"\"It would work for me too\": How Online Communities Shape Software Developers' Trust in AI-Powered Code Generation Tools"},{"paperId":"5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","title":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming"},{"paperId":"971e875e28f26240987d2c9470d1ee74ad204205","title":"Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction"},{"paperId":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"}],"references":[{"paperId":"84e8912ba21d9dd6eb2e3a92282afef3fc361691","title":"End-user encounters with lambda abstraction in spreadsheets: Apollo’s bow or Achilles’ heel?"},{"paperId":"0509c25103939d59ed4e27b1393e74ad5734c453","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot"},{"paperId":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"340535d68dedefd64b530154eeda5c4569da4a36","title":"Is Explainable AI a Race Against Model Complexity? 192-199"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"1f3a97d255a6d0b2d5535af75737ed43b1e96fc4","title":"Is GitHub Copilot a Substitute for Human Pair-programming? An Empirical Study"},{"paperId":"4bc040835fbff57ce6612306d794b8c6c8226086","title":"Discovering the Syntax and Strategies of Natural Language Programming with Generative Language Models"},{"paperId":"9203a57752ba96bde4e54dd7fb3f84e8161895b8","title":"“It’s Freedom to Put Things Where My Mind Wants”: Understanding and Improving the User Experience of Structuring Data in Spreadsheets"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"e67c6c075916da9ba9c2067418ddf961307b1c59","title":"GridBook: Natural Language Formulas for the Spreadsheet Grid"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"},{"paperId":"4a160efbe80c38cd5eb2f92c7c095b49b113397d","title":"In-IDE Code Generation from Natural Language: Promise and Challenges"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"d7a7ebd1565c3795bc2bcdec4334d42a65ad17c5","title":"Pretrained Language Models for Text Generation: A Survey"},{"paperId":"aa2eac44e9725b6201c272a1a1bb75adee837058","title":"TweakIt: Supporting End-User Programmers Who Transmogrify Code"},{"paperId":"6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"ceaff192479db6faee58ae88e053b0b319cf1893","title":"Retrieval-Augmented Generation for Code Summarization via Hybrid GNN"},{"paperId":"24bf9857fbb90270b97a7d462ab150de75580364","title":"FAccT '21: 2021 ACM Conference on Fairness, Accountability, and Transparency, Virtual Event / Toronto, Canada, March 3-10, 2021"},{"paperId":null,"title":"Github copilot research recitation"},{"paperId":"77b50a2c0dd166d8f36aec05d1539312bb0422c4","title":"The dilemma of the direct answer"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"3e7898bee9e13b92545d4815af6af9b20f674c48","title":"Understanding and Inferring Units in Spreadsheets"},{"paperId":"29fc7ff02e68537a86d32978bfd2e8e45258e77d","title":"LambdaNet: Probabilistic Type Inference using Graph Neural Networks"},{"paperId":"27e83a8ea690dd26267f9e6dcb9a6d95948ec963","title":"Do We Need Natural Language?: Exploring Restricted Language Interfaces for Complex Domains"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"6370d741f01b6cd59671d995c97dfd9a08827c36","title":"How do people learn to use spreadsheets? (Work in progress)"},{"paperId":"0dfef27d777569d92965c2dfa2b3c6f1b355ca2e","title":"How should compilers explain problems to developers?"},{"paperId":"7af1150059f3c6e7062e9bb8ac8af2a23098d826","title":"No half-measures: A study of manual and tool-assisted end-user programming tasks in Excel"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"ebd874cc9a886ea6283227c5eef034f5c7a4293c","title":"Interactive analytical modelling"},{"paperId":null,"title":"Introducing visual studio intellicode. Microsoft. Retrieved from https://devblogs.microsoft.com/visualstudio/introducing-visual -studio-intellicode"},{"paperId":"d93c6df00eb08a93a9e3567de506456db79009cd","title":"Exploring exploratory programming"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"6cf6dc8bb7995a1f529d51b11f1677e045337337","title":"SmartPaste: Learning to Adapt Source Code"},{"paperId":"f4f874c431b75432ed6c1badd1359c2af1cdd70e","title":"Bing developer assistant: improving developer productivity by recommending sample code"},{"paperId":"5008cd9c1f65c34088bebdd1e86e033265d61c6a","title":"The practices of programming"},{"paperId":"6d1f937d56ef4251d52bbe3c85a6ff4090925884","title":"Improving API usability"},{"paperId":"814e62bb4f0ec6facd640fbd3e3fcbd189716d20","title":"\"Like Having a Really Bad PA\": The Gulf between User Expectation and Experience of Conversational Agents"},{"paperId":"27fdb872558ea94409418ef1831e1bde92d3ab4a","title":"Foraging Among an Overabundance of Similar Variants"},{"paperId":"a5d0d3d23b3fbe1ac6c2ac0310e94fb1de7b0415","title":"Pair Programming vs. Solo Programming: What Do We Know After 15 Years of Research?"},{"paperId":"49512270b39636375880d611d7b2192d324f4ba6","title":"Convolutional Neural Networks over Tree Structures for Programming Language Processing"},{"paperId":"f4c6dbcc0a5f3d7f29aac705ccd9bb991d63ae07","title":"Software and How it Lives On - Embedding Live Programs in the World Around Them"},{"paperId":"2095acfccc9750d681c78d919e3f884c64818aad","title":"API Usability at Scale"},{"paperId":"2ef43455fe58699374d8fc5c6ed57eb753978a86","title":"Interactive visual machine learning in spreadsheets"},{"paperId":"927aaa20318d18564bd331ed37428e05820d647e","title":"Divide-and-Conquer Approach for Multi-phase Statistical Migration for Source Code (T)"},{"paperId":"a747e4db56a78c94e63608649fbdb3889be0c811","title":"Idea Garden: Situated Support for Problem Solving by End-User Programmers"},{"paperId":"e0641bbac3a091c932b81c449a0c6facf71de4d1","title":"I heart hacker news: expanding qualitative research findings by analyzing social news websites"},{"paperId":"4ac8cf91eeac90bb91f2419316747ae0934ba765","title":"Third-wave HCI, 10 years later---participation and sharing"},{"paperId":"cfab0a9412f2aff106080b2adb66e51f6dca497c","title":"Supporting Selective Undo in a Code Editor"},{"paperId":"75ae9a4a66a50a02cd1396e3e22036080f56d496","title":"Building Bing Developer Assistant"},{"paperId":"be6665462fd6c56e8756dadf23373bb0c90b2b19","title":"Predicting Program Properties from \"Big Code\""},{"paperId":"62cc6be687cddeadf5c77b221d49615e11b5e86d","title":"An empirical investigation of code completion usage by professional software developers"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"c8ffe876bfbe85231b236a9af35ad7a2d2ccfcef","title":"Structured labeling for facilitating concept evolution in machine learning"},{"paperId":"cf616c398eda90178db3520994dbf42bd22e627c","title":"The patchworks code editor: toward faster navigation with less code arranging and fewer navigation mistakes"},{"paperId":"1edaa5cd10740839433c83b64b996258fd8b4767","title":"Detecting and refactoring code smells in spreadsheet formulas"},{"paperId":"693932c48cb315c373e48fed8ba4e2725fb492a4","title":"An Empirical Study of API Usability"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"cd95f3f397e967005d6f108d0dbf3c5c7ffbcce9","title":"A perspective on the evolution of live programming"},{"paperId":"8f880975990a8e0ef817d997e64f65cf00742b7c","title":"Axiomatic Basis for Computer Programming"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":null,"title":"Research notebook: Computational thinking—what and why"},{"paperId":"3685e0436523d8c06bbf954b73b506bdc9a66ca1","title":"The effectiveness of pair programming: A meta-analysis"},{"paperId":"3896a88ad7b6a01f49ac37eb317eceddd51a5ff5","title":"Reducing Overconfidence in Spreadsheet Development"},{"paperId":"7b03a16039decbc8f949fc267f9661902bf0977a","title":"Aspects of PROLOG History: Logic Programming and Professional Dynamics"},{"paperId":"4d52e4f8037d797b7f9829bfa1854141790c1d7e","title":"Feasibility Studies for Programming in Natural Language"},{"paperId":"01dbd4ceafcd428bbb7b15db10d53952d0ad8ae7","title":"Designing the whyline: a debugging interface for asking questions about program behavior"},{"paperId":"448c454d10dccdda8b3a2457be2b2b4665819376","title":"First steps in programming: a rationale for attention investment models"},{"paperId":null,"title":"What is programming? In Ppig (p"},{"paperId":"1cc3f5cdd4204f8e55e46d9cbaef730d17ca647c","title":"Your Wish is My Command: Programming By Example"},{"paperId":"06b5370585b0d853de37e9671d831221399146c0","title":"All I really need to know about pair programming I learned in kindergarten"},{"paperId":"5df85ae89af55c6d82a1a14836ea6bcfbfc2c0ec","title":"Principles of mixed-initiative user interfaces"},{"paperId":"2058347c34d15dba2051c8474048902c1e70e585","title":"Cognitive Dimensions of Information Artefacts: a tutorial"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"6d32d1f65434d4cc94bd2cee0f5a889fa29d8edd","title":"Watch what I do: programming by demonstration"},{"paperId":"169f585547b17ed2b3c64bd37a5b58688646ef05","title":"A Small Matter of Programming: Perspectives on End User Computing"},{"paperId":"e7316c33d627b94d2fb9674e4af6408d70dbb52f","title":"The birth of Prolog"},{"paperId":null,"title":"1.1 direct manipulation: a step beyond programming. Sparks of innovation in human-computer interaction"},{"paperId":"46835cd4618dab26589d5a64a712f87668515db4","title":"Demonstrational interfaces: A step beyond direct manipulation"},{"paperId":"5d294399ae7141784563525f8fdefc9eabc6285f","title":"When Visual Programs are Harder to Read than Textual Programs"},{"paperId":"4ca24a6a487c3fa92d60a17b760cc3515708896a","title":"Cognitive dimensions of notations"},{"paperId":"67a614470d28b44a6e3b50f91cb4b99403e41389","title":"Direct Manipulation Interfaces"},{"paperId":"2a49ee0e7fa2048c483856cbbf5b24b05340ae8f","title":"Natural Language Programming: Styles, Strategies, and Contrasts"},{"paperId":null,"title":"Don't fully trust AI in dev work! /yet"},{"paperId":null,"title":"Blog / tabnine announcements / announcing our next-generation ai models. Tabnine"},{"paperId":null,"title":"Using Github copilot to get the tweets for a keyword and find the sentiment of each tweet in 2 mins"},{"paperId":null,"title":"Building games and apps entirely through natural language using OpenAI's code-davinci model"},{"paperId":null,"title":"Miscellaneous Hacker News discussions 1"},{"paperId":null,"title":"Building a No-Code Machine Learning Model by Chatting with GitHub Copilot"},{"paperId":null,"title":"A.1. Blog posts and corresponding Hacker News discussions"},{"paperId":null,"title":"One Month of Using GitHub Copilot"}],"id":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","summary":"This paper explores how programming with large language models (LLM-assisted programming) is similar to, and differs from, prior conceptualisations of programmer assistance, and draws upon publicly available experience reports of LLM- assisted programming, as well as prior usability and design studies."},{"url":"https://www.semanticscholar.org/paper/f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":83,"citationCount":12,"influentialCitationCount":0,"publicationDate":"10/06/2021","authors":"Tal Schuster,A. Kalyan,Oleksandr Polozov,A. Kalai","citations":[{"paperId":"d3a7a4543d83f568f79d1febe8379465ff0140c9","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"4c2534f9b03ac2f3810c07abc398a11bcf47258e","title":"Transformers Learn Shortcuts to Automata"},{"paperId":"eb5c1c666ce2fe1c531ecabfcdf264ae831fad89","title":"Understanding and Supporting Debugging Workflows in Multiverse Analysis"},{"paperId":"ace0745f4449f20e4f4297476941fcd7dc7ab05c","title":"Higher Cognition: A Mechanical Perspective"},{"paperId":"63c670ba8018da0a7e33c34ffd84c2a3ca54b894","title":"Language Models Can Teach Themselves to Program Better"},{"paperId":"2ef60a4ea4ea53056be811ff55679eb59fb4b586","title":"Confident Adaptive Language Modeling"},{"paperId":"35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs"},{"paperId":"f40aeae3e522ada1f6a9f326841b01ef5c8657b6","title":"Unifying Language Learning Paradigms"},{"paperId":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"}],"references":[{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"77a096d80eb4dd4ccd103d1660c5a5498f7d026b","title":"Dynabench: Rethinking Benchmarking in NLP"},{"paperId":"21ec9c0f869bdb33b06c7dbc8880169db0397d08","title":"UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark"},{"paperId":"eebc1811c55c2e5e8b3b78d0b0382ad50f22e32a","title":"Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"1ec69f1a1a9d4ff5c5bc70db0e5087157b620570","title":"You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion"},{"paperId":null,"title":"Research recitation: A first look at rote learning in github copilot suggestions"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"5fe0a4af3bd1479d5e39fbda2215c86bce54722b","title":"Generative Language Modeling for Automated Theorem Proving"},{"paperId":"f430773cdc0fdd62192a48a372601d6aa40f3d7b","title":"Task-Oriented Dialogue as Dataflow Synthesis"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"db392858262b17aa9c8ff8659738f68fbf832ebe","title":"Structural Language Models of Code"},{"paperId":"305da9d093cde16cd568f16bf0cbbc0950d8d1f1","title":"Guide to Competitive Programming: Learning and Improving Algorithms Through Contests"},{"paperId":null,"title":"Transformers: State-of-theart natural language processing"},{"paperId":null,"title":"CodeBERT: A pre-trained 455 model for programming and natural languages. In Findings of the Association for Com- 456 putational Linguistics: EMNLP 2020, pages 1536–1547"},{"paperId":null,"title":"Python One-Liners: Write Concise, Eloquent Python Like a Professional"},{"paperId":null,"title":"Karel the robot learns python"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"6dfc2ff03534a4325d06c6f88c3144831996629b","title":"From Recognition to Cognition: Visual Commonsense Reasoning"},{"paperId":"421cb75cc91e8e5683d41ee6a918121aedf6d24d","title":"Social IQA: Commonsense Reasoning about Social Interactions"},{"paperId":"c21a4d70d83e0f6eb2a9e1c41d034842dd561e47","title":"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"f9717d29840f4d8f1cc19d1b1e80c5d12ec40608","title":"A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play"},{"paperId":"af5c4b80fbf847f69a202ba5a780a3dd18c1a027","title":"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"},{"paperId":"d6b7abd5c17f9e7d83c5b709c27da3a82a9788f2","title":"NAPS: Natural Program Synthesis Dataset"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"d4c85ef35c5224792186100c29df141761d6abd8","title":"Neural Program Search: Solving Data Processing Tasks from Description and Examples"},{"paperId":"d102c0ed7c2559cd22fd167c1c2acff2b1023042","title":"Glass-Box Program Synthesis: A Machine Learning Approach"},{"paperId":"c27db32efa8137cbf654902f8f728f338e55cd1c","title":"Mastering the game of Go without human knowledge"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":null,"title":"George van den Driessche, Thore Graepel, and Demis Hassabis"},{"paperId":null,"title":"Five $1,000 problems (update 2017)"},{"paperId":"15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7","title":"Gaussian Error Linear Units (GELUs)"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"7f6245d527e84092111a6adb0f1939d0209d3360","title":"FlashMeta: a framework for inductive program synthesis"},{"paperId":"4d376d6978dad0374edfa6709c9556b42d3594d3","title":"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"},{"paperId":"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd","title":"ImageNet Large Scale Visual Recognition Challenge"},{"paperId":"c51274cdd2a0305cab0bbaf0298cab63db1b9152","title":"Bootstrap Learning via Modular Concept Discovery"},{"paperId":"43dcda631f8a0d39949ba3f9e3e22101db4daba0","title":"A Machine Learning Framework for Programming by Example"},{"paperId":"f48eed915cbb9c6592cdb9df80c1edaeb46959af","title":"A measure of intelligence"},{"paperId":"168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74","title":"Scikit-learn: Machine Learning in Python"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"f04af7291d49a0912def77a432b9ad79baa2096d","title":"Interactive Theorem Proving and Program Development: Coq'Art The Calculus of Inductive Constructions"},{"paperId":"a298defcbe68f5a09b0b8f6e5379164f7fd79bc5","title":"Handbook of Satisfiability"},{"paperId":"7130de3696c9e1942fbdbc8fba3cc78cd9ce7eed","title":"Computational Complexity: A Modern Approach"},{"paperId":"f491c61a874b049981053d86894bdcef9a0e5530","title":"Bebras International Contest on Informatics and Computer Literacy: Criteria for Good Tasks"},{"paperId":"ef78e1bd3bea4122217a93a72533a59f1b18a18c","title":"RSA factoring challenge"},{"paperId":"f87f327f64573fd79a5c3feabb608cf04837881c","title":"Noise-tolerant learning, the parity problem, and the statistical query model"},{"paperId":"fd77430f6f5c5e35e8a45ff3478032b680fa0b0c","title":"To all authors"},{"paperId":"5ed59f49c1bb7de06cfa2a9467d5efb535103277","title":"Temporal difference learning and TD-Gammon"},{"paperId":"18c1fdffdce47d319bd6874302dadb81f5d4f000","title":"The 3x + 1 Problem and its Generalizations"},{"paperId":"2fb9729c0a3b046c390ad6748773527eae6cfc84","title":"What Is Program Synthesis?"},{"paperId":"5e29000d24d5ded11e7a32216a91bdadaa9877f1","title":"On the inherent intractability of certain coding problems (Corresp.)"},{"paperId":"c903d23753bae10adf9b2821658249230e909033","title":"Finite Groups of Automorphisms: Course Given at the University of Southampton, October-December 1969"},{"paperId":null,"title":"Mathematical Society, London Mathematical Society lecture note series, and S.P.G.N.J"},{"paperId":"a973eb892f233acb3093589393181ae633d3a244","title":"ACTIVITY ANALYSIS OF PRODUCTION AND ALLOCATION"},{"paperId":"60b1cdf630031816f71cd6dd321195e9aaa5ec31","title":"An Introduction to the Theory of Numbers"},{"paperId":null,"title":"Association 605 for Computational Linguistics"},{"paperId":null,"title":"]): return len(bi) == len(set(bi)) and {(i, j) for i, j in g1} == {(bi[i], bi[j]) for i"},{"paperId":null,"title":"From conversation to code: Microsoft introduces its first product features powered by GPT-3"},{"paperId":null,"title":"668 (a) Did you include the full text of instructions given to participants and screenshots, if 669 applicable? [Yes] See the figures in G 670 (b) Did you describe any potential participant risks"},{"paperId":null,"title":"code, data, models) or curating/releasing new assets"},{"paperId":null,"title":"The Monkey and the coconuts\" ancient puzzle) def f86(n: int): for i in range"},{"paperId":null,"title":"If you are including theoretical results"},{"paperId":null,"title":", Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander M . Rush . Transformers : State - ofthe - art natural language processing"},{"paperId":null,"title":"663 (d) Did you discuss whether and how consent was obtained from people whose data you're 664 using/curating?"},{"paperId":null,"title":"Bootstrap solved after 1039 tries: def f61(li: List[int]): return all(sum(li[:i]) == 2 ** i -1 for i in range"},{"paperId":null,"title":"Board (IRB) approvals, if applicable? [Yes] See 5.1 and G for details about the 672 recruiting and agreement for the user study"},{"paperId":null,"title":"Hint\" return len(nums) == 3 and sum([i ** 3 for i in nums"},{"paperId":null,"title":"Bootstrap solved after 151 tries: def f26(s: str, target='foobarbazwow', length=6): return target[(len(target) -length) // 2:(len(target) + length) // 2] == s SOL: \"str"},{"paperId":null,"title":"627 (a) Do the main claims made in the abstract and introduction accurately reflect the paper's 628 contributions and scope? [Yes] as mentioned at the end of the introduction"},{"paperId":null,"title":"list(map(lambda i: 2**i, range"},{"paperId":null,"title":"and abs(sum(probs) -1) < 1e-6 return max(probs[(i + 2) % 3] -probs[(i + 1) % 3] for i in range(3)) < 1e-6 SOL"},{"paperId":null,"title":"639 (a) Did you state the full set of assumptions of all theoretical results? [Yes] 640 (b) Did you include complete proofs of all theoretical results?"}],"id":"f7664102a451332ed7e1286561b2f621eaff128d","summary":"A positive correlation between puzzlesolving performance and coding experience, and between the puzzle difficulty for humans and AI solvers are found, and further improvements on P3 could have a significant impact on many program synthesis areas."},{"url":"https://www.semanticscholar.org/paper/045a6f92b58f08d5b305dd5d661316a506ee4d43","title":"Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation","venue":"International Conference on Automated Software Engineering","year":2022,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Xin Wang,Xiao Liu,Pingyi Zhou,Qixia Liu,Jin Liu,Hao Wu,Xiao Cui","citations":[],"references":[{"paperId":"903689ffa22535e0c9daeccb794542d5a4d5f269","title":"Jointly learning invocations and descriptions for context-aware mashup tagging with graph attention network"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"0121c151c96b32cd7851e4bcda2a468b279c1e6f","title":"CODE-MVP: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training"},{"paperId":"2c71912f388811abd0a820b58b13dffde889f3bc","title":"Graph4Web: A relation-aware graph attention network for web service classification"},{"paperId":"2b556fcf2ac634f03c1fb0ace5e602e829418e65","title":"ReACC: A Retrieval-Augmented Code Completion Framework"},{"paperId":"7a1069dafaeb484e22f2473d5545f1e45ce30656","title":"Compilable Neural Code Generation with Compiler Feedback"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"f29599399def2cadb8e9a7fafec5b7bdf58382aa","title":"Exploiting gated graph neural network for detecting and explaining self-admitted technical debts"},{"paperId":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis"},{"paperId":"9426793c4eb8e582bae529e7b712b2959725482e","title":"Neural Program Repair with Execution-based Backpropagation"},{"paperId":"3563d6e51a39fc2e878a270ae4166510a45cc37a","title":"Keyword-Driven Service Recommendation via Deep Reinforced Steiner Tree Search"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"c56aced0f0c5cfebefadb530cb08d736c3ac5c05","title":"Retrieval Augmented Code Generation and Summarization"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"14b104d1a1c0b784e4e74454d809455cc47d0093","title":"SynCoBERT: Syntax-Guided Multi-Modal Contrastive Pre-Training for Code Representation"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"1311adebe439ccd9e373d594a8df1d720a685190","title":"Code Prediction by Feeding Trees to Transformers"},{"paperId":"6806b0d1e425639a42da4763f170569b86448c87","title":"A Neural Network Solves and Generates Mathematics Problems by Program Synthesis: Calculus, Differential Equations, Linear Algebra, and More"},{"paperId":"f44ae7cf4396d1d2837347d9aa705eb791902848","title":"ServiceBERT: A Pre-trained Model for Web Service Tagging and Recommendation"},{"paperId":"036fda02d93139ad0ef37d892d41796a2a39fdcd","title":"CLSEBERT: Contrastive Learning for Syntax Enhanced Code Pre-Trained Model"},{"paperId":"4f09c97321628688c7f3cbaa76ef132656a4b830","title":"Detecting and Explaining Self-Admitted Technical Debts with Attention-based Neural Networks"},{"paperId":"581151d7bab87fc3f0bf04c9f91599ce994ca8c7","title":"On the Generalizability of Neural Program Analyzers with respect to Semantic-Preserving Program Transformations"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"0071ee55f1749e6be586645bab08f0385ee856b4","title":"Graph-based, Self-Supervised Program Repair from Diagnostic Feedback"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"40df572b0fbeae0f3db9b364be838c6467d189f2","title":"A Self-Attentional Neural Architecture for Code Completion with Multi-Task Learning"},{"paperId":"ee0aa02ed9e424364b2b5c1403026ccff409807b","title":"A Spatial and Sequential Combined Method for Web Service Classification"},{"paperId":"0aeee6e779f43558446ea20d4b6e69ac15a0a137","title":"Web API Recommendation with Features Ensemble and Learning-to-Rank"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"045a6f92b58f08d5b305dd5d661316a506ee4d43","summary":"A method combining program analysis with deep learning for neural code generation, where functionally equivalent code snippets and test execution feedback will be considered at the training stage, and preliminary results on a newly published dataset demonstrate the effectiveness."},{"url":"https://www.semanticscholar.org/paper/1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization","venue":"ArXiv","year":2022,"referenceCount":27,"citationCount":2,"influentialCitationCount":0,"publicationDate":"11/08/2022","authors":"Shuvendu K. Lahiri,Aaditya Naik,Georgios Sakkas,Piali Choudhury,Curtis von Veh,M. Musuvathi,J. Inala,Chenglong Wang,Jianfeng Gao","citations":[{"paperId":"8210cef990b8e5cddbc95000e46309bdd25337f7","title":"Asking Clarification Questions for Code Generation in General-Purpose Programming Language"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"}],"references":[{"paperId":"7d180ff2a86d259794aa85466d87c215d86b5c92","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis"},{"paperId":"8c40f0259aac79db7203ffdb452fffe006cd7e18","title":"TOGA: A Neural Method for Test Oracle Generation"},{"paperId":"ad18af95ba8125d2c0eb9f9941205678cad38ad2","title":"Generating Accurate Assert Statements for Unit Test Cases using Pretrained Transformers"},{"paperId":null,"title":"Fiedel. Palm: Scaling language modeling with pathways, 2022"},{"paperId":"a8863de15a5ee8eed98107f423138a1a8f5a2ba8","title":"Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"ac3cdb50606f7770eef8e4cd951840a4f71287a0","title":"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"},{"paperId":"15a60130ea9cb4a00e0dac6d11a368894974cedf","title":"Unit Test Case Generation with Transformers and Focal Context"},{"paperId":"eff168312f519f6cc4cb867147a105c75abef735","title":"Question selection for interactive program synthesis"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":null,"title":"Abhishek Udupa, and Sumit Gulwani"},{"paperId":null,"title":"Program synthesis. Found"},{"paperId":null,"title":"Interactive program synthesis, 2017"},{"paperId":"363c9c645dc8c303c3d7ad995f60beae32ce10fa","title":"An empirical analysis of flaky tests"},{"paperId":"698fe1da46e077cb3a0050ab986bcfdd82af114a","title":"Evolutionary Generation of Whole Test Suites"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"2d38fad1c1d1b9cdd0ca8e0f3061b32fce12e987","title":"The Sketching Approach to Program Synthesis"},{"paperId":"6dc7017e357767dbc210bcb27080260fb0816701","title":"Feedback-Directed Random Test Generation"},{"paperId":null,"title":"Gomez , Łukasz Kaiser , and Illia Polosukhin . Attention is all you need"}],"id":"1c336c18e53ad878bf4688c864acd99f137ae29f","summary":"This paper proposes the workflow of test-driven user-intent formalization (TDUIF), which leverages lightweight user feedback to jointly formalize the user intent as tests (a partial specification), and generates code that meets the formal user intent."},{"url":"https://www.semanticscholar.org/paper/5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","title":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/10/2022","authors":"Hussein Mozannar,Gagan Bansal,Adam Fourney,E. Horvitz","citations":[],"references":[{"paperId":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?"},{"paperId":"654cd297b307c0c6fa08732511ba852b8dce1977","title":"Out of the BLEU: how should we assess quality of the Code Generation models?"},{"paperId":"114eb5a35a3cd802cd1f46fff35c284e32ef6c54","title":"GitHub Copilot AI pair programmer: Asset or Liability?"},{"paperId":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"4bc040835fbff57ce6612306d794b8c6c8226086","title":"Discovering the Syntax and Strategies of Natural Language Programming with Generative Language Models"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"c18624382afc57072446b056bf590ee891c078e7","title":"Is GitHub's Copilot as Bad As Humans at Introducing Vulnerabilities in Code?"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"},{"paperId":null,"title":"Ml-enhanced code completion improves developer productivity, Jul 2022"},{"paperId":null,"title":"Ml-powered coding companion -amazon codewhisperer"},{"paperId":null,"title":"A Details User Study A.1 Interfaces Figure 13: Screenshot of Labeling Tool represented in Figure 4 Figure 14: Screenshot of Virtual Machine interface with VS Code"},{"paperId":null,"title":"Task Instructions The tasks are shown to participants as image files to deter copying of the instructions as a prompt"},{"paperId":null,"title":"Probability of Accept by State Table 4: Probability of accepting suggestion in next two events given the user was in the particular CUPS"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"2a36402451c4fcab33c9e7f59fc027167c070aa6","title":"Perfection Not Required? Human-AI Partnerships in Code Translation"},{"paperId":"6c107f38e218d967f5ad0618dfce2e9edf796063","title":"The SPACE of Developer Productivity"},{"paperId":"5ff9032d0f7f246d01ae7b2c231ab03469a7344a","title":"Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?"},{"paperId":"28797c288403828f29d3faca6ab9d64d134a4ea3","title":"The SPACE of Developer Productivity: There's more to it than you think"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"d02285385fd2b53a6100b8d62421bba6693b788c","title":"When Code Completion Fails: A Case Study on Real-World Completions"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"8d115c3b2ee80e0754360a154a9369bc1658b607","title":"Obtaining Well Calibrated Probabilities Using Bayesian Binning"},{"paperId":null,"title":"Xgboost: extreme gradient boosting"},{"paperId":"2244595cc05a687ff04390f63a4443e1cd0cb3b0","title":"Flow and the Foundations of Positive Psychology"},{"paperId":"b722582997d8ed7a49d0582a2140acbce4464683","title":"The GOMS family of user interface analysis techniques: comparison and contrast"},{"paperId":"9fa713bc9971717aeed966676d8018ee73687bec","title":"The Entropy Of Markov Trajectories"},{"paperId":"82f8fbc66004ac438ac742c1ad6016d07d1ae037","title":"The keystroke-level model for user performance time with interactive systems"},{"paperId":null,"title":"Research: Quantifying github copilot's impact on developer productivity and happiness"}],"id":"5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","summary":"This work studied GitHub Copilot, developed CUPS– a taxonomy of 12 programmer activities common to AI code completion systems, and conducted a study with 21 programmers who completed coding tasks and used the labeling tool to retrospectively label their sessions with CUPS."},{"url":"https://www.semanticscholar.org/paper/ef29fb8cc6bdda4b011288f51da521d3c25fc53d","title":"Learning to Prevent Profitless Neural Code Completion","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/09/2022","authors":"Zhensu Sun,Xiaoning Du,Fu Song,Shangwen Wang,Mingze Ni,Li Li","citations":[],"references":[{"paperId":"654cd297b307c0c6fa08732511ba852b8dce1977","title":"Out of the BLEU: how should we assess quality of the Code Generation models?"},{"paperId":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"205ac1373eb7981aca2d08f2ab651871a001271e","title":"CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"97010ef4c4bd80b1f959df236501cf7741053d04","title":"On the Importance of Building High-quality Training Datasets for Neural Code Search"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"02183e69f1dfd6e9b2d0fb876153299bab4bb82b","title":"CoProtector: Protect Open-Source Code against Unauthorized Training Usage with Data Poisoning"},{"paperId":null,"title":"Code faster with ai completions — tabnine"},{"paperId":null,"title":"Turing-nlg: A 17-billion-parameter language model by microsoft - microsoft research"},{"paperId":null,"title":"aixcoder"},{"paperId":null,"title":"Amazon ec2 update – inf1 instances with aws inferentia chips for high performance cost-effective inferencing"},{"paperId":null,"title":"Ml-powered coding companion – amazon codewhisperer – amazon web services"},{"paperId":null,"title":"Code conventions for the java programming language: 9. naming conventions"},{"paperId":null,"title":"Github copilot · your ai pair programmer."},{"paperId":"01ce9f7d1c35e88f6ab3abe51bf1e1370da718b5","title":"Compute and Energy Consumption Trends in Deep Learning Inference"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"8e136c0944c9181b4b853df4f642d8bb7afddecb","title":"Reassessing automatic evaluation metrics for code summarization tasks"},{"paperId":"a3223d46f3091307525e32b5a059a4bc4a5980a8","title":"Efficient Neural Architecture Search with Performance Prediction"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"79b8ef3905a42b771248719495a2117271906445","title":"Carbon Emissions and Large Neural Network Training"},{"paperId":"774591fdd988eaaff3917e7c5171d044b0843e63","title":"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"},{"paperId":"eb90f7a7f281ccbf64084e11adf822fae2d9bc3f","title":"Sustainable AI: AI for sustainability and the sustainability of AI"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"1ec69f1a1a9d4ff5c5bc70db0e5087157b620570","title":"You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion"},{"paperId":"1728cb805a9573b59330890ba9723e73d6c3c974","title":"Knowledge Distillation: A Survey"},{"paperId":"c9c944146322321d2565a735b6fd10676e6bd69b","title":"Meta-learning Hyperparameter Performance Prediction with Neural Processes"},{"paperId":"da5d78b3e3a1544fde98fba86088e1215e97cbe8","title":"All NLP Tasks Are Generation Tasks: A General Pretraining Framework"},{"paperId":null,"title":"Ai and memory wall"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"deedb9b61a01d686b28e6034770fccc142e77fab","title":"Predicting Performance for Natural Language Processing Tasks"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"eb606d9ce65139754232cee62f6ab77f3e0c665f","title":"Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"8323c591e119eb09b28b29fd6c7bc76bd889df7a","title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"},{"paperId":"79b7a61abd4a4b4cf62c26cec815b55b0778f48d","title":"Pythia: AI-assisted Code Completion System"},{"paperId":"d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","title":"Energy and Policy Considerations for Deep Learning in NLP"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"Aws to offer nvidiaś t4 gpus for ai inferencing"},{"paperId":"dd908c09eaf8f3a3c20d64fff798ec902433737d","title":"Code Completion with Neural Attention and Pointer Networks"},{"paperId":"9d34951c328c02c062d829f7e3c2ebf657b9d031","title":"Determining Sample Size for Research Activities"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"442e10a3c6640ded9408622005e3c2a8906ce4c2","title":"A Unified Approach to Interpreting Model Predictions"},{"paperId":"230579a14d54ae00073d6c3522ffcef313320be9","title":"Compression of Neural Machine Translation Models via Pruning"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"443516aeb2819d4d362ffe7d5418a54e5427a016","title":"ORANGE: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"2153bd13bf4b094bd4be8e5e764a344930d5209f","title":"Source Code"},{"paperId":"1e41ed1ac234cba0138329047e16a8a424389e77","title":"A Complexity Measure"}],"id":"ef29fb8cc6bdda4b011288f51da521d3c25fc53d","summary":"An early-rejection mechanism to turn down low-return prompts by foretelling the completion qualities without sending them to the LCM is proposed and a lightweight Transformer-based estimator is proposed to demonstrate the feasibility of the mechanism."}]}