{"papers":[{"url":"https://www.semanticscholar.org/paper/b2c4fdb49bdb23e6ad0ac3272029324157046ea7","title":"Automated Repair of Code from Language Models","venue":"","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Zhiyu Fan,Xiang Gao,M. Mirchev,Abhik Roychoudhury,Shin Hwei Tan","id":"b2c4fdb49bdb23e6ad0ac3272029324157046ea7","summary":"The study revealed that automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to auto-generated code, and given bug location information provided by a statistical fault localization approach, Codex edit mode is similar to or better than existing Java repair tools TBar and Recoder in correcting incorrect solutions.","score":7},{"url":"https://www.semanticscholar.org/paper/0bcd59da541fdae66884afba8d25475a54a9da1a","title":"Automated Repair of Programs from Large Language Models","venue":"","year":2022,"referenceCount":49,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/05/2022","authors":"Zhiyu Fan,Xiang Gao,M. Mirchev,Abhik Roychoudhury,Shin Hwei Tan","id":"0bcd59da541fdae66884afba8d25475a54a9da1a","summary":"The study revealed that automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to auto-generated code, and given bug location information provided by a statistical fault localization approach, Codex edit mode is similar to or better than existing Java repair tools TBar and Recoder in correcting incorrect solutions.","score":7},{"url":"https://www.semanticscholar.org/paper/e73a16490c29530c37b49f6a30592790e7caaaa4","title":"Don't Complete It! Preventing Unhelpful Code Completion for Productive and Sustainable Neural Code Completion Systems","venue":"","year":2022,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/09/2022","authors":"Zhensu Sun,Xiaoning Du,Fu Song,Shangwen Wang,Mingze Ni,Li Li","id":"e73a16490c29530c37b49f6a30592790e7caaaa4","summary":"An early-rejection mechanism to turn down low-return prompts by foretelling the code completion qualities without sending them to the codepletion system is proposed and a lightweight Transformer-based estimator is proposed to demonstrate the feasibility of the mechanism.","score":6},{"url":"https://www.semanticscholar.org/paper/6074c7b75f27ca9adb6d74b080c07d5d079c3ea0","title":"Improving automatically generated code from Codex via Automated Program Repair","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":4,"influentialCitationCount":0,"publicationDate":2022,"authors":"Zhiyu Fan,Xiang Gao,Abhik Roychoudhury,Shin Hwei Tan","id":"6074c7b75f27ca9adb6d74b080c07d5d079c3ea0","summary":"This study systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests, revealing that automatically generated codes share some common programming mistakes with human-crafted solutions, indicating existing APR tools have the potential to fix auto-generated code.","score":6},{"url":"https://www.semanticscholar.org/paper/1b4c19168410fb2690d285b205ab2281793db81a","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages","venue":"ArXiv","year":2022,"referenceCount":27,"citationCount":4,"influentialCitationCount":1,"publicationDate":2022,"authors":"Federico Cassano,John Gouwar,Daniel Nguyen,S. Nguyen,Luna Phipps-Costin,Donald Pinckney,Ming-Ho Yee,Yangtian Zi,Carolyn Jane Anderson,Molly Q. Feldman,Arjun Guha,M. Greenberg,Abhinav Jangda","id":"1b4c19168410fb2690d285b205ab2281793db81a","summary":"It is shown that on several languages, Codex matches and even exceeds its performance on Python, and a general approach is described for easily adding support for new benchmarks and languages to MultiPL-E, the first multi-language parallel benchmark for natural-language-to-code-generation.","score":6},{"url":"https://www.semanticscholar.org/paper/fba0b0817dbc8200b41a1de22654b54b778a11e9","title":"Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models","venue":"ArXiv","year":2023,"referenceCount":64,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/01/2023","authors":"Toufique Ahmed,Supriyo Ghosh,Chetan Bansal,T. Zimmermann,Xuchao Zhang,S. Rajmohan","id":"fba0b0817dbc8200b41a1de22654b54b778a11e9","summary":"This work does the first large-scale study to evaluate the effectiveness of GPT-3.x models for helping engineers root cause and mitigate production incidents and compares several large language models in zero-shot, fine-tuned and multi-task setting using semantic and lexical metrics.","score":6},{"url":"https://www.semanticscholar.org/paper/68edfd62d2619fb4af7c2469edb95b9e2fe4544a","title":"Execution-based Code Generation using Deep Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Parshin Shojaee,Aneesh Jain,S. Tipirneni,C. Reddy","id":"68edfd62d2619fb4af7c2469edb95b9e2fe4544a","summary":"PPOCoder is a new framework for code generation that combines pretrained PL models with Proximal Policy Optimization (PPO) deep reinforcement learning and employs execution feedback as the external source of knowledge into the model optimization, which is transferable across different code generation tasks and PLs.","score":6},{"url":"https://www.semanticscholar.org/paper/3d5463a16d85d9d1d0d8ebb4117a31aca3c240ff","title":"Fooling MOSS Detection with Pretrained Language Models","venue":"International Conference on Information and Knowledge Management","year":2022,"referenceCount":70,"citationCount":2,"influentialCitationCount":0,"publicationDate":"19/01/2022","authors":"Stella Rose Biderman,Edward Raff","id":"3d5463a16d85d9d1d0d8ebb4117a31aca3c240ff","summary":"It is found that a student using GPT-J can complete introductory level programming assignments without triggering suspicion from MOSS, a widely used software similarity and plagiarism detection tool.","score":5},{"url":"https://www.semanticscholar.org/paper/6032212d5790b6a580d68d469a9895aad6238c89","title":"Diverse Title Generation for Stack Overflow Posts with Multiple Sampling Enhanced Transformer","venue":"ArXiv","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/08/2022","authors":"Fengji Zhang,Jin Liu,Yao Wan,Xiao Yu,Xiao Liu,J. Keung","id":"6032212d5790b6a580d68d469a9895aad6238c89","summary":"A novel approach to automatically generate multiple post titles from the given code snippets, using the maximal marginal multiple nucleus sampling strategy to generate multiple high-quality and diverse title candidates at a time for the developers to choose from.","score":5},{"url":"https://www.semanticscholar.org/paper/ebd4bec684808aff360b5f255d15c0d112ba13d3","title":"Explicit Knowledge Transfer for Weakly-Supervised Code Generation","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/11/2022","authors":"Zhangir Azerbayev,Ansong Ni,Hailey Schoelkopf,Dragomir R. Radev","id":"ebd4bec684808aff360b5f255d15c0d112ba13d3","summary":"This paper proposes explicit knowledge transfer (EKT), which uses the few-shot capabilities of a teacher LLM to create NL-code pairs that are filter for correctness and fine-tune the student on, and finds that EKT not only yields better performance than training with expert iteration, but also outperforms knowledge distillation, another form of knowledge transfer.","score":5},{"url":"https://www.semanticscholar.org/paper/d3a7a4543d83f568f79d1febe8379465ff0140c9","title":"A Survey of Deep Learning for Mathematical Reasoning","venue":"ArXiv","year":2022,"referenceCount":216,"citationCount":1,"influentialCitationCount":1,"publicationDate":"20/12/2022","authors":"Pan Lu,Liang Qiu,Wenhao Yu,S. Welleck,Kai-Wei Chang","id":"d3a7a4543d83f568f79d1febe8379465ff0140c9","summary":"This survey paper reviews the key tasks, datasets, and methods at the intersec-tion of mathematical reasoning and deep learning over the past decade, and evaluates existing benchmarks and methods and discusses future research directions.","score":5},{"url":"https://www.semanticscholar.org/paper/d8405996b4d08c304098636aedd9e1c1a1e262ee","title":"The Premature Obituary of Programming","venue":"Communications of the ACM","year":2023,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/01/2023","authors":"D. Yellin","id":"d8405996b4d08c304098636aedd9e1c1a1e262ee","summary":"Why deep learning will not replace programming and why deep learning should not be considered as a programming language.","score":5},{"url":"https://www.semanticscholar.org/paper/1c8e15f15d67c5974445634bb971e2275e957aff","title":"Security Implications of Large Language Model Code Assistants: A User Study","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":4,"influentialCitationCount":0,"publicationDate":"20/08/2022","authors":"Gustavo Sandoval,H. Pearce,Teo Nys,R. Karri,Brendan Dolan-Gavitt,S. Garg","id":"1c8e15f15d67c5974445634bb971e2275e957aff","summary":"A security-driven user study to assess code written by student programmers when assisted by LLMs and indicates that the security impact in this setting is small: AI-assisted users produce critical security bugs at a rate no greater than 10% more than the control.","score":4},{"url":"https://www.semanticscholar.org/paper/a7435722d8ab595da5a9c70ac9160f57d0dcd75a","title":"Enabling Transformers to Understand Low-Level Programs","venue":"IEEE Conference on High Performance Extreme Computing","year":2022,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/09/2022","authors":"Zifan Carl Guo,William S. Moses","id":"a7435722d8ab595da5a9c70ac9160f57d0dcd75a","summary":"This work applies transfer learning to low-level (LLVM) programs and study how low- level programs can be made more amenable to Transformer models through various techniques, including preprocessing, infix/prefix operators, and information deduplication.","score":4},{"url":"https://www.semanticscholar.org/paper/971e875e28f26240987d2c9470d1ee74ad204205","title":"Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/09/2022","authors":"Sungmin Kang,Juyeon Yoon,Shin Yoo","id":"971e875e28f26240987d2c9470d1ee74ad204205","summary":"Results show L IBRO has the potential to enhance developerency by automatically generating tests from bug reports, and is proposed as a framework that uses Large Language Models (LLMs), which have been shown to be capable of performing code-related tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/825333b7efe2cade106eaf36c7e731f757974806","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot","venue":"37th IEEE/ACM International Conference on Automated Software Engineering","year":2022,"referenceCount":28,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Naser Al Madi","id":"825333b7efe2cade106eaf36c7e731f757974806","summary":"The results suggest that model generated code is comparable in complexity and readability to code written by human pair programmers, and eye tracking data suggests, to a statistically significant level, that programmers direct less visual attention to model generate code.","score":4},{"url":"https://www.semanticscholar.org/paper/39e40821b7207125e54e6ed7112e55cd38c6f0c3","title":"Language Models of Code are Few-Shot Commonsense Learners","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":38,"citationCount":11,"influentialCitationCount":1,"publicationDate":"13/10/2022","authors":"Aman Madaan,Shuyan Zhou,Uri Alon,Yiming Yang,Graham Neubig","id":"39e40821b7207125e54e6ed7112e55cd38c6f0c3","summary":"This paper shows that when this task is frame as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than L Ms of natural language, even when the downstream task does not involve source code at all.","score":4},{"url":"https://www.semanticscholar.org/paper/a6ef5bad716091fb1888bf365f6129628ab3a5ee","title":"ObSynth: An Interactive Synthesis System for Generating Object Models from Natural Language Specifications","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/10/2022","authors":"Alex Gu,Tamara Mitrovska,D. Vélez,Jacob Andreas,Armando Solar-Lezama","id":"a6ef5bad716091fb1888bf365f6129628ab3a5ee","summary":"ObSynth is introduced, an interactive system leveraging the domain knowledge em-bedded in large language models (LLMs) to help users design object models from high level natural language prompts, showing that it often synthesizes objects, methods, and methods users might have otherwise omitted.","score":4},{"url":"https://www.semanticscholar.org/paper/0566c1c3eeeef5c968fced6d80b77fe22d02bbd9","title":"Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language","venue":"ArXiv","year":2022,"referenceCount":24,"citationCount":5,"influentialCitationCount":0,"publicationDate":"27/10/2022","authors":"Paul Denny,Viraj Kumar,Nasser Giacaman","id":"0566c1c3eeeef5c968fced6d80b77fe22d02bbd9","summary":"Evaluating the performance of Copilot on a publicly available dataset of 166 programming problems finds that it successfully solves around half of these problems on its very first attempt, and that it solves 60% of the remaining problems using only natural language changes to the problem description.","score":4},{"url":"https://www.semanticscholar.org/paper/ec7324a15009a9bd0b676f6b17762f759cf5dd9a","title":"Large Language Models Are Human-Level Prompt Engineers","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":12,"influentialCitationCount":2,"publicationDate":"03/11/2022","authors":"Yongchao Zhou,Andrei Ioan Muresanu,Ziwen Han,Keiran Paster,Silviu Pitis,Harris Chan,Jimmy Ba","id":"ec7324a15009a9bd0b676f6b17762f759cf5dd9a","summary":"It is shown that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts.","score":4},{"url":"https://www.semanticscholar.org/paper/71280dba5bda65c162f9deaffed7d3d20692ca0a","title":"SecurityEval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques","venue":"","year":2022,"referenceCount":28,"citationCount":3,"influentialCitationCount":1,"publicationDate":"07/11/2022","authors":"Mohammed Latif Siddiq,msiddiq","id":"71280dba5bda65c162f9deaffed7d3d20692ca0a","summary":"SecurityEval, an evaluation dataset that contains 130 samples for 75 vulnerability types, which are mapped to the Common Weakness Enumeration (CWE) and demonstrated using one open-source and one closed-source code generation model to evaluate.","score":4},{"url":"https://www.semanticscholar.org/paper/c192fb33f308f19ac8a5c4c2d623d56385b839be","title":"Systematic Literature Review on Solving Competitive Programming Problem with Artificial Intelligence (AI)","venue":"2022 1st International Conference on Software Engineering and Information Technology (ICoSEIT)","year":2022,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/11/2022","authors":"Francis Alexander,Edwin Ario Abdiwijaya,Felix Pherry,A. A. Gunawan,Anderies","id":"c192fb33f308f19ac8a5c4c2d623d56385b839be","summary":"It can be concluded that the code auto-completion and code-generation tools that are available now still do not meet the necessary benchmark which is solving CP tasks, and AI still has a long way to go before competing at the highest level of CP.","score":4},{"url":"https://www.semanticscholar.org/paper/8175ce2cbb99b6a394bdac152ae39d413f4f1380","title":"Codex Hacks HackerRank: Memorization Issues and a Framework for Code Synthesis Evaluation","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"publicationDate":"06/12/2022","authors":"Anjan Karmakar,Julian Aron Prenner,Marco D'Ambros,R. Robbes","id":"8175ce2cbb99b6a394bdac152ae39d413f4f1380","summary":"This work evaluates the code synthesis capabilities of the Codex model based on a set of 115 Python problem statements from a popular competitive programming portal: HackerRank, and proposes a framework for code-synthesis evaluation using variations of problem statements based on mutations.","score":4},{"url":"https://www.semanticscholar.org/paper/9e8f0d9ba4e7af673c8b214b3764e020706dd1f3","title":"Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments","venue":"ArXiv","year":2022,"referenceCount":66,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Yu Gu,Xiang Deng,Yu Su","id":"9e8f0d9ba4e7af673c8b214b3764e020706dd1f3","summary":"Pangu is proposed, a generic framework for grounded language understanding that capitalizes on the discriminative ability of LMs instead of their generative ability, and enables, for the first time, effective few-shot in-context learning for KBQA with large LMs such as Codex.","score":4},{"url":"https://www.semanticscholar.org/paper/12a4e62c43b829dabdb8afc60eee76aa80fa3f6e","title":"Fuzzing Deep-Learning Libraries via Large Language Models","venue":"ArXiv","year":2022,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/12/2022","authors":"Yinlin Deng,Chun Xia,Haoran Peng,Chenyuan Yang,Lingming Zhang","id":"12a4e62c43b829dabdb8afc60eee76aa80fa3f6e","summary":"LLMFuzz is the first automated approach to directly leveraging Large Pre-trained Language Models (LLMs) to generate input programs for fuzzing DL libraries, and is able to detect 65 bugs, with 41 already confirmed as previously unknown bugs.","score":4},{"url":"https://www.semanticscholar.org/paper/c72ac81c4ac414314b52cf8f5b77370f8d0875d4","title":"Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models","venue":"ArXiv","year":2023,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/01/2023","authors":"Tung Phung,J. Cambronero,Sumit Gulwani,Tobias Kohn,R. Majumdar,A. Singla,Gustavo Soares","id":"c72ac81c4ac414314b52cf8f5b77370f8d0875d4","summary":"The key idea behind PyFiXV is to use a novel run-time validation mechanism to decide whether the generated feedback is suitable for sharing with the student; notably, this validation mechanism also provides a precision knob to educators.","score":4},{"url":"https://www.semanticscholar.org/paper/0885556b71b24a641b4ffe139afd4d2712228cff","title":"Beware of the Unexpected: Bimodal Taint Analysis","venue":"ArXiv","year":2023,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/01/2023","authors":"Yiu Wai Chow,Max Schäfer,Michael Pradel","id":"0885556b71b24a641b4ffe139afd4d2712228cff","summary":"Fluffy is presented, a bimodal taint analysis that combines static analysis, which reasons about data flow, with machine learning, which probabilistically determines which flows are potentially problematic.","score":4},{"url":"https://www.semanticscholar.org/paper/e9a64e58855dbbc725203d0202ceb9e7f8b7bb36","title":"A Survey of Learning-based Automated Program Repair","venue":"ArXiv","year":2023,"referenceCount":185,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/01/2023","authors":"Quanjun Zhang,Chunrong Fang,Yuxiang Ma,Weisong Sun,Zhenyu Chen","id":"e9a64e58855dbbc725203d0202ceb9e7f8b7bb36","summary":"This work presents a meta-modelling system that automates the very labor-intensive and therefore time-heavy and therefore expensive and expensive process of manually cataloging and cataloging individual neurons in the brain.","score":3},{"url":"https://www.semanticscholar.org/paper/b1eebb2df3b9ff7ff2b00fb1a786f6ada2caebce","title":"Towards a Mathematics Formalisation Assistant using Large Language Models","venue":"ArXiv","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"Ayush Agrawal,Siddhartha Gadgil,Navin Goyal,Ashvni Narayanan,Anand Tadipatri","id":"b1eebb2df3b9ff7ff2b00fb1a786f6ada2caebce","summary":"The abilities of a large language model (Codex) to help with formalisation in the Lean theorem prover are explored, finding that with careful inputdependent prompt selection and postprocessing, Codex is able to formalise short mathematical statements at undergrad level with nearly 75% accuracy for 120 theorem statements.","score":3},{"url":"https://www.semanticscholar.org/paper/4ddc26b3a5fe9044b97b408d163f7464d769ebbf","title":"CODEP: Grammatical Seq2Seq Model for General-Purpose Code Generation","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Yihong Dong,Ge Li,Zhi Jin","id":"4ddc26b3a5fe9044b97b408d163f7464d769ebbf","summary":"This paper proposes CODEP, a grammatical Seq2Seq code generation framework equipped with a Pushdown automaton (PDA) module, and constructs the DPA for the most popular GPL Python and conducts extensive experiments to evaluate the effectiveness.","score":3},{"url":"https://www.semanticscholar.org/paper/7497360b0f411a44aa6afbd8b830050c40ec8aed","title":"Dataset of Student Solutions to Algorithm and Data Structure Programming Assignments","venue":"International Conference on Language Resources and Evaluation","year":2022,"referenceCount":18,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Fynn Petersen-Frey,Marcus Soll,Louis Kobras,Melf Johannsen,Peter Kling,Chris Biemann","id":"7497360b0f411a44aa6afbd8b830050c40ec8aed","summary":"This paper presents a dataset containing source code solutions to algorithmic programming exercises solved by hundreds of Bachelor-level students at the University of Hamburg, and plans to extend the dataset with tasks and solutions from upcoming courses.","score":3},{"url":"https://www.semanticscholar.org/paper/4f278ab5ad629267e06196e273252262854c1c57","title":"BF++: a language for general-purpose program synthesis","venue":"","year":2021,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/01/2021","authors":"Vadim Liventsev,A. Harma,M. Petkovi'c","id":"4f278ab5ad629267e06196e273252262854c1c57","summary":"A new programming language, BF ++ is proposed, designed speciﬁcally for automatic programming of agents in a Partially Observable Markov Decision Process (POMDP) setting and apply neural program synthesis to solve standard OpenAI Gym benchmarks.","score":3},{"url":"https://www.semanticscholar.org/paper/92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models","venue":"ArXiv","year":2021,"referenceCount":30,"citationCount":112,"influentialCitationCount":11,"publicationDate":"30/11/2021","authors":"Maxwell Nye,Anders Andreassen,Guy Gur-Ari,H. Michalewski,Jacob Austin,David Bieber,David Dohan,Aitor Lewkowycz,Maarten Bosma,D. Luan,Charles Sutton,Augustus Odena","id":"92173d081b15824d22a9ef070e118744ceee8052","summary":"Surprisingly, large pre-trained language models are able to perform complex multistep computations—even in the few-shot regime—when asked to perform the operation “step by step”, showing the results of intermediate computations.","score":3},{"url":"https://www.semanticscholar.org/paper/a5731122200fbb8b37f048010a1e1ca4474aa606","title":"Examining Zero-Shot Vulnerability Repair with Large Language Models","venue":"","year":2021,"referenceCount":56,"citationCount":7,"influentialCitationCount":0,"publicationDate":"03/12/2021","authors":"H. Pearce,B. Tan,Baleegh Ahmad,R. Karri,Brendan Dolan-Gavitt","id":"a5731122200fbb8b37f048010a1e1ca4474aa606","summary":"This work examines the use of large language models for code (such as OpenAI’s Codex and AI21's Jurassic J-1) for zero-shot vulnerability repair, and investigates challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code.","score":3},{"url":"https://www.semanticscholar.org/paper/5ff9032d0f7f246d01ae7b2c231ab03469a7344a","title":"Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?","venue":"ArXiv","year":2021,"referenceCount":56,"citationCount":20,"influentialCitationCount":1,"publicationDate":2021,"authors":"H. Pearce,B. Tan,Baleegh Ahmad,R. Karri,Brendan Dolan-Gavitt","id":"5ff9032d0f7f246d01ae7b2c231ab03469a7344a","summary":"This work examines the use of large language models for code (such as OpenAI's Codex and AI21's Jurassic J-1) for zero-shot vulnerability repair, and investigates challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code.","score":3},{"url":"https://www.semanticscholar.org/paper/9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"C OMPOSITIONAL G ENERALIZATION AND D ECOMPOSITION IN N EURAL P ROGRAM S YNTHESIS","venue":"","year":null,"referenceCount":0,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"","id":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","summary":"A suite of generalization tasks, which measure different types of compositional generalization that are desirable for program synthesis and are particularly difﬁcult for current sequence to sequence models, are proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/a2b6e1f7d8a7963d321f29fca7c01eeb1ebd7f0f","title":"P ATCH G ENERATION WITH L ANGUAGE M ODELS : F EASIBILITY AND S CALING B EHAVIOR","venue":"","year":2022,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Sophia Kolak,Ruben Martins,Claire Le Goues,V. Hellendoorn","id":"a2b6e1f7d8a7963d321f29fca7c01eeb1ebd7f0f","summary":"This work highlights a noticeable correlation of model size with test-passing accuracy and patch ranking quality, and the propensity for especially the largest models to generate candidate patches that closely resemble (if not exactly match), the original developer patch.","score":3},{"url":"https://www.semanticscholar.org/paper/78863000eb70945cc8d791d45d4a3fe8a6521cb6","title":"Open-ended Knowledge Tracing for Computer Science Education","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Naiming Liu,Zichao Wang","id":"78863000eb70945cc8d791d45d4a3fe8a6521cb6","summary":"An initial solution to the OKT problem is developed, a student knowledge-guided code generation approach that combines program synthesis methods using language models with student knowledge tracing methods and a series of quantitative and qualitative experiments on a real-world student code dataset to validate and demonstrate the promise of OKT.","score":3},{"url":"https://www.semanticscholar.org/paper/56e6d62c638a24411f12d15cdc8821a31fc495c8","title":"Source Code Generation from Descriptions in a Natural Language","venue":"","year":2022,"referenceCount":70,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Bc. Jan Pašek","id":"56e6d62c638a24411f12d15cdc8821a31fc495c8","summary":"This work introduces CodeFormer, a Python source code generator pretrained on a massive GitHub crawl consisting of 230M Python functions, and releases the resulting model, built on BART architecture, which generates Python functions based on descriptions in English.","score":3},{"url":"https://www.semanticscholar.org/paper/1aed58bd07026492194672adec494dc37c894a28","title":"Leveraging Automated Unit Tests for Unsupervised Code Translation","venue":"International Conference on Learning Representations","year":2021,"referenceCount":60,"citationCount":19,"influentialCitationCount":2,"publicationDate":"13/10/2021","authors":"Baptiste Rozière,J Zhang,François Charton,M. Harman,Gabriel Synnaeve,Guillaume Lample","id":"1aed58bd07026492194672adec494dc37c894a28","summary":"This work proposes to leverage an automated unit-testing system to filter out invalid translations, thereby creating a fully tested parallel corpus, and finds that fine-tuning an unsupervised model with this filtered data set significantly reduces the noise in the translations so-generated, comfortably outperforming the state of the art for all language pairs studied.","score":3},{"url":"https://www.semanticscholar.org/paper/52db8674337e5d86dcb96d013734befc8c3d4581","title":"Large Language Models are not Models of Natural Language: they are Corpus Models.","venue":"IEEE Access","year":2021,"referenceCount":63,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/12/2021","authors":"C. Veres","id":"52db8674337e5d86dcb96d013734befc8c3d4581","summary":"It is argued that the term language model is misleading because deep learning models are not theoretical models of language and proposed the adoption of corpus model instead, which better reflects the genesis and contents of the model.","score":3},{"url":"https://www.semanticscholar.org/paper/7b5aa186ca8abc585607c5ec91562e127a398601","title":"Open-Ended Knowledge Tracing","venue":"ArXiv","year":2022,"referenceCount":64,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/02/2022","authors":"Naiming Liu,Zichao Wang,Richard Baraniuk,Andrew S. Lan","id":"7b5aa186ca8abc585607c5ec91562e127a398601","summary":"This paper develops an initial solution to the OKT problem, a student knowledge-guided code generation approach that combines program synthesis methods using language models with student knowledge tracing methods and conducts a series of quantitative and qualitative experiments on a real-world student code dataset to validate OKT and demonstrate its promise in educational applications.","score":3},{"url":"https://www.semanticscholar.org/paper/1f11601f9eb8c0bee1d1d2cc7f4fa187e8c5e525","title":"The impact of lexical and grammatical processing on generating code from natural language","venue":"Findings","year":2022,"referenceCount":19,"citationCount":4,"influentialCitationCount":2,"publicationDate":"28/02/2022","authors":"Nathanael Beau,Benoit Crabb'e","id":"1f11601f9eb8c0bee1d1d2cc7f4fa187e8c5e525","summary":"The paper highlights the importance of the lexical substitution component in the current natural language to code systems with a state-of-the-art architecture that relies on BERT encoder and a grammar-based decoder for which a formalization is provided.","score":3},{"url":"https://www.semanticscholar.org/paper/771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":59,"influentialCitationCount":22,"publicationDate":2022,"authors":"Erik Nijkamp,Bo Pang,Hiroaki Hayashi,Lifu Tu,Haiquan Wang,Yingbo Zhou,S. Savarese,Caiming Xiong","id":"771371fb288da26a9812f5808535847a0a9c9a80","summary":"This work proposes and trains C ODE G EN, an interactive code generation model for program synthesis, and suggests that the capacity of conversational program synthesis scales as a function of the model size and data size.","score":3},{"url":"https://www.semanticscholar.org/paper/094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways","venue":"ArXiv","year":2022,"referenceCount":173,"citationCount":540,"influentialCitationCount":78,"publicationDate":"05/04/2022","authors":"Aakanksha Chowdhery,Sharan Narang,Jacob Devlin,Maarten Bosma,Gaurav Mishra,Adam Roberts,P. Barham,Hyung Won Chung,Charles Sutton,Sebastian Gehrmann,Parker Schuh,Kensen Shi,Sasha Tsvyashchenko,Joshua Maynez,Abhishek Rao,Parker Barnes,Yi Tay,Noam M. Shazeer,Vinodkumar Prabhakaran,Emily Reif,Nan Du,B. Hutchinson,Reiner Pope,James Bradbury,Jacob Austin,M. Isard,Guy Gur-Ari,Pengcheng Yin,Toju Duke,Anselm Levskaya,S. Ghemawat,Sunipa Dev,H. Michalewski,Xavier García,Vedant Misra,Kevin Robinson,L. Fedus,Denny Zhou,Daphne Ippolito,D. Luan,Hyeontaek Lim,Barret Zoph,A. Spiridonov,Ryan Sepassi,David Dohan,Shivani Agrawal,Mark Omernick,Andrew M. Dai,T. S. Pillai,Marie Pellat,Aitor Lewkowycz,Erica Moreira,Rewon Child,Oleksandr Polozov,Katherine Lee,Zongwei Zhou,Xuezhi Wang,Brennan Saeta,Mark Díaz,Orhan Firat,Michele Catasta,Jason Wei,K. Meier-Hellstern,D. Eck,J. Dean,Slav Petrov,Noah Fiedel","id":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","summary":"A 540-billion parameter, densely activated, Transformer language model, which is called PaLM achieves breakthrough performance, outperforming the state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark.","score":3},{"url":"https://www.semanticscholar.org/paper/6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis","venue":"ArXiv","year":2022,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/04/2022","authors":"Kensen Shi,Joey Hong,M. Zaheer,Pengcheng Yin,Charles Sutton","id":"6a250b904965732840a75b6a13e35ac15f5cce4d","summary":"A suite of generalization tasks, which measure different types of compositional generalization that are desirable for program synthesis and are particularly difﬁcult for current sequence to sequence models, are proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/1f87dc41bdf2c4c78e2dce9c5c8adfef5e25a70c","title":"Passport: Improving Automated Formal Verification Using Identifiers","venue":"ArXiv","year":2022,"referenceCount":73,"citationCount":4,"influentialCitationCount":0,"publicationDate":"21/04/2022","authors":"Alex Sanchez-Stern,E. First,Timothy Zhou,Zhanna Kaufman,Yuriy Brun,T. Ringer","id":"1f87dc41bdf2c4c78e2dce9c5c8adfef5e25a70c","summary":"Passport is a fully-automated proof-synthesis tool that encodes one additional aspect of that rich proof data: identifiers, suggesting that modeling identifiers can play a significant role in improving proof synthesis, leading to higher-quality software.","score":3},{"url":"https://www.semanticscholar.org/paper/f7820b52e3cdff6625e6bd0430a8d48ca66cca3f","title":"Self-Programming Artificial Intelligence Using Code-Generating Language Models","venue":"","year":2022,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/04/2022","authors":"Alex Sheng,Shankar Padmanabhan","id":"f7820b52e3cdff6625e6bd0430a8d48ca66cca3f","summary":"It is empirically show that a self-programming AI implemented using a code generation model can successfully modify its own source code to improve performance and program sub-models to perform auxiliary tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/9a7d4c2f4309a2a38ea8e5b23c6d616fa0952d44","title":"Neural language models for network configuration: Opportunities and reality check","venue":"Computer Communications","year":2022,"referenceCount":63,"citationCount":4,"influentialCitationCount":0,"publicationDate":"03/05/2022","authors":"Zied Ben-Houidi,Dario Rossi","id":"9a7d4c2f4309a2a38ea8e5b23c6d616fa0952d44","summary":"Recent advances in deep learning applied to programming languages are surveyed, for the purpose of code veriﬁcation, synthesis and translation: in particularly, their training requirements and expected performance are reviewed, and qualitatively assess whether similar techniques can bene⬁t corresponding use-cases in networking.","score":3},{"url":"https://www.semanticscholar.org/paper/0efa0441da820b1905572666ba1974a06a9663fb","title":"NaturalProver: Grounded Mathematical Proof Generation with Language Models","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":6,"influentialCitationCount":0,"publicationDate":"25/05/2022","authors":"S. Welleck,Jiacheng Liu,Ximing Lu,Hannaneh Hajishirzi,Yejin Choi","id":"0efa0441da820b1905572666ba1974a06a9663fb","summary":"N ATURAL P ROVER is capable of proving some theorems that require short (2-6 step) proofs, and providing next-step suggestions that are rated as correct and useful over 40% of the time, which is to the authors' knowledge the first demonstration of these capabilities using neural language models.","score":3},{"url":"https://www.semanticscholar.org/paper/0d08ffccc982781e310bb184397bbe64b9aef157","title":"Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models","venue":"International Computing Education Research Workshop","year":2022,"referenceCount":99,"citationCount":19,"influentialCitationCount":1,"publicationDate":"03/06/2022","authors":"Sami Sarsa,Paul Denny,Arto Hellas,Juho Leinonen","id":"0d08ffccc982781e310bb184397bbe64b9aef157","summary":"The analysis suggests that there is significant value in massive generative machine learning models as a tool for instructors, although there remains a need for some oversight to ensure the quality of the generated content before it is delivered to students.","score":3},{"url":"https://www.semanticscholar.org/paper/2edc8efcda27c944a46f367acf6a5280b8f65525","title":"FixEval: Execution-based Evaluation of Program Fixes for Competitive Programming Problems","venue":"ArXiv","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Md. Mahim Anjum Haque,W. Ahmad,Ismini Lourentzou,Chris Brown","id":"2edc8efcda27c944a46f367acf6a5280b8f65525","summary":"This work introduces F IX E VAL, a benchmark comprising of buggy code submissions to competitive programming problems and their respective ﬁxes, and believes it provides a step towards real-world automatic bugﬁxing and model-generated code evaluation.","score":3},{"url":"https://www.semanticscholar.org/paper/ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":77,"influentialCitationCount":14,"publicationDate":"29/06/2022","authors":"Aitor Lewkowycz,Anders Andreassen,David Dohan,Ethan Dyer,H. Michalewski,V. Ramasesh,Ambrose Slone,Cem Anil,Imanol Schlag,Theo Gutman-Solo,Yuhuai Wu,Behnam Neyshabur,Guy Gur-Ari,Vedant Misra","id":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/99f85119f113b5498517928eff74a904b69e37b7","title":"CCTEST: Testing and Repairing Code Completion Systems","venue":"ArXiv","year":2022,"referenceCount":81,"citationCount":1,"influentialCitationCount":0,"publicationDate":"17/08/2022","authors":"Zongjie Li,Chaozheng Wang,Zhibo Liu,Haoxuan Wang,Shuai Wang,Cuiyun Gao","id":"99f85119f113b5498517928eff74a904b69e37b7","summary":"This research proposes CCT EST, a framework to test and repair code completion systems in black-box settings, which features a set of novel mutation strategies, namely program structure-consistent (PSC) mutations, to generate mutated code completion inputs.","score":3},{"url":"https://www.semanticscholar.org/paper/95fa2b27ab7eb84738441ee16da97323538938f9","title":"I Speak, You Verify: Toward Trustworthy Neural Program Synthesis","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/09/2022","authors":"Darren Key,Wen-Ding Li,Kevin Ellis","id":"95fa2b27ab7eb84738441ee16da97323538938f9","summary":"An approach for improving the trustworthiness and overall accuracy of program synthesizers based on large language models for source code by analyzing the agreement between programs and predicates to judge both which program is most likely to be correct and whether the language model is able to solve the programming problem in the first place.","score":3},{"url":"https://www.semanticscholar.org/paper/0b8772b7790c69f40897b5eb7f8fd57f24138f3d","title":"ContraGen: Effective Contrastive Learning For Causal Language Model","venue":"ArXiv","year":2022,"referenceCount":66,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/10/2022","authors":"Nihal Jain,Dejiao Zhang,Wasi Uddin Ahmad,Zijian Wang,Feng Nan,Xiaopeng Li,M. Tan,Ramesh Nallapati,Baishakhi Ray,Parminder Bhatia,Xiaofei Ma,Bing Xiang","id":"0b8772b7790c69f40897b5eb7f8fd57f24138f3d","summary":"It is shown that C ONTRA G EN can effectively enhance both uniformity and discrimination of the representations and lead to the desired improvement on various language understanding tasks where discriminative representations are crucial for attaining good performance.","score":3},{"url":"https://www.semanticscholar.org/paper/0c78a473e33a81246d5c0fbbda7e7de168814c18","title":"FlexType: A Plug-and-Play Framework for Type Inference Models","venue":"International Conference on Automated Software Engineering","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Sivani Voruganti,Kevin Jesse,Prem Devanbu","id":"0c78a473e33a81246d5c0fbbda7e7de168814c18","summary":"This work introduces FlexType, an IDE extension that can be used on both JavaScript and TypeScript to infer types in an interactive or automatic fashion and believes the interactive Visual Studio Code extension is inherently useful in both TypeScript and JavaScript especially when resolving types is taxing for the developer.","score":3},{"url":"https://www.semanticscholar.org/paper/663a41c866d49ce052801fbc88947d39764cad29","title":"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them","venue":"ArXiv","year":2022,"referenceCount":55,"citationCount":23,"influentialCitationCount":5,"publicationDate":"17/10/2022","authors":"Mirac Suzgun,Nathan Scales,Nathanael Scharli,Sebastian Gehrmann,Yi Tay,Hyung Won Chung,Aakanksha Chowdhery,Quoc V. Le,E. Chi,Denny Zhou,Jason Wei","id":"663a41c866d49ce052801fbc88947d39764cad29","summary":"It is found that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex to surpass it on 17 of the23 tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/8ffe7b7eeddbd4c22d642b0a48379d17e61c3bab","title":"Soft-Labeled Contrastive Pre-training for Function-level Code Representation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":42,"citationCount":2,"influentialCitationCount":0,"publicationDate":"18/10/2022","authors":"Xiaonan Li,Daya Guo,Yeyun Gong,Yun Lin,Yelong Shen,Xipeng Qiu,Daxin Jiang,Weizhu Chen,Nan Duan","id":"8ffe7b7eeddbd4c22d642b0a48379d17e61c3bab","summary":"This paper presents SCodeR, aoft-labeled contrastive pre-training framework with two positive sample construction methods to learn functional-level code representation and shows the effectiveness of the proposed pre- training method.","score":3},{"url":"https://www.semanticscholar.org/paper/4c2534f9b03ac2f3810c07abc398a11bcf47258e","title":"Transformers Learn Shortcuts to Automata","venue":"ArXiv","year":2022,"referenceCount":104,"citationCount":4,"influentialCitationCount":0,"publicationDate":"19/10/2022","authors":"Bingbin Liu,J. Ash,Surbhi Goel,A. Krishnamurthy,Cyril Zhang","id":"4c2534f9b03ac2f3810c07abc398a11bcf47258e","summary":"The theoretical results completely characterize shortcut solutions, whereby a shallow Transformer with only o(T ) layers can exactly replicate the computation of an automaton on an input sequence of length T .","score":3},{"url":"https://www.semanticscholar.org/paper/f031ba42cf82f106200bb03fbb91dd5671a59b9c","title":"Practical Program Repair in the Era of Large Pre-trained Language Models","venue":"ArXiv","year":2022,"referenceCount":69,"citationCount":4,"influentialCitationCount":0,"publicationDate":"25/10/2022","authors":"Chun Xia,Yuxiang Wei,Lingming Zhang","id":"f031ba42cf82f106200bb03fbb91dd5671a59b9c","summary":"This study demonstrates that directly applying state-of-the-art PLMs can already substantially outperform all existing APR techniques on all the authors' datasets and shows that PLM-based APR can be further substantially boosted via: increasing the sample size, and incorporating inﬁx template information.","score":3},{"url":"https://www.semanticscholar.org/paper/621009f1c30951b7c952c65c45ef0064a204e91e","title":"Early Experience with Transformer-Based Similarity Analysis for DataRaceBench","venue":"International Workshop on Software Correctness for HPC Applications","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Winson X. Chen,T. Vanderbruggen,Pei-Hung Lin,C. Liao,M. Emani","id":"621009f1c30951b7c952c65c45ef0064a204e91e","summary":"The challenges and the solutions when applying transformer-based similarity analysis to new source codes which are unseen by pre-trained transformers are explored, and comparative experiments of different variants of similarity analysis are used to comment on the strengths and limitations of the transformer- based approach and point out future research directions.","score":3},{"url":"https://www.semanticscholar.org/paper/2abed82162c47a0cc32cd62afcf46b0745541017","title":"Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":3,"influentialCitationCount":0,"publicationDate":"04/11/2022","authors":"S. MacNeil,Andrew Tran,Arto Hellas,Joanne Kim,Sami Sarsa,Paul Denny,Seth Bernstein,Juho Leinonen","id":"2abed82162c47a0cc32cd62afcf46b0745541017","summary":"The preliminary results show that all varieties of explanations were viewed by students and that the majority of students perceived the code explanations as helpful to them, however, student engagement appeared to vary by code snippet complexity, explanation type, and code snippet length.","score":3},{"url":"https://www.semanticscholar.org/paper/327f1561b544b0a3b9d8d5d0e6d82c2a5911fca9","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","venue":"ArXiv","year":2022,"referenceCount":157,"citationCount":53,"influentialCitationCount":9,"publicationDate":"09/11/2022","authors":"Teven Le Scao,Angela Fan,Christopher Akiki,Elizabeth-Jane Pavlick,Suzana Ili'c,Daniel Hesslow,Roman Castagn'e,A. Luccioni,Franccois Yvon,Matthias Gallé,J. Tow,Alexander M. Rush,Stella Rose Biderman,Albert Webson,Pawan Sasanka Ammanamanchi,Thomas Wang,Benoît Sagot,Niklas Muennighoff,Albert Villanova del Moral,Olatunji Ruwase,Rachel Bawden,Stas Bekman,Angelina McMillan-Major,Iz Beltagy,Huu Nguyen,Lucile Saulnier,Samson Tan,Pedro Ortiz Suarez,Victor Sanh,Hugo Laurenccon,Yacine Jernite,Julien Launay,Margaret Mitchell,Colin Raffel,Aaron Gokaslan,Adi Simhi,Aitor Soroa Etxabe,Alham Fikri Aji,Amit Alfassy,Anna Rogers,Ariel Kreisberg Nitzav,Canwen Xu,Chenghao Mou,Chris C. Emezue,Christopher Klamm,Colin Leong,Daniel Alexander van Strien,David Ifeoluwa Adelani,Dragomir R. Radev,Eduardo G. Ponferrada,Efrat Levkovizh,Ethan Kim,E. Natan,F. Toni,Gérard Dupont,Germán Kruszewski,Giada Pistilli,Hady ElSahar,Hamza Benyamina,Hieu Tran,Ian Yu,Idris Abdulmumin,Isaac Johnson,Itziar Gonzalez-Dios,Javier de la Rosa,Jenny Chim,Jesse Dodge,Jian Zhu,Jonathan Chang,Jorg Frohberg,Josephine L. Tobing,J. Bhattacharjee,Khalid Almubarak,Kimbo Chen,Kyle Lo,Leandro von Werra,Leon Weber,Long Phan,Loubna Ben Allal,Ludovic Tanguy,Manan Dey,M. Muñoz,Maraim Masoud,Mar'ia Grandury,Mario vSavsko,Max Huang,Maximin Coavoux,Mayank Singh,Mike Tian-Jian Jiang,Minh Chien Vu,M. A. Jauhar,Mustafa Ghaleb,Nishant Subramani,Nora Kassner,Nurulaqilla Khamis,Olivier Nguyen,Omar Espejel,Ona de Gibert,Paulo Villegas,Peter Henderson,Pierre Colombo,Priscilla Amuok,Quentin Lhoest,Rheza Harliman,Rishi Bommasani,R. L'opez,Rui Ribeiro,Salomey Osei,Sampo Pyysalo,Sebastian Nagel,Shamik Bose,Shamsuddeen Hassan Muhammad,Shanya Sharma,S. Longpre,Somaieh Nikpoor,Stanislav Silberberg,S. Pai,S. Zink,Tiago Timponi Torrent,Timo Schick,Tristan Thrush,V. Danchev,Vassilina Nikoulina,Veronika Laippala,Violette Lepercq,V. Prabhu,Zaid Alyafeai,Zeerak Talat,Arun Raja,Benjamin Heinzerling,Chenglei Si,Elizabeth Salesky,Sabrina J. Mielke,Wilson Y. Lee,Abheesht Sharma,Andrea Santilli,Antoine Chaffin,Arnaud Stiegler,Debajyoti Datta,Eliza Szczechla,Gunjan Chhablani,Han Wang,Harshit Pandey,Hendrik Strobelt,Jason Alan Fries,Jos Rozen,Leo Gao,Lintang Sutawika,M Saiful Bari,Maged S. Al-shaibani,Matteo Manica,Nihal V. Nayak,Ryan Teehan,Samuel Albanie,Sheng Shen,Srulik Ben-David,Stephen H. Bach,Taewoon Kim,T. Bers,Thibault Févry,Trishala Neeraj,Urmish Thakker,Vikas Raunak,Xiang Tang,Zheng Xin Yong,Zhiqing Sun,Shaked Brody,Y. Uri,Hadar Tojarieh,Adam Roberts,Hyung Won Chung,Jaesung Tae,Jason Phang,Ofir Press,Conglong Li,D. Narayanan,Hatim Bourfoune,J. Casper,Jeff Rasley,Max Ryabinin,Mayank Mishra,Minjia Zhang,M. Shoeybi,Myriam Peyrounette,N. Patry,Nouamane Tazi,Omar Sanseviero,Patrick von Platen,Pierre Cornette,Pierre Franccois Lavall'ee,R. Lacroix,Samyam Rajbhandari,Sanchit Gandhi,Shaden Smith,S. Requena,Suraj Patil,Tim Dettmers,Ahmed Baruwa,Amanpreet Singh,Anastasia Cheveleva,Anne-Laure Ligozat,Arjun Subramonian,Aur'elie N'ev'eol,Charles Lovering,Daniel H Garrette,D. Tunuguntla,Ehud Reiter,Ekaterina Taktasheva,E. Voloshina,Eli Bogdanov,Genta Indra Winata,Hailey Schoelkopf,Jan-Christoph Kalo,Jekaterina Novikova,J. Forde,Jordan Clive,Jungo Kasai,Ken Kawamura,Liam Hazan,Marine Carpuat,Miruna Clinciu,Najoung Kim,Newton Cheng,Oleg Serikov,Omer Antverg,Oskar van der Wal,Rui Zhang,Ruochen Zhang,Sebastian Gehrmann,S. Pais,Tatiana Shavrina,Thomas Scialom,Tian Yun,Tomasz Limisiewicz,Verena Rieser,Vitaly Protasov,V. Mikhailov,Yada Pruksachatkun,Yonatan Belinkov,Zachary Bamberger,Zdenvek Kasner,Alice Rueda,Amanda Pestana,A. Feizpour,Ammar Khan,Amy Faranak,A. Santos,A. Hevia,Antigona Unldreaj,Arash Aghagol,Arezoo Abdollahi,A. Tammour,Azadeh HajiHosseini,Bahareh Behroozi,B. Ajibade,B. Saxena,Carlos Muñoz Ferrandis,Danish Contractor,D. Lansky,Davis David,Douwe Kiela,D. A. Nguyen,Edward Tan,Emily Baylor,Ezinwanne Ozoani,Fatim T Mirza,Frankline Ononiwu,Habib Rezanejad,H.A. Jones,Indrani Bhattacharya,Irene Solaiman,Irina Sedenko,I. Nejadgholi,J. Passmore,Joshua Seltzer,Julio Bonis Sanz,Karen Fort,L. Dutra,Mairon Samagaio,Maraim Elbadri,M. Mieskes,Marissa Gerchick,Martha Akinlolu,Michael McKenna,Mike Qiu,M. Ghauri,Mykola Burynok,Nafis Abrar,Nazneen Rajani,Nour Elkott,N. Fahmy,O. Samuel,Ran An,R. Kromann,Ryan Hao,S. Alizadeh,Sarmad Shubber,Silas L. Wang,Sourav Roy,S. Viguier,Thanh-Cong Le,Tobi Oyebade,T. Le,Yoyo Yang,Z. Nguyen,Abhinav Ramesh Kashyap,Alfredo Palasciano,A. Callahan,Anima Shukla,Antonio Miranda-Escalada,A. Singh,Benjamin Beilharz,Bo Wang,C. Brito,Chenxi Zhou,Chirag Jain,Chuxin Xu,Clémentine Fourrier,Daniel Le'on Perin'an,Daniel Molano,Dian Yu,Enrique Manjavacas,Fabio Barth,Florian Fuhrimann,Gabriel Altay,Giyaseddin Bayrak,Gully A. Burns,Helena U. Vrabec,I. Bello,Isha Dash,J. Kang,John Giorgi,J. Golde,J. Posada,Karthi Sivaraman,Lokesh Bulchandani,Lu Liu,Luisa Shinzato,Madeleine Hahn de Bykhovetz,Maiko Takeuchi,Marc Pàmies,M. A. Castillo,Marianna Nezhurina,Mario Sanger,M. Samwald,Michael Cullan,Michael Weinberg,M. Wolf,Mina Mihaljcic,Minna Liu,M. Freidank,Myungsun Kang,Natasha Seelam,N. Dahlberg,N. Broad,N. Muellner,Pascale Fung,Patricia Haller,R. Chandrasekhar,R. Eisenberg,Robert Martin,Rodrigo L. Canalli,Rosaline Su,Ruisi Su,Samuel Cahyawijaya,Samuele Garda,Shlok S Deshmukh,Shubhanshu Mishra,Sid Kiblawi,Simon Ott,Sinee Sang-aroonsiri,Srishti Kumar,Stefan Schweter,S. Bharati,T. A. Laud,Th'eo Gigant,Tomoya Kainuma,Wojciech Kusa,Yanis Labrak,Yashasvi Bajaj,Y. Venkatraman,Yifan Xu,Ying Xu,Yun-chao Xu,Z. Tan,Zhongli Xie,Zifan Ye,M. Bras,Younes Belkada,Thomas Wolf","id":"327f1561b544b0a3b9d8d5d0e6d82c2a5911fca9","summary":"BLOOM is a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers and achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning.","score":3},{"url":"https://www.semanticscholar.org/paper/a4bdc300db297756f36bedee2859b62df8e268c2","title":"Follow the Wisdom of the Crowd: Effective Text Generation via Minimum Bayes Risk Decoding","venue":"ArXiv","year":2022,"referenceCount":83,"citationCount":3,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"Mirac Suzgun,Luke Melas-Kyriazi,Dan Jurafsky","id":"a4bdc300db297756f36bedee2859b62df8e268c2","summary":"This work presents crowd sampling, a family of decoding methods based on Bayesian risk minimization, to ad-dress this diversity-quality trade-off in open-ended natural-language generation.","score":3},{"url":"https://www.semanticscholar.org/paper/20b60fb3993d2e9a5af04611f7bdf248e5a3a736","title":"Programming by Example and Text-to-Code Translation for Conversational Code Generation","venue":"ArXiv","year":2022,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Eli Whitehouse,William Gerard,Yauhen Klimovich,Marc Franco-Salvador","id":"20b60fb3993d2e9a5af04611f7bdf248e5a3a736","summary":"Modular Programs for Text-guided Hierarchical Synthesis (MPaTHS), a method for integrating Programming by Example and text-to-code systems which uses an accessible natural language interface for synthesizing general programs, is proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/30624a18720bf93a85dc3efe570df271a8c9f4c3","title":"Program Repair","venue":"ArXiv","year":2022,"referenceCount":105,"citationCount":2,"influentialCitationCount":0,"publicationDate":"23/11/2022","authors":"Xiang Gao,Yannic Noller,Abhik Roychoudhury","id":"30624a18720bf93a85dc3efe570df271a8c9f4c3","summary":"Keeping intermediate rules, instead of producing the most generalized rule by generalizing all concrete transformations, enables us to apply the most suitable rule to transform a given code.","score":3},{"url":"https://www.semanticscholar.org/paper/dca3bc28a7d404b28780a813ea7072eda809e6c0","title":"Programming Is Hard - Or at Least It Used to Be: Educational Opportunities And Challenges of AI Code Generation","venue":"ArXiv","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/12/2022","authors":"Brett A. Becker,Paul Denny,James Finnie-Ansley,Andrew Luxton-Reilly,J. Prather,E. Santos","id":"dca3bc28a7d404b28780a813ea7072eda809e6c0","summary":"It is argued that the community needs to act quickly in deciding what possible opportunities can and should be leveraged and how, while also working on how to overcome or otherwise mitigate the possible challenges.","score":3},{"url":"https://www.semanticscholar.org/paper/42630c03d3817b1153d245f20742ad4b30a80b75","title":"JEMMA: An Extensible Java Dataset for ML4Code Applications","venue":"ArXiv","year":2022,"referenceCount":93,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/12/2022","authors":"Anjan Karmakar,Miltiadis Allamanis,R. Robbes","id":"42630c03d3817b1153d245f20742ad4b30a80b75","summary":"JEMMA is introduced, which is a largescale, diverse, and high-quality dataset targeted at ML4Code applications, and becomes a workbench that researchers can use to experiment with novel representations and tasks operating on source code.","score":3},{"url":"https://www.semanticscholar.org/paper/bf5fbd690f24f873df86d1b0a06579cf42f7dc36","title":"Dialog2API: Task-Oriented Dialogue with API Description and Example Programs","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Raphael Shu,Elman Mansimov,Tamer Alkhouli,Nikolaos Pappas,Salvatore Romeo,Arshit Gupta,Saab Mansour,Yi Zhang,D. Roth","id":"bf5fbd690f24f873df86d1b0a06579cf42f7dc36","summary":"An approach tailored for the Dialog2API, where the dialogue states are represented by a stack of programs, with most recently mentioned program on the top of the stack, is proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/c532f1df90925e5c69789f0cd99248d8a2a2e5bc","title":"My AI Wants to Know if This Will Be on the Exam: Testing OpenAI’s Codex on CS2 Programming Exercises","venue":"IFAC Symposium on Advances in Control Education","year":2023,"referenceCount":27,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"James Finnie-Ansley,Paul Denny,Andrew Luxton-Reilly,E. Santos,J. Prather,Brett A. Becker","id":"c532f1df90925e5c69789f0cd99248d8a2a2e5bc","summary":"This paper presents results detailing how Codex performs on more advanced CS2 exam questions taken from past exams, and compares these results to those of students who took the same exams under normal conditions, demonstrating that Codex outscores most students.","score":3},{"url":"https://www.semanticscholar.org/paper/63396fceb84286b02796dc58e55c07ec1095c4dc","title":"FLAME: A small language model for spreadsheet formulas","venue":"ArXiv","year":2023,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Harshit Joshi,Abishai Ebenezer,J. Cambronero,Sumit Gulwani,Aditya Kanade,Vu Le,Ivan Radivcek,Gust Verbruggen","id":"63396fceb84286b02796dc58e55c07ec1095c4dc","summary":"This work presents FLAME, a T5-based model trained on Excel formulas that leverages domain insights to achieve competitive performance with a substantially smaller model (60M parameters) and two orders of magnitude less training data.","score":3},{"url":"https://www.semanticscholar.org/paper/782f3d43b37790a83c98d5fd3ef142b296f20616","title":"CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code Models","venue":"ArXiv","year":2023,"referenceCount":76,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"Changan Niu,Chuanyi Li,Vincent Ng,Bin Luo","id":"782f3d43b37790a83c98d5fd3ef142b296f20616","summary":"A large-scale benchmark that includes 216 existing code-related tasks is proposed and it is demonstrated that the cross-task generalization of models can be largely improved by in-context learning methods such as few-shot learning and learning from task instructions, which shows the promising prospects of conducting cross- task learning research on this benchmark.","score":3},{"url":"https://www.semanticscholar.org/paper/b0335eb2b9a1684fc16ff79234f0b206b30da169","title":"A Study of Editor Features in a Creative Coding Classroom","venue":"ArXiv","year":2023,"referenceCount":107,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"Andrew M. McNutt,Anton Outkine,Ravi Chugh","id":"b0335eb2b9a1684fc16ff79234f0b206b30da169","summary":"An experimental editor is implemented and used to teach a sequence of college and high-school creative coding courses to identify opportunities to improve creativity- and novice-focused IDEs and highlight ten-sions in their design.","score":2},{"url":"https://www.semanticscholar.org/paper/0bc2753f59e653de718b5c7a2a0a7e00d13778c7","title":"NL2CMD: An Updated Workflow for Natural Language to Bash Commands Translation","venue":"","year":2022,"referenceCount":77,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Quchen Fu,Zhongwei Teng,Marco Georgaklis,Jules White,Douglas,C. Schmidt","id":"0bc2753f59e653de718b5c7a2a0a7e00d13778c7","summary":"A state-of-the-art translation model used to generate Bash Commands from the corresponding English text is described and a new NL2CMD dataset is introduced that is automatically generated, involves minimal human intervention, and is over six times larger than prior datasets.","score":2},{"url":"https://www.semanticscholar.org/paper/d388b3cbc820975da21c57c9fb9cef2d9c6d08f0","title":"CodeRetriever: Large-scale Contrastive Pre-training for Code Search","venue":"","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Xiaonan Li,Yeyun Gong,Yelong Shen,Xipeng Qiu,Hang Zhang,Bolun Yao,Weizhen Qi,Daxin Jiang,Weizhu Chen,Nan Duan","id":"d388b3cbc820975da21c57c9fb9cef2d9c6d08f0","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/22df866f9605d27d1e5cca9b3ab721f33673e158","title":"ProgramTransformer: A tool for generating semantically equivalent transformed programs","venue":"Softw. Impacts","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/10/2022","authors":"Md Rafiqul Islam Rabin,Mohammad Amin Alipour","id":"22df866f9605d27d1e5cca9b3ab721f33673e158","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/862c0b672c9defded3111924310a07760cfa27ff","title":"Can NMT Understand Me? Towards Perturbation-based Evaluation of NMT Models for Code Generation","venue":"2022 IEEE/ACM 1st International Workshop on Natural Language-Based Software Engineering (NLBSE)","year":2022,"referenceCount":58,"citationCount":1,"influentialCitationCount":0,"publicationDate":"29/03/2022","authors":"Pietro Liguori,Cristina Improta,S. D. Vivo,R. Natella,B. Cukic,Domenico Cotroneo","id":"862c0b672c9defded3111924310a07760cfa27ff","summary":"This work identifies a set of perturbations and metrics tailored for the robustness assessment of NMT models, and presents a preliminary experimental evaluation, showing what type of perturbedations affect the model the most and deriving useful insights for future directions.","score":2},{"url":"https://www.semanticscholar.org/paper/24c5450d8fa785e5f85d9427d2d65cf66476ac3a","title":"Toward General Design Principles for Generative AI Applications","venue":"ArXiv","year":2023,"referenceCount":115,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/01/2023","authors":"Justin D. Weisz,Michael J. Muller,Jessica He,Stephanie Houde","id":"24c5450d8fa785e5f85d9427d2d65cf66476ac3a","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/2ad6b59ff14d6bec3f14d8b6480025bbebc50e46","title":"Optimal Neural Program Synthesis from Multimodal Specifications","venue":"Conference on Empirical Methods in Natural Language Processing","year":2020,"referenceCount":48,"citationCount":8,"influentialCitationCount":0,"publicationDate":"04/10/2020","authors":"Xi Ye,Qiaochu Chen,Işil Dillig,Greg Durrett","id":"2ad6b59ff14d6bec3f14d8b6480025bbebc50e46","summary":"The experimental results on a multimodal synthesis dataset show that the proposed optimal neural synthesis approach substantially outperforms prior state-of-the-art techniques in terms of accuracy %, finds model-optimal programs more frequently, and explores fewer states during search.","score":2},{"url":"https://www.semanticscholar.org/paper/6df98ac2300c6e9c232440147ba976b4f501ca67","title":"C ODEX HACKS H ACKER R ANK : B ENEFITS AND R ISKS OF L ARGE -S CALE S OURCE C ODE M ODELS","venue":"","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","id":"6df98ac2300c6e9c232440147ba976b4f501ca67","summary":"These studies evaluate Codex on code synthesis, similar to the approach, but their evaluation efforts remain limited to math problems.","score":2},{"url":"https://www.semanticscholar.org/paper/8bfc22de7fe66286ad9ae705d677246757fbf8a8","title":"Large Language Models Can Be Easily Distracted by Irrelevant Context","venue":"ArXiv","year":2023,"referenceCount":57,"citationCount":1,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Freda Shi,Xinyun Chen,Kanishka Misra,Nathan Scales,David Dohan,E. Chi,Nathanael Scharli,Denny Zhou","id":"8bfc22de7fe66286ad9ae705d677246757fbf8a8","summary":"This work investigates the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context, and introduces Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description.","score":2},{"url":"https://www.semanticscholar.org/paper/cc80244d69c2c5bb7fe79fc6ca2561f2bed830ca","title":"Learning to Represent Programs with Heterogeneous Graphs","venue":"IEEE International Conference on Program Comprehension","year":2020,"referenceCount":70,"citationCount":15,"influentialCitationCount":3,"publicationDate":"28/09/2020","authors":"Wenhan Wang,Kechi Zhang,Ge Li,Zhi Jin","id":"cc80244d69c2c5bb7fe79fc6ca2561f2bed830ca","summary":"This paper proposes the heterogeneous program graph (HPG), which provides the types of the nodes and the edges explicitly, and employs the heterogeneity transformer (HGT) architecture to generate representations based on HPG, considering the type of information during processing.","score":2},{"url":"https://www.semanticscholar.org/paper/c125b0be73c8493ebc27beb572f6c1b21d6b4ae4","title":"Static Prediction of Runtime Errors by Learning to Execute Programs with External Resource Descriptions","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":2,"influentialCitationCount":0,"publicationDate":"07/03/2022","authors":"David Bieber,Rishab Goel,Daniel Zheng,H. Larochelle,Daniel Tarlow","id":"c125b0be73c8493ebc27beb572f6c1b21d6b4ae4","summary":"Surprisingly, it is shown that the model can also predict the location of the error, despite being trained only on labels indicating the presence/absence and kind of error.","score":2},{"url":"https://www.semanticscholar.org/paper/703f79763d534dbf9674132cec890f432dcc19ec","title":"A Survey of Deep Learning Models for Structural Code Understanding","venue":"ArXiv","year":2022,"referenceCount":180,"citationCount":1,"influentialCitationCount":0,"publicationDate":"03/05/2022","authors":"Ruoting Wu,Yu-xin Zhang,Qibiao Peng,Liang Chen,Zibin Zheng","id":"703f79763d534dbf9674132cec890f432dcc19ec","summary":"This survey presents a comprehensive overview of the structures formed from code data, categorizing the models for understanding code in recent years into two groups: sequence-based and graph-based models, and makes some suggestions for future research in structural code understanding field.","score":2},{"url":"https://www.semanticscholar.org/paper/713bd2971116098211ef06336dfbe91a69854404","title":"Probing Semantic Grounding in Language Models of Code with Representational Similarity Analysis","venue":"International Conference on Advanced Data Mining and Applications","year":2022,"referenceCount":18,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/07/2022","authors":"Shounak Naik,Rajaswa Patil,Swati Agarwal,V. Baths","id":"713bd2971116098211ef06336dfbe91a69854404","summary":"This paper probes representations from the CodeBERT model for semantic grounding by using the data from the IBM CodeNet dataset, and shows that using bimodalinputs over unimodal inputs gives better semantic grounding and sample eﬃciency during semantic ﬁne-tuning.","score":2},{"url":"https://www.semanticscholar.org/paper/27e3ca4fd7b8290a0e12ca4fd2b7ad5bcd5900f2","title":"Are Transformers All That Karel Needs?","venue":"","year":2021,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Abhay Garg,Anand Sriraman,Kunal Pagarey,S. Karande","id":"27e3ca4fd7b8290a0e12ca4fd2b7ad5bcd5900f2","summary":"By changing the base architecture to a transformer based one, speciﬁcally GPT2, this work is able to apply simple execution guidance on top to achieve a generalization accurary of 89.64%, which is within 2.36 percentage points of the current state-of-the-art on Karel which uses ensembling.","score":2},{"url":"https://www.semanticscholar.org/paper/27d16a7f2ce8f2b787b34ff1f9b4fece079700c3","title":"Figuring out Figures: Using Textual References to Caption Scientific Figures","venue":"","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Stanley Cao,Kevin Liu","id":"27d16a7f2ce8f2b787b34ff1f9b4fece079700c3","summary":"This work uses the S CI C AP datasets curated by Hsu et al. and uses a variant of a CLIP+GPT-2 encoder-decoder model with cross-attention to generate captions conditioned on the image, and uses SciBERT to encode the textual metadata and uses this encoding alongside the figure embedding.","score":2},{"url":"https://www.semanticscholar.org/paper/317208b423d24d52ba04221cfb46956962364e22","title":"Extracting Meaningful Attention on Source Code: An Empirical Study of Developer and Neural Model Code Exploration","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/10/2022","authors":"Matteo Paltenghi,Rahul Pandita,Austin Z. Henley,Albert Ziegler","id":"317208b423d24d52ba04221cfb46956962364e22","summary":"This work empirically evaluates attention-agnostic heuris-tics and ten attention-based post processing approaches of the attention signal against the ground truth of developers exploring code, including the novel concept of follow-up attention which exhibits the highest agreement.","score":2},{"url":"https://www.semanticscholar.org/paper/e5993b3afe6384b5e6f90093989773ad1f868f71","title":"Towards Top-Down Deep Code Generation in Limited Scopes","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/09/2022","authors":"Jian Gu,H. Gall","id":"e5993b3afe6384b5e6f90093989773ad1f868f71","summary":"A semantic pyramid framework (SPF) is proposed as the approach, focusing on softwares of high modularity and low complexity, and introduces a three-layer semantic pyramid (SP) to associate text data and code data.","score":2},{"url":"https://www.semanticscholar.org/paper/bab6893ee48d168d27c227c3b0867f6d471fbea8","title":"Language Models are not Models of Language","venue":"ArXiv","year":2021,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"C. Veres","id":"bab6893ee48d168d27c227c3b0867f6d471fbea8","summary":"It is argued that the term language model is misleading because deep learning models are not theoretical models of language and proposed the adoption of corpus model instead, which better reflects the genesis and contents of the model.","score":2},{"url":"https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":65,"citationCount":91,"influentialCitationCount":20,"publicationDate":"05/03/2021","authors":"Dan Hendrycks,Collin Burns,Saurav Kadavath,Akul Arora,Steven Basart,Eric Tang,D. Song,J. Steinhardt","id":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","summary":"This work introduces MATH, a new dataset of 12, 500 challenging competition mathematics problems which can be used to teach models to generate answer derivations and explanations, and shows that accuracy remains relatively low, even with enormous Transformer models.","score":2},{"url":"https://www.semanticscholar.org/paper/98485ce6532d69f34a8ec67de6b09a39532bd221","title":"Communicating Natural Programs to Humans and Machines","venue":"ArXiv","year":2021,"referenceCount":87,"citationCount":8,"influentialCitationCount":0,"publicationDate":"15/06/2021","authors":"Samuel Acquaviva,Yewen Pu,Marta Kryven,Catherine Wong,Gabrielle Ecanow,Maxwell Nye,Theo Sechopoulos,Michael Henry Tessler,J. Tenenbaum","id":"98485ce6532d69f34a8ec67de6b09a39532bd221","summary":"LARC, the Language-complete ARC is presented, a collection of natural language descriptions by a group of human participants who instruct each other on how to solve ARC tasks using language alone, which contains successful instructions for 88% of the ARC tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/58a6ca2ae28a618126f71a07262cb958a8c37904","title":"Latent Execution for Neural Program Synthesis","venue":"","year":2021,"referenceCount":60,"citationCount":2,"influentialCitationCount":0,"publicationDate":"29/06/2021","authors":"Xinyun Chen,D. Song,Yuandong Tian","id":"58a6ca2ae28a618126f71a07262cb958a8c37904","summary":"LaSynth learns the latent representation to approximate the execution of partially generated programs, even if they are incomplete in syntax, and significantly improves the performance of next token prediction over existing approaches, facilitating search.","score":2},{"url":"https://www.semanticscholar.org/paper/5436193122dff271796bca07df7cecb7a8d6dea6","title":"Natural language-guided programming","venue":"SIGPLAN symposium on New ideas, new paradigms, and reflections on programming and software","year":2021,"referenceCount":53,"citationCount":6,"influentialCitationCount":1,"publicationDate":"11/08/2021","authors":"Geert Heyman,Rafael Huysegems,P. Justen,Tom Van Cutsem","id":"5436193122dff271796bca07df7cecb7a8d6dea6","summary":"The key idea is to adapt code autocompletion tools such that they take into account not only the developer’s already-written code but also the intent of the task the developer is trying to achieve next, formulated in plain natural language.","score":2},{"url":"https://www.semanticscholar.org/paper/4885e616e85d420576196b2578525cbc501137ec","title":"Programming and execution models for next generation code intelligence systems (keynote)","venue":"ESEC/SIGSOFT FSE","year":2021,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/08/2021","authors":"M. Mezini","id":"4885e616e85d420576196b2578525cbc501137ec","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/3f97c2067cde9377e50b3160bbd7982c94abd88a","title":"An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions","venue":"ArXiv","year":2021,"referenceCount":29,"citationCount":21,"influentialCitationCount":1,"publicationDate":2021,"authors":"H. Pearce,Baleegh Ahmad,Benjamin Tan,Brendan Dolan-Gavitt,R. Karri","id":"3f97c2067cde9377e50b3160bbd7982c94abd88a","summary":"This work systematically investigates the prevalence and conditions that can cause GitHub Copilot to recommend insecure code, and explores Copilot’s performance on three distinct code generation axes—examining how it performs given diversity of weaknesses, diversity of prompts, and diversity of domains.","score":2},{"url":"https://www.semanticscholar.org/paper/a176b0de62840f7118006277d94bbc1547162a4d","title":"Learning to Synthesize Programs as Interpretable and Generalizable Policies","venue":"Neural Information Processing Systems","year":2021,"referenceCount":131,"citationCount":15,"influentialCitationCount":2,"publicationDate":"31/08/2021","authors":"Dweep Trivedi,Jesse Zhang,Shao-Hua Sun,Joseph J. Lim","id":"a176b0de62840f7118006277d94bbc1547162a4d","summary":"Experimental results demonstrate that the proposed framework not only learns to reliably synthesize task-solving programs but also outperforms DRL and program synthesis baselines while producing interpretable and more generalizable policies.","score":2},{"url":"https://www.semanticscholar.org/paper/a8863de15a5ee8eed98107f423138a1a8f5a2ba8","title":"Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis","venue":"Proc. ACM Program. Lang.","year":2021,"referenceCount":77,"citationCount":17,"influentialCitationCount":0,"publicationDate":"03/09/2021","authors":"Kia Rahmani,Mohammad Raza,Sumit Gulwani,Vu Le,Daniel Morris,Arjun Radhakrishna,Gustavo Soares,A. Tiwari","id":"a8863de15a5ee8eed98107f423138a1a8f5a2ba8","summary":"This work presents an approach that combines PTMs with component-based synthesis (CBS): PTMs are used to generate candidates programs from the natural language description of the task, which are then used to guide the CBS procedure to find the program that matches the precise examples-based specification.","score":2},{"url":"https://www.semanticscholar.org/paper/bc9598dc4ed0472d8b59b87ed3a139f8347d40ee","title":"Towards A Measure Of General Machine Intelligence","venue":"ArXiv","year":2021,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/09/2021","authors":"Gautham Venkatasubramanian,Sibesh Kar,Abhimanyu Singh,Shubham Mishra,Dushyant Yadav,Shreyansh Chandak","id":"bc9598dc4ed0472d8b59b87ed3a139f8347d40ee","summary":"A common language of instruction is proposed, a programming language that allows the expression of programs in the form of directed acyclic graphs across a wide variety of real-world domains and computing platforms and evaluates the suitability of some well-known models as general intelligence systems by calculating their g-index scores.","score":2},{"url":"https://www.semanticscholar.org/paper/05c2e1ee203be217f100d2da05bdcc52004f00b6","title":"Unsolved Problems in ML Safety","venue":"ArXiv","year":2021,"referenceCount":217,"citationCount":62,"influentialCitationCount":5,"publicationDate":"28/09/2021","authors":"Dan Hendrycks,Nicholas Carlini,J. Schulman,J. Steinhardt","id":"05c2e1ee203be217f100d2da05bdcc52004f00b6","summary":"This work provides a new roadmap for ML Safety and presents four problems ready for research, namely withstanding hazards, identifying hazards, steering ML systems, and reducing deployment hazards.","score":2},{"url":"https://www.semanticscholar.org/paper/6c2d43e71e240e354b5790a38da78a291ceffe7c","title":"Learning to Superoptimize Real-world Programs","venue":"ArXiv","year":2021,"referenceCount":37,"citationCount":2,"influentialCitationCount":0,"publicationDate":"28/09/2021","authors":"Alex Shypula,P. Yin,Jeremy Lacomis,Claire Le Goues,E. Schwartz,Graham Neubig","id":"6c2d43e71e240e354b5790a38da78a291ceffe7c","summary":"A framework to learn to superoptimize real-world programs by using neural sequence-to-sequence models, and an approach to implement and outperforms a standard policy gradient learning approach on this dataset.","score":2},{"url":"https://www.semanticscholar.org/paper/dace03e57056d736f9e24937bdf486e894f8e866","title":"Is neural machine translation approach accurate enough for coding assistance?","venue":"BCNC@SPLASH","year":2021,"referenceCount":20,"citationCount":2,"influentialCitationCount":1,"publicationDate":"15/10/2021","authors":"Yuka Akinobu,Momoka Obara,Teruno Kajiura,Shiho Takano,Miyu Tamura,Mayu Tomioka,Kimio Kuramitsu","id":"dace03e57056d736f9e24937bdf486e894f8e866","summary":"A transcompiler-based back-translation, a data augmentation method that generates parallel corpora from numerous source code repositories and the resulting BLEU indicates that the proposed model is accurate enough to allow coding assistance in the future.","score":2},{"url":"https://www.semanticscholar.org/paper/21e8e76386aaaa00e0971af70ce84a8a544e1aa1","title":"Cascaded Fast and Slow Models for Efficient Semantic Code Search","venue":"ArXiv","year":2021,"referenceCount":24,"citationCount":4,"influentialCitationCount":0,"publicationDate":"15/10/2021","authors":"Akhilesh Deepak Gotmare,Junnan Li,Shafiq R. Joty,S. Hoi","id":"21e8e76386aaaa00e0971af70ce84a8a544e1aa1","summary":"An efficient and accurate semantic code search framework with cascaded fast and slow models, in which a fast transformer encoder model is learned to optimize a scalable index for fast retrieval followed by learning a slow classification-based re-ranking model to improve the performance of the top K results from the fast retrieval.","score":2},{"url":"https://www.semanticscholar.org/paper/570a6a5b8ec2827c3f33bb1b1bd027190a0d3e07","title":"Neural Program Generation Modulo Static Analysis","venue":"Neural Information Processing Systems","year":2021,"referenceCount":56,"citationCount":11,"influentialCitationCount":0,"publicationDate":"26/10/2021","authors":"Rohan Mukherjee,Yeming Wen,Dipak Chaudhari,T. Reps,Swarat Chaudhuri,C. Jermaine","id":"570a6a5b8ec2827c3f33bb1b1bd027190a0d3e07","summary":"The neurosymbolic method allows a deep generative model to symbolically compute, using calls to a static-analysis tool, long-distance semantic relationships in the code that it has already generated, and learns to generate programs conditioned on them.","score":2},{"url":"https://www.semanticscholar.org/paper/091fa84bdc07dcb22a34060c3996d8c58d71cd20","title":"Towards Neural Functional Program Evaluation","venue":"ArXiv","year":2021,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/12/2021","authors":"Torsten Scholak,Jonathan Pilault,Joey Velez-Ginorio","id":"091fa84bdc07dcb22a34060c3996d8c58d71cd20","summary":"A new program generation mechanism is introduced that allows control over syntactic sugar for semantically equivalent programs in transformer-based language models for program evaluation of simple functional programming languages.","score":2},{"url":"https://www.semanticscholar.org/paper/75c8f2916a2067f7549cf58ea8c9061565eb1dab","title":"C ROSS B EAM : L EARNING TO S EARCH IN B OTTOM -U P P ROGRAM S YNTHESIS","venue":"","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"H. Dai,Kevin Ellis,Charles Sutton","id":"75c8f2916a2067f7549cf58ea8c9061565eb1dab","summary":"This work uses a neural model to learn a hands-on search policy for bottom-up synthesis, instead of relying on a combinatorial search algorithm, and observes that CROSSBEAM learns to search efficiently, exploring much smaller portions of the program space compared to the state-of-the-art.","score":2},{"url":"https://www.semanticscholar.org/paper/9bf75110ea0923bbed49256b5491f1ec284019ec","title":"From BERT to GPT-3 Codex: Harnessing the Potential of Very Large Language Models for Data Management","venue":"Proceedings of the VLDB Endowment","year":2022,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Immanuel Trummer","id":"9bf75110ea0923bbed49256b5491f1ec284019ec","summary":"The goal of the tutorial is to introduce database researchers to the latest generation of language models, and to their use cases in the domain of data management.","score":2},{"url":"https://www.semanticscholar.org/paper/ddab94478a7647ee136b1f6b5076417db3074d0f","title":"Machine Programming: Turning Data into Programmer Productivity","venue":"Proceedings of the VLDB Endowment","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"A. Wasay,Nesime Tatbul,Justin Emile Gottschlich","id":"ddab94478a7647ee136b1f6b5076417db3074d0f","summary":"An introduction to machine programming is introduced introducing its three pillars: intention, invention, and adaptation, and an overview of the data ecosystem central to all machine programming systems is provided, highlighting challenges and novel opportunities relevant to the data systems community.","score":2},{"url":"https://www.semanticscholar.org/paper/2443179d421e1faf7474add557b45add554723c7","title":"Formal Premise Selection With Language Models","venue":"","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Szymon Tworkowski,Maciej Miku la,Tomasz Odrzygóźdź,K. Czechowski,Szymon Antoniak,Albert Qiaochu Jiang,Christian Szegedy,Lukasz Kucinski,Piotr Mi loś,Yuhuai Wu","id":"2443179d421e1faf7474add557b45add554723c7","summary":"This work provides a solution to the problem of selecting a useful premise to prove a new theorem by combining a premise selection model with a language model, and shows that this retrieval-augmented prover achieves significant improvements in proof rates compared to the language model alone.","score":2},{"url":"https://www.semanticscholar.org/paper/b98d6fe8f0ef02ec0d1bb2bcfb924c8f01feb7d4","title":"Convergent Representations of Computer Programs in Human and Artificial Neural Networks","venue":"","year":2022,"referenceCount":79,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Shashank Srikant,Benjamin Lipkin,Anna A. Ivanova,Evelina Fedorenko,Una-May O’Reilly","id":"b98d6fe8f0ef02ec0d1bb2bcfb924c8f01feb7d4","summary":"Analysis of brain recordings derived from functional magnetic resonance imaging studies of programmers comprehending Python code suggests at least two distinct neural mechanisms mediating computer program comprehension and evaluation, prompting the design of code model objectives that go beyond static language modeling.","score":2},{"url":"https://www.semanticscholar.org/paper/15ef2d1b88f54fa32a32927463a7116219b89529","title":"L EARNING TO S UPEROPTIMIZE R EAL - WORLD P ROGRAMS","venue":"","year":null,"referenceCount":0,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"","id":"15ef2d1b88f54fa32a32927463a7116219b89529","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/4797e960f8e7a2b47ae0d95c7071ef84fa5d4b5b","title":"Code Generation Using Machine Learning: A Systematic Review","venue":"IEEE Access","year":2022,"referenceCount":117,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Enrique Dehaerne,Bappaditya Dey,Sandip Halder,S. De Gendt,Wannes Meert","id":"4797e960f8e7a2b47ae0d95c7071ef84fa5d4b5b","summary":"This review provides a broad and detailed overview of studies for code generation using ML, and summarizes the applications, models, datasets, results, limitations, and future work of 37 publications.","score":2},{"url":"https://www.semanticscholar.org/paper/660ca9e15e19409903a0605f0584d0f263c35c67","title":"S YNCHROMESH : R ELIABLE C ODE G ENERATION FROM P RE - TRAINED L ANGUAGE M ODELS","venue":"","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Gabriel Poesia,A. Tiwari,Gustavo Soares,Christopher Meek","id":"660ca9e15e19409903a0605f0584d0f263c35c67","summary":"A framework for substantially improving the reliability of pre-trained models for code generation and observing substantial complementary gains from CSD and TST in prediction accuracy and in effectively preventing run-time errors is proposed.","score":2},{"url":"https://www.semanticscholar.org/paper/ba5d21b7c65c6598c7bd39a5d992308c205df374","title":"A S YSTEMATIC E VALUATION OF L ARGE L ANGUAGE M ODELS OF C ODE","venue":"","year":2022,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Graham Neubig,V. Hellendoorn","id":"ba5d21b7c65c6598c7bd39a5d992308c205df374","summary":"It is found that existing open-source models do achieve close results in some programming languages, although targeted mainly for natural language modeling, and a new model, PolyCoder, is released that was trained on 249GB of code across 12 programming languages on a single machine.","score":2},{"url":"https://www.semanticscholar.org/paper/6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions","venue":"IEEE Symposium on Security and Privacy","year":2021,"referenceCount":35,"citationCount":34,"influentialCitationCount":6,"publicationDate":"20/08/2021","authors":"H. Pearce,Baleegh Ahmad,Benjamin Tan,Brendan Dolan-Gavitt,R. Karri","id":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","summary":"This work systematically investigates the prevalence and conditions that can cause GitHub Copilot to recommend insecure code, and explores Copilot’s performance on three distinct code generation axes—examining how it performs given diversity of weaknesses, diversity of prompts, and diversity of domains.","score":2},{"url":"https://www.semanticscholar.org/paper/ff0b2681d7b05e16c46dfb71d980cc2f605907cd","title":"Finetuned Language Models Are Zero-Shot Learners","venue":"International Conference on Learning Representations","year":2021,"referenceCount":167,"citationCount":349,"influentialCitationCount":82,"publicationDate":"03/09/2021","authors":"Jason Wei,Maarten Bosma,Vincent Zhao,Kelvin Guu,A. Yu,Brian Lester,Nan Du,Andrew M. Dai,Quoc V. Le","id":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","summary":"It is shown that instruction tuning —ﬁnetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks and outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze.","score":2},{"url":"https://www.semanticscholar.org/paper/6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a","title":"Few-Shot Semantic Parsing with Language Models Trained on Code","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":18,"citationCount":19,"influentialCitationCount":3,"publicationDate":"16/12/2021","authors":"Richard Shin,Benjamin Van Durme","id":"6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a","summary":"This paper evaluates OpenAI Codex on Overnight and SMCalFlow and finds that unlike GPT-3, Codex performs similarly when targeting meaning representations directly, perhaps because meaning representations are structured similar to code in these datasets.","score":2},{"url":"https://www.semanticscholar.org/paper/53c0abe83fe9b4fdaf2208295d8504fcf5241694","title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":118,"citationCount":90,"influentialCitationCount":12,"publicationDate":"16/01/2022","authors":"Tianbao Xie,Chen Henry Wu,Peng Shi,Ruiqi Zhong,Torsten Scholak,Michihiro Yasunaga,Chien-Sheng Wu,Ming Zhong,Pengcheng Yin,Sida I. Wang,Victor Zhong,Bailin Wang,Chengzu Li,Connor Boyle,Ansong Ni,Ziyu Yao,Dragomir R. Radev,Caiming Xiong,Lingpeng Kong,Rui Zhang,Noah A. Smith,Luke Zettlemoyer,Tao Yu","id":"53c0abe83fe9b4fdaf2208295d8504fcf5241694","summary":"The UnifiedSKG framework is proposed, which unifies 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset.","score":2},{"url":"https://www.semanticscholar.org/paper/92a8f7f09f3705cb5a6009a42220a6f01ea084e8","title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents","venue":"International Conference on Machine Learning","year":2022,"referenceCount":55,"citationCount":92,"influentialCitationCount":13,"publicationDate":"18/01/2022","authors":"Wenlong Huang,P. Abbeel,Deepak Pathak,Igor Mordatch","id":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","summary":"This paper investigates the possibility of grounding high-level tasks, expressed in natural language, to a chosen set of actionable steps and proposes a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions.","score":2},{"url":"https://www.semanticscholar.org/paper/b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models","venue":"International Conference on Learning Representations","year":2022,"referenceCount":31,"citationCount":32,"influentialCitationCount":4,"publicationDate":"26/01/2022","authors":"Gabriel Poesia,Oleksandr Polozov,Vu Le,A. Tiwari,Gustavo Soares,Christopher Meek,Sumit Gulwani","id":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","summary":"A framework for substantially improving the reliability of pre-trained models for code generation and observing substantial complementary gains from CSD and TST in prediction accuracy and in effectively preventing run-time errors is proposed.","score":2},{"url":"https://www.semanticscholar.org/paper/7428f9b16a82839e2cb6e6c7a77c1ffeab898813","title":"HEAT: Hyperedge Attention Networks","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":3,"influentialCitationCount":0,"publicationDate":"28/01/2022","authors":"Dobrik Georgiev,Marc Brockschmidt,Miltiadis Allamanis","id":"7428f9b16a82839e2cb6e6c7a77c1ffeab898813","summary":"This work presents HEAT, a neural model capable of representing typed and qualified hypergraphs, where each hyperedge explicitly qualifies how participating nodes con-tribute is treated, which can be viewed as a generalization of both message passing neural networks and Transformers.","score":2},{"url":"https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models","venue":"ArXiv","year":2022,"referenceCount":103,"citationCount":338,"influentialCitationCount":92,"publicationDate":"28/01/2022","authors":"Jason Wei,Xuezhi Wang,Dale Schuurmans,Maarten Bosma,E. Chi,Quoc Le,Denny Zhou","id":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","summary":"Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/763792c655e591c5d61f67d7ac9cbecbcb5f4508","title":"Pop Quiz! Can a Large Language Model Help With Reverse Engineering?","venue":"ArXiv","year":2022,"referenceCount":27,"citationCount":4,"influentialCitationCount":0,"publicationDate":"02/02/2022","authors":"H. Pearce,B. Tan,P. Krishnamurthy,F. Khorrami,R. Karri,Brendan Dolan-Gavitt","id":"763792c655e591c5d61f67d7ac9cbecbcb5f4508","summary":"An extensive quantitative analysis of the measured performance of the language model on a set of program purpose identification and information extraction tasks shows that LLMs are not yet ready for zero-shot reverse engineering.","score":2},{"url":"https://www.semanticscholar.org/paper/9ac5e1ffa8f837671a89354d4e298b1adeb08d79","title":"Enabling Automatic Repair of Source Code Vulnerabilities Using Data-Driven Methods","venue":"2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/02/2022","authors":"Anastasiia Grishina","id":"9ac5e1ffa8f837671a89354d4e298b1adeb08d79","summary":"This work proposes ways to improve code representations for vulnerability repair from three perspectives: input data type, data-driven models, and downstream tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/9cbc044e315cdefe9a255119037ac7c23e9abdd5","title":"Predictability and Surprise in Large Generative Models","venue":"Conference on Fairness, Accountability and Transparency","year":2022,"referenceCount":92,"citationCount":27,"influentialCitationCount":2,"publicationDate":"15/02/2022","authors":"Deep Ganguli,Danny Hernandez,Liane Lovitt,Nova DasSarma,T. Henighan,Andy Jones,Nicholas Joseph,John Kernion,Benjamin Mann,Amanda Askell,Yuntao Bai,Anna Chen,Tom Conerly,Dawn Drain,Nelson Elhage,Sheer El Showk,Stanislav Fort,Zac Hatfield-Dodds,Scott Johnston,S. Kravec,Neel Nanda,Kamal Ndousse,Catherine Olsson,Daniela Amodei,Dario Amodei,Tom B. Brown,Jared Kaplan,Sam McCandlish,C. Olah,Jack Clark","id":"9cbc044e315cdefe9a255119037ac7c23e9abdd5","summary":"This paper highlights a counterintuitive property of large-scale generative models, which have a paradoxical combination of predictable loss on a broad training distribution, and unpredictable specific capabilities, inputs, and outputs, and analyzed how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment.","score":2},{"url":"https://www.semanticscholar.org/paper/76f023c3a819fc58989a064a1b50825b11fce95d","title":"Capturing Failures of Large Language Models via Human Cognitive Biases","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":7,"influentialCitationCount":0,"publicationDate":"24/02/2022","authors":"Erik Jones,J. Steinhardt","id":"76f023c3a819fc58989a064a1b50825b11fce95d","summary":"The results indicate that experimental methodology from cognitive science can help characterize how machine learning systems behave, and draw inspiration from human cognitive biases as motivation to generate hypotheses for problems that models may have and develop experiments that elicit these problems.","score":2},{"url":"https://www.semanticscholar.org/paper/d26fe2a7a7cc940d8485488e97460b144dc7d69e","title":"From Natural Language to Simulations: Applying GPT-3 Codex to Automate Simulation Modeling of Logistics Systems","venue":"SSRN Electronic Journal","year":2022,"referenceCount":60,"citationCount":1,"influentialCitationCount":0,"publicationDate":"24/02/2022","authors":"I. Jackson,M. J. Sáenz","id":"d26fe2a7a7cc940d8485488e97460b144dc7d69e","summary":"This work demonstrated that the framework built on top of the GPT-3 Codex, a Transformer-based language model, could produce functionally valid simulations of queuing and inventory control systems given the verbal description.","score":2},{"url":"https://www.semanticscholar.org/paper/2b556fcf2ac634f03c1fb0ace5e602e829418e65","title":"ReACC: A Retrieval-Augmented Code Completion Framework","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":70,"citationCount":15,"influentialCitationCount":2,"publicationDate":"15/03/2022","authors":"Shuai Lu,Nan Duan,Hojae Han,Daya Guo,Seung-won Hwang,Alexey Svyatkovskiy","id":"2b556fcf2ac634f03c1fb0ace5e602e829418e65","summary":"This work proposes a retrieval-augmented code completion framework, leveraging both lexical copying and referring to code with similar semantics by retrieval, and adopts a stage-wise training approach that combines a source code retriever and an auto-regressive language model for programming language.","score":2},{"url":"https://www.semanticscholar.org/paper/c347093e2dca530ce347526380b0b7aedf03a6b2","title":"CrossBeam: Learning to Search in Bottom-Up Program Synthesis","venue":"International Conference on Learning Representations","year":2022,"referenceCount":56,"citationCount":5,"influentialCitationCount":0,"publicationDate":"20/03/2022","authors":"Kensen Shi,H. Dai,Kevin Ellis,Charles Sutton","id":"c347093e2dca530ce347526380b0b7aedf03a6b2","summary":"This work uses a neural model to learn a hands-on search policy for bottom-up synthesis, instead of relying on a combinatorial search algorithm, and observes that CROSSBEAM learns to search efficiently, exploring much smaller portions of the program space compared to the state-of-the-art.","score":2},{"url":"https://www.semanticscholar.org/paper/fb5c11bbf63884f75d2da615fbf37a3bcfa2bd20","title":"Wordcraft: Story Writing With Large Language Models","venue":"International Conference on Intelligent User Interfaces","year":2022,"referenceCount":41,"citationCount":16,"influentialCitationCount":1,"publicationDate":"22/03/2022","authors":"Ann Yuan,Andy Coenen,Emily Reif,Daphne Ippolito","id":"fb5c11bbf63884f75d2da615fbf37a3bcfa2bd20","summary":"This work built Wordcraft, a text editor in which users collaborate with a generative language model to write a story, and shows that large language models enable novel co-writing experiences.","score":2},{"url":"https://www.semanticscholar.org/paper/237f5ca6fcccef2b77a2212b34fb06a1dbd09b72","title":"Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting","venue":"ArXiv","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/03/2022","authors":"Gabriel Orlanski","id":"237f5ca6fcccef2b77a2212b34fb06a1dbd09b72","summary":"Collect and standardize prompts from a diverse range of tasks for use with tasks they were not designed for and evaluate these prompts across multiple choice datasets for a quantitative analysis of how certain attributes of a prompt affect performance.","score":2},{"url":"https://www.semanticscholar.org/paper/2ba7104f7b93d77940312664f3467b8f090d6d16","title":"On Distribution Shift in Learning-based Bug Detectors","venue":"International Conference on Machine Learning","year":2022,"referenceCount":49,"citationCount":5,"influentialCitationCount":1,"publicationDate":"21/04/2022","authors":"Jingxuan He,Luca Beurer-Kellner,Martin T. Vechev","id":"2ba7104f7b93d77940312664f3467b8f090d6d16","summary":"This work proposes to train a bug detector in two phases, first on a synthetic bug distribution to adapt the model to the bug detection domain, and then on a real bug distributionTo drive the model towards the real distribution, which leverage a multi-task hierarchy, focal loss, and contrastive learning to further boost performance.","score":2},{"url":"https://www.semanticscholar.org/paper/6050454e0446a3068617f73b0301453f3f67844d","title":"Stylette: Styling the Web with Natural Language","venue":"International Conference on Human Factors in Computing Systems","year":2022,"referenceCount":65,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/04/2022","authors":"Tae Soo Kim,Yoonseo Choi,D. Choi,Juho Kim","id":"6050454e0446a3068617f73b0301453f3f67844d","summary":"Stylette is a browser extension that enables users to change the style of websites by expressing goals in natural language, and shows that Stylette lowered the learning curve, helping participants perform styling changes 35% faster than those using developer tools.","score":2},{"url":"https://www.semanticscholar.org/paper/4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models","venue":"CHI Extended Abstracts","year":2022,"referenceCount":53,"citationCount":38,"influentialCitationCount":5,"publicationDate":"27/04/2022","authors":"Ganesha Upadhyaya,Anastasia Reinhardt,Hridesh Rajan,Miryung Kim,Elena L. Glassman,B. Hartmann,Joseph Pinedo","id":"4054fc9e8776dc0324cfc215462d606eb75916c0","summary":"It was found that, while Copilot did not necessarily improve the task completion time or success rate, most participants preferred to use Copilot in daily programming tasks, since Copilot often provided a useful starting point and saved the effort of searching online.","score":2},{"url":"https://www.semanticscholar.org/paper/4bc040835fbff57ce6612306d794b8c6c8226086","title":"Discovering the Syntax and Strategies of Natural Language Programming with Generative Language Models","venue":"International Conference on Human Factors in Computing Systems","year":2022,"referenceCount":36,"citationCount":11,"influentialCitationCount":2,"publicationDate":"29/04/2022","authors":"Ellen Jiang,Edwin Toh,A. Molina,Kristen Olson,Claire Kayacik,Aaron Donsbach,Carrie J. Cai,Michael Terry","id":"4bc040835fbff57ce6612306d794b8c6c8226086","summary":"A natural language code synthesis tool, GenLine, backed by a large generative language model and a set of task-specific prompts that create or change code is presented, indicating that while naturallanguage code synthesis can sometimes provide a magical experience, participants still faced challenges.","score":2},{"url":"https://www.semanticscholar.org/paper/f8aa0cab09bc0668276e2cb5690bae47fa50d350","title":"An Initial Look at Self-Reprogramming Artificial Intelligence","venue":"ArXiv","year":2022,"referenceCount":14,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Alex Sheng","id":"f8aa0cab09bc0668276e2cb5690bae47fa50d350","summary":"This paper develops and experimentally validate the first fully self-reprogramming AI system with the ability to continuously modify and rewrite its own neural network source code.","score":2},{"url":"https://www.semanticscholar.org/paper/1f3a97d255a6d0b2d5535af75737ed43b1e96fc4","title":"Is GitHub Copilot a Substitute for Human Pair-programming? An Empirical Study","venue":"2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)","year":2022,"referenceCount":11,"citationCount":10,"influentialCitationCount":0,"publicationDate":"01/05/2022","authors":"Saki Imai","id":"1f3a97d255a6d0b2d5535af75737ed43b1e96fc4","summary":"The results suggest that although Copilot increases productivity as measured by Lines of code added, the quality of code produced is inferior by having more lines of code deleted in the subsequent trial.","score":2},{"url":"https://www.semanticscholar.org/paper/cdfe9580f63070f311151444f9df32818cc858bf","title":"An Empirical Evaluation of GitHub Copilot's Code Suggestions","venue":"IEEE Working Conference on Mining Software Repositories","year":2022,"referenceCount":29,"citationCount":22,"influentialCitationCount":3,"publicationDate":"01/05/2022","authors":"N. Nguyen,Sarah Nadi","id":"cdfe9580f63070f311151444f9df32818cc858bf","summary":"Overall, Copilot's suggestions have low complexity with no notable differences between the programming languages and some potential Copilot shortcomings are found.","score":2},{"url":"https://www.semanticscholar.org/paper/5922f437512158970c417f4413bface021df5f78","title":"A Generalist Agent","venue":"ArXiv","year":2022,"referenceCount":102,"citationCount":133,"influentialCitationCount":19,"publicationDate":"12/05/2022","authors":"S. Reed,Konrad Zolna,Emilio Parisotto,Sergio Gomez Colmenarejo,Alexander Novikov,Gabriel Barth-Maron,Mai Gimenez,Yury Sulsky,Jackie Kay,J. T. Springenberg,Tom Eccles,Jake Bruce,Ali Razavi,Ashley D. Edwards,N. Heess,Yutian Chen,R. Hadsell,Oriol Vinyals,Mahyar Bordbar,N. D. Freitas","id":"5922f437512158970c417f4413bface021df5f78","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/c61ce808818308566124df2c8725c98d6bd38dc3","title":"A Precis of Language Models are not Models of Language","venue":"ArXiv","year":2022,"referenceCount":16,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/05/2022","authors":"C. Veres","id":"c61ce808818308566124df2c8725c98d6bd38dc3","summary":"It is shown that despite their many successes at performing linguistic tasks, Large Neural Language Models are ill suited as comprehensive models of natural language.","score":2},{"url":"https://www.semanticscholar.org/paper/58aacb967cc7fc25cfc9d51b7ad3e57ac00d119b","title":"Can Foundation Models Wrangle Your Data?","venue":"Proceedings of the VLDB Endowment","year":2022,"referenceCount":91,"citationCount":7,"influentialCitationCount":1,"publicationDate":"20/05/2022","authors":"A. Narayan,Ines Chami,Laurel J. Orr,Christopher R'e","id":"58aacb967cc7fc25cfc9d51b7ad3e57ac00d119b","summary":"It is found that large FMs generalize and achieve SoTA performance on data cleaning and integration tasks, even though they are not trained for these data tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/636f854b1a3a983e6803eae0277179596cc2cb95","title":"Summarize and Generate to Back-translate: Unsupervised Translation of Programming Languages","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/05/2022","authors":"Wasi Uddin Ahmad,Saikat Chakraborty,Baishakhi Ray,Kai-Wei Chang","id":"636f854b1a3a983e6803eae0277179596cc2cb95","summary":"This work proposes a method for performing back-translation via code summarization and generation of natural language summaries given code snippets that performs competitively with state-of-the-art methods.","score":2},{"url":"https://www.semanticscholar.org/paper/f1b6b34b4440a77ba86493f7062e8974062508c5","title":"Applying genetic programming to PSB2: the next generation program synthesis benchmark suite","venue":"Genetic Programming and Evolvable Machines","year":2022,"referenceCount":62,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/06/2022","authors":"Thomas Helmuth,Peter Kelly","id":"f1b6b34b4440a77ba86493f7062e8974062508c5","summary":"25 new general program synthesis benchmark problems that make up PSB2, a new benchmark suite curated from a variety of sources, including programming katas and college courses are described.","score":2},{"url":"https://www.semanticscholar.org/paper/8c7fd98b6bc4f32772e76471afe8babc323f10d3","title":"CitySpec: An Intelligent Assistant System for Requirement Specification in Smart Cities","venue":"International Conference on Smart Computing","year":2022,"referenceCount":18,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/06/2022","authors":"Zirong Chen,Isaac Li,Haoxiang Zhang,S. Preum,J. Stankovic,Meiyi Ma","id":"8c7fd98b6bc4f32772e76471afe8babc323f10d3","summary":"This work builds CitySpec, the first intelligent assistant system for requirement specification in smart cities, and builds a translation model and enhance it through requirement synthesis and develops a novel online learning framework with validation under uncertainty.","score":2},{"url":"https://www.semanticscholar.org/paper/40edfa97cd02268fccff75eb9c693b11c1a968b2","title":"Formal Specifications from Natural Language","venue":"ArXiv","year":2022,"referenceCount":97,"citationCount":1,"influentialCitationCount":0,"publicationDate":"04/06/2022","authors":"Christopher Hahn,Frederik Schmitt,Julia J. Tillman,Niklas Metzger,Julian Siber,B. Finkbeiner","id":"40edfa97cd02268fccff75eb9c693b11c1a968b2","summary":"These experiments show that language models maintain their generalization capabilities from pre-trained knowledge of natural language to generalize, e.g., to new variable names or operator descriptions, and achieve competitive performance, and even outperform the state-of-the-art for translating into regular expressions.","score":2},{"url":"https://www.semanticscholar.org/paper/a64871352f8ac7f8aa226fda5cce70251a18a4fc","title":"Assessing Project-Level Fine-Tuning of ML4SE Models","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/06/2022","authors":"Egor Bogomolov,Sergey Zhuravlev,Egor Spirin,T. Bryksin","id":"a64871352f8ac7f8aa226fda5cce70251a18a4fc","summary":"It is shown that per-project fine-tuning can greatly improve the models’ quality as they capture the project’s domain and naming conventions.","score":2},{"url":"https://www.semanticscholar.org/paper/9aed848be4e9e401b0e61a2e5d60dbdafa0c6cc1","title":"A Dataset and Benchmark for Automatically Answering and Generating Machine Learning Final Exams","venue":"ArXiv","year":2022,"referenceCount":15,"citationCount":2,"influentialCitationCount":1,"publicationDate":"11/06/2022","authors":"Sarah Zhang,Reece Shuttleworth,Derek Austin,Yann Hicke,Leonard Tang,Sathwik Karnik,Darnell Granberry,Iddo Drori","id":"9aed848be4e9e401b0e61a2e5d60dbdafa0c6cc1","summary":"A student survey comparing the quality, appropriateness, andulty of machine- generated questions with human-written questions shows that across multiple aspects, machine-generated questions are indistinguishable from human-generated Questions and are suitable for ﬁnal exams.","score":2},{"url":"https://www.semanticscholar.org/paper/52ca0f589fdcde1c45fd6dfb1b72248d4ecaefc0","title":"Code Translation with Compiler Representations","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":2,"influentialCitationCount":0,"publicationDate":"30/06/2022","authors":"Marc Szafraniec,Baptiste Rozière,Hugh Leather Francois Charton,Patrick Labatut,Gabriel Synnaeve","id":"52ca0f589fdcde1c45fd6dfb1b72248d4ecaefc0","summary":"This paper proposes to augment code translation with IRs, speciﬁcally LLVM IR, with results on the C++, Java, Rust, and Go languages, and improves upon the state of the art for unsupervised code translation.","score":2},{"url":"https://www.semanticscholar.org/paper/53661ff6fdbfb8557c5b19895fad151792c62da7","title":"Few-shot training LLMs for project-specific code-summarization","venue":"International Conference on Automated Software Engineering","year":2022,"referenceCount":25,"citationCount":2,"influentialCitationCount":0,"publicationDate":"09/07/2022","authors":"Toufique Ahmed,Prem Devanbu","id":"53661ff6fdbfb8557c5b19895fad151792c62da7","summary":"This paper investigates the use few-shot training with the very large GPT (Generative Pre-trained Transformer) Codex model, and finds evidence suggesting that one can significantly surpass state-of-the-art models for code-summarization, leveraging project-specific training.","score":2},{"url":"https://www.semanticscholar.org/paper/f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":19,"influentialCitationCount":5,"publicationDate":"11/07/2022","authors":"Cem Anil,Yuhuai Wu,Anders Andreassen,Aitor Lewkowycz,Vedant Misra,V. Ramasesh,Ambrose Slone,Guy Gur-Ari,Ethan Dyer,Behnam Neyshabur","id":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","summary":"It is shown that combining pretrained large language models’ in-context learning abilities with scratchpad prompting results in a dramatic improvement in length generalization, and is run to identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.","score":2},{"url":"https://www.semanticscholar.org/paper/e37155d21818513bd40d64ee212099aac82bd6f8","title":"Less training, more repairing please: revisiting automated program repair via zero-shot learning","venue":"ESEC/SIGSOFT FSE","year":2022,"referenceCount":77,"citationCount":9,"influentialCitationCount":1,"publicationDate":"17/07/2022","authors":"Chun Xia,Lingming Zhang","id":"e37155d21818513bd40d64ee212099aac82bd6f8","summary":"This paper proposesAlphaRepair, the first cloze-style (or infilling-style) APR approach to directly leveraging large pre-trained code models for APR without any fine-tuning/retraining on historical bug fixes, and implements AlphaRepair as a practical multilingual APR tool based on the recent CodeBERT model.","score":2},{"url":"https://www.semanticscholar.org/paper/2f2750b48a6f958ff12cba90e99695123d1e2f47","title":"Using pre-trained language models to resolve textual and semantic merge conflicts (experience paper)","venue":"International Symposium on Software Testing and Analysis","year":2022,"referenceCount":29,"citationCount":2,"influentialCitationCount":0,"publicationDate":"18/07/2022","authors":"Jialu Zhang,Todd Mytkowicz,Mike Kaufman,R. Piskac,Shuvendu K. Lahiri","id":"2f2750b48a6f958ff12cba90e99695123d1e2f47","summary":"The feasibility of automatically repairing merge conflicts using k-shot learning with pre-trained large neural language models (LM) such as GPT-3 is explored, and LMs provide the state-of-the-art (SOTA) performance on semantic merge conflict resolution for Edge compared to earlier symbolic approaches.","score":2},{"url":"https://www.semanticscholar.org/paper/1e3ec5709b2ca43233c566cd77dbabbd6892bfa6","title":"Neurosymbolic repair for low-code formula languages","venue":"Proc. ACM Program. Lang.","year":2022,"referenceCount":69,"citationCount":3,"influentialCitationCount":0,"publicationDate":"24/07/2022","authors":"Rohan Bavishi,Harshit Joshi,Jos'e Pablo Cambronero S'anchez,Anna Fariha,Sumit Gulwani,Vu Le,Ivan Radicek,A. Tiwari","id":"1e3ec5709b2ca43233c566cd77dbabbd6892bfa6","summary":"LaMirage, a LAst-MIle RepAir-engine GEnerator that combines symbolic and neural techniques to perform last-mile repair in low-code formula languages, is developed and compared to state-of-the-art neural and symbolic approaches on 400 real Excel and Power Fx formulas, where LaMirage outperforms all baselines.","score":2},{"url":"https://www.semanticscholar.org/paper/ec4c8d99eb1c028c43af6d8bbf727392d351cb59","title":"Efficient Training of Language Models to Fill in the Middle","venue":"ArXiv","year":2022,"referenceCount":60,"citationCount":14,"influentialCitationCount":1,"publicationDate":"28/07/2022","authors":"Mohammad Bavarian,Heewoo Jun,N. Tezak,J. Schulman,C. McLeavey,Jerry Tworek,Mark Chen","id":"ec4c8d99eb1c028c43af6d8bbf727392d351cb59","summary":"There is extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales.","score":2},{"url":"https://www.semanticscholar.org/paper/def2e28863338cb20782eb2015a39d32df697ed6","title":"Learning to Improve Code Efficiency","venue":"ArXiv","year":2022,"referenceCount":35,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/08/2022","authors":"Bing Chen,Daniel Tarlow,Kevin Swersky,M. Maas,P. Heiber,Ashish Naik,Milad Hashemi,P. Ranganathan","id":"def2e28863338cb20782eb2015a39d32df697ed6","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/8c2d9d2aa891e3b53bbd9166c1b804a0741fc44a","title":"Finding Reusable Machine Learning Components to Build Programming Language Processing Pipelines","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":1,"influentialCitationCount":0,"publicationDate":"11/08/2022","authors":"Patrick Flynn,T. Vanderbruggen,C. Liao,Pei-Hung Lin,M. Emani,Xipeng Shen","id":"8c2d9d2aa891e3b53bbd9166c1b804a0741fc44a","summary":"To improve theability, accessibility, interoperability and reusability (FAIRness) of machine learning components, a set of representative papers in the domain of machineLearning-based PLP are collected and analyzed.","score":2},{"url":"https://www.semanticscholar.org/paper/9b61de7038290751377b64293baaf42f3e7cf441","title":"An Empirical Evaluation of Competitive Programming AI: A Case Study of AlphaCode","venue":"International Workshop on Software Clones","year":2022,"referenceCount":35,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/08/2022","authors":"Sila Lertbanjongngam,Bodin Chinthanet,T. Ishio,R. Kula,P. Leelaprute,Bundit Manaskasemsak,A. Rungsawang,Kenichi Matsumoto","id":"9b61de7038290751377b64293baaf42f3e7cf441","summary":"An empirical study to find code similarities and performance differences between AlphaCode-generated codes and human codes shows that the generated codes are similar to human codes and the generated code performs on par with or worse than the human code in terms of execution time and memory usage.","score":2},{"url":"https://www.semanticscholar.org/paper/befde3e07ce97f02f12fe92ab27e99f23ccd17aa","title":"Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation","venue":"medRxiv","year":2022,"referenceCount":33,"citationCount":1,"influentialCitationCount":0,"publicationDate":"31/08/2022","authors":"F. Yu,M. Endo,R. Krishnan,I. Pan,A. Tsai,E. P. Reis,E. Fonseca,H. M. H. Lee,Z. H. Abad,A. Y. Ng,C. Langlotz,V. Venugopal,P. Rajpurkar","id":"befde3e07ce97f02f12fe92ab27e99f23ccd17aa","summary":"This study quantitatively examines the correlation between automated metrics and the scoring of reports by radiologists, and proposes a composite metric, called RadCliQ, that is able to rank the quality of reports similarly to radiologists and better than existing metrics.","score":2},{"url":"https://www.semanticscholar.org/paper/6b5d1e50894b1f28e4798cf20e9ffa88b9ec011a","title":"How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models","venue":"ArXiv","year":2022,"referenceCount":22,"citationCount":2,"influentialCitationCount":0,"publicationDate":"03/09/2022","authors":"Hai Dang,Lukas Mecke,Florian Lehmann,Sven Goller,D. Buschek","id":"6b5d1e50894b1f28e4798cf20e9ffa88b9ec011a","summary":"This work discusses the key opportunities and challenges for interactive creative applications that use prompting as a new paradigm for Human-AI interaction and proposes four design goals for user interfaces that support prompting.","score":2},{"url":"https://www.semanticscholar.org/paper/9360390b02b9a09ece9a2486055b17e18dc5d3f6","title":"Automatic Code Documentation Generation Using GPT-3","venue":"International Conference on Automated Software Engineering","year":2022,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/09/2022","authors":"Junaed Younus Khan,Gias Uddin","id":"9360390b02b9a09ece9a2486055b17e18dc5d3f6","summary":"Codec is a GPT-3 based model pre-trained on both natural and programming languages that outperforms existing techniques even with basic settings like one-shot learning and achieves an overall BLEU score of 20.6 for six different programming languages.","score":2},{"url":"https://www.semanticscholar.org/paper/363758e9e296adc9391ed731e834809cf5d4c19b","title":"Are machine programming systems using right source-code measures to select code repositories?","venue":"MaLTeSQuE@ESEC/SIGSOFT FSE","year":2022,"referenceCount":32,"citationCount":1,"influentialCitationCount":0,"publicationDate":"24/09/2022","authors":"N. Hasabnis","id":"363758e9e296adc9391ed731e834809cf5d4c19b","summary":"A framework to rank open-source repositories on quality, maintainability, and popularity by leveraging existing research on this topic is developed and some correlation between the quality measures used in GitRank and ControlFlag's performance is revealed, suggesting that some of the measures used by GitRank are applicable to ControlFlag.","score":2},{"url":"https://www.semanticscholar.org/paper/cd6496bc404e18a24f634e3dded2ed1cdca03e0f","title":"Learning to Learn with Generative Models of Neural Network Checkpoints","venue":"ArXiv","year":2022,"referenceCount":77,"citationCount":8,"influentialCitationCount":0,"publicationDate":"26/09/2022","authors":"William S. Peebles,Ilija Radosavovic,Tim Brooks,Alexei A. Efros,J. Malik","id":"cd6496bc404e18a24f634e3dded2ed1cdca03e0f","summary":"This model is a conditional diffusion transformer that, given an initial input parameter vector and a prompted loss, error, or return, predicts the distribution over parameter updates that achieve the desired metric.","score":2},{"url":"https://www.semanticscholar.org/paper/55e3fe05598be7c3dd357d51166869f6571b824f","title":"Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/10/2022","authors":"Mohammad Reza Taesiri,F. Macklon,Yihe Wang,Hengshuo Shen,C. Bezemer","id":"55e3fe05598be7c3dd357d51166869f6571b824f","summary":"This study explores the possibil-ity of leveraging the zero-shot capabilities of large language models for video game bug detection by formulating the bug detection problem as a question-answering task, and shows thatLarge language models can identify which event is buggy in a sequence of textual descriptions of events from a game.","score":2},{"url":"https://www.semanticscholar.org/paper/62f0db3a5ad5c795ec18fc7a6e7b01836809df57","title":"Language Models are Multilingual Chain-of-Thought Reasoners","venue":"ArXiv","year":2022,"referenceCount":50,"citationCount":17,"influentialCitationCount":1,"publicationDate":"06/10/2022","authors":"Freda Shi,Mirac Suzgun,Markus Freitag,Xuezhi Wang,Suraj Srivats,Soroush Vosoughi,Hyung Won Chung,Yi Tay,Sebastian Ruder,Denny Zhou,Dipanjan Das,Jason Wei","id":"62f0db3a5ad5c795ec18fc7a6e7b01836809df57","summary":"It is shown that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment, and that models have strikingly strong mult bilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili.","score":2},{"url":"https://www.semanticscholar.org/paper/259b7a01700c39d5669e88d1434873ea38a13528","title":"In-Context Policy Iteration","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/10/2022","authors":"Ethan Brooks,Logan Walls,Richard L. Lewis,Satinder Singh","id":"259b7a01700c39d5669e88d1434873ea38a13528","summary":"An algorithm, ICPI, that learns to perform RL tasks without expert demonstrations or gradients, and is presented as a policy-iteration method in which the prompt content is the entire locus of learning.","score":2},{"url":"https://www.semanticscholar.org/paper/5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","title":"Scaling Instruction-Finetuned Language Models","venue":"ArXiv","year":2022,"referenceCount":104,"citationCount":36,"influentialCitationCount":10,"publicationDate":"20/10/2022","authors":"Hyung Won Chung,Le Hou,S. Longpre,Barret Zoph,Yi Tay,W. Fedus,Eric Li,Xuezhi Wang,M. Dehghani,Siddhartha Brahma,Albert Webson,S. Gu,Zhuyun Dai,Mirac Suzgun,Xinyun Chen,Aakanksha Chowdhery,Dasha Valter,Sharan Narang,Gaurav Mishra,A. Yu,Vincent Zhao,Yanping Huang,Andrew M. Dai,Hongkun Yu,Slav Petrov,E. Chi,J. Dean,Jacob Devlin,Adam Roberts,Denny Zhou,Quoc Le,Jason Wei","id":"5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","summary":"This result shows that instruction and UL2 continued pre-training are complementary compute-eﬃcient methods to improve the performance of language models without increasing model scale.","score":2},{"url":"https://www.semanticscholar.org/paper/d26f616699a122e5455a13189e276002ee4cf923","title":"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":2,"influentialCitationCount":0,"publicationDate":"21/10/2022","authors":"Albert Qiaochu Jiang,S. Welleck,J. Zhou,Wenda Li,Jiacheng Liu,M. Jamnik,Timothée Lacroix,Yuhuai Wu,Guillaume Lample","id":"d26f616699a122e5455a13189e276002ee4cf923","summary":"This work introduces Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems.","score":2},{"url":"https://www.semanticscholar.org/paper/472d87be3dc298102e058be55a814cc6d2085b39","title":"Towards Deceptive Defense in Software Security with Chaff Bugs","venue":"International Symposium on Recent Advances in Intrusion Detection","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Zhenghao Hu,Yu Hu,Brendan Dolan-Gavitt","id":"472d87be3dc298102e058be55a814cc6d2085b39","summary":"A new defensive technique called chaff bugs is proposed, which instead targets the bug discovery and exploit creation stages of this process, and can serve as an effective deterrent against both human attackers and automated bug-finding tools.","score":2},{"url":"https://www.semanticscholar.org/paper/ac3d7ae8b4acb137492d4a8d8bcff480b89fa000","title":"Programming Pedagogy and Assessment in the Era of AI/ML: A Position Paper","venue":"Compute","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/11/2022","authors":"A. Raman,Viraj Kumar","id":"ac3d7ae8b4acb137492d4a8d8bcff480b89fa000","summary":"This work surveys recent research on automated systems for writing code, and examines the components of the code-writing task using a six-step framework proposed in the literature, and identifies the impact of automated systems at each step.","score":2},{"url":"https://www.semanticscholar.org/paper/632ab7663e6d64578ceda1d1df9ec525b503bacb","title":"Steps towards prompt-based creation of virtual worlds","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/11/2022","authors":"Jasmine Roberts,Andrzej Banburski-Fahey,J. Lanier","id":"632ab7663e6d64578ceda1d1df9ec525b503bacb","summary":"This work shows that prompt-based methods can both accelerate in-VR level editing, as well as can become part of gameplay rather than just part of game development.","score":2},{"url":"https://www.semanticscholar.org/paper/048ed70192de0232086eb32a95ffb3be8d336c76","title":"Metaphors We Learn By","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/11/2022","authors":"R. Memisevic","id":"048ed70192de0232086eb32a95ffb3be8d336c76","summary":"This essay relates parameter sharing (“weight sharing”) to analogy making and the school of thought of cognitive metaphor, and discusses how recurrent and auto-regressive models can be thought of as extending analogy making from static features to dynamic skills and procedures.","score":2},{"url":"https://www.semanticscholar.org/paper/403028df4fe52786f1748f1c314b6eb2cf867197","title":"CLAWSAT: Towards Both Robust and Accurate Code Models","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Jinghan Jia,Shashank Srikant,Tamara Mitrovska,Chuang Gan,Shiyu Chang,Sijia Liu,Una-May O’Reilly","id":"403028df4fe52786f1748f1c314b6eb2cf867197","summary":"This is the first systematic study to explore and exploit the robustness and accuracy benefits of (multi-view) code obfuscations in code models and demonstrate the effectiveness of adversarial learning in CLAW.","score":2},{"url":"https://www.semanticscholar.org/paper/477d0b2abf07ee92732698f9aeb3c784f28ffa88","title":"On the Security Vulnerabilities of Text-to-SQL Models","venue":"ArXiv","year":2022,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/11/2022","authors":"Xutan Peng,Yipeng Zhang,Jingfeng Yang,Mark Stevenson","id":"477d0b2abf07ee92732698f9aeb3c784f28ffa88","summary":"Vulnerability tests on Text-to-SQL, a technique that builds natural language interfaces for databases, showed that the Text- to-SQL modules of two commercial black boxes can be manipulated to produce malicious code, potentially leading to data breaches and Denial of Service.","score":2},{"url":"https://www.semanticscholar.org/paper/6d4a12ea469ff08634eeb24c47b265a7dca2fce2","title":"PADL: Language-Directed Physics-Based Character Control","venue":"ACM SIGGRAPH Conference and Exhibition on Computer Graphics and Interactive Techniques in Asia","year":2022,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/11/2022","authors":"Jordan Juravsky,Yunrong Guo,S. Fidler,X. B. Peng","id":"6d4a12ea469ff08634eeb24c47b265a7dca2fce2","summary":"PADL, which leverages recent innovations in NLP in order to take steps towards developing language-directed controllers for physics-based character animation, is presented and it is shown that the framework can be applied to effectively direct a simulated humanoid character to perform a diverse array of complex motor skills.","score":2},{"url":"https://www.semanticscholar.org/paper/ea55ae4c82b45e3857f37406c94e9f642a2b3d84","title":"Genetic Programming with Local Scoring","venue":"ArXiv","year":2022,"referenceCount":9,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/11/2022","authors":"Max Vistrup","id":"ea55ae4c82b45e3857f37406c94e9f642a2b3d84","summary":"New techniques for synthesizing programs through se-quences of mutations, including a method of local scoring assigning a score to each expression in a program, and cyclic evolution in which programs evolve programs through phases of expansion and reduction are presented.","score":2},{"url":"https://www.semanticscholar.org/paper/4290a70025f29d7054c550c75ae6b24c38a79d12","title":"SE Factual Knowledge in Frozen Giant Code Model: A Study on FQN and its Retrieval","venue":"ArXiv","year":2022,"referenceCount":74,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"Qing Huang,Dianshu Liao,Zhenchang Xing,Zhiqiang Yuan,Qinghua Lu,Xiwei Xu,Jiaxing Lu","id":"4290a70025f29d7054c550c75ae6b24c38a79d12","summary":"This work conducts the first systematic study on the SE factual knowledge in the state-of-the-art PCM CoPilot, focusing on APIs’ Fully Qualiﬁed Names (FQNs), the fundamental knowledge for effective code analysis, search and reuse.","score":2},{"url":"https://www.semanticscholar.org/paper/ae441f7305dc2cd58c708528b3ecee3501cc5c46","title":"Plansformer: Generating Symbolic Plans using Transformers","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":1,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"Vishal Pallagani,Bharath Muppasani,K. Murugesan,F. Rossi,L. Horesh,Biplav Srivastava,F. Fabiano,Andrea Loreggia","id":"ae441f7305dc2cd58c708528b3ecee3501cc5c46","summary":"The use of LLMs for automated planning - a branch of AI concerned with the realization of action sequences (plans) to achieve a goal, typically executed by intelligent agents, autonomous robots, and unmanned vehicles are explored.","score":2},{"url":"https://www.semanticscholar.org/paper/3b8ccc7ec80b8775de603e248ac1ca2b919d6b70","title":"Chatbots in a Botnet World","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":2,"influentialCitationCount":0,"publicationDate":"18/12/2022","authors":"Forrest McKee,David Noever","id":"3b8ccc7ec80b8775de603e248ac1ca2b919d6b70","summary":"Questions-and-answer formats provide a novel experimental platform for investigating cybersecurity questions and illustrate cases that support the broad gain of functionality, including self-replication and self-modification, evasion, and strategic understanding of complex cybersecurity goals.","score":2},{"url":"https://www.semanticscholar.org/paper/6756fcd998caeb7b23702e08559e63710179334c","title":"Reasoning with Language Model Prompting: A Survey","venue":"ArXiv","year":2022,"referenceCount":150,"citationCount":5,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng,Chuanqi Tan,Fei Huang,Huajun Chen","id":"6756fcd998caeb7b23702e08559e63710179334c","summary":"This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting and introduces research works with comparisons and summaries and provides systematic resources to help beginners.","score":2},{"url":"https://www.semanticscholar.org/paper/a8e0ba16346b72c3a04dd0b1da84bc5f28900174","title":"Using GitHub Copilot to Solve Simple Programming Problems","venue":"","year":2023,"referenceCount":15,"citationCount":1,"influentialCitationCount":1,"publicationDate":2023,"authors":"M. Wermelinger","id":"a8e0ba16346b72c3a04dd0b1da84bc5f28900174","summary":"Evaluating Copilot, a natural language machine learning model trained on billions of lines of code, and looking qualitatively at the generated suggestions, to understand the limitations of Copilot.","score":2},{"url":"https://www.semanticscholar.org/paper/490f1e8ff352bded30bcde01d5b4769d6c2d2dd5","title":"Adversarial Attacks on Neural Models of Code via Code Difference Reduction","venue":"ArXiv","year":2023,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/01/2023","authors":"Zhao Tian,Junjie Chen,Zhi Jin","id":"490f1e8ff352bded30bcde01d5b4769d6c2d2dd5","summary":"This work proposes a novel adversarial attack technique, CODA, that uses the code differences between the target input and reference inputs to guide the generation of adversarial examples and considers both structure differences and identiﬁer differences to preserve the original semantics.","score":2},{"url":"https://www.semanticscholar.org/paper/468992bf970c37bd1fef58b78a6c2fcd8c018868","title":"Scaling Laws for Generative Mixed-Modal Language Models","venue":"ArXiv","year":2023,"referenceCount":41,"citationCount":1,"influentialCitationCount":0,"publicationDate":"10/01/2023","authors":"Armen Aghajanyan,L. Yu,Alexis Conneau,Wei-Ning Hsu,Karen Hambardzumyan,Susan Zhang,Stephen Roller,Naman Goyal,Omer Levy,Luke Zettlemoyer","id":"468992bf970c37bd1fef58b78a6c2fcd8c018868","summary":"New mixed-modal scaling laws that unify the contributions of individual modalities and the interactions between them are reported, and the optimal synergy and competition due to data and model size is explicitly model as an additive term to previous uni-modAL scaling laws.","score":2},{"url":"https://www.semanticscholar.org/paper/1f22de83d912176cb8857efa1c6d65b14d6a2f5c","title":"ChatGPT is not all you need. A State of the Art Review of large Generative AI models","venue":"ArXiv","year":2023,"referenceCount":34,"citationCount":5,"influentialCitationCount":0,"publicationDate":"11/01/2023","authors":"Roberto Gozalo-Brizuela,E.C. Garrido-Merchán","id":"1f22de83d912176cb8857efa1c6d65b14d6a2f5c","summary":"This work consists on an attempt to describe in a concise way the main models are sectors that aresector that are aﬀected by generative AI and to provide a taxonomy of the main generative models published recently.","score":2},{"url":"https://www.semanticscholar.org/paper/907a77639069bb7dd270f017068745706133cffc","title":"Inaccessible Neural Language Models Could Reinvigorate Linguistic Nativism","venue":"ArXiv","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/01/2023","authors":"Patrick Perrine","id":"907a77639069bb7dd270f017068745706133cffc","summary":"This work argues that this lack of accessibility could instill a nativist bias in researchers new to computational linguistics, and calls upon researchers to open source their LLM code wherever possible to allow both empircist and hybrid approaches to remain accessible.","score":2},{"url":"https://www.semanticscholar.org/paper/a20875e70a38cb053cd34e170038c4746f85dac9","title":"A Case Study in Engineering a Conversational Programming Assistant's Persona","venue":"ArXiv","year":2023,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/01/2023","authors":"Steven I. Ross,Michael J. Muller,Fernando Martinez,Stephanie Houde,Justin D. Weisz","id":"a20875e70a38cb053cd34e170038c4746f85dac9","summary":"The Programmer’s Assistant is an experimental prototype software development environment that integrates a chatbot with a code editor that establishes a conversational interaction pattern, a set of conventions.","score":2},{"url":"https://www.semanticscholar.org/paper/074baf835aec44a100990178859b35451975f339","title":"Navigating Complexity in Software Engineering: A Prototype for Comparing GPT-n Solutions","venue":"ArXiv","year":2023,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/01/2023","authors":"Christoph Treude","id":"074baf835aec44a100990178859b35451975f339","summary":"A work-in-progress prototype “GPTC OMPARE” is presented, which allows programmers to visually compare multiple source code solutions generated by GPT-n models for the same programming- related query by highlighting their similarities and differences.","score":2},{"url":"https://www.semanticscholar.org/paper/1d34b6cffe67077cdd4df41950b1195f09ae0cb8","title":"Hierarchical Programmatic Reinforcement Learning via Learning to Compose Programs","venue":"ArXiv","year":2023,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"Guanhui. Liu,En-Pei Hu,Pu-Jen Cheng,Hung-yi Lee,Shao-Hua Sun","id":"1d34b6cffe67077cdd4df41950b1195f09ae0cb8","summary":"This work proposes to learn a meta-policy that composes a series of programs sampled from the learned program embedding space that can produce program policies that describe out-of-distributionally complex behaviors and directly assign credits to programs that induce desired behaviors.","score":2},{"url":"https://www.semanticscholar.org/paper/861916af6428277a3ce2e18034e4b40dc6616eb9","title":"On the Robustness of Code Generation Techniques: An Empirical Study on GitHub Copilot","venue":"ArXiv","year":2023,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"A. Mastropaolo,L. Pascarella,Emanuela Guglielmi,Matteo Ciniselli,Simone Scalabrino,R. Oliveto,G. Bavota","id":"861916af6428277a3ce2e18034e4b40dc6616eb9","summary":"This paper presents an empirical study in which it is aimed at understanding whether different but semantically equivalent natural language descriptions result in the same recommended function, and investigates the extent to which predictions generated by Copilot changed.","score":2},{"url":"https://www.semanticscholar.org/paper/2af6a21a1b682ceb585165359d3605e89f4cf6b0","title":"Fixing Hardware Security Bugs with Large Language Models","venue":"ArXiv","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/02/2023","authors":"Baleegh Ahmad,Shailja Thakur,B. Tan,R. Karri,H. Pearce","id":"2af6a21a1b682ceb585165359d3605e89f4cf6b0","summary":"The results show that LLMs can repair hardware security bugs and the framework designed and implemented are an important step towards the ultimate goal of an automated end-to-end bug repair framework.","score":2},{"url":"https://www.semanticscholar.org/paper/d79e0cb459ba751e719889fbb9ae9398d35c37fb","title":"KNOD: Domain Knowledge Distilled Tree Decoder for Automated Program Repair","venue":"ArXiv","year":2023,"referenceCount":63,"citationCount":1,"influentialCitationCount":0,"publicationDate":"03/02/2023","authors":"Nan Jiang,Thibaud Lutellier,Yiling Lou,Lin Tan,Dan Goldwasser,Xiangyu Zhang","id":"d79e0cb459ba751e719889fbb9ae9398d35c37fb","summary":"KNOD is proposed, which incorporates domain knowledge to guide patch generation in a direct and comprehensive way and is evaluated on three widely-used benchmarks, outperforming all existing APR tools.","score":2},{"url":"https://www.semanticscholar.org/paper/fb243dfd1234b8f76dfda740a62402663da74085","title":"Exploring Data Augmentation for Code Generation Tasks","venue":"ArXiv","year":2023,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/02/2023","authors":"Pinzhen Chen,Gerasimos Lampouras","id":"fb243dfd1234b8f76dfda740a62402663da74085","summary":"Focusing on data utilization for downstream tasks, this work proposes and adapt augmentation methods that yield consistent improvements in code translation and summarization by up to 6.9% and 7.5% respectively.","score":2},{"url":"https://www.semanticscholar.org/paper/8927db4ee890bf42608752bb840bc9d7db556da1","title":"ChatGPT and Software Testing Education: Promises & Perils","venue":"ArXiv","year":2023,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/02/2023","authors":"Sajed Jalil,Suzzana Rafi,Thomas D. LaToza,Kevin Moran,Wing Lam","id":"8927db4ee890bf42608752bb840bc9d7db556da1","summary":"How well ChatGPT performs when tasked with solving common questions in a popular software testing curriculum is examined, and the potential promise, and dangers related to the use ofChatGPT by students and instructors are discussed.","score":2},{"url":"https://www.semanticscholar.org/paper/1586dd245ae4cca00ea519fb35f27e2a0980476d","title":"Teaching Structured Vision&Language Concepts to Vision&Language Models","venue":"ArXiv","year":2022,"referenceCount":83,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Sivan Doveh,Assaf Arbelle,Sivan Harary,R. Panda,Roei Herzig,Eli Schwartz,Donghyun Kim,R. Giryes,R. Feris,S. Ullman,Leonid Karlinsky","id":"1586dd245ae4cca00ea519fb35f27e2a0980476d","summary":"Various techniques based on language structure understanding can be used to manipulate the textual part of off-the-shelf paired VL datasets that makes more effective use of existing VL pre-training datasets and does not require any additional data.","score":1},{"url":"https://www.semanticscholar.org/paper/6d4e540e1bed26679097139bf90c8652919e4e5c","title":"Explanation Regeneration via Information Bottleneck","venue":"ArXiv","year":2022,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Qintong Li,Zhiyong Wu,Lingpeng Kong,Wei Bi","id":"6d4e540e1bed26679097139bf90c8652919e4e5c","summary":"This work develops an information bottleneck method EIB that regenerates the free-text explanation by polishing the single-pass output from the pretrained language model but retaining the information that supports the contents being explained.","score":1},{"url":"https://www.semanticscholar.org/paper/6db13f58ff662eefa823a660fa86faf8ddf75533","title":"Controllable Text Generation with Language Constraints","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Howard Chen,Huihan Li,Danqi Chen,Karthik Narasimhan","id":"6db13f58ff662eefa823a660fa86faf8ddf75533","summary":"A solution to leverage a language model’s own internal knowledge to guide generation and propose three forms of guidance (binary veriﬁer, top-k token, textual example), and employ pre-tuning approaches to distill the guidance to tackle diverse natural language constraints.","score":1},{"url":"https://www.semanticscholar.org/paper/bbe93c90b7b87939cd064c805858feca61a3234d","title":"Self-Instruct: Aligning Language Model with Self Generated Instructions","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":4,"influentialCitationCount":2,"publicationDate":"20/12/2022","authors":"Yizhong Wang,Yeganeh Kordi,Swaroop Mishra,Alisa Liu,Noah A. Smith,Daniel Khashabi,Hannaneh Hajishirzi","id":"bbe93c90b7b87939cd064c805858feca61a3234d","summary":"S ELF -I NSTRUCT provides an almost annotation-free method for aligning pre-trained language models with instructions, and is released to facili-tate future studies on instruction tuning.","score":1},{"url":"https://www.semanticscholar.org/paper/430aaacce4147faef7940d1908cf715c3938e51f","title":"Using Language Models to Convert Between Natural Language and Game Commands","venue":"","year":null,"referenceCount":13,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"Stefan Papazov,W. Gill,Marta Garcia Ferreiro,Andrew Zhu,Lara J. Martin,Chris Callison-Burch","id":"430aaacce4147faef7940d1908cf715c3938e51f","summary":"This paper looks at enhancing a Discord Bot called Avrae that is developed by D&D Beyond to help with online play, and uses GPT-3’s few shot learning and fine tuning capabilities to achieve 64% accuracy.","score":1},{"url":"https://www.semanticscholar.org/paper/134a83d99b4913af4ed265310dfaa670e2689ce0","title":"Translating Natural Language to Bash Commands using Deep Neural Networks","venue":"","year":2022,"referenceCount":9,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Daniel Jenson,Yingxiao Liu","id":"134a83d99b4913af4ed265310dfaa670e2689ce0","summary":"It was found that while cross-entropy loss decreased steadily for all models, only T5 was able to continue learning the structure of Bash commands, and after post-processing, all models improved, but only T4 and BART exceeded the performance of the GPT-3 baseline model.","score":1},{"url":"https://www.semanticscholar.org/paper/9ed21b38ed48773048a6749c48748d3b88974a17","title":"A large-scale empirical study of commit message generation: models, datasets and evaluation","venue":"Empirical Software Engineering","year":2022,"referenceCount":67,"citationCount":1,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Wei Tao,Yanlin Wang,Ensheng Shi,Lun Du,Shi Han,Hongyu Zhang,Dongmei Zhang,Wenqiang Zhang","id":"9ed21b38ed48773048a6749c48748d3b88974a17","summary":"This paper conducts a systematic and in-depth analysis of the state-of-the-art models and datasets for automatic commit message generation and collects a large-scale, information-rich, multi-programming-language, MCMD.","score":1},{"url":"https://www.semanticscholar.org/paper/e772bf6ad9b87a333291f427d328e24cc57f23a1","title":"Human perceiving behavior modeling in evaluation of code generation models","venue":"IEEE Games Entertainment Media Conference","year":2022,"referenceCount":14,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"S. Kovalchuk,Vadim Lomshakov,Artem Aliev","id":"e772bf6ad9b87a333291f427d328e24cc57f23a1","summary":"A 5-level Likert scale assessment of the model output using a perceiving model based on the Theory of Planned Behavior (TPB) shows an extension of model assessment as well as a deeper understanding of the quality and applicability of generated code for practical question answering.","score":1},{"url":"https://www.semanticscholar.org/paper/1ad47393da793fbf866c3ed071c088aa540359ac","title":"Who Evaluates the Evaluators? On Automatic Metrics for Assessing AI-based Offensive Code Generators","venue":"ArXiv","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/12/2022","authors":"Cristina Improta,Pietro Liguori,R. Natella,B. Cukic,Domenico Cotroneo","id":"1ad47393da793fbf866c3ed071c088aa540359ac","summary":"This practical experience report analyzes a large set of output similarity metrics on offensive code generators using two state-of-the-art NMT models using two datasets containing offensive assembly and Python code with their de- scriptions in the English language.","score":1},{"url":"https://www.semanticscholar.org/paper/cbbacf9ba52942fd6a43588295385e2ab39545b0","title":"Interacting with next-phrase suggestions: How suggestion systems aid and influence the cognitive processes of writing","venue":"","year":2022,"referenceCount":49,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/08/2022","authors":"Advait Bhat,Saaket Agashe,Niharika Mohile,Parth Oberoi,R. Jangir,Anirudha N. Joshi","id":"cbbacf9ba52942fd6a43588295385e2ab39545b0","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/1b13cf2971199d48722192b8c290d4c4eb63ca80","title":"Semi-Supervised Code Generation by Editing Intents","venue":"","year":2018,"referenceCount":15,"citationCount":0,"influentialCitationCount":0,"publicationDate":2018,"authors":"Edgar Chen","id":"1b13cf2971199d48722192b8c290d4c4eb63ca80","summary":"This work proposes a deep generative model to rewrite and augment utterances such as StackOverflow questions into an acceptable form by adding variable names and other important information.","score":1},{"url":"https://www.semanticscholar.org/paper/a9d240d911e338e6083cee83570c10af6f9dafb8","title":"A Comprehensive Study of StaQC for Deep Code Summarization","venue":"","year":2018,"referenceCount":51,"citationCount":3,"influentialCitationCount":0,"publicationDate":2018,"authors":"Jayavardhan Reddy Peddamail,Ziyu Yao,Zhen Wang,Huan Sun","id":"a9d240d911e338e6083cee83570c10af6f9dafb8","summary":"StaQC is described, a large-scale and high-quality dataset of <NL, code> pairs in Python and SQL domain, systematically mined from the Stack Overflow forum (SO), showing that StaQC helps achieve substantially better results.","score":1},{"url":"https://www.semanticscholar.org/paper/8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task","venue":"Conference on Empirical Methods in Natural Language Processing","year":2018,"referenceCount":45,"citationCount":427,"influentialCitationCount":114,"publicationDate":"24/09/2018","authors":"Tao Yu,Rui Zhang,Kai-Chou Yang,Michihiro Yasunaga,Dongxu Wang,Zifan Li,James Ma,Irene Z Li,Qingning Yao,Shanelle Roman,Zilin Zhang,Dragomir R. Radev","id":"8e773b1840b894603c06b677a0f15ebcf0f26378","summary":"This work defines a new complex and cross-domain semantic parsing and text-to-SQL task so that different complicated SQL queries and databases appear in train and test sets and experiments with various state-of-the-art models show that Spider presents a strong challenge for future research.","score":1},{"url":"https://www.semanticscholar.org/paper/e6b035ff8d839810593769b96edb18dd188c118c","title":"Reinforcing Diversity Company Policies: Insights from StackOverflow Developers Survey","venue":"International Conference on Enterprise Information Systems","year":2019,"referenceCount":29,"citationCount":2,"influentialCitationCount":0,"publicationDate":2019,"authors":"K. Silveira,S. Musse,I. Manssour,R. Vieira,R. Prikladnicki","id":"e6b035ff8d839810593769b96edb18dd188c118c","summary":"This work is interested in understanding the pain points in software engineering regarding diversity and provide insights to support the attraction, hiring and retention policies for more diverse software engineering environments and proposes a discussion about the unconscious bias, stereotypes, and impostor syndrome.","score":1},{"url":"https://www.semanticscholar.org/paper/970d34f38106023cfdacfb0bf59b7f3f64dcc4c3","title":"A Multi-Modal Intelligent Agent that Learns from Demonstrations and Natural Language Instructions","venue":"","year":2019,"referenceCount":146,"citationCount":0,"influentialCitationCount":0,"publicationDate":2019,"authors":"Toby Jia-Jun Li","id":"970d34f38106023cfdacfb0bf59b7f3f64dcc4c3","summary":"The preliminary lab usability evaluation results showed that the prototype of SUGILITE allowed users with little or no programming expertise to successfully teach the agent common smartphone tasks, as well as the appropriate conditionals for triggering these actions and the relevant concepts for determining these conditions.","score":1},{"url":"https://www.semanticscholar.org/paper/8cd26c197ad9635bd7508f01bdad67b6562099dd","title":"Deep Code-Comment Understanding and Assessment","venue":"IEEE Access","year":2019,"referenceCount":29,"citationCount":4,"influentialCitationCount":0,"publicationDate":2019,"authors":"Deze Wang,Yong Guo,Wei Dong,Zhiming Wang,Haoran Liu,Shanshan Li","id":"8cd26c197ad9635bd7508f01bdad67b6562099dd","summary":"This paper converts the quality assessment of code comments into a classification problem based on the multi-input neural network and concatenates the code vectors as the input of the Multiple-Layer Perceptron classifier for the comment quality assessment.","score":1},{"url":"https://www.semanticscholar.org/paper/4fbddbe0f23f76bd779f8d1e524374d6bf1bea81","title":"Learning to Map Natural Language to General Purpose Source Code","venue":"","year":2019,"referenceCount":161,"citationCount":0,"influentialCitationCount":0,"publicationDate":2019,"authors":"Srini Iyer","id":"4fbddbe0f23f76bd779f8d1e524374d6bf1bea81","summary":"This dissertation presents efficient deep learning models and training paradigms to map language to general purpose source code that will enable numerous applications for non-expert users as well as developers.","score":1},{"url":"https://www.semanticscholar.org/paper/4fdbf9af4058ff17c31db8bc8ca751d69b90ae43","title":"Semantic Parser and Natural Language Generator via Dual Information Maximization","venue":"","year":2019,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":2019,"authors":"Hai Ye","id":"4fdbf9af4058ff17c31db8bc8ca751d69b90ae43","summary":"This paper proposes the method of dual information maximization (DIM) to regularize the learning process, where DIM empirically maximizes the variational lower bounds of expected joint distributions of NL and MRs.","score":1},{"url":"https://www.semanticscholar.org/paper/3092325d55f6aa9ba28b0841bdcfd61991a38d48","title":"A Neural Model for Generating Natural Language Summaries of Program Subroutines","venue":"International Conference on Software Engineering","year":2019,"referenceCount":59,"citationCount":169,"influentialCitationCount":40,"publicationDate":"05/02/2019","authors":"Alexander LeClair,Siyuan Jiang,Collin McMillan","id":"3092325d55f6aa9ba28b0841bdcfd61991a38d48","summary":"This paper presents a neural model that combines words from code with code structure from an AST, which allows the model to learn code structure independent of the text in code.","score":1},{"url":"https://www.semanticscholar.org/paper/f44ceb54fa773920b767c1e93ea0bc8725f248df","title":"Automatic Acquisition of Annotated Training Corpora for Test-Code Generation","venue":"Inf.","year":2019,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/02/2019","authors":"Magdalena Kacmajor,John D. Kelleher","id":"f44ceb54fa773920b767c1e93ea0bc8725f248df","summary":"The experiments show that a neural MT model trained on a generated dataset can generate syntactically correct and semantically relevant short Java functions from quasi-natural language descriptions of functionality.","score":1},{"url":"https://www.semanticscholar.org/paper/dfd252415b37208617da78d09cfd87480b9800eb","title":"Modeling Vocabulary for Big Code Machine Learning","venue":"ArXiv","year":2019,"referenceCount":80,"citationCount":22,"influentialCitationCount":1,"publicationDate":"03/04/2019","authors":"Hlib Babii,A. Janes,R. Robbes","id":"dfd252415b37208617da78d09cfd87480b9800eb","summary":"It is shown that a subset of decisions have decisive characteristics, allowing to train accurate Neural Language Models quickly on a large corpus of 10,106 projects.","score":1},{"url":"https://www.semanticscholar.org/paper/a57131646a16445df5ddbc86da917fb497cc27da","title":"Cleaning StackOverflow for Machine Translation","venue":"IEEE Working Conference on Mining Software Repositories","year":2019,"referenceCount":44,"citationCount":4,"influentialCitationCount":1,"publicationDate":"26/05/2019","authors":"Musfiqur Rahman,Peter C. Rigby,Dharani Palani,T. Nguyen","id":"a57131646a16445df5ddbc86da917fb497cc27da","summary":"This paper cleans StackOverflow, one of the most popular online discussion forums for programmers, to generate a parallel English-Code corpus from Android posts, and creates a simple maximum likelihood MT model to show that English words in the corpus map to a small number of specific code elements.","score":1},{"url":"https://www.semanticscholar.org/paper/1fbaed00dbda975a6209761857dd1a78618c6585","title":"Jointly Learning Semantic Parser and Natural Language Generator via Dual Information Maximization","venue":"Annual Meeting of the Association for Computational Linguistics","year":2019,"referenceCount":50,"citationCount":19,"influentialCitationCount":1,"publicationDate":"03/06/2019","authors":"Hai Ye,Wenjie Li,Lu Wang","id":"1fbaed00dbda975a6209761857dd1a78618c6585","summary":"A novel method of dual information maximization (DIM) is proposed to regularize the learning process, where DIM empirically maximizes the variational lower bounds of expected joint distributions of NL and MRs.","score":1},{"url":"https://www.semanticscholar.org/paper/735ce0447e459e13f89ae751b19323d76a2af786","title":"Program Synthesis and Semantic Parsing with Learned Code Idioms","venue":"Neural Information Processing Systems","year":2019,"referenceCount":43,"citationCount":58,"influentialCitationCount":7,"publicationDate":"26/06/2019","authors":"Richard Shin,Miltiadis Allamanis,Marc Brockschmidt,Oleksandr Polozov","id":"735ce0447e459e13f89ae751b19323d76a2af786","summary":"PATOIS is a system that allows a neural program synthesizer to explicitly interleave high-level and low-level reasoning at every generation step, and it accomplishes this by automatically mining common code idioms from a given corpus and incorporating them into the underlying language for neural synthesis.","score":1},{"url":"https://www.semanticscholar.org/paper/3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c","title":"Reranking for Neural Semantic Parsing","venue":"Annual Meeting of the Association for Computational Linguistics","year":2019,"referenceCount":31,"citationCount":45,"influentialCitationCount":10,"publicationDate":"01/07/2019","authors":"Pengcheng Yin,Graham Neubig","id":"3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c","summary":"This paper presents a simple approach to quickly iterate and improve the performance of an existing neural semantic parser by reranking an n-best list of predicted MRs, using features that are designed to fix observed problems with baseline models.","score":1},{"url":"https://www.semanticscholar.org/paper/561b279dd17a232f7466a5dda89d879f75a2bf3b","title":"Identifying Algorithm Names in Code Comments","venue":"ArXiv","year":2019,"referenceCount":18,"citationCount":2,"influentialCitationCount":0,"publicationDate":"10/07/2019","authors":"Jakapong Klainongsuang,Yusuf Sulistyo Nugroho,Hideaki Hata,Bundit Manaskasemsak,A. Rungsawang,P. Leelaprute,Ken-ichi Matsumoto","id":"561b279dd17a232f7466a5dda89d879f75a2bf3b","summary":"This paper proposes an automatic method of algorithm name identification by extracting important N-gram words containing the word `algorithm' in the last from active FLOSS projects written in seven programming languages.","score":1},{"url":"https://www.semanticscholar.org/paper/98ec93df77d6f672b4f682cbe315fedf0e2d4ee7","title":"PUMICE: A Multi-Modal Agent that Learns Concepts and Conditionals from Natural Language and Demonstrations","venue":"ACM Symposium on User Interface Software and Technology","year":2019,"referenceCount":64,"citationCount":50,"influentialCitationCount":6,"publicationDate":"30/08/2019","authors":"Toby Jia-Jun Li,Marissa Radensky,Justin Jia,Kirielle Singarajah,Tom Michael Mitchell,B. Myers","id":"98ec93df77d6f672b4f682cbe315fedf0e2d4ee7","summary":"A new multi-modal domain-independent approach that combines natural language programming and programming-by-demonstration to allow users to first naturally describe tasks and associated conditions at a high level, and then collaborate with the agent to recursively resolve any ambiguities or vagueness through conversations and demonstrations is described.","score":1},{"url":"https://www.semanticscholar.org/paper/fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2019,"referenceCount":33,"citationCount":34,"influentialCitationCount":9,"publicationDate":"01/10/2019","authors":"R. Agashe,Srini Iyer,Luke Zettlemoyer","id":"fa5b139b08ef9ce0529d68c929f786412edc2898","summary":"To study code generation conditioned on a long context history, JuICe is presented, a corpus of 1.5 million examples with a curated test set of 3.7K instances based on online programming assignments that provides refined human-curated data, open-domain code, and an order of magnitude more training data.","score":1},{"url":"https://www.semanticscholar.org/paper/5ad7ce7810964bde2c86e07d63156270e0b17e0b","title":"Extracting Code-relevant Description Sentences Based on Structural Similarity","venue":"Asia-Pacific Symposium on Internetware","year":2019,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/10/2019","authors":"Yingkui Cao,Yanzhen Zou,Bing Xie","id":"5ad7ce7810964bde2c86e07d63156270e0b17e0b","summary":"To quantify the relevance between code line and natural language sentence, the authors represent them with structure trees and calculate their structural similarity.","score":1},{"url":"https://www.semanticscholar.org/paper/539268757e7d29201a13e5ca90454eb151d3d022","title":"Abstraction, Generalization, and Embodiment in Neural Program Synthesis","venue":"","year":2020,"referenceCount":129,"citationCount":0,"influentialCitationCount":0,"publicationDate":2020,"authors":"Richard Shin","id":"539268757e7d29201a13e5ca90454eb151d3d022","summary":"This paper presents a meta-modelling framework that automates the very labor-intensive and therefore time-heavy and expensive process of designing and implementing neural program Synthesis systems.","score":1},{"url":"https://www.semanticscholar.org/paper/fd551f9b634b43b4b6be6fbda8369f95868ae94a","title":"POSIT: Simultaneously TaggingNatural and Programming Languages","venue":"","year":2020,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":2020,"authors":"Profir-Petru Pârt","id":"fd551f9b634b43b4b6be6fbda8369f95868ae94a","summary":"This paper borrows code-switching techniques from Natural Language Processing and adapt them to apply to mixed text to solve two problems: language identification and token tagging, and develops POSIT, a technique that improves the state-of-the-art on language identification by 10.6% and PoS/AST tagging by 23.7% in accuracy.","score":1},{"url":"https://www.semanticscholar.org/paper/d6fef46fd53c3bb13dc2cd32d8b8050b3f67058c","title":"Neural Semantic Parsing for Syntax-Aware Code Generation","venue":"","year":2020,"referenceCount":68,"citationCount":0,"influentialCitationCount":0,"publicationDate":2020,"authors":"Artur Baranowski","id":"d6fef46fd53c3bb13dc2cd32d8b8050b3f67058c","summary":"This thesis presents an efficient neural semantic parser that exploits the underlying grammar of logical forms to enforce well-formed expressions and shows that the proposed model outperforms the standard encoder-decoder model across datasets and is competitive with comparable grammar-guided semantic parsing approaches.","score":1},{"url":"https://www.semanticscholar.org/paper/1f8885c1dea94b97be0d0a89afe642c37ebfbc55","title":"Learning Based Methods for Code Runtime Complexity Prediction","venue":"European Conference on Information Retrieval","year":2019,"referenceCount":17,"citationCount":2,"influentialCitationCount":0,"publicationDate":"04/11/2019","authors":"Jagriti Sikka,K. Satya,Yaman Kumar Singla,Shagun Uppal,R. Shah,Roger Zimmermann","id":"1f8885c1dea94b97be0d0a89afe642c37ebfbc55","summary":"This work model this problem as a machine learning task and check its feasibility with thorough analysis, establishing baselines using two different approaches: feature engineering and code embeddings, to achieve state of the art results and compare their performances.","score":1},{"url":"https://www.semanticscholar.org/paper/a470dfc3dd30e4a27c8480d6fb817ece6a11d813","title":"Associating Natural Language Comment and Source Code Entities","venue":"AAAI Conference on Artificial Intelligence","year":2019,"referenceCount":39,"citationCount":14,"influentialCitationCount":0,"publicationDate":"13/12/2019","authors":"Sheena Panthaplackel,Miloš Gligorić,R. Mooney,Junyi Jessy Li","id":"a470dfc3dd30e4a27c8480d6fb817ece6a11d813","summary":"A binary classifier and a sequence labeling model is developed by crafting a rich feature set which encompasses various aspects of code, comments, and the relationships between them and shows that these systems outperform several baselines learning from the proposed supervision.","score":1},{"url":"https://www.semanticscholar.org/paper/b68f2d939ea3acaf4bee6a487522c27ef7d04cee","title":"Are the Code Snippets What We Are Searching for? A Benchmark and an Empirical Study on Code Search with Natural-Language Queries","venue":"IEEE International Conference on Software Analysis, Evolution, and Reengineering","year":2020,"referenceCount":51,"citationCount":29,"influentialCitationCount":1,"publicationDate":"01/02/2020","authors":"Shuhan Yan,Hang Yu,Yuting Chen,Beijun Shen,Lingxiao Jiang","id":"b68f2d939ea3acaf4bee6a487522c27ef7d04cee","summary":"This paper builds CosBench, a dataset that consists of 1000 projects, 52 code-independent natural-language queries with ground truths, and a set of scripts for calculating four metrics on code research results, and evaluated four IR (Information Retrieval)-based and two DL (Deep Learning)-based code search methods on CosBench.","score":1},{"url":"https://www.semanticscholar.org/paper/0dfe706526e5234338411489e1826b4060acc4e8","title":"CC2Vec: Distributed Representations of Code Changes","venue":"International Conference on Software Engineering","year":2020,"referenceCount":70,"citationCount":77,"influentialCitationCount":18,"publicationDate":"12/03/2020","authors":"Thong Hoang,Hong Jin Kang,J. Lawall,David Lo","id":"0dfe706526e5234338411489e1826b4060acc4e8","summary":"This work proposed CC2Vec, a neural network model that learns a representation of code changes guided by their accompanying log messages, which represent the semantic intent of the code changes, which outperform the state-of-the-art techniques.","score":1},{"url":"https://www.semanticscholar.org/paper/3944354c42ddfff7414ad06022f96c72858d5fa6","title":"Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code","venue":"International Conference on Software Engineering","year":2020,"referenceCount":95,"citationCount":149,"influentialCitationCount":27,"publicationDate":"17/03/2020","authors":"Rafael-Michael Karampatsis,Hlib Babii,R. Robbes,Charles Sutton,A. Janes","id":"3944354c42ddfff7414ad06022f96c72858d5fa6","summary":"This paper presents an open vocabulary source code NLM that can scale to such a corpus, 100 times larger than in previous work; and shows that such models outperform the state of the art on three distinct code corpora (Java, C, Python).","score":1},{"url":"https://www.semanticscholar.org/paper/5e9b611a476e993f03c90424847311cd84e36a06","title":"Optimising the fit of stack overflow code snippets into existing code","venue":"GECCO Companion","year":2020,"referenceCount":25,"citationCount":6,"influentialCitationCount":0,"publicationDate":"16/04/2020","authors":"Brittany Reid,Christoph Treude,Markus Wagner","id":"5e9b611a476e993f03c90424847311cd84e36a06","summary":"An automated code reuse tool for the Eclipse IDE, NLP2TestableCode, which can not only search for Java code snippets using natural language tasks, but also evaluate code snippets based on a user's existing code, modify snippets to improve fit and correct errors, before presenting the user with the best snippet, all without leaving the editor.","score":1},{"url":"https://www.semanticscholar.org/paper/77910e51a40d17157fc798325d06edfa6cff18d6","title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":31,"citationCount":50,"influentialCitationCount":9,"publicationDate":"20/04/2020","authors":"Frank F. Xu,Zhengbao Jiang,Pengcheng Yin,Bogdan Vasilescu,Graham Neubig","id":"77910e51a40d17157fc798325d06edfa6cff18d6","summary":"Evaluations show that combining the two sources with data augmentation and retrieval-based data re-sampling improves the current state-of-the-art by up to 2.2% absolute BLEU score on the code generation testbed CoNaLa.","score":1},{"url":"https://www.semanticscholar.org/paper/57348a5e75b89c1d3e8d5b85027872c35ebc6d36","title":"Relevance Transformer: Generating Concise Code Snippets with Relevance Feedback","venue":"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","year":2020,"referenceCount":21,"citationCount":8,"influentialCitationCount":2,"publicationDate":"23/04/2020","authors":"Carlos Gemmell,Federico Rossetto,Jeffrey Dalton","id":"57348a5e75b89c1d3e8d5b85027872c35ebc6d36","summary":"The Relevance Transformer model shows the potential of Transformer-based architectures for code generation and introduces a method of incorporating pseudo-relevance feedback during inference, shown over state-of-the-art methods based on BLEU evaluation.","score":1},{"url":"https://www.semanticscholar.org/paper/a266b37f98928f27fddd863d11b38a5563043315","title":"Learning to Update Natural Language Comments Based on Code Changes","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":42,"citationCount":40,"influentialCitationCount":4,"publicationDate":"25/04/2020","authors":"Sheena Panthaplackel,Pengyu Nie,Miloš Gligorić,Junyi Jessy Li,R. Mooney","id":"a266b37f98928f27fddd863d11b38a5563043315","summary":"This work proposes an approach that learns to correlate changes across two distinct language representations, to generate a sequence of edits that are applied to the existing comment to reflect the source code modifications.","score":1},{"url":"https://www.semanticscholar.org/paper/037aa837d95b5f0edef494a2392b1788dc840a47","title":"A Multi-Perspective Architecture for Semantic Code Search","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":13,"citationCount":23,"influentialCitationCount":3,"publicationDate":"06/05/2020","authors":"Rajarshi Haldar,Lingfei Wu,Jinjun Xiong,J. Hockenmaier","id":"037aa837d95b5f0edef494a2392b1788dc840a47","summary":"A novel multi-perspective cross-lingual neural framework for code–text matching is proposed, inspired in part by a previous model for monolingual text-to-text matching, to capture both global and local similarities.","score":1},{"url":"https://www.semanticscholar.org/paper/eccd7060c4f81e92d65601f5c7ac7cade2f68807","title":"TAG : Type Auxiliary Guiding for Code Comment Generation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":40,"citationCount":14,"influentialCitationCount":2,"publicationDate":"06/05/2020","authors":"Ruichu Cai,Zhihao Liang,Boyan Xu,Zijian Li,Yuexing Hao,Yao Chen","id":"eccd7060c4f81e92d65601f5c7ac7cade2f68807","summary":"A Type Auxiliary Guiding encoder-decoder framework for the code comment generation task which considers the source code as an N-ary tree with type information associated with each node and a hierarchical reinforcement learning method to resolve the training difficulties of this proposed framework.","score":1},{"url":"https://www.semanticscholar.org/paper/c9aac0037e8abe2da081490cc9d10610aa8fdb3f","title":"Semantic code search using Code2Vec: A bag-of-paths model","venue":"","year":2020,"referenceCount":46,"citationCount":4,"influentialCitationCount":0,"publicationDate":"14/05/2020","authors":"Lakshmanan Arumugam","id":"c9aac0037e8abe2da081490cc9d10610aa8fdb3f","summary":"This thesis uses Code2Vec, a model that learns distributed representations of source code called code embeddings, to evaluate its performance against the task of semantically searching code snippets to create a hybrid model that outperforms previous benchmark baseline models developed in the CodeSearchNet challenge.","score":1},{"url":"https://www.semanticscholar.org/paper/0a01de6020e43db362e92664d1832a488d4a4c5b","title":"Improving Quality of a Post’s Set of Answers in Stack Overflow","venue":"EUROMICRO Conference on Software Engineering and Advanced Applications","year":2020,"referenceCount":19,"citationCount":4,"influentialCitationCount":0,"publicationDate":"30/05/2020","authors":"M. Tavakoli,M. Izadi,A. Heydarnoori","id":"0a01de6020e43db362e92664d1832a488d4a4c5b","summary":"An approach to automate the identification process of deficient posts on Stack Overflow and boost their set of answers, utilizing the help of related experts, and develops an Eclipse plugin named SOPI and integrated the prediction model in the plugin to link these deficient posts to related developers and help them improve the answer set.","score":1},{"url":"https://www.semanticscholar.org/paper/4e0daa62e8467c46123bc957f0068f7901e1e94f","title":"Mining API usage scenarios from stack overflow","venue":"Information and Software Technology","year":2020,"referenceCount":108,"citationCount":23,"influentialCitationCount":0,"publicationDate":"01/06/2020","authors":"Gias Uddin,F. Khomh,C. Roy","id":"4e0daa62e8467c46123bc957f0068f7901e1e94f","summary":"A framework to automatically mine API usage scenarios from Stack Overflow, supported by three novel algorithms is proposed and implemented and deployed in the proof-of-concept online tool, Opiner.","score":1},{"url":"https://www.semanticscholar.org/paper/eeb37b1b3019a03669b9b8b8173efbbd7c236184","title":"POSIT: Simultaneously Tagging Natural and Programming Languages","venue":"International Conference on Software Engineering","year":2020,"referenceCount":35,"citationCount":7,"influentialCitationCount":0,"publicationDate":"27/06/2020","authors":"Profir-Petru Pârtachi,S. K. Dash,Christoph Treude,Earl T. Barr","id":"eeb37b1b3019a03669b9b8b8173efbbd7c236184","summary":"This paper borrows code-switching techniques from Natural Language Processing and adapt them to apply to mixed text to solve two problems: language identification and token tagging, and produces POSIT, which improves the state-of-the-art on language identification by 10.6% and PoS/Ast tagging by 23.7% in accuracy.","score":1},{"url":"https://www.semanticscholar.org/paper/4a7e7b24389d190d20f214054156e43dc269e595","title":"POSIT","venue":"Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering","year":2020,"referenceCount":17,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/06/2020","authors":"Profir-Petru Pârtachi,S. K. Dash,Christoph Treude,Earl T. Barr","id":"4a7e7b24389d190d20f214054156e43dc269e595","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/8fbb41e21a8ba6072de60980d3c96e5e50c8e8fa","title":"CC2Vec","venue":"Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering","year":2020,"referenceCount":35,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/06/2020","authors":"Thong Hoang,Hong Jin Kang,D. Lo,Julia L. Lawall","id":"8fbb41e21a8ba6072de60980d3c96e5e50c8e8fa","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/ca0810e7658b72cdce6ea807d2ed0ed39ca70d4e","title":"Bug severity prediction using question-and-answer pairs from Stack Overflow","venue":"Journal of Systems and Software","year":2020,"referenceCount":41,"citationCount":26,"influentialCitationCount":1,"publicationDate":"01/07/2020","authors":"Youshuai Tan,Sijie Xu,Zhaowei Wang,Tao Zhang,Zhou Xu,Xiapu Luo","id":"ca0810e7658b72cdce6ea807d2ed0ed39ca70d4e","summary":"This paper extracts all the posts related to bug repositories from Stack Overflow and combines them with bug reports to obtain enhanced versions of bug reports and achieves severity prediction on three popular open source projects with Naive Bayesian, k-Nearest Neighbor algorithm (KNN), and Long Short-Term Memory (LSTM).","score":1},{"url":"https://www.semanticscholar.org/paper/7c41e58832f3af5fd9e09674924d6b5f822e8eac","title":"Exploring Unexplored Generalization Challenges for Cross-Database Semantic Parsing","venue":"Annual Meeting of the Association for Computational Linguistics","year":2020,"referenceCount":62,"citationCount":52,"influentialCitationCount":17,"publicationDate":"01/07/2020","authors":"Alane Suhr,Ming-Wei Chang,Peter Shaw,Kenton Lee","id":"7c41e58832f3af5fd9e09674924d6b5f822e8eac","summary":"This work re-purpose eight semantic parsing datasets that have been well-studied in the setting where in-domain training data is available, and instead use them as additional evaluation data for XSP systems instead, to uncovers several generalization challenges for cross-database semantic parsing.","score":1},{"url":"https://www.semanticscholar.org/paper/f30a7e6a1f7bbaf34493586fa61972a4789f7594","title":"Hierarchical Embedding for Code Search in Software Q&A Sites","venue":"IEEE International Joint Conference on Neural Network","year":2020,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/07/2020","authors":"Ruitong Li,Gang Hu,Min Peng","id":"f30a7e6a1f7bbaf34493586fa61972a4789f7594","summary":"A novel deep neural network named HECS 1 (Hierarchical embedding for code search), which can understand the difference between positive and negative samples more accurately and achieves state-of-the-art performance.","score":1},{"url":"https://www.semanticscholar.org/paper/b5b9ed1e6d3cd2e51d155831cdfae39f2e0f4578","title":"Synergy between Machine/Deep Learning and Software Engineering: How Far Are We?","venue":"ArXiv","year":2020,"referenceCount":155,"citationCount":3,"influentialCitationCount":0,"publicationDate":"12/08/2020","authors":"Simin Wang,LiGuo Huang,Jidong Ge,Tengfei Zhang,Haitao Feng,Ming Li,He Zhang,Vincent Ng","id":"b5b9ed1e6d3cd2e51d155831cdfae39f2e0f4578","summary":"A 10-year Systematic Literature Review on 906 ML/DL-related SE papers published between 2009 and 2018 demonstrated the mutual impacts that ML/ DL and SE have had on each other and identified five factors that influence their replicability and reproducibility.","score":1},{"url":"https://www.semanticscholar.org/paper/2dc59238ad0f010f505238fcd0dd0695681200ef","title":"Neural Code Search Revisited: Enhancing Code Snippet Retrieval through Natural Language Intent","venue":"ArXiv","year":2020,"referenceCount":31,"citationCount":15,"influentialCitationCount":3,"publicationDate":"27/08/2020","authors":"Geert Heyman,T. V. Cutsem","id":"2dc59238ad0f010f505238fcd0dd0695681200ef","summary":"A domain-specific retrieval model for code annotated with a natural language description is created that yields significantly more relevant search results compared to state-of-the-art code retrieval methods that do not use descriptions.","score":1},{"url":"https://www.semanticscholar.org/paper/d944bf7942297f5670192c5cd33191c26a87973e","title":"Code to Comment “Translation”: Data, Metrics, Baselining & Evaluation","venue":"International Conference on Automated Software Engineering","year":2020,"referenceCount":64,"citationCount":38,"influentialCitationCount":2,"publicationDate":"01/09/2020","authors":"David Gros,Hariharan Sezhiyan,Prem Devanbu,Zhou Yu","id":"d944bf7942297f5670192c5cd33191c26a87973e","summary":"It is argued that fairly naive information retrieval methods do well enough at this task to be considered a reasonable baseline, and some suggestions on how the findings might be used in future research in this area are made.","score":1},{"url":"https://www.semanticscholar.org/paper/179076fe7aad3d5f085535b37bae85cbbae3c240","title":"CROKAGE: effective solution recommendation for programming tasks by leveraging crowd knowledge","venue":"Empirical Software Engineering","year":2020,"referenceCount":75,"citationCount":7,"influentialCitationCount":1,"publicationDate":"02/09/2020","authors":"R. F. Silva,C. Roy,M. M. Rahman,Kevin A. Schneider,K. V. R. Paixão,C. E. Dantas,M. Maia","id":"179076fe7aad3d5f085535b37bae85cbbae3c240","summary":"The proposed CROKAGE (CrowdKnowledge Answer Generator), a tool that takes the description of a programming task as input and delivers a comprehensible solution for the task, outperforms the state-of-art tool in terms of relevance of the suggested code examples, benefit of the code explanations and the overall solution quality.","score":1},{"url":"https://www.semanticscholar.org/paper/c6f608a3731a1fde355835a0e10a65ac71f80643","title":"Towards Full-line Code Completion with Neural Language Models","venue":"ArXiv","year":2020,"referenceCount":26,"citationCount":7,"influentialCitationCount":0,"publicationDate":"18/09/2020","authors":"Wenhan Wang,Sijie Shen,Ge Li,Zhi Jin","id":"c6f608a3731a1fde355835a0e10a65ac71f80643","summary":"This paper conducts experiments on two real-world python corpora and evaluates existing neural models based on source code tokens or syntactical actions and shows that neural language models can achieve acceptable results on the authors' tasks, with significant room for improvements.","score":1},{"url":"https://www.semanticscholar.org/paper/e6e8a2c56243847b77b604259cde9e10a2daccb8","title":"Semantic Evaluation for Text-to-SQL with Distilled Test Suite","venue":"Conference on Empirical Methods in Natural Language Processing","year":2020,"referenceCount":44,"citationCount":26,"influentialCitationCount":3,"publicationDate":"06/10/2020","authors":"Ruiqi Zhong,Tao Yu,D. Klein","id":"e6e8a2c56243847b77b604259cde9e10a2daccb8","summary":"This work proposes test suite accuracy to approximate semantic accuracy for Text-to-SQL models by distilling a small test suite of databases that achieves high code coverage for the gold query from a large number of randomly generated databases.","score":1},{"url":"https://www.semanticscholar.org/paper/3421e608d478161146d795752b6bcd222ed4c106","title":"When Does it Pay Off to Learn a New Skill? Revealing the Complementary Benefit of Cross-Skilling","venue":"SSRN Electronic Journal","year":2020,"referenceCount":113,"citationCount":6,"influentialCitationCount":0,"publicationDate":"22/10/2020","authors":"F. Stephany","id":"3421e608d478161146d795752b6bcd222ed4c106","summary":"The results indicate that the added economic value of learning a new skill strongly depends on the already existing skill bundle but that acquiring a skill from a different domain is often beneficial.","score":1},{"url":"https://www.semanticscholar.org/paper/f0a9ff1391bdefaf6d712e5c27e747d7d710bb40","title":"AlgoLabel: A Large Dataset for Multi-Label Classification of Algorithmic Challenges","venue":"Mathematics","year":2020,"referenceCount":52,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/11/2020","authors":"R. Iacob,V. Monea,D. Radulescu,Andrei-Florin Ceapă,Traian Rebedea,Stefan Trausan-Matu","id":"f0a9ff1391bdefaf6d712e5c27e747d7d710bb40","summary":"This work proposes a large multi-modal dataset of text and code pairs consisting of algorithmic challenges and their solutions, called AlgoLabel and proposes a dual text-code neural model for detecting the algorithmic solution type for a programming challenge.","score":1},{"url":"https://www.semanticscholar.org/paper/2df83c8604ac9260a77d85241b9da0539b29effa","title":"Statistical machine translation outperforms neural machine translation in software engineering: why and how","venue":"","year":2020,"referenceCount":48,"citationCount":7,"influentialCitationCount":0,"publicationDate":"13/11/2020","authors":"H. Phan,A. Jannesari","id":"2df83c8604ac9260a77d85241b9da0539b29effa","summary":"This work provides a hypothesis that SE corpus has inherent characteristics that NMT will confront challenges compared to the state-of-the-art translation engine based on Statistical Machine Translation, and implements and optimize the original SMT and NMT to mitigate those challenges.","score":1},{"url":"https://www.semanticscholar.org/paper/35f0482b4eadd92d5cd00932a729f00c64ce9e76","title":"Neural joint attention code search over structure embeddings for software Q&A sites","venue":"Journal of Systems and Software","year":2020,"referenceCount":84,"citationCount":5,"influentialCitationCount":0,"publicationDate":"01/12/2020","authors":"Gang Hu,Min Peng,Yihan Zhang,Qianqian Xie,Mengting Yuan","id":"35f0482b4eadd92d5cd00932a729f00c64ce9e76","summary":"NJDACS is a novel two-way attention-based neural network for retrieving code fragments in software Q&A sites, which aligns and focuses the more structure informative parts of source codes to natural query.","score":1},{"url":"https://www.semanticscholar.org/paper/b28ca9fb2056c8fd3f37e066f7809e7af27ef9b0","title":"BF++: a language for general-purpose neural program synthesis","venue":"ArXiv","year":2021,"referenceCount":47,"citationCount":2,"influentialCitationCount":0,"publicationDate":2021,"authors":"Vadim Liventsev,Aki Härmä,M. Petkovic","id":"b28ca9fb2056c8fd3f37e066f7809e7af27ef9b0","summary":"A new programming language, BF++, is proposed, designed specifically for neural program synthesis in a Partially Observable Markov Decision Process (POMDP) setting and generate programs for a number of standard OpenAI Gym benchmarks.","score":1},{"url":"https://www.semanticscholar.org/paper/ed2a639dd4a6b3abdcc0040733a264cb1dbcd7d4","title":"PlotCoder: Hierarchical Decoding for Synthesizing Visualization Code in Programmatic Context","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":42,"citationCount":10,"influentialCitationCount":5,"publicationDate":2021,"authors":"Xinyun Chen,Linyuan Gong,Alvin Cheung,D. Song","id":"ed2a639dd4a6b3abdcc0040733a264cb1dbcd7d4","summary":"This paper introduces PlotCoder, a new hierarchical encoder-decoder architecture that models both the code context and the input utterance and uses it to first determine the template of the visualization code, followed by predicting the data to be plotted.","score":1},{"url":"https://www.semanticscholar.org/paper/e13d317fe0178a8b8b67f4af995e7fac12c35014","title":"Analysis of Tree-Structured Architectures for Code Generation","venue":"Findings","year":2021,"referenceCount":35,"citationCount":3,"influentialCitationCount":1,"publicationDate":2021,"authors":"Samip Dahal,A. Maharana,Mohit Bansal","id":"e13d317fe0178a8b8b67f4af995e7fac12c35014","summary":"This work presents an empirical analysis of the significance of input parse trees for code generation, and finds that structure-aware encodings are better at modelling inputs with multiple variables and capturing long-range dependencies for codegeneration.","score":1},{"url":"https://www.semanticscholar.org/paper/8e5b4aad131263457a38adbbffebe1b252802f1e","title":"Text2PyCode: Machine Translation of Natural Language Intent to Python Source Code","venue":"International Cross-Domain Conference on Machine Learning and Knowledge Extraction","year":2021,"referenceCount":2,"citationCount":3,"influentialCitationCount":1,"publicationDate":2021,"authors":"Sridevi Bonthu,S. R. Sree,M. K. Prasad","id":"8e5b4aad131263457a38adbbffebe1b252802f1e","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/62aceee1eca236841fddbad25833802750a92656","title":"Institutional Knowledge at Singapore Management University Institutional Knowledge at Singapore Management University CC2Vec: Distributed representations of code changes CC2Vec: Distributed representations of code changes","venue":"","year":2021,"referenceCount":63,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Thong Hoang","id":"62aceee1eca236841fddbad25833802750a92656","summary":"This work proposed CC2Vec, a neural network model that learns a representation of code changes guided by their accompanying log messages, which represent the semantic intent of the code changes, which outperform the state-of-the-art techniques.","score":1},{"url":"https://www.semanticscholar.org/paper/4d8bfc6b35d46a5a1dbad7aa273e81f49f5f6e4b","title":"Deep Learning Based Code Generation from Requirements Text: Are We There Yet?","venue":"","year":2021,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Hui Liu,Mingzhu Shen,Jiaqi Zhu,Nan Niu,Ge Li,Lu Zhang","id":"4d8bfc6b35d46a5a1dbad7aa273e81f49f5f6e4b","summary":"A popularity based approach is proposed that always generates the most popular statements in training programs regardless of the input (software requirements), and evaluation results suggest that none of the state-of-the-art approaches can outperform this simple statistics-based approach.","score":1},{"url":"https://www.semanticscholar.org/paper/b029346bfa85fa2fc7a12498ae5f018922ce55f9","title":"Code Generation from Natural Language with Less Prior Knowledge and More Monolingual Data","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":35,"citationCount":7,"influentialCitationCount":1,"publicationDate":2021,"authors":"Sajad Norouzi,Keyi Tang,Yanshuai Cao","id":"b029346bfa85fa2fc7a12498ae5f018922ce55f9","summary":"This work investigates whether a generic transformer-based seq2seq model can achieve competitive performance with minimal code-generation-specific inductive bias design and achieves 81.03% exact match accuracy on Django and 32.57 BLEU score on CoNaLa.","score":1},{"url":"https://www.semanticscholar.org/paper/ceeac1741f8c8dee5a1365828f79e8d3e84b0623","title":"Semantic Parsing with Less Prior and More Monolingual Data","venue":"ArXiv","year":2021,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Sajad Norouzi,Yanshuai Cao","id":"ceeac1741f8c8dee5a1365828f79e8d3e84b0623","summary":"This work investigates whether a generic transformerbased seq2seq model can achieve competitive performance with minimal semantic-parsing specific inductive bias design, and achieves positive evidence of a potentially easier path toward building accurate semantic parsers in the wild.","score":1},{"url":"https://www.semanticscholar.org/paper/010c9f63c51ccc504de36d2d1693e0ea9d525da1","title":"Deep Learning for Source Code Modeling and Generation","venue":"ACM Computing Surveys","year":2020,"referenceCount":297,"citationCount":57,"influentialCitationCount":1,"publicationDate":"13/02/2020","authors":"T. H. Le,Hao Chen,M. Babar","id":"010c9f63c51ccc504de36d2d1693e0ea9d525da1","summary":"A comprehensive review to categorize and investigate existing DL methods for source code modeling and generation, and formulate common program learning tasks under an encoder-decoder framework to address the limitations of the traditional source code models.","score":1},{"url":"https://www.semanticscholar.org/paper/8c779ca55bef46f2380c1aea163ce9415f47b335","title":"One size does not fit all: Constructing complementary digital reskilling strategies using online labour market data","venue":"","year":2021,"referenceCount":37,"citationCount":9,"influentialCitationCount":1,"publicationDate":"01/01/2021","authors":"F. Stephany","id":"8c779ca55bef46f2380c1aea163ce9415f47b335","summary":"This commentary argues that, over the last decade, online labour platforms have become the ‘laboratories’ of skill rebundling; the combination of skills from different occupational domains that allows a new taxonomy on the individual complementarity of skills to be established.","score":1},{"url":"https://www.semanticscholar.org/paper/216425e1407875c94a754389b20bf983afe242c5","title":"Teach me how to Label: Labeling Functions from Natural Language with Text-to-text Transformers","venue":"ArXiv","year":2021,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/01/2021","authors":"Yannis Papanikolaou","id":"216425e1407875c94a754389b20bf983afe242c5","summary":"The task of turning natural language descriptions into Python labeling functions by following a novel approach to semantic parsing with pre-trained text-to-text Transformers achieves a new state of the art on the semantic parsing benchmark CoNaLa, surpassing the previous best approach by 3.7 BLEU points.","score":1},{"url":"https://www.semanticscholar.org/paper/69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":115,"citationCount":257,"influentialCitationCount":65,"publicationDate":"09/02/2021","authors":"Shuai Lu,Daya Guo,Shuo Ren,Junjie Huang,Alexey Svyatkovskiy,Ambrosio Blanco,Colin B. Clement,Dawn Drain,Daxin Jiang,Duyu Tang,Ge Li,Lidong Zhou,Linjun Shou,Long Zhou,Michele Tufano,Ming Gong,Ming Zhou,Nan Duan,Neel Sundaresan,Shao Kun Deng,Shengyu Fu,Shujie Liu","id":"69a72ff5b30642d11c96635e99aadad3140d33a7","summary":"This paper introduces CodeXGLUE, a benchmark dataset to foster machine learning research for program understanding and generation that includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison.","score":1},{"url":"https://www.semanticscholar.org/paper/f389c09ad85263bdd1bb2518d61c90a8a558441d","title":"MulCode: A Multi-task Learning Approach for Source Code Understanding","venue":"IEEE International Conference on Software Analysis, Evolution, and Reengineering","year":2021,"referenceCount":56,"citationCount":7,"influentialCitationCount":1,"publicationDate":"01/03/2021","authors":"Deze Wang,Yue Yu,Shanshan Li,Wei Dong,Ji Wang,Qing Liao","id":"f389c09ad85263bdd1bb2518d61c90a8a558441d","summary":"This work proposes MulCode, a multi-task learning approach for source code understanding that learns unified representation space for tasks, with the pre-trained BERT model for the token sequence and the Tree-LSTM model for abstract syntax trees.","score":1},{"url":"https://www.semanticscholar.org/paper/b6262aa661b5195ad36b6a588c7160b1681abc2a","title":"Mining software architecture knowledge: Classifying stack overflow posts using machine learning","venue":"Concurrency and Computation","year":2021,"referenceCount":38,"citationCount":1,"influentialCitationCount":0,"publicationDate":"31/03/2021","authors":"Mubashir Ali,Husnain Mushtaq,M. B. Rasheed,Anees Baqir,T. Alquthami","id":"b6262aa661b5195ad36b6a588c7160b1681abc2a","summary":"A supervised machine learning‐based approach to classify the architectural knowledge into predefined categories, that is, analysis, synthesis, evaluation, and implementation of software architectural knowledge management (AKM) is proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/969e8c2c7cdf26c35e6c3fc19a9a56b3e7fcd6f9","title":"Generating Code with the Help of Retrieved Template Functions and Stack Overflow Answers","venue":"ArXiv","year":2021,"referenceCount":19,"citationCount":2,"influentialCitationCount":0,"publicationDate":"12/04/2021","authors":"Dawn Drain,Changran Hu,Chen Wu,Mikhail Breslav,Neel Sundaresan","id":"969e8c2c7cdf26c35e6c3fc19a9a56b3e7fcd6f9","summary":"This work presents a novel framework to precisely retrieve template functions as well as intent-snippet pairs and effectively train such a retrieval-guided code generator.","score":1},{"url":"https://www.semanticscholar.org/paper/b15fa9e57fb791899154a0f6c321eb703f1c0b09","title":"Shellcode_IA32: A Dataset for Automatic Shellcode Generation","venue":"NLP4PROG","year":2021,"referenceCount":30,"citationCount":11,"influentialCitationCount":1,"publicationDate":"27/04/2021","authors":"Pietro Liguori,Erfan Al-Hossami,Domenico Cotroneo,R. Natella,B. Cukic,Samira Shaikh","id":"b15fa9e57fb791899154a0f6c321eb703f1c0b09","summary":"The first step to address the task of automatically generating shellcodes, i.e., small pieces of code used as a payload in the exploitation of a software vulnerability, starting from natural language comments is taken, consisting of challenging but common assembly instructions with their natural language descriptions.","score":1},{"url":"https://www.semanticscholar.org/paper/43e8ad04354bb40c1d10cb58b3187473a8275f82","title":"One Size Does not Fit All: Constructing Complementary Digital Re-Skilling Strategies Using Online Labour Market Data","venue":"","year":2021,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/04/2021","authors":"F. Stephany","id":"43e8ad04354bb40c1d10cb58b3187473a8275f82","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/0c21334be0228431d619a180c809b43be0065bdd","title":"CoSQA: 20,000+ Web Queries for Code Search and Question Answering","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":40,"citationCount":30,"influentialCitationCount":5,"publicationDate":"27/05/2021","authors":"Junjie Huang,Duyu Tang,Linjun Shou,Ming Gong,Ke Xu,Daxin Jiang,Ming Zhou,Nan Duan","id":"0c21334be0228431d619a180c809b43be0065bdd","summary":"A contrastive learning method dubbed CoCLR is introduced to enhance text-code matching, which works as a data augmenter to bring more artificially generated training instances to bring better semantic matching between query and code.","score":1},{"url":"https://www.semanticscholar.org/paper/8c4f89a9ac30cf94186916be1bfaa02dbfb3600d","title":"CoDesc: A Large Code–Description Parallel Dataset","venue":"Findings","year":2021,"referenceCount":33,"citationCount":6,"influentialCitationCount":0,"publicationDate":"29/05/2021","authors":"Masum Hasan,Tanveer Muttaqueen,Abdullah Al Ishtiaq,Kazi Sajeed Mehrab,Md. Mahim Anjum Haque,Tahmid Hasan,Wasi Uddin Ahmad,Anindya Iqbal,Rifat Shahriyar","id":"8c4f89a9ac30cf94186916be1bfaa02dbfb3600d","summary":"It is shown that the dataset helps improve code search by up to 22% and achieves the new state-of-the-art in code summarization and CoDesc’s effectiveness in pre-training–finetuning setup, opening possibilities in building pretrained language models for Java.","score":1},{"url":"https://www.semanticscholar.org/paper/47fa6b2645a95165399fc5c2e966f67ac2110df0","title":"Enriching API Documentation with Code Samples and Usage Scenarios from Crowd Knowledge","venue":"IEEE Transactions on Software Engineering","year":2021,"referenceCount":59,"citationCount":16,"influentialCitationCount":2,"publicationDate":"01/06/2021","authors":"Jingxuan Zhang,He Jiang,Zhilei Ren,Tao Zhang,Zhiqiu Huang","id":"47fa6b2645a95165399fc5c2e966f67ac2110df0","summary":"A novel approach named ADECK is proposed towards enriching API documentation with code samples and corresponding usage scenarios by leveraging crowd knowledge from Stack Overflow, a popular technical Question and Answer (Q&A) website attracting millions of developers.","score":1},{"url":"https://www.semanticscholar.org/paper/3ca1430fb5bbf6ffa8f377f6b603648268f7546e","title":"Exploring Dynamic Selection of Branch Expansion Orders for Code Generation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":32,"citationCount":7,"influentialCitationCount":1,"publicationDate":"01/06/2021","authors":"Hui Jiang,Chulun Zhou,Fandong Meng,Biao Zhang,Jie Zhou,Degen Huang,Qingqiang Wu,Jinsong Su","id":"3ca1430fb5bbf6ffa8f377f6b603648268f7546e","summary":"This paper proposes to equip the Seq2Tree model with a context-based Branch Selector, which is able to dynamically determine optimal expansion orders of branches for multi-branch nodes, and optimize the selector through reinforcement learning, and formulate the reward function as the difference of model losses obtained through different expansion orders.","score":1},{"url":"https://www.semanticscholar.org/paper/48db3e5425e1582f5659d98154fc8406cea0dc54","title":"A Globally Normalized Neural Model for Semantic Parsing","venue":"SPNLP","year":2021,"referenceCount":35,"citationCount":2,"influentialCitationCount":0,"publicationDate":"07/06/2021","authors":"Chenyang Huang,Wei Yang,Yanshuai Cao,O. Zaiane,Lili Mou","id":"48db3e5425e1582f5659d98154fc8406cea0dc54","summary":"This paper proposes a globally normalized model for context-free grammar (CFG)-based semantic parsing that predicts a real-valued score at each step and does not suffer from the label bias problem.","score":1},{"url":"https://www.semanticscholar.org/paper/cc9ad384ec0d0176ce05865d3866b44d2519bd68","title":"Reading StackOverflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation","venue":"NLP4PROG","year":2021,"referenceCount":42,"citationCount":9,"influentialCitationCount":2,"publicationDate":"08/06/2021","authors":"Gabriel Orlanski,Alex Gittens","id":"cc9ad384ec0d0176ce05865d3866b44d2519bd68","summary":"A corpus of over 40,000 StackOverflow question texts to be used in conjunction with the corresponding intents from the CoNaLa dataset to prove that BART is an unsupervised multimodal learner and examine its extractive behavior.","score":1},{"url":"https://www.semanticscholar.org/paper/83a86fdf5d42fc70a07a2badd4fc9d42863f9b64","title":"SpreadsheetCoder: Formula Prediction from Semi-structured Context","venue":"International Conference on Machine Learning","year":2021,"referenceCount":57,"citationCount":13,"influentialCitationCount":5,"publicationDate":"26/06/2021","authors":"Xinyun Chen,Petros Maniatis,Rishabh Singh,Charles Sutton,H. Dai,Max Lin,Denny Zhou","id":"83a86fdf5d42fc70a07a2badd4fc9d42863f9b64","summary":"This work proposes SPREADSHEETCODER, a BERT-based model architecture to represent the tabular context in both row-based and column-based formats, and achieves top-1 prediction accuracy of 42.51%, which is a considerable improvement over baselines that do not employ richtabular context.","score":1},{"url":"https://www.semanticscholar.org/paper/04a9caa68ea24d183dce56903c5c05e49ae1d289","title":"Exploiting API Description Information to Improve Code Comment Generation","venue":"Advances in Artificial Intelligence and Security","year":2021,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/07/2021","authors":"Guang Yang,Qian Zhang,Yufei Wu,T. Zhou,Huan Liu,Wenting Feng","id":"04a9caa68ea24d183dce56903c5c05e49ae1d289","summary":"This paper has designed two models that combine code sequences and API description information to implement the code comment generation task, and conducted experiments on two open source data sets that demonstrate the effectiveness of API function description information for code commentgeneration task.","score":1},{"url":"https://www.semanticscholar.org/paper/1601a4b1af2c22e047790ab189861368a5008f5f","title":"APIzation: Generating Reusable APIs from StackOverflow Code Snippets","venue":"International Conference on Automated Software Engineering","year":2021,"referenceCount":63,"citationCount":3,"influentialCitationCount":0,"publicationDate":"02/09/2021","authors":"Valerio Terragni,P. Salza","id":"1601a4b1af2c22e047790ab189861368a5008f5f","summary":"This paper presents APIZATOR, a static analysis algorithm that automatically extracts the method parameters and return statements of JAVA code snippets automatically and is grounded by four common patterns that were extracted by studying real APIzations in GitHub.","score":1},{"url":"https://www.semanticscholar.org/paper/0916d3112978bbe5f123553b5460ac1d05c6a8fd","title":"Mapping Language to Programs using Multiple Reward Components with Inverse Reinforcement Learning","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":49,"citationCount":1,"influentialCitationCount":0,"publicationDate":"02/10/2021","authors":"Sayan Ghosh,Shashank Srivastava","id":"0916d3112978bbe5f123553b5460ac1d05c6a8fd","summary":"This paper poses program generation from language as Inverse Reinforcement Learning as a challenge, and introduces several interpretable reward components that jointly learn a reward function that linearly combines them and a policy for program generation.","score":1},{"url":"https://www.semanticscholar.org/paper/fa25e37fbd55dd05c0563de8f1e277f90c4ea589","title":"Text Classification for Task-based Source Code Related Questions","venue":"ArXiv","year":2021,"referenceCount":20,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/10/2021","authors":"Sairamvinay Vijayaraghavan,Jinxiao Song,David A. Tomassi,Siddhartha Punj,Jailan Sabet","id":"fa25e37fbd55dd05c0563de8f1e277f90c4ea589","summary":"The task of determining if a pair of a question/problem and a corresponding code snippet is appropriate or not as a binary classification problem is posed.","score":1},{"url":"https://www.semanticscholar.org/paper/e1d534c754d821bc405ecb0a954c8a3761d48a20","title":"Repo4QA: Answering Coding Questions via Dense Retrieval on GitHub Repositories","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Minyu Chen,Guoqiang Li,Chen Ma,Jingyang Li,Hongfei Fu","id":"e1d534c754d821bc405ecb0a954c8a3761d48a20","summary":"This paper introduces a specialized dataset named Repo4QA, which includes over 12,000 question-repository pairs constructed from Stack Overflow and GitHub, and proposes QuRep, a CodeBERT-based model that jointly learns the representation of both questions and repositories.","score":1},{"url":"https://www.semanticscholar.org/paper/445e5b31fc98f7acd2274aaa77d4de98816d89e3","title":"Towards Usable Neural Comment Generation Via Code-Comment Linkage Interpretation: Method and Empirical Study","venue":"IEEE Transactions on Software Engineering","year":2022,"referenceCount":70,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Shuyao Jiang,Jiacheng Shen,Shengnan Wu,Yu Cai,Yue Yu,Yangfan Zhou","id":"445e5b31fc98f7acd2274aaa77d4de98816d89e3","summary":"CCLink is proposed, a novel model-independent framework, namely CCLink, to interpret the auto-generated comments of Neural Comment Generation (NCG), and is promising in making NCG more usable with a proper interpretation of theAuto- generated comments.","score":1},{"url":"https://www.semanticscholar.org/paper/c317f4d5ddeb11da9a5c8fea1e3cdec2f7de485d","title":"Deep Learning Based Program Generation From Requirements Text: Are We There Yet?","venue":"IEEE Transactions on Software Engineering","year":2020,"referenceCount":67,"citationCount":14,"influentialCitationCount":2,"publicationDate":"21/08/2020","authors":"Hui Liu,Mingzhu Shen,Jiaqi Zhu,Nan Niu,Ge Li,Lu Zhang","id":"c317f4d5ddeb11da9a5c8fea1e3cdec2f7de485d","summary":"A popularity-based approach is proposed that always generates the most popular statements in training programs regardless of the input (software requirements), and Evaluation results suggest that none of the state-of-the-art approaches can outperform this simple statistics- based approach.","score":1},{"url":"https://www.semanticscholar.org/paper/1334674796f2122c4ede1eb6c1ad07cec9d77a28","title":"A Systematic Literature Review on the Use of Deep Learning in Software Engineering Research","venue":"ACM Transactions on Software Engineering and Methodology","year":2020,"referenceCount":203,"citationCount":28,"influentialCitationCount":0,"publicationDate":"14/09/2020","authors":"Cody Watson,N. Cooper,David Nader-Palacio,Kevin Moran,D. Poshyvanyk","id":"1334674796f2122c4ede1eb6c1ad07cec9d77a28","summary":"A systematic literature review of research at the intersection of SE & DL, from its modern inception to the present, that delineates the foundations of DL techniques applied to SE research and highlights likely areas of fertile exploration for the future.","score":1},{"url":"https://www.semanticscholar.org/paper/bea6af010fc02f5ac29edfc17096be6078edab46","title":"A Survey on Deep Learning for Software Engineering","venue":"ACM Computing Surveys","year":2020,"referenceCount":263,"citationCount":24,"influentialCitationCount":0,"publicationDate":"30/11/2020","authors":"Yanming Yang,Xin Xia,David Lo,J. Grundy","id":"bea6af010fc02f5ac29edfc17096be6078edab46","summary":"A survey to analyze the relevant studies published since 2006 and presents a set of current challenges remaining to be investigated and outline a proposed research road map highlighting key opportunities for future work.","score":1},{"url":"https://www.semanticscholar.org/paper/4a160efbe80c38cd5eb2f92c7c095b49b113397d","title":"In-IDE Code Generation from Natural Language: Promise and Challenges","venue":"ACM Transactions on Software Engineering and Methodology","year":2021,"referenceCount":134,"citationCount":41,"influentialCitationCount":5,"publicationDate":"27/01/2021","authors":"Frank F. Xu,Bogdan Vasilescu,Graham Neubig","id":"4a160efbe80c38cd5eb2f92c7c095b49b113397d","summary":"This article develops a plugin for the PyCharm IDE that implements a hybrid of code generation and code retrieval functionality and asks developers with various backgrounds to complete 7 varieties of 14 Python programming tasks ranging from basic file manipulation to machine learning or data visualization, with or without the help of the plugin.","score":1},{"url":"https://www.semanticscholar.org/paper/7ad2b12526e77badd629b10880db147afce4864a","title":"Lyra: A Benchmark for Turducken-Style Code Generation","venue":"International Joint Conference on Artificial Intelligence","year":2021,"referenceCount":43,"citationCount":2,"influentialCitationCount":1,"publicationDate":"27/08/2021","authors":"Qingyuan Liang,Zeyu Sun,Qihao Zhu,Wenjie Zhang,Lian Yu,Yingfei Xiong,Lu Zhang","id":"7ad2b12526e77badd629b10880db147afce4864a","summary":"This paper defines a new code generation task: given a natural language comment, this task aims to generate a program in a base imperative language with an embedded declarative language, and presents Lyra: a dataset in Python with embedded SQL, which they believe provides a new challenge for code generation.","score":1},{"url":"https://www.semanticscholar.org/paper/5bc74a4add942c340e6038235507e5f00b02d3b2","title":"Learning to Describe Solutions for Bug Reports Based on Developer Discussions","venue":"Findings","year":2021,"referenceCount":60,"citationCount":1,"influentialCitationCount":0,"publicationDate":"08/10/2021","authors":"Sheena Panthaplackel,J. Li,Miloš Gligorić,R. Mooney","id":"5bc74a4add942c340e6038235507e5f00b02d3b2","summary":"A corpus for this task is built using a novel technique for obtaining noisy supervision from repository changes linked to bug reports, with which it is found to form an ideal testbed for complex reasoning in long, bimodal dialogue context.","score":1},{"url":"https://www.semanticscholar.org/paper/2e375d8f4c4dbbf2fdf209f54eda70c9546e4096","title":"AstBERT: Enabling Language Model for Financial Code Understanding with Abstract Syntax Trees","venue":"FINNLP","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/01/2022","authors":"Rong Liang,Tiehu Zhang,Y. Lu,Yuze Liu,Zhengqing Huang,Xin Chen","id":"2e375d8f4c4dbbf2fdf209f54eda70c9546e4096","summary":"This work proposes the AstBERT model, a pre-trained PL model aiming to better understand the financial codes using the abstract syntax tree (AST), and evaluates the performance of the proposed model on three tasks, including code question answering, code clone detection and code refinement.","score":1},{"url":"https://www.semanticscholar.org/paper/758d85dd98f5caf59d51f9f3c09c2423471d56fd","title":"AstBERT: Enabling Language Model for Code Understanding with Abstract Syntax Tree","venue":"ArXiv","year":2022,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Rong Liang,Yujie Lu,Zhen Huang,Tiehu Zhang,Yuze Liu","id":"758d85dd98f5caf59d51f9f3c09c2423471d56fd","summary":"The AstBERT model is proposed, a pre-trained language model aiming to better understand the PL using the abstract syntax tree (AST), which collects a colossal amount of source codes from GitHub and incorporates the contextual code knowledge into the model through the help of code parsers.","score":1},{"url":"https://www.semanticscholar.org/paper/532f32be1e918d6b75650947318e57fc8f4fb415","title":"CodeRetriever: Unimodal and Bimodal Contrastive Learning","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":5,"influentialCitationCount":0,"publicationDate":2022,"authors":"Xiaonan Li,Yeyun Gong,Yelong Shen,Xipeng Qiu,Hang Zhang,Bolun Yao,Weizhen Qi,Daxin Jiang,Weizhu Chen,Nan Duan","id":"532f32be1e918d6b75650947318e57fc8f4fb415","summary":"The CodeRetriever model, which combines the unimodal and bimodal contrastive learning to train functionlevel code semantic representations, specifically for the code search task, achieves the new state-ofthe-art performance with significant improvement over existing code pre-trained models.","score":1},{"url":"https://www.semanticscholar.org/paper/f5b3be8b0f06eba6d26ed02656fac82928b1ae05","title":"Natural Language to Code Using Transformers","venue":"ArXiv","year":2022,"referenceCount":13,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/02/2022","authors":"Uday Kusupati,Venkata Ravi Teja Ailavarapu","id":"f5b3be8b0f06eba6d26ed02656fac82928b1ae05","summary":"The self-attention based transformer architecture is used and it is shown that it performs better than recurrent attention-based encoder decoder and a modified form of back translation and use cycle consistent losses to train the model in an end-to-end fashion.","score":1},{"url":"https://www.semanticscholar.org/paper/2236b6036cd1ecba1792d5e1fedbae7cefc3d43b","title":"Can we generate shellcodes via natural language? An empirical study","venue":"International Conference on Automated Software Engineering","year":2022,"referenceCount":91,"citationCount":8,"influentialCitationCount":0,"publicationDate":"08/02/2022","authors":"Pietro Liguori,Erfan Al-Hossami,Domenico Cotroneo,R. Natella,B. Cukic,Samira Shaikh","id":"2236b6036cd1ecba1792d5e1fedbae7cefc3d43b","summary":"The empirical analysis shows that NMT can generate assembly code snippets from the natural language with high accuracy and that in many cases can generate entire shellcodes with no errors.","score":1},{"url":"https://www.semanticscholar.org/paper/97010ef4c4bd80b1f959df236501cf7741053d04","title":"On the Importance of Building High-quality Training Datasets for Neural Code Search","venue":"International Conference on Software Engineering","year":2022,"referenceCount":61,"citationCount":12,"influentialCitationCount":1,"publicationDate":"14/02/2022","authors":"Zhensu Sun,Li Li,Y. Liu,Xiaoning Du","id":"97010ef4c4bd80b1f959df236501cf7741053d04","summary":"This is the first framework that applies semantic query cleaning to code search datasets and training the popular DeepCS model with the filtered dataset from this framework improves its performance by 19.2% MRR and 21.3% Answer@l, with the three validation benchmarks.","score":1},{"url":"https://www.semanticscholar.org/paper/7ae55d0c0f501846deb1b6f13a03a249d9a2db4a","title":"Code Generation for Unknown Libraries via Reading API Documentations","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2022","authors":"Koki Washio,Yusuke Miyao","id":"7ae55d0c0f501846deb1b6f13a03a249d9a2db4a","summary":"This paper implements a model that can extract relevant code signatures from API documentations based on a natural language intent and copy primitives from the extracted signatures and can properly generate unknown primitives when extracted code signatures are noiseless.","score":1},{"url":"https://www.semanticscholar.org/paper/f557534feabd3fa25052fd590a64d687b9754986","title":"Can we generate shellcodes via natural language? An empirical study","venue":"International Conference on Automated Software Engineering","year":2022,"referenceCount":77,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/03/2022","authors":"Pietro Liguori,Erfan Al-Hossami,Domenico Cotroneo,R. Natella,Bojan Cukic,Samira Shaikh","id":"f557534feabd3fa25052fd590a64d687b9754986","summary":"The empirical analysis shows that NMT can generate assembly code snippets from the natural language with high accuracy and that in many cases can generate entire shellcodes with no errors.","score":1},{"url":"https://www.semanticscholar.org/paper/56a75a76b86ea27c106f3e3e4a4d546c24e8678c","title":"Programming Language Agnostic Mining of Code and Language Pairs with Sequence Labeling Based Question Answering","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/03/2022","authors":"Changran Hu,Akshara Reddi Methukupalli,Yutong Zhou,Chen Wu,Yubo Chen","id":"56a75a76b86ea27c106f3e3e4a4d546c24e8678c","summary":"This paper proposes a Sequence Labeling based Question Answering (SLQA) method to mine NL-PL pairs in a PL-agnostic manner and proposes to apply the BIO tagging scheme instead of the conventional binary scheme to mine the code solutions which are often composed of multiple blocks of a post.","score":1},{"url":"https://www.semanticscholar.org/paper/8bf6109ed9dc5379d2340913e4184904d342918c","title":"DETECTION OF SOURCE CODE IN INTERNET TEXTS USING AUTOMATICALLY GENERATED MACHINE LEARNING MODELS","venue":"Applied Computer Science","year":2022,"referenceCount":19,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/03/2022","authors":"M. Badurowicz","id":"8bf6109ed9dc5379d2340913e4184904d342918c","summary":"The software system was prepared for a discussion forum for software developers to find fragments of source code that were published without marking them as code snippets to be used for archiving purposes.","score":1},{"url":"https://www.semanticscholar.org/paper/66b7836001407120ad0000369af8b30c44788a33","title":"Transformer with Tree-order Encoding for Neural Program Generation","venue":"ArXiv","year":2022,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/05/2022","authors":"Klaudia Thellmann,Bernhard Stadler,Ricardo Usbeck,Jens Lehmann","id":"66b7836001407120ad0000369af8b30c44788a33","summary":"This work has extended the positional encoding of the Transformer to allow the attention mechanism to also attend over hierarchical positions in the input and realized a decoder based on a restrictive grammar graph model to improve the generation accuracy and ensure the well-formedness of the generated code.","score":1},{"url":"https://www.semanticscholar.org/paper/02da1f11b2620ee4a6c8b010fd1c8fe6f5ab5119","title":"Antecedent Predictions Are Dominant for Tree-Based Code Generation","venue":"ArXiv","year":2022,"referenceCount":22,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"Yihong Dong,Ge Li,Zhi Jin","id":"02da1f11b2620ee4a6c8b010fd1c8fe6f5ab5119","summary":"This paper proposes an effective method, named APTRANX (Antecedent Prioritized TRANX), on the basis of TRANX, which helps the model attach importance to antecedent predictions by exploiting the position information of the generated AST nodes.","score":1},{"url":"https://www.semanticscholar.org/paper/b109588511459bf46e94cb4eb68b5cf79b092795","title":"Antecedent Predictions Are More Important Than You Think: An Effective Method for Tree-Based Code Generation","venue":"","year":2022,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/08/2022","authors":"Yihong Dong,Ge Li,Zhi Jin","id":"b109588511459bf46e94cb4eb68b5cf79b092795","summary":"This paper designs an AST-to-Vector (AST2Vec) method, that maps AST node positions to two-dimensional vectors, to model the position information of AST nodes, and implements and trains an Antecedent Prioritized Tree-based code generation model called APT, which improves the performance of existing Seq2Tree methods.","score":1},{"url":"https://www.semanticscholar.org/paper/52fb239ea5cea1e9a2636f8f7922c8ede3e50ba7","title":"LILA: A Unified Benchmark for Mathematical Reasoning","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":18,"citationCount":13,"influentialCitationCount":2,"publicationDate":"31/10/2022","authors":"Swaroop Mishra,Matthew Finlayson,Pan Lu,Leonard Tang,S. Welleck,Chitta Baral,Tanmay Rajpurohit,Oyvind Tafjord,Ashish Sabharwal,Peter Clark,A. Kalyan","id":"52fb239ea5cea1e9a2636f8f7922c8ede3e50ba7","summary":"It is found that multi-tasking leads to significant improvements (average relative improvement of 21.83% F1 score vs. single-task models), indicating the room for improvement in general mathematical reasoning and understanding.","score":1},{"url":"https://www.semanticscholar.org/paper/5fd0533ba1068af1cb075d4eb71ee551691a71ac","title":"ExploitGen: Template-augmented exploit code generation based on CodeBERT","venue":"Journal of Systems and Software","year":2022,"referenceCount":38,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Guang Yang,Yu Zhou,Xiang Chen,Xiangyu Zhang,Tingting Han,Taolue Chen","id":"5fd0533ba1068af1cb075d4eb71ee551691a71ac","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/f40aeae3e522ada1f6a9f326841b01ef5c8657b6","title":"Unifying Language Learning Paradigms","venue":"ArXiv","year":2022,"referenceCount":113,"citationCount":39,"influentialCitationCount":6,"publicationDate":2022,"authors":"Yi Tay,M. Dehghani,V. Tran,Xavier García,Dara Bahri,Tal Schuster,Huaixiu Zheng,N. Houlsby,Donald Metzler","id":"f40aeae3e522ada1f6a9f326841b01ef5c8657b6","summary":"UL2 achieves SOTA performance on 50 well-established supervised NLP tasks ranging from language generation, language understanding, text classiﬁcation, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval.","score":1},{"url":"https://www.semanticscholar.org/paper/2ef60a4ea4ea53056be811ff55679eb59fb4b586","title":"Confident Adaptive Language Modeling","venue":"ArXiv","year":2022,"referenceCount":89,"citationCount":11,"influentialCitationCount":0,"publicationDate":"14/07/2022","authors":"Tal Schuster,Adam Fisch,Jai Gupta,M. Dehghani,Dara Bahri,V. Tran,Yi Tay,Donald Metzler","id":"2ef60a4ea4ea53056be811ff55679eb59fb4b586","summary":"This work introduces Conﬁdent Adaptive Language Modeling (CALM), a framework for dynamically allocating different amounts of compute per input and generation timestep, and demonstrates the efﬂcacy of the framework in reducing compute—speedup of up to × 3 —while provably maintaining high performance.","score":1},{"url":"https://www.semanticscholar.org/paper/ace0745f4449f20e4f4297476941fcd7dc7ab05c","title":"Higher Cognition: A Mechanical Perspective","venue":"Encyclopedia","year":2022,"referenceCount":69,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/08/2022","authors":"Robert Friedman","id":"ace0745f4449f20e4f4297476941fcd7dc7ab05c","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/eb5c1c666ce2fe1c531ecabfcdf264ae831fad89","title":"Understanding and Supporting Debugging Workflows in Multiverse Analysis","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/10/2022","authors":"Ken Gu,Eunice Jun,Tim Althoff","id":"eb5c1c666ce2fe1c531ecabfcdf264ae831fad89","summary":"In understanding the composition of a multiverse, data strong in statistical analysis and design implications for future multiverse analysis authoring systems are concluded.","score":1},{"url":"https://www.semanticscholar.org/paper/5e8bc5f84f3a550319b0d2b54cc0062b410d2328","title":"Taking Flight with Copilot","venue":"Queue","year":2022,"referenceCount":12,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/12/2022","authors":"C. Bird,Denae Ford,T. Zimmermann,Nicole Forsgren,Eirini Kalliamvakou,Travis Lowdermilk,Idan Gazit","id":"5e8bc5f84f3a550319b0d2b54cc0062b410d2328","summary":"This study of Copilot shows that developers spend more time reviewing code than actually writing code, and developer roles will shift so that more time is spent assessing suggestions related to the task than doing the task itself.","score":1},{"url":"https://www.semanticscholar.org/paper/5a05d7f6a2ee8fdb20f2e27baa95bd1e1a71c634","title":"Practitioners' Expectations on Code Completion","venue":"ArXiv","year":2023,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/01/2023","authors":"Chaozheng Wang,Junhao Hu,Cuiyun Gao,Yu Jin,Tao Xie,Hailiang Huang,Zhenyu Lei,Yuetang Deng","id":"5a05d7f6a2ee8fdb20f2e27baa95bd1e1a71c634","summary":"This work compares the practitioners’ demands with current research via conducting a literature review of papers on code completion published in premier publication venues from 2012 to 2022 and highlights the directions desirable for researchers to invest efforts towards developing code completion techniques for meeting practitioners' expectations.","score":1},{"url":"https://www.semanticscholar.org/paper/522069bf612482f913fd83b2982127a19b2ab9b3","title":"Source Code Recommender Systems: The Practitioners' Perspective","venue":"ArXiv","year":2023,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"Matteo Ciniselli,L. Pascarella,Emad Aghajani,Simone Scalabrino,R. Oliveto,G. Bavota","id":"522069bf612482f913fd83b2982127a19b2ab9b3","summary":"A study involving 80 software developers to investigate the characteristics of code recommender systems they consider important is presented, and a taxonomy of 70 requirements that should be considered when designing codeRecommender systems is presented.","score":1},{"url":"https://www.semanticscholar.org/paper/43112f25190a9e19dc84cc7a0851318fdd1d9f71","title":"INTENT: Interactive Tensor Transformation Synthesis","venue":"ACM Symposium on User Interface Software and Technology","year":2022,"referenceCount":58,"citationCount":1,"influentialCitationCount":0,"publicationDate":"28/10/2022","authors":"Zhanhui Zhou,Man To Tang,Qiping Pan,Shangyin Tan,Xinyu Wang,Tianyi Zhang","id":"43112f25190a9e19dc84cc7a0851318fdd1d9f71","summary":"INTENT, an interactive system that infers user intent and generates corresponding TensorFlow code on behalf of users, helps users understand and validate the semantics of generated code by rendering individual tensor transformation steps with intermediate results and element-wise data provenance.","score":1},{"url":"https://www.semanticscholar.org/paper/6fe61d77b8a4a090899867b79e32efd658f848e7","title":"Explainable Natural Language to Bash Translation using Abstract Syntax Tree","venue":"Conference on Computational Natural Language Learning","year":2021,"referenceCount":21,"citationCount":4,"influentialCitationCount":0,"publicationDate":2021,"authors":"Shikhar Bharadwaj,S. Shevade","id":"6fe61d77b8a4a090899867b79e32efd658f848e7","summary":"This work proposes a novel transformer based solution by utilizing Bash Abstract Syntax Trees and manual pages that performs on par with the state of the art performance on Natural Language Context to Command task and performs better than fine-tuned T5 and Seq2Seq models.","score":1},{"url":"https://www.semanticscholar.org/paper/012d5d4346e84b6e158b252de9c87589dd62b16e","title":"Efficient Constituency Tree based Encoding for Natural Language to Bash Translation","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":31,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Shikhar Bharadwaj,S. Shevade","id":"012d5d4346e84b6e158b252de9c87589dd62b16e","summary":"A Segmented Invocation Transformer (SIT) that utilizes the information from the constituency parse tree of the natural language text and Bash command components is proposed that improves the performance of the model.","score":1},{"url":"https://www.semanticscholar.org/paper/94ac02f13ac3252a55b8740b7b310383cdf53445","title":"ShellFusion: Answer Generation for Shell Programming Tasks via Knowledge Fusion","venue":"International Conference on Software Engineering","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/05/2022","authors":"Neng Zhang,Chao Liu,Xin Xia,Christoph Treude,Ying Zou,David Lo,Zibin Zheng","id":"94ac02f13ac3252a55b8740b7b310383cdf53445","summary":"This work proposes an approach, i.e., ShellFusion, to automatically generate comprehensive answers (including relevant shell commands, scripts, and explanations) for shell programming tasks, which significantly outperforms Magnum and DeepAns (a recent answer recommendation baseline).","score":1},{"url":"https://www.semanticscholar.org/paper/988cb68d6510f3c4477b8c8ffe9cbdbea7971474","title":"Towards NLP-based Processing of Honeypot Logs","venue":"2022 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)","year":2022,"referenceCount":11,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/06/2022","authors":"Matteo Boffa,Giulia Milan,L. Vassio,I. Drago,M. Mellia,Zied Ben-Houidi","id":"988cb68d6510f3c4477b8c8ffe9cbdbea7971474","summary":"This work considers a widely used SSH/Telnet honeypot to record more than 200000 sessions, including 61000 unique shell scripts, some containing sequences of more than 100 Bash commands, to evaluate whether Natural Language Processing approaches can provide meaningful representations to find common traits in attackers' activity.","score":1},{"url":"https://www.semanticscholar.org/paper/4b85c2ee560ccb3c62c75ee52fb5dda94353591a","title":"Code2Snapshot: Using Code Snapshots for Learning Representations of Source Code","venue":"","year":2021,"referenceCount":32,"citationCount":3,"influentialCitationCount":0,"publicationDate":"01/11/2021","authors":"Md Rafiqul Islam Rabin,Mohammad Amin Alipour","id":"4b85c2ee560ccb3c62c75ee52fb5dda94353591a","summary":"Interestingly, obscuring input programs have insigniﬁcant impacts on the C ODE 2S NAPSHOT performance, suggesting that, for some tasks, neural models may provide high performance by relying merely on the structure of input programs.","score":1},{"url":"https://www.semanticscholar.org/paper/6a3b6512de2caa712311feb876f1be599a7c0b68","title":"Encoding Program as Image: Evaluating Visual Representation of Source Code","venue":"ArXiv","year":2021,"referenceCount":26,"citationCount":1,"influentialCitationCount":0,"publicationDate":2021,"authors":"Md Rafiqul Islam Rabin,Mohammad Amin Alipour","id":"6a3b6512de2caa712311feb876f1be599a7c0b68","summary":"This paper investigates Code2Snapshot, a novel representation of the source code that is based on the snapshots of input programs, and evaluates several variations of this representation and compares its performance with state-of-the-art representations that utilize the rich syntactic and semantic features ofinput programs.","score":1},{"url":"https://www.semanticscholar.org/paper/87bb5593d04450bbbd29afc5b2ef395127d1ba6a","title":"Testing the Robustness of a BiLSTM-based Structural Story Classifier","venue":"ArXiv","year":2022,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/01/2022","authors":"Aftab Hussain,Sai Durga Prasad Nanduri,Sneha Seenuvasavarathan","id":"87bb5593d04450bbbd29afc5b2ef395127d1ba6a","summary":"This work examines the impact of noise on a state-of-the-art, structural model based on BiLSTM (Bidirectional Long-Short Term Model) for fake news detection, Hierarchical Discourse-level Structure for Fake News Detection by Karimi and Tang.","score":1},{"url":"https://www.semanticscholar.org/paper/f0aacc7a0379883c4ab67d9a2d852c7bd99d9797","title":"Extracting Label-specific Key Input Features for Neural Code Intelligence Models","venue":"ArXiv","year":2022,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/02/2022","authors":"Md Rafiqul Islam Rabin","id":"f0aacc7a0379883c4ab67d9a2d852c7bd99d9797","summary":"Extracting key input features from reduced programs reveals that the syntax-guided reduced programs contain more label-specific key input Features that may help to understand the reasoning of models’ prediction from different perspectives and increase the trustworthiness to correct classification given by CI models.","score":1},{"url":"https://www.semanticscholar.org/paper/b070d2c844d5b18d0c94fb6b20bef3946d60abfd","title":"Readle: A Formal Framework for Designing AI-based Edge Systems","venue":"ArXiv","year":2022,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/05/2022","authors":"Aftab Hussain","id":"b070d2c844d5b18d0c94fb6b20bef3946d60abfd","summary":"A new systematic, extendable, manual approach, R EADLE, is proposed for creating representations of speciﬁcations in edge intelligent systems, capturing constraints in the edge system design space and constraint in the deep learning space in a coherent fashion.","score":1},{"url":"https://www.semanticscholar.org/paper/6042c51ccce53b94b84d1bdbcb33c3ab493323b4","title":"Syntax-guided program reduction for understanding neural code intelligence models","venue":"MAPS@PLDI","year":2022,"referenceCount":29,"citationCount":4,"influentialCitationCount":0,"publicationDate":"28/05/2022","authors":"Md Rafiqul Islam Rabin,Aftab Hussain,Mohammad Amin Alipour","id":"6042c51ccce53b94b84d1bdbcb33c3ab493323b4","summary":"A syntax-guided program reduction technique that considers the grammar of the input programs during reduction that is faster and provides smaller sets of key tokens in reduced programs is applied.","score":1},{"url":"https://www.semanticscholar.org/paper/81ce2664e892fc5f71fa4f8d61e7b42314dccb5e","title":"FeatureExtractor: A tool for extracting key input features of code intelligence models","venue":"Softw. Impacts","year":2022,"referenceCount":18,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/10/2022","authors":"Md Rafiqul Islam Rabin,Mohammad Amin Alipour","id":"81ce2664e892fc5f71fa4f8d61e7b42314dccb5e","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/66eae7128c34dd7967d79224eb9dbc978773c3d0","title":"I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation","venue":"ArXiv","year":2022,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Chandra Bhagavatula,Jena D. Hwang,Doug Downey,Ronan Le Bras,Ximing Lu,Keisuke Sakaguchi,Swabha Swayamdipta,Peter West,Yejin Choi","id":"66eae7128c34dd7967d79224eb9dbc978773c3d0","summary":"The key intellectual question is whether it is possible, if at all, to design a learning algorithm that does not beneﬁt from scale, yet leads to a competitive level of commonsense acquisition.","score":1},{"url":"https://www.semanticscholar.org/paper/459176532c85ae72f8b5cb35589b72468401d844","title":"SelfAPR: Self-supervised Program Repair with Test Execution Diagnostics","venue":"International Conference on Automated Software Engineering","year":2022,"referenceCount":98,"citationCount":6,"influentialCitationCount":1,"publicationDate":"23/03/2022","authors":"He Ye,Matias Martinez,Xiapu Luo,Tao Zhang,Monperrus Martin","id":"459176532c85ae72f8b5cb35589b72468401d844","summary":"SelfAPR correctly repairs 110 bugs from Defects4J, outperforming all the supervised learning repair approaches and executes all training samples and extracts and encodes test execution diagnostics into the input representation, steering the neural model to fix the kind of fault.","score":1},{"url":"https://www.semanticscholar.org/paper/3994eb8e237a94dae1efc6e767a09044b8550ace","title":"FCM: Forgetful Causal Masking Makes Causal Language Models Better Zero-Shot Learners","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Hao Liu,Xinyang Geng,Lisa Lee,Igor Mordatch,S. Levine,Sharan Narang,P. Abbeel","id":"3994eb8e237a94dae1efc6e767a09044b8550ace","summary":"Experimental results show that the proposed technique improves PaLM’s zero and few-shot performance on a diverse suite of tasks, including commonsense reasoning, natural language inference and cloze completion, and also helps representation learning.","score":1},{"url":"https://www.semanticscholar.org/paper/96f0f08e2dbeacc89a30d419a9cfb24312bd8da7","title":"BigIssue: A Realistic Bug Localization Benchmark","venue":"ArXiv","year":2022,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/07/2022","authors":"Paul Kassianik,Erik Nijkamp,Bo Pang,Yingbo Zhou,Caiming Xiong","id":"96f0f08e2dbeacc89a30d419a9cfb24312bd8da7","summary":"The introduction of BigIssue is proposed, a general benchmark for realistic bug localization and a motivation to improve bug localization capabilities of models through attention to the full repository context, to advance the state of the art in bug localization.","score":1},{"url":"https://www.semanticscholar.org/paper/8fbd7ddf1ea30c991f3b1152a245df77caa18e16","title":"Learning by Distilling Context","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":6,"influentialCitationCount":0,"publicationDate":"30/09/2022","authors":"Charles Burton Snell,D. Klein,Ruiqi Zhong","id":"8fbd7ddf1ea30c991f3b1152a245df77caa18e16","summary":"This work shows that context distillation is a general method to train language models, and it can effectively internalize 3 types of training signals, and can internalize step-by-step reasoning for complex tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/78f1ca609cd6f789749365c2870e2c2efd8f1fdf","title":"UniMASK: Unified Inference in Sequential Decision Problems","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":4,"influentialCitationCount":1,"publicationDate":"20/11/2022","authors":"Micah Carroll,Orr Paradise,Jessy Lin,Raluca Georgescu,Mingfei Sun,David Bignell,Stephanie Milani,Katja Hofmann,Matthew J. Hausknecht,A. Dragan,Sam Devlin","id":"78f1ca609cd6f789749365c2870e2c2efd8f1fdf","summary":"The Uni [MASK] framework is introduced, which provides a uniﬁed way to specify models which can be trained on many different sequential decision making tasks, and it is shown that a single Uni [ MASK] model is often capable of carrying out many tasks with performance similar to or better than single-task models.","score":1},{"url":"https://www.semanticscholar.org/paper/ef58e99dbb90bebbdc6187b5cba94dd5973dbb16","title":"Retrieval-Augmented Multimodal Language Modeling","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":3,"influentialCitationCount":1,"publicationDate":2022,"authors":"Michihiro Yasunaga,Armen Aghajanyan,Weijia Shi,Rich James,J. Leskovec,Percy Liang,M. Lewis,Luke Zettlemoyer,Wen-tau Yih","id":"ef58e99dbb90bebbdc6187b5cba94dd5973dbb16","summary":"Retrieval-Augmented CM3, the first retrieval-augmented multimodal model that can retrieve and generate mixtures of text and images, is proposed and shows that RA-CM3 exhibits novel capabilities such as knowledge-intensive image generation and multimodals in-context.","score":1},{"url":"https://www.semanticscholar.org/paper/7ed237af793f43c442b3e8e1bc9ace906a276b2a","title":"Transfer Knowledge from Natural Language to Electrocardiography: Can We Detect Cardiovascular Disease Through Language Models?","venue":"ArXiv","year":2023,"referenceCount":95,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/01/2023","authors":"Jielin Qiu,W. Han,Jiacheng Zhu,Mengdi Xu,Michael Rosenberg,Emerson Liu,Douglas Weber,Ding Zhao","id":"7ed237af793f43c442b3e8e1bc9ace906a276b2a","summary":"This work proposes an approach for cardiovascular disease diagnosis and automatic ECG diagnosis report generation and introduces an additional loss function by Optimal Transport to align the distribution between ECG and language embedding and proves the feasibility of transferring knowledge from LLMs to the cardiac domain.","score":1},{"url":"https://www.semanticscholar.org/paper/99752e255a866484291866a5ff5cf94e96d6bdc4","title":"Is a Question Decomposition Unit All We Need?","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":40,"citationCount":7,"influentialCitationCount":1,"publicationDate":"25/05/2022","authors":"Pruthvi H. Patel,Swaroop Mishra,Mihir Parmar,Chitta Baral","id":"99752e255a866484291866a5ff5cf94e96d6bdc4","summary":"The findings indicate that Human-in-the-loop Question Decomposition (HQD) can potentially provide an alternate path to building large LMs and provides a viable option to involve people in NLP research in a meaningful way.","score":1},{"url":"https://www.semanticscholar.org/paper/a81d05e8812cd4adbd76bf408efdcab05d6bb8d7","title":"Creative Use of XAI In Socio-Technical Systems: A Case Study","venue":"","year":2022,"referenceCount":14,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Michaela Benk","id":"a81d05e8812cd4adbd76bf408efdcab05d6bb8d7","summary":"It is shown that AI and explainability methods are used in creative ways in daily workflows, resulting in a divergence between their intended and actual use.","score":1},{"url":"https://www.semanticscholar.org/paper/3fbea6c84b8c78c59392d7bf864dbe681924015f","title":"How Can We Develop Explainable Systems? Insights from a Literature Review and an Interview Study","venue":"ICSSP/ICGSE","year":2022,"referenceCount":87,"citationCount":5,"influentialCitationCount":0,"publicationDate":"04/04/2022","authors":"Larissa Chazette,J. Klünder,Merve Balci,K. Schneider","id":"3fbea6c84b8c78c59392d7bf864dbe681924015f","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/ba77991d19cf8c50ae2d2efcc9b5fb141acaa7b4","title":"Requirements Engineering for Machine Learning: A Review and Reflection","venue":"2022 IEEE 30th International Requirements Engineering Conference Workshops (REW)","year":2022,"referenceCount":182,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/08/2022","authors":"Zhong Pei,Lin Liu,Chen Wang,Jianmin Wang","id":"ba77991d19cf8c50ae2d2efcc9b5fb141acaa7b4","summary":"This paper aims to provide an overview of the requirements engineering process for machine learning applications in terms of cross domain collaborations, and goes through the collaborative requirements analysis process step-by-step.","score":1},{"url":"https://www.semanticscholar.org/paper/be747953df4b3269534c54addddc889986550343","title":"Psychological Impact and Influence of Animation on Viewer's Visual Attention and Cognition: A Systematic Literature Review, Open Challenges, and Future Research Directions","venue":"Computational and Mathematical Methods in Medicine","year":2022,"referenceCount":85,"citationCount":1,"influentialCitationCount":0,"publicationDate":"31/08/2022","authors":"C. K. Praveen,Kathiravan Srinivasan","id":"be747953df4b3269534c54addddc889986550343","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/edb6c93255bbaa879f6d4af173a947a2026ed4c6","title":"A nascent design theory for explainable intelligent systems","venue":"Electronic Markets","year":2022,"referenceCount":132,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"L. Herm,Th. Steinbach,Jonas Wanner,Christian Janiesch","id":"edb6c93255bbaa879f6d4af173a947a2026ed4c6","summary":"A nascent design theory for explainable intelligent systems is derived and evaluated based on a structured literature review, two qualitative expert studies, a real-world use case application, and quantitative research about how to socio-technically design these systems to address acceptance barriers among different user groups.","score":1},{"url":"https://www.semanticscholar.org/paper/80a6501f518eaaf495e72373fc4128d374d25395","title":"Charting the Sociotechnical Gap in Explainable AI: A Framework to Address the Gap in XAI","venue":"ArXiv","year":2023,"referenceCount":167,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Upol Ehsan,Koustuv Saha,Munmun De Choudhury,Mark O. Riedl","id":"80a6501f518eaaf495e72373fc4128d374d25395","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/36589346063ff26506330451976280011273b935","title":"Towards Teachable Reasoning Systems","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":10,"influentialCitationCount":3,"publicationDate":2022,"authors":"Bhavana Dalvi,Oyvind Tafjord,Peter Clark","id":"36589346063ff26506330451976280011273b935","summary":"Generated chains of reasoning show how answers are implied by the system’s own internal beliefs, and are both faithful and truthful, which suggests new opportunities for using language models in an interactive setting where users can inspect, debug, correct, and improve a system‘s performance over time.","score":1},{"url":"https://www.semanticscholar.org/paper/18bda734a1546eae13f6b13600023ff73f95b6e3","title":"Program Synthesis Through Learning the Input-Output Behavior of Commands","venue":"IEEE Access","year":2022,"referenceCount":20,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Sihyung Lee,S. Nam,Jiyeon Kim","id":"18bda734a1546eae13f6b13600023ff73f95b6e3","summary":"The proposed system receives the syntax of the available commands and learns their meanings independently by writing programs and observing their input-output behavior, and believes that the proposed system provides a basis for synthesis systems based on learning input- Output behavior.","score":1},{"url":"https://www.semanticscholar.org/paper/29ed68d701f8450853938827b5124c9613c56aff","title":"ET-BERT: A Contextualized Datagram Representation with Pre-training Transformers for Encrypted Traffic Classification","venue":"The Web Conference","year":2022,"referenceCount":47,"citationCount":4,"influentialCitationCount":0,"publicationDate":"13/02/2022","authors":"Xinjie Lin,G. Xiong,Gaopeng Gou,Zhen Li,Junzheng Shi,J. Yu","id":"29ed68d701f8450853938827b5124c9613c56aff","summary":"This paper proposes a new traffic representation model called Encrypted Traffic Bidirectional Encoder Representations from Transformer (ET-BERT), which pre-trains deep contextualized datagram-level representation from large-scale unlabeled data and achieves state-of-the-art performance across five encrypted traffic classification tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/608df5bd7a0ad0f4d335ae4d071b8cfe60e2f3c5","title":"On the Effectiveness of Pretrained Models for API Learning","venue":"IEEE International Conference on Program Comprehension","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/04/2022","authors":"M. Hadi,Imam Nur Bani Yusuf,Ferdian Thung,K. Luong,Lingxiao Jiang,F. Fard,David Lo","id":"608df5bd7a0ad0f4d335ae4d071b8cfe60e2f3c5","summary":"This work uses a dataset that contains 7 million annotations collected from GitHub to evaluate the effectiveness of recent Pre-trained Transformer based Models (PTMs) for the API learning task and identifies two different tokenization approaches that can contribute to a significant boost in PTMs' performance for theAPI sequence generation task.","score":1},{"url":"https://www.semanticscholar.org/paper/3eda53506586216acc96f4f34446f697874f360c","title":"Learning to Induce Causal Structure","venue":"ArXiv","year":2022,"referenceCount":81,"citationCount":9,"influentialCitationCount":0,"publicationDate":"11/04/2022","authors":"Nan Rosemary Ke,S. Chiappa,Jane X. Wang,J. Bornschein,T. Weber,Anirudh Goyal,Matthew Botvinic,M. Mozer,Danilo Jimenez Rezende","id":"3eda53506586216acc96f4f34446f697874f360c","summary":"The performance of the model improves as it observes more interventions, this suggest that the model is able to extract useful information from interventions in order to predict the causal structure.","score":1},{"url":"https://www.semanticscholar.org/paper/905cbe787b20fca3917d2afd6a7a5f073a50386e","title":"MP-CodeCheck: Evolving Logical Expression Code Anomaly Learning with Iterative Self-Supervision","venue":"ArXiv","year":2022,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/04/2022","authors":"Urs C. Muff,Celine Lee,Paul Gottschlich,Justin Emile Gottschlich","id":"905cbe787b20fca3917d2afd6a7a5f073a50386e","summary":"This work presents MP-CodeCheck, an MP system that tries to identify anomalous code patterns within logical program expressions and compares it against ControlFlag, a state-of-the-art self-supervised code anomaly detection system; it is found that MPCC is more spatially and temporally efficient.","score":1},{"url":"https://www.semanticscholar.org/paper/abab9ae27efb40bbe6ffb9f6d27d56001412d856","title":"Scaling Genetic Improvement and Automated Program Repair","venue":"APR","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/05/2022","authors":"M. Harman","id":"abab9ae27efb40bbe6ffb9f6d27d56001412d856","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/74600cebaec0ecd6a9bde7e3830d813899bf8a91","title":"From {Solution Synthesis} to {Student Attempt Synthesis} for Block-Based Visual Programming Tasks","venue":"ArXiv","year":2022,"referenceCount":52,"citationCount":2,"influentialCitationCount":0,"publicationDate":"03/05/2022","authors":"A. Singla,Nikitas Theodoropoulos","id":"74600cebaec0ecd6a9bde7e3830d813899bf8a91","summary":"This work inves-tigate the crucial component of student modeling, the ability to automatically infer students’ misconceptions for predicting (synthesizing) their behavior, and introduces a novel benchmark, StudentSyn, centered around the following challenge: For a given student, synthesize the student’s attempt on a new target task after observing theStudentSyn.","score":1},{"url":"https://www.semanticscholar.org/paper/e7e1feff05edf89cac6c2e6de46815a3f89144ef","title":"Tensor Program Optimization with Probabilistic Programs","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":2,"influentialCitationCount":0,"publicationDate":"26/05/2022","authors":"Junru Shao,Xiyou Zhou,Siyuan Feng,Bohan Hou,Ruihang Lai,Hongyi Jin,Wuwei Lin,Masahiro Masuda,Cody Hao Yu,Tianqi Chen","id":"e7e1feff05edf89cac6c2e6de46815a3f89144ef","summary":"Experimental results show that MetaSchedule can cover the search space used in the state-of-the-art tensor program optimization frameworks in a modular way, and it empowers domain experts to conveniently grow theSearch space and modularly enhance the system, which brings 48% speedup on end-to-end deep learning workloads.","score":1},{"url":"https://www.semanticscholar.org/paper/29acc890e521f7a6415666ab9eb3432c49b4587a","title":"Self-critiquing models for assisting human evaluators","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":17,"influentialCitationCount":1,"publicationDate":"12/06/2022","authors":"W. Saunders,Catherine Yeh,Jeff Wu,Steven Bills,Long Ouyang,Jonathan Ward,J. Leike","id":"29acc890e521f7a6415666ab9eb3432c49b4587a","summary":"This work fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning, and suggests that even large models may still have relevant knowledge they cannot or do not articulate as critiques with both topic-based summarization and synthetic tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/a82d6acc26d34f25d572da5dace6c29f4acfddfc","title":"Actionable Guidance for High-Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic Risks","venue":"ArXiv","year":2022,"referenceCount":72,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/06/2022","authors":"A. Barrett,Dan Hendrycks,J. Newman,Brandie Nonnecke","id":"a82d6acc26d34f25d572da5dace6c29f4acfddfc","summary":"This document provides detailed actionable-guidance recommendations focused on identifying and managing risks of events with very high or catastrophic consequences, intended as a risk management practices resource for NIST for AI RMF version 1.0 (scheduled for release in early 2023), or for AIRMF users, or for other AI risk management guidance and standards as appropriate.","score":1},{"url":"https://www.semanticscholar.org/paper/3a2aa950971a46167b6da9431098b02facffe342","title":"Questions Are All You Need to Train a Dense Passage Retriever","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":6,"influentialCitationCount":2,"publicationDate":"21/06/2022","authors":"Devendra Singh Sachan,M. Lewis,Dani Yogatama,Luke Zettlemoyer,J. Pineau,M. Zaheer","id":"3a2aa950971a46167b6da9431098b02facffe342","summary":"ART is introduced, a new corpus-level autoencoding approach for training dense retrieval models that does not require any labeled training data and removes the need for labeled data and task-speciﬁc losses.","score":1},{"url":"https://www.semanticscholar.org/paper/746b6108a72b2b1bd78f70d4b1a211cdebfd8f49","title":"Multi-objective Grammar-guided Genetic Programming with Code Similarity Measurement for Program Synthesis","venue":"IEEE Congress on Evolutionary Computation","year":2022,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/07/2022","authors":"Ning Tao,Anthony Ventresque,Takfarinas Saber","id":"746b6108a72b2b1bd78f70d4b1a211cdebfd8f49","summary":"A novel multi-objective G3P approach that combines the similarity to the target program and the traditional input/output error rate is proposed and shown to improve the success rate of specific problems and has great potential to improve on the traditional G 3P system.","score":1},{"url":"https://www.semanticscholar.org/paper/45875d5f55c72c1bdfa6d7c312eead7dcc93123d","title":"Borch: A Deep Universal Probabilistic Programming Language","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/09/2022","authors":"Lewis Belcher,Johan Gudmundsson,Michael Green","id":"45875d5f55c72c1bdfa6d7c312eead7dcc93123d","summary":"Borch is presented, a scalable deep universal probabilistic programming language, built on top of PyTorch, which aims to force the models it creates to represent, learn, and report uncertainty in every prediction that is made.","score":1},{"url":"https://www.semanticscholar.org/paper/558b7245a54575e16143324df98129254d5a244c","title":"Enhancing Code Similarity with Augmented Data Filtering and Ensemble Strategies","venue":"JOIV: International Journal on Informatics Visualization","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/09/2022","authors":"Gyeongmin Kim,Minseok Kim,Jaechoon Jo","id":"558b7245a54575e16143324df98129254d5a244c","summary":"This algorithm is the first automated development system for increasing software productivity that addresses the current situation—a worldwide shortage of software dramatically improves performance in various downstream natural language processing tasks (NLP).","score":1},{"url":"https://www.semanticscholar.org/paper/d400a649f0f0a3de22b89a268f48aff2dcb06a09","title":"Entailer: Answering Questions with Faithful and Truthful Chains of Reasoning","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":38,"citationCount":2,"influentialCitationCount":0,"publicationDate":"21/10/2022","authors":"Oyvind Tafjord,Bhavana Dalvi,Peter Clark","id":"d400a649f0f0a3de22b89a268f48aff2dcb06a09","summary":"This work recursively combines a trained backward-chaining model, capable of generating a set of premises entailing an answer hypothesis, with a verifier that checks that the model itself believes those premises (and the entailment itself) through self-querying.","score":1},{"url":"https://www.semanticscholar.org/paper/4af2891ce1aab624c4917e8a69fcee5c8a1f41db","title":"NL2Viz: natural language to visualization via constrained syntax-guided synthesis","venue":"ESEC/SIGSOFT FSE","year":2022,"referenceCount":40,"citationCount":1,"influentialCitationCount":0,"publicationDate":"07/11/2022","authors":"Zhengkai Wu,Vu Le,A. Tiwari,Sumit Gulwani,Arjun Radhakrishna,Ivan Radicek,Gustavo Soares,Xinyu Wang,Zhenwen Li,Tao Xie","id":"4af2891ce1aab624c4917e8a69fcee5c8a1f41db","summary":"This work proposes a new approach and its supporting tool named NL2VIZ with three salient features: leveraging not only the user's NL input but also the data and program context that the NL query is upon, and providing support for result refinement and reuse.","score":1},{"url":"https://www.semanticscholar.org/paper/3d3012bfcc8bc7e4dc84c177e94650e66f03bc5b","title":"Are ChatGPT and AlphaCode going to replace programmers?","venue":"Nature","year":2022,"referenceCount":1,"citationCount":8,"influentialCitationCount":0,"publicationDate":"08/12/2022","authors":"D. Castelvecchi","id":"3d3012bfcc8bc7e4dc84c177e94650e66f03bc5b","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/000b8567d17dd982ae226c29505027ed692911dd","title":"AlphaCode and “data-driven” programming","venue":"Science","year":2022,"referenceCount":3,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/12/2022","authors":"J. Kolter","id":"000b8567d17dd982ae226c29505027ed692911dd","summary":"The AlphaCode system is presented, which represents a substantial step forward in the development of machine learning models that can synthesize computer programs to solve these types of challenging problems, and what is perhaps most surprising about the system is what AlphaCode does not do: it contains no explicit built-in knowledge about the structure of computer code.","score":1},{"url":"https://www.semanticscholar.org/paper/e8aa5f51aaf29344174f90d7edca49cc153a6b00","title":"Economic impacts of AI-augmented R&D","venue":"","year":2022,"referenceCount":112,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/12/2022","authors":"T. Besiroglu,Nicholas Emery-Xu,Neil C. Thompson","id":"e8aa5f51aaf29344174f90d7edca49cc153a6b00","summary":"This work estimates the idea production function for AI in two computer vision tasks that are considered key test-beds for deep learning and suggests that AI-augmented R&D has the potential to speed up technological change and economic growth.","score":1},{"url":"https://www.semanticscholar.org/paper/b7823997fb185f208b6a6723b60413ff179d2639","title":"Standing on the Shoulders of AI Giants","venue":"Computer","year":2023,"referenceCount":12,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/01/2023","authors":"Hsiao-Ying Lin","id":"b7823997fb185f208b6a6723b60413ff179d2639","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/e42843bfb05263df00837fa1b287bc816296e1fc","title":"Transformers Meet Directed Graphs","venue":"ArXiv","year":2023,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Simon Geisler,Yujia Li,D. Mankowitz,A. Cemgil,Stephan Gunnemann,Cosmin Paduraru","id":"e42843bfb05263df00837fa1b287bc816296e1fc","summary":"This work proposes two direction- and structure-aware positional encodings for directed graphs: the eigenvectors of the Magnetic Laplacian - a direction-aware generalization of the combinatorial LaPLacian; and directional random walkencodings.","score":1},{"url":"https://www.semanticscholar.org/paper/be09ed6cd73654a23f78416433a1b23ea623ea79","title":"Symbolic Behaviour in Artificial Intelligence","venue":"ArXiv","year":2021,"referenceCount":139,"citationCount":19,"influentialCitationCount":0,"publicationDate":"05/02/2021","authors":"Adam Santoro,Andrew Kyle Lampinen,K. Mathewson,T. Lillicrap,David Raposo","id":"be09ed6cd73654a23f78416433a1b23ea623ea79","summary":"This work argues that the path towards symbolically fluent artificial intelligence (AI) begins with a reinterpretation of what symbols are, how they come to exist, and how a system behaves when it uses them, and suggests that AI research explore social and cultural engagement as a tool to develop the cognitive machinery necessary for symbolic behaviour to emerge.","score":1},{"url":"https://www.semanticscholar.org/paper/abd77509ef1cc739c0757ae657025fcee13c98dc","title":"Program Synthesis Guided Reinforcement Learning for Partially Observed Environments","venue":"Neural Information Processing Systems","year":2021,"referenceCount":83,"citationCount":5,"influentialCitationCount":0,"publicationDate":"22/02/2021","authors":"Yichen Yang,J. Inala,O. Bastani,Yewen Pu,Armando Solar-Lezama,M. Rinard","id":"abd77509ef1cc739c0757ae657025fcee13c98dc","summary":"This work proposes a new approach, model predictive program synthesis (MPPS), that uses program synthesis to automatically generate the guiding programs for program-guided reinforcement learning without requiring the user to provide a new guiding program for every new task.","score":1},{"url":"https://www.semanticscholar.org/paper/1093adc2a47212ef599e4708fe3e41ff7c9ec6d0","title":"GenLine and GenForm: Two Tools for Interacting with Generative Language Models in a Code Editor","venue":"ACM Symposium on User Interface Software and Technology","year":2021,"referenceCount":8,"citationCount":6,"influentialCitationCount":0,"publicationDate":"10/10/2021","authors":"Ellen Jiang,Edwin Toh,A. Molina,Aaron Donsbach,Carrie J. Cai,Michael Terry","id":"1093adc2a47212ef599e4708fe3e41ff7c9ec6d0","summary":"A macro system with two tools that allow users to invoke language model prompts as macros in a code editor, and a form-like interface where the user provides input that is then transformed into multiple pieces of output at the same time.","score":1},{"url":"https://www.semanticscholar.org/paper/3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e","title":"A General Language Assistant as a Laboratory for Alignment","venue":"ArXiv","year":2021,"referenceCount":52,"citationCount":53,"influentialCitationCount":2,"publicationDate":"01/12/2021","authors":"Amanda Askell,Yuntao Bai,Anna Chen,Dawn Drain,Deep Ganguli,T. Henighan,Andy Jones,Nicholas Joseph,Benjamin Mann,Nova DasSarma,Nelson Elhage,Zac Hatfield-Dodds,Danny Hernandez,John Kernion,Kamal Ndousse,Catherine Olsson,Dario Amodei,Tom B. Brown,Jack Clark,Sam McCandlish,C. Olah,Jared Kaplan","id":"3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e","summary":"A ‘preference model pre-training’ stage of training is studied, with the goal of improving sample efﬁciency when ﬁnetuning on human preferences, and investigating scaling trends for several training objectives relevant to alignment.","score":1},{"url":"https://www.semanticscholar.org/paper/4b70f356f50d3e8e1f72c4a10f0ce2a26da95b5a","title":"Controlling Conditional Language Models with Distributional Policy Gradients","venue":"ArXiv","year":2021,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Tomasz Korbak,Hady ElSahar,Germán Kruszewski,Marc Dymetman","id":"4b70f356f50d3e8e1f72c4a10f0ce2a26da95b5a","summary":"The results show that fine-tuning using CDPG robustly moves these pretrained models closer towards meeting control objectives and — in contrast with baseline approaches — does not result in catastrophic forgetting.","score":1},{"url":"https://www.semanticscholar.org/paper/54d00fc330248b3b2f89193da31bb17851ebd2b7","title":"M EMORIZING T RANSFORMERS","venue":"","year":2022,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Memorizing Transformers,Yuhuai Wu,Markus N. Rabe,DeLesley S. Hutchins,Christian Szegedy","id":"54d00fc330248b3b2f89193da31bb17851ebd2b7","summary":"It is demonstrated that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext, math papers, books, code, as well as formal theorems (Isabelle).","score":1},{"url":"https://www.semanticscholar.org/paper/2aec574791fd33e9be32fd5191a66734f805a6a1","title":"Training Language Models with Natural Language Feedback","venue":"","year":2022,"referenceCount":35,"citationCount":4,"influentialCitationCount":0,"publicationDate":2022,"authors":"J. Scheurer,Jon Ander Campos,Jun Shern Chan,Angelica Chen,Kyunghyun Cho,Ethan Perez","id":"2aec574791fd33e9be32fd5191a66734f805a6a1","summary":"This work proposes to learn from natural language feedback, which conveys more information per human evaluation, from a GPT-3 model to roughly human-level summarization ability using a three-step learning algorithm.","score":1},{"url":"https://www.semanticscholar.org/paper/d3640eb3b542eaf36fee2261f037a6bf0d8eac9c","title":"AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts","venue":"International Conference on Human Factors in Computing Systems","year":2021,"referenceCount":96,"citationCount":36,"influentialCitationCount":1,"publicationDate":"04/10/2021","authors":"Tongshuang Sherry Wu,Michael Terry,Carrie J. Cai","id":"d3640eb3b542eaf36fee2261f037a6bf0d8eac9c","summary":"Chaining LLM steps together is introduced, where the output of one step becomes the input for the next, thus aggregating the gains per step, and found that Chaining not only improved the quality of task outcomes, but also significantly enhanced system transparency, controllability, and sense of collaboration.","score":1},{"url":"https://www.semanticscholar.org/paper/05af6c968ef8c7f9b07b0d67f138780179f29511","title":"Sparks: Inspiration for Science Writing using Language Models","venue":"IN2WRITING","year":2021,"referenceCount":84,"citationCount":8,"influentialCitationCount":0,"publicationDate":"14/10/2021","authors":"K. Gero,Vivian Liu,Lydia B. Chilton","id":"05af6c968ef8c7f9b07b0d67f138780179f29511","summary":"This work presents a system for generating “sparks”, sentences related to a scientific concept intended to inspire writers, and finds three main use cases of sparks—inspiration, translation, and perspective—each of which correlates with a unique interaction pattern.","score":1},{"url":"https://www.semanticscholar.org/paper/edc07f490c1c1b773094f236157219677f6a2f71","title":"Better Modeling the Programming World with Code Concept Graphs-augmented Multi-modal Learning","venue":"2022 IEEE/ACM 44th International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER)","year":2022,"referenceCount":39,"citationCount":1,"influentialCitationCount":0,"publicationDate":"10/01/2022","authors":"M. Weyssow,H. Sahraoui,Bang Liu","id":"edc07f490c1c1b773094f236157219677f6a2f71","summary":"This paper proposes to enhance an existing pretrained language model of code by joint-learning it with a graph neural network based on a concept graphs based on the idea of leveraging multi-modal learning approaches to modeling the programming world.","score":1},{"url":"https://www.semanticscholar.org/paper/0e802c0739771acf70e60d59c2df51cd7e8c50c0","title":"Memorizing Transformers","venue":"International Conference on Learning Representations","year":2022,"referenceCount":57,"citationCount":40,"influentialCitationCount":9,"publicationDate":"16/03/2022","authors":"Yuhuai Wu,Markus N. Rabe,DeLesley S. Hutchins,Christian Szegedy","id":"0e802c0739771acf70e60d59c2df51cd7e8c50c0","summary":"It is demonstrated that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext, math papers, books, code, as well as formal theorems (Isabelle).","score":1},{"url":"https://www.semanticscholar.org/paper/9a6730534295335247eebdec59b7decdeb83d59a","title":"On the Transferability of Pre-trained Language Models for Low-Resource Programming Languages","venue":"IEEE International Conference on Program Comprehension","year":2022,"referenceCount":49,"citationCount":4,"influentialCitationCount":0,"publicationDate":"05/04/2022","authors":"Fuxiang Chen,F. Fard,David Lo,T. Bryksin","id":"9a6730534295335247eebdec59b7decdeb83d59a","summary":"The results show that multilingual PLMs have a lower Performance-to-Time Ratio as compared to monolingual PLMs, and the proposed strategy to select target programming languages to fine-tune mult bilingual PLMs is effective — it reduces the time to Fine-Tune yet achieves higher performance in Code Summarization and Code Search tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/fd7c3c8fbe8cf88bd967ead02738b43081e306a7","title":"Training Language Models with Language Feedback","venue":"","year":2022,"referenceCount":43,"citationCount":6,"influentialCitationCount":1,"publicationDate":"29/04/2022","authors":"J. Scheurer,Jon Ander Campos,Jun Shern Chan,Angelica Chen,Kyunghyun Cho,Ethan Perez","id":"fd7c3c8fbe8cf88bd967ead02738b43081e306a7","summary":"This work proposes to learn from natural language feedback, which conveys more information per human evaluation, from a GPT-3 model to roughly human-level summarization ability using a three-step learning algorithm.","score":1},{"url":"https://www.semanticscholar.org/paper/16168520c7efbfa84bcb609a05362916b04022bb","title":"Context-Aware Abbreviation Expansion Using Large Language Models","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":45,"citationCount":3,"influentialCitationCount":0,"publicationDate":"08/05/2022","authors":"Shanqing Cai,Subhashini Venugopalan,Katrin Tomanek,Ajit Narayanan,M. Morris,Michael P. Brenner","id":"16168520c7efbfa84bcb609a05362916b04022bb","summary":"This work proposes a paradigm in which phrases are abbreviated aggressively as primarily word-initial letters, and aims to expand the abbreviations into full-phrase options by leveraging conversation context with the power of pretrained large language models (LLMs).","score":1},{"url":"https://www.semanticscholar.org/paper/6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":84,"citationCount":10,"influentialCitationCount":2,"publicationDate":"24/05/2022","authors":"Linlu Qiu,Peter Shaw,Panupong Pasupat,Tianze Shi,Jonathan Herzig,Emily Pitler,Fei Sha,Kristina Toutanova","id":"6e10343767ab09dde83cf99ea3442907402a9810","summary":"Limits of current techniques for effectively leveraging model scale for compositional generalization are highlighted, while the analysis also suggests promising directions for future work.","score":1},{"url":"https://www.semanticscholar.org/paper/0f86d5ae106a53f40f89b60dff24074f6c2cd127","title":"The Case for a Single Model that can Both Generate Continuations and Fill in the Blank","venue":"NAACL-HLT","year":2022,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/06/2022","authors":"Daphne Ippolito,Liam Dugan,Emily Reif,Ann Yuan,Andy Coenen,Chris Callison-Burch","id":"0f86d5ae106a53f40f89b60dff24074f6c2cd127","summary":"This work shows that models pre-trained with a F IT B- 012 style objective are capable of both tasks, while model pre- trained for continuation are not, and shows how these models can be easily tuned to allow forained control over the length and word choice of the generation.","score":1},{"url":"https://www.semanticscholar.org/paper/18a831422a4b4c89bbf1cc4baaa2cfcbf29daaf1","title":"Exploring and evaluating personalized models for code generation","venue":"ESEC/SIGSOFT FSE","year":2022,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/08/2022","authors":"Andrei Zlotchevski,Dawn Drain,Alexey Svyatkovskiy,Colin B. Clement,Neel Sundaresan,Michele Tufano","id":"18a831422a4b4c89bbf1cc4baaa2cfcbf29daaf1","summary":"This paper explores and evaluates transformer model fine-tuning for personalization in the context of generating unit tests for Java methods, and evaluates learning to personalize to a specific software project using several personalization techniques.","score":1},{"url":"https://www.semanticscholar.org/paper/3ee3f425482cf86989d809155cc8cf2bf8d8113e","title":"Understanding HTML with Large Language Models","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":1,"influentialCitationCount":0,"publicationDate":"08/10/2022","authors":"Izzeddin Gur,Ofir Nachum,Yingjie Miao,Mustafa Safdari,Austin Huang,Aakanksha Chowdhery,Sharan Narang,Noah Fiedel,Aleksandra Faust","id":"3ee3f425482cf86989d809155cc8cf2bf8d8113e","summary":"It is shown that LLMs pretrained on standard natural language corpora transfer re-markably well to HTML understanding tasks, and evidence that T5-based models are ideal due to their bidirectional encoder-decoder architecture is shown.","score":1},{"url":"https://www.semanticscholar.org/paper/13d3733e0dabbb4ccdd036a5f04fd5b3e39eecb0","title":"Vision Transformers provably learn spatial structure","venue":"ArXiv","year":2022,"referenceCount":77,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"Samy Jelassi,Michael E. Sander,Yuan-Fang Li","id":"13d3733e0dabbb4ccdd036a5f04fd5b3e39eecb0","summary":"This paper proposes a spatially structured dataset and a simplified ViT model and proposes a mechanism that implicitly learns the spatial structure of the dataset while generalizing, and proves that patch association helps to sample-efficiently transfer to downstream datasets that share the same structure as the pre-training one but differ in the features.","score":1},{"url":"https://www.semanticscholar.org/paper/e8db669c8cb1c07557ede15e2771968f9370330b","title":"Large language models are not zero-shot communicators","venue":"ArXiv","year":2022,"referenceCount":100,"citationCount":2,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Laura Ruis,Akbir Khan,Stella Rose Biderman,Sara Hooker,Tim Rocktaschel,Edward Grefenstette","id":"e8db669c8cb1c07557ede15e2771968f9370330b","summary":"A simple task is designed and widely used state-of-the-art models are evaluated, finding that, despite only evaluating on utterances that require a binary inference (yes or no), most perform close to random.","score":1},{"url":"https://www.semanticscholar.org/paper/e7d0a8eb7e98863f37b51d89b5ca305b04aaba99","title":"SemanticOn: Specifying Content-Based Semantic Conditions for Web Automation Programs","venue":"ACM Symposium on User Interface Software and Technology","year":2022,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/10/2022","authors":"Kevin Pu,Rainey Fu,Rui Dong,Xinyu Wang,Yuanchun Chen,Tovi Grossman","id":"e7d0a8eb7e98863f37b51d89b5ca305b04aaba99","summary":"SemanticOn is introduced, a system that enables users to specify, refine, and incorporate visual and textual semantic conditions in web automation programs via two methods: natural language description via prompts or information highlighting.","score":1},{"url":"https://www.semanticscholar.org/paper/9b5fb07df99b0dd65f3058701d7f017c3a70c144","title":"Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data","venue":"ArXiv","year":2023,"referenceCount":102,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/01/2023","authors":"Jing Wei,Sungdong Kim,Hyunhoon Jung,Young-Ho Kim","id":"9b5fb07df99b0dd65f3058701d7f017c3a70c144","summary":"This work formulated four prompt designs with different structures and personas to explore what design factors of prompts can help steer chatbots to talk naturally and collect data reliably and discusses the opportunities and challenges of building chatbots with LLMs.","score":1},{"url":"https://www.semanticscholar.org/paper/fd0bccf5e7c1fb5dadd75972e3212554fb255fe2","title":"MISIM: A Neural Code Semantics Similarity System Using the Context-Aware Semantics Structure","venue":"","year":2020,"referenceCount":62,"citationCount":3,"influentialCitationCount":0,"publicationDate":"05/06/2020","authors":"Fangke Ye,Sheng-Tian Zhou,Anand Venkat,Ryan Marcus,Nesime Tatbul,Jesmin Jahan Tithi,N. Hasabnis,Paul Petersen,T. Mattson,Tim Kraska,P. Dubey,Vivek Sarkar,Justin Emile Gottschlich","id":"fd0bccf5e7c1fb5dadd75972e3212554fb255fe2","summary":"This work presents Machine Inferred Code Similarity (MISIM), a neural code semantics similarity system consisting of two core components: a novel context-aware semantics structure, which was purpose-built to lift semantics from code syntax and an extensible neural code similarity scoring algorithm, which can be used for various neural network architectures with learned parameters.","score":1},{"url":"https://www.semanticscholar.org/paper/0c7cb854756f6b69f070a925cd497c2970b136f2","title":"DeSkew-LSH based Code-to-Code Recommendation Engine","venue":"ArXiv","year":2021,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Fran Silavong,S. Moran,Antonios Georgiadis,Rohan Saphal,R. Otter","id":"0c7cb854756f6b69f070a925cd497c2970b136f2","summary":"Senatus, a new code-to-code recommendation engine that addresses both the scale gracefully to large codebases and the global statistics of code repositories in the ranking function, such as the distribution of code snippet lengths, leads to sub-optimal retrieval results.","score":1},{"url":"https://www.semanticscholar.org/paper/21363c1ac138d8df80b20af4848b5113bd3bf6f8","title":"Clone-advisor: recommending code tokens and clone methods with deep learning and information retrieval","venue":"PeerJ Computer Science","year":2021,"referenceCount":107,"citationCount":1,"influentialCitationCount":1,"publicationDate":"09/11/2021","authors":"Muhammad Hammad,Önder Babur,H. Basit,M. Brand","id":"21363c1ac138d8df80b20af4848b5113bd3bf6f8","summary":"This paper applies an information retrieval technique on top of DeepClone output to recommend real clone methods closely matching the predicted clone method, thus improving the original output by DeepClones.","score":1},{"url":"https://www.semanticscholar.org/paper/58b142663367ef6ed67507e3d7591b6e384a6937","title":"Deep Distilling: automated code generation using explainable deep learning","venue":"ArXiv","year":2021,"referenceCount":20,"citationCount":1,"influentialCitationCount":0,"publicationDate":"16/11/2021","authors":"Paul J. Blazek,Kesavan Venkatesh,M. Lin","id":"58b142663367ef6ed67507e3d7591b6e384a6937","summary":"Deep distilling is introduced, a machine learning method that learns patterns from data using explainable deep learning and then condenses it into concise, executable computer code that generalizes out-of-distribution to solve problems orders of-magnitude larger and more complex than the training data.","score":1},{"url":"https://www.semanticscholar.org/paper/4e7de32c8da8c910285acdaf397347dc94ca3594","title":"Many Heads but One Brain: Fusion Brain -- a Competition and a Single Multimodal Multitask Architecture","venue":"","year":2021,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/11/2021","authors":"Daria Bakshandaeva,Denis Dimitrov,Alex Shonenkov,M. Potanin,V.Ya. Arkhipkin,Denis Karachev,Vera Davydova,Anton Voronov,Mikhail Martynov,Natalia Semenova,Mikhail Stepnov,Elena Tutubalina,Andrey Chertok,Aleksandr Petiushko","id":"4e7de32c8da8c910285acdaf397347dc94ca3594","summary":"The proposed Fusion approach proves to be competitive and more energy-efﬁcient compared to the task- speci⬁c one.","score":1},{"url":"https://www.semanticscholar.org/paper/7fba3100768fa3aac0fbf961d5e894b2f629e6e6","title":"Federated Data Science to Break Down Silos [Vision]","venue":"SIGMOD record","year":2021,"referenceCount":42,"citationCount":4,"influentialCitationCount":0,"publicationDate":"25/11/2021","authors":"Essam Mansour,Kavitha Srinivas,K. Hose","id":"7fba3100768fa3aac0fbf961d5e894b2f629e6e6","summary":"KEK is proposed, an open federated data science platform that does not only allow for sharing data science pipelines and their (meta) data but also provides methods for efficient search and, in the ideal case, even allows for combining and defining pipelines across platforms in a federated manner.","score":1},{"url":"https://www.semanticscholar.org/paper/eeadabea580953c14bb00ca99b41ee9b2cef6300","title":"Energy-bounded Learning for Robust Models of Code","venue":"ArXiv","year":2021,"referenceCount":70,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/12/2021","authors":"Nghi D. Q. Bui,Yijun Yu","id":"eeadabea580953c14bb00ca99b41ee9b2cef6300","summary":"This paper proposes the use of an energy-bounded learning objective function to assign a higher score to in-distribution samples and a lower score to out-of-distributed samples in order to incorporate such out- of-dist distribution samples into the training process of source code models.","score":1},{"url":"https://www.semanticscholar.org/paper/d4f16dae4ab1d21db3c0b0bd7b54f4b62e2d1b85","title":"The Effectiveness of Transformer Models for Analyzing Low-Level Programs","venue":"","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Zifan Carl,William S. Moses","id":"d4f16dae4ab1d21db3c0b0bd7b54f4b62e2d1b85","summary":"It is shown that transformer models can translate C to LLVM-IR with high accuracy, by training on a parallel corpus of functions extract from 1 million compilable, open-sourced C programs (AnghaBench) and its corresponding LL VM-IR after compiling with Clang.","score":1},{"url":"https://www.semanticscholar.org/paper/7d500d6bd3ae49fa3acb213fd25d5b11566e64fd","title":"Labeling-Free Comparison Testing of Deep Learning Models","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Yuejun Guo,Qiang Hu,Maxime Cordy,Xiaofei Xie,Mike Papadakis,Yves Le Traon","id":"7d500d6bd3ae49fa3acb213fd25d5b11566e64fd","summary":"A labeling-free comparison testing approach to overcome the limitations of labeling effort and sampling randomness and learns a Bayesian model to infer the models’ specialty only based on predicted labels.","score":1},{"url":"https://www.semanticscholar.org/paper/b2c0e903b79835b6ee8fd553c2213ea8abbf7864","title":"Senatus - A Fast and Accurate Code-to-Code Recommendation Engine","venue":"IEEE Working Conference on Mining Software Repositories","year":2021,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/11/2021","authors":"Fran Silavong,S. Moran,Antonios Georgiadis,Rohan Saphal,R. Otter","id":"b2c0e903b79835b6ee8fd553c2213ea8abbf7864","summary":"De-Skew LSH a new locality sensitive hashing (LSH) algorithm that indexes the data for fast (sub-linear time) retrieval while also counteracting the skewness in the snippet length distribution using novel abstract syntax tree-based feature scoring and selection algorithms is developed.","score":1},{"url":"https://www.semanticscholar.org/paper/9f0852ce9338c00135fe39426d893a36a289e5d5","title":"GraphCode2Vec: Generic Code Embedding via Lexical and Program Dependence Analyses","venue":"IEEE Working Conference on Mining Software Repositories","year":2021,"referenceCount":78,"citationCount":7,"influentialCitationCount":0,"publicationDate":"02/12/2021","authors":"Wei Ma,Mengjie Zhao,Ezekiel O. Soremekun,Q. Hu,Jie Zhang,Mike Papadakis,Maxime Cordy,Xiaofei Xie,Yves Le Traon","id":"9f0852ce9338c00135fe39426d893a36a289e5d5","summary":"Graphcode2vec, the first self-supervised pre-training approach which produces task-agnostic embedding of lexical and program dependence features, is proposed and demonstrated to be more effective than both generic and task-specific learning-based baselines.","score":1},{"url":"https://www.semanticscholar.org/paper/7438626a757c5442b9c0fb37b54ec0fe7e1889c3","title":"Better Together? An Evaluation of AI-Supported Code Translation","venue":"International Conference on Intelligent User Interfaces","year":2022,"referenceCount":103,"citationCount":7,"influentialCitationCount":0,"publicationDate":"15/02/2022","authors":"Justin D. Weisz,Michael J. Muller,Steven I. Ross,Fernando Martinez,Stephanie Houde,Mayank Agarwal,Kartik Talamadupula,John T. Richards","id":"7438626a757c5442b9c0fb37b54ec0fe7e1889c3","summary":"This work motivates the need for intelligent user interfaces that help software engineers effectively work with generative code models in order to understand and evaluate their outputs and achieve superior outcomes to working alone.","score":1},{"url":"https://www.semanticscholar.org/paper/4b27f18bff43d605805c92696a979714ced0b805","title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":36,"citationCount":42,"influentialCitationCount":10,"publicationDate":"08/03/2022","authors":"Daya Guo,Shuai Lu,Nan Duan,Yanlin Wang,Ming Zhou,Jian Yin","id":"4b27f18bff43d605805c92696a979714ced0b805","summary":"Results show that the model achieves state-of-the-art performance on most tasks and analysis reveals that comment and AST can both enhance UniXcoder.","score":1},{"url":"https://www.semanticscholar.org/paper/8c8bf30828bc789be679f29ee08cc6cdebd36600","title":"Characterizing and Understanding the Behavior of Quantized Models for Reliable Deployment","venue":"ArXiv","year":2022,"referenceCount":64,"citationCount":1,"influentialCitationCount":0,"publicationDate":"08/04/2022","authors":"Qiang Hu,Yuejun Guo,Maxime Cordy,Xiaofei Xie,Wei Ma,Mike Papadakis,Yves Le Traon","id":"8c8bf30828bc789be679f29ee08cc6cdebd36600","summary":"This study considers 4 datasets spanning from image to text, 8 DNN architectures including feed-forward neural networks and recurrent neural networks, and 42 shifted sets with both synthetic and natural distribution shifts and reveals that data with distribution shifts happen more disagreements than without.","score":1},{"url":"https://www.semanticscholar.org/paper/5e5a7f8423e0b990bbe1c85a999da86f16ee68a3","title":"LaF: Labeling-Free Model Selection for Automated Deep Neural Network Reusing","venue":"","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/04/2022","authors":"Qiang Hu,Yuejun Guo,Maxime Cordy,Xiaofei Xie,Mike Papadakis,Yves Le Traon","id":"5e5a7f8423e0b990bbe1c85a999da86f16ee68a3","summary":"A labeling-free (LaF) model selection approach to overcome the limitations of labeling efforts for automated model reusing and statistically learn a Bayesian model to infer the models’ specialty only based on predicted labels.","score":1},{"url":"https://www.semanticscholar.org/paper/1413acc991434ee36248b282b4cedac77ade1737","title":"Evaluating few shot and Contrastive learning Methods for Code Clone Detection","venue":"ArXiv","year":2022,"referenceCount":49,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/04/2022","authors":"Mohamad Khajezade,F. Fard,M. Shehata","id":"1413acc991434ee36248b282b4cedac77ade1737","summary":"This research assesses the generalizability of the state of the art models for CCD in few shot settings and employs Model Agnostic Meta-learning (MAML), where the model learns a meta-learner capable of extracting transferable knowledge from the train set so that the model can be fine-tuned using a few samples.","score":1},{"url":"https://www.semanticscholar.org/paper/c765091cb8bec8448669351f3662101c307c03c4","title":"Zero-Shot Program Representation Learning","venue":"IEEE International Conference on Program Comprehension","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/04/2022","authors":"Nan Cui,Yuze Jiang,Xiaodong Gu,Beijun Shen","id":"c765091cb8bec8448669351f3662101c307c03c4","summary":"Zecoler is a zero-shot learning approach for code representations built upon a pre-trained programming language model that significantly outperforms baseline models in both zero- shot and few-shot settings.","score":1},{"url":"https://www.semanticscholar.org/paper/80b2b006ed2f26ec3ddc91e303dc9861fb456a26","title":"On The Cross-Modal Transfer from Natural Language to Code through Adapter Modules","venue":"IEEE International Conference on Program Comprehension","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/04/2022","authors":"Divyam Goel,Raman Grover,F. Fard","id":"80b2b006ed2f26ec3ddc91e303dc9861fb456a26","summary":"Studying the bimodality of adapters for two tasks of cloze test and code clone detection, compared to their benchmarks from the CodeXGLUE platform confirms the success of the adapters in knowledge transfer to software engineering, which sometimes are in par with or exceed the results of a PTLM trained on source code while being more efficient in terms of the number of parameters, memory usage, and inference time.","score":1},{"url":"https://www.semanticscholar.org/paper/c1ddf0006e1aa0d5551e1ba1ad734ec0ecf27fd0","title":"CV4Code: Sourcecode Understanding via Visual Code Representations","venue":"ArXiv","year":2022,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/05/2022","authors":"Ruibo Shi,Lili Tao,Rohan Saphal,Fran Silavong,S. Moran","id":"c1ddf0006e1aa0d5551e1ba1ad734ec0ecf27fd0","summary":"This work presents CV4Code, a compact and effective computer vision method for sourcecode understanding that leverages the contextual and the structural information available from the code snippet by treating each snippet as a two-dimensional image, which naturally encodes the context and retains the underlying structural information through an explicit spatial representation.","score":1},{"url":"https://www.semanticscholar.org/paper/6fb1a9d5278b85dfbbb0be3731d480ab36a0372e","title":"CodeAttack: Code-based Adversarial Attacks for Pre-Trained Programming Language Models","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":3,"influentialCitationCount":0,"publicationDate":"31/05/2022","authors":"Akshita Jha,C. Reddy","id":"6fb1a9d5278b85dfbbb0be3731d480ab36a0372e","summary":"This work proposes, CodeAttack, a simple yet effective black- box attack model that uses code structure to generate effective, efﬁcient, and imperceptible adversarial code samples and demonstrates the vulnerabilities of the state-of-the-art PL models to code-species adversarial attacks.","score":1},{"url":"https://www.semanticscholar.org/paper/5514b87e34db2b34bd9a9b995894243f91435efc","title":"Learning to Represent Programs with Code Hierarchies","venue":"ArXiv","year":2022,"referenceCount":66,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/05/2022","authors":"Minh Nguyen,Nghi D. Q. Bui","id":"5514b87e34db2b34bd9a9b995894243f91435efc","summary":"A method for representing code as a hierarchy ( Code Hierarchy), in which different code components are represented separately at various levels of granular- ity, and a novel pretraining objective called Miss- ing Subtree Prediction to complement this method.","score":1},{"url":"https://www.semanticscholar.org/paper/c6e6cb19e7055f3d0616c3314a85b0914132ae40","title":"CodeS: A Distribution Shift Benchmark Dataset for Source Code Learning","venue":"ArXiv","year":2022,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Qiang Hu,Yuejun Guo,Xiaofei Xie,Maxime Cordy,L. Ma,Mike Papadakis,Yves Le Traon","id":"c6e6cb19e7055f3d0616c3314a85b0914132ae40","summary":"The proposed CodeS is a distribution shift benchmark dataset for source code learning that supports 2 programming languages and 5 types of code distribution shifts, and is the best of its knowledge to be the first to propose the code representation-based (token and CST) distribution shifts.","score":1},{"url":"https://www.semanticscholar.org/paper/73b4f4c31852273b38868f2bd362abeafce40232","title":"CodeS: Towards Code Model Generalization Under Distribution Shift","venue":"","year":2022,"referenceCount":28,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/06/2022","authors":"Qiang Hu,Yuejun Guo,Xiaofei Xie,Maxime Cordy,Lei Ma,Mike Papadakis,Yves Le Traon","id":"73b4f4c31852273b38868f2bd362abeafce40232","summary":"This paper initiates to propose CodeS, a distribution shift benchmark dataset, for source code learning, which supports two programming languages and five shift types and reveals that out-of-distribution detectors from other domains do not generalize to source code, all code classification models suffer from distribution shifts.","score":1},{"url":"https://www.semanticscholar.org/paper/62851a515ea0ee3a547d94e8a493d978c22d0be9","title":"Multilingual Code Snippets Training for Program Translation","venue":"AAAI Conference on Artificial Intelligence","year":2022,"referenceCount":27,"citationCount":8,"influentialCitationCount":0,"publicationDate":"28/06/2022","authors":"Ming-Yuan Zhu,Karthik Suresh,C. Reddy","id":"62851a515ea0ee3a547d94e8a493d978c22d0be9","summary":"CoST is introduced, a new multilingual Code Snippet Translation dataset that contains parallel data from 7 commonly used programming languages and outperforms the baselines on both snippet-level and program-level translation, and achieves state-of-the-art performance on CodeXGLUE translation task.","score":1},{"url":"https://www.semanticscholar.org/paper/8bf89cf8f18f08a43fd3d058687987666996b995","title":"PST: Measuring Skill Proficiency in Programming Exercise Process via Programming Skill Tracing","venue":"Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","year":2022,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/07/2022","authors":"Ruixin Li,Yu Yin,Le Dai,Shuanghong Shen,Xin Lin,Yu Su,Enhong Chen","id":"8bf89cf8f18f08a43fd3d058687987666996b995","summary":"A model that measures skill proficiency in programming exercise process named Programming Skill Tracing (PST) is proposed, which divided programming skill into programming knowledge and coding ability to get more fine-grained assessment.","score":1},{"url":"https://www.semanticscholar.org/paper/1444ed03083523a4413d9f15f2200007447771db","title":"A Library for Representing Python Programs as Graphs for Machine Learning","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":1,"influentialCitationCount":1,"publicationDate":"15/08/2022","authors":"David Bieber,Kensen Shi,Petros Maniatis,Charles Sutton,V. Hellendoorn,Daniel D. Johnson,Daniel Tarlow","id":"1444ed03083523a4413d9f15f2200007447771db","summary":"An open source Python library python_graphs is introduced that applies static analysis to construct graph representations of Python programs suitable for training machine learning models.","score":1},{"url":"https://www.semanticscholar.org/paper/da78bab10019e530a93584f60b9224e353d90f2a","title":"A Tree-structured Transformer for Program Representation Learning","venue":"ArXiv","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Wenhan Wang,Kechi Zhang,Ge Li,Shangqing Liu,Zhi Jin,Yang Liu","id":"da78bab10019e530a93584f60b9224e353d90f2a","summary":"The extensive experimental results show that the Tree-Transformer outperforms existing tree-based or graph-based neural networks in program-related tasks with tree-level and node-level prediction tasks, indicating that Tree- transformer performs well on learning both tree- level and nodes-level representations.","score":1},{"url":"https://www.semanticscholar.org/paper/89b58765614bd6c52baca0006d67f64985d2204e","title":"Topical: Learning Repository Embeddings from Source Code using Attention","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/08/2022","authors":"Agathe Lherondelle,Yash Satsangi,Fran Silavong,Shaltiel Eloul,S. Moran","id":"89b58765614bd6c52baca0006d67f64985d2204e","summary":"Topical a deep neural network to generate repository level embeddings of publicly available GitHub code repositories directly from source code is introduced and it is shown that Topical’s attention mechanism out- performs naive aggregation methods when computing repository-level representations from script-level representation generated by existing methods.","score":1},{"url":"https://www.semanticscholar.org/paper/939b4b1ff5a21108bb2f8c81117f1d5b230180a9","title":"Extreme Multi-Domain, Multi-Task Learning With Unified Text-to-Text Transfer Transformers","venue":"ArXiv","year":2022,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/09/2022","authors":"Adebayo Oshingbesan,Courage Ekoh,Germann Atakpa,Yonah Byaruagaba","id":"939b4b1ff5a21108bb2f8c81117f1d5b230180a9","summary":"It is shown that while negative knowledge transfer and catastrophic forgetting are still considerable challenges for all the models, the GPT-style joint pretraining + joint ﬁnetuning strategy showed the most promise in multi-domain, multi-task learning as it performs well across all four tasks while still keeping its multi- domain knowledge.","score":1},{"url":"https://www.semanticscholar.org/paper/05f225085154e4326af07f7c8f273156f132aa70","title":"Geração Automática de Benchmarks para Compilação Preditiva","venue":"Brazilian Symposium on Programming Languages","year":2022,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/10/2022","authors":"Cecília Kind,João Coelho,Bruno Kind,F. Pereira","id":"05f225085154e4326af07f7c8f273156f132aa70","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/8b58130ecb302a2f0e78e9ffb7115cb4906cb966","title":"Towards Robust Models of Code via Energy-Based Learning on Auxiliary Datasets","venue":"International Conference on Automated Software Engineering","year":2022,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Nghi D. Q. Bui,Yijun Yu","id":"8b58130ecb302a2f0e78e9ffb7115cb4906cb966","summary":"This work proposes to use an auxiliary dataset (out-of-distribution) such that, when trained together with the main dataset, they will enhance the model’s robustness and demonstrate a greater robustness for existing source code models to become more accurate at recognizing OOD data while being more resistant to adversarial attacks at the same time.","score":1},{"url":"https://www.semanticscholar.org/paper/00aacec39159bcd92a412aa314b376c3378c49cb","title":"Improvement of Vulnerable Code Dataset Based on Program Equivalence Transformation","venue":"Journal of Physics: Conference Series","year":2022,"referenceCount":15,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Dejiang Jing","id":"00aacec39159bcd92a412aa314b376c3378c49cb","summary":"A generator of complex code vulnerability dataset based on program equivalence transformation is proposed, which improves the complexity of program structure and code size while preserving the labels of the original case.","score":1},{"url":"https://www.semanticscholar.org/paper/e052fd3e1480a501c3145f53ad5ddb4526efbb21","title":"Source Code Preprocessing Method Analysis in Unit Test Code Classification","venue":"2022 1st International Conference on Software Engineering and Information Technology (ICoSEIT)","year":2022,"referenceCount":18,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/11/2022","authors":"A. Rasyid,M. J. Alibasa,N. Selviandro,Y. Priyadi","id":"e052fd3e1480a501c3145f53ad5ddb4526efbb21","summary":"The results show that the preprocessing methods and their combinations used significantly influences the performance of the machine learning model.","score":1},{"url":"https://www.semanticscholar.org/paper/9431181f8115a2360621df5ed76e1a23b88e3b2f","title":"Evaluating Human-Language Model Interaction","venue":"ArXiv","year":2022,"referenceCount":168,"citationCount":2,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Mina Lee,Megha Srivastava,Amelia Hardy,John Thickstun,Esin Durmus,Ashwin Paranjape,Ines Gerard-Ursin,Xiang Lisa Li,Faisal Ladhak,Frieda Rong,Rose E. Wang,Minae Kwon,Joon Sung Park,Hancheng Cao,Tony Lee,Rishi Bommasani,Michael Bernstein,Percy Liang","id":"9431181f8115a2360621df5ed76e1a23b88e3b2f","summary":"A framework, Human-AI Language-based Interaction Evaluation (H-LINE), is developed that expands non-interactive evaluation along three dimensions, capturing the interactive process, not only the output of the system, and notions of preference beyond quality.","score":1},{"url":"https://www.semanticscholar.org/paper/1d74875aa4f415cb2c60b17fd1eb3e4ae543bfe1","title":"Keeping Pace with Ever-Increasing Data: Towards Continual Learning of Code Intelligence Models","venue":"ArXiv","year":2023,"referenceCount":62,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/02/2023","authors":"Shuzheng Gao,Hongyu Zhang,Cuiyun Gao,Chaozheng Wang","id":"1d74875aa4f415cb2c60b17fd1eb3e4ae543bfe1","summary":"This paper proposes REPEAT, a novel method for continual learning of code intelligence models that addresses the catastrophic forgetting problem with representative exemplars replay and adaptive parameter regularization.","score":1},{"url":"https://www.semanticscholar.org/paper/f5eb526492798dd7a53fe78f28431f5f489192da","title":"A Survey on Semantic Parsing for Machine Programming","venue":"","year":2021,"referenceCount":89,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Celine Lee,Justin Emile Gottschlich,D. Roth","id":"f5eb526492798dd7a53fe78f28431f5f489192da","summary":"An overview of the growing body of research in natural language semantic parsing techniques and extracting lessons from the evolution of semantic parsing is provided, drawing parallels between modern efforts in neural semantic parsing and program synthesis.","score":1},{"url":"https://www.semanticscholar.org/paper/6a1b25f7a67395ad1e676027322913acbb0a0635","title":"CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":26,"citationCount":42,"influentialCitationCount":10,"publicationDate":"10/03/2021","authors":"Dan Hendrycks,Collin Burns,Anya Chen,Spencer Ball","id":"6a1b25f7a67395ad1e676027322913acbb0a0635","summary":"It is found that Transformer models have nascent performance, but that this performance is strongly influenced by model design and training dataset size, so there is still substantial room for improvement.","score":1},{"url":"https://www.semanticscholar.org/paper/04ff95e0edc3759fc5d18a1b929b3ccf79b032b2","title":"Deconstructing Distributions: A Pointwise Framework of Learning","venue":"ArXiv","year":2022,"referenceCount":90,"citationCount":6,"influentialCitationCount":0,"publicationDate":"20/02/2022","authors":"Gal Kaplun,Nikhil Ghosh,S. Garg,B. Barak,Preetum Nakkiran","id":"04ff95e0edc3759fc5d18a1b929b3ccf79b032b2","summary":"This work studies a point’s profile : the relationship between models’ average performance on the test distribution and their pointwise performance on this individual point, and finds that profiles can yield new insights into the structure of both models and data—in and out-of-distribution.","score":1},{"url":"https://www.semanticscholar.org/paper/1c1ca2392155ddf30408a442e6b504b5d60d4f2a","title":"When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment","venue":"ArXiv","year":2022,"referenceCount":78,"citationCount":1,"influentialCitationCount":0,"publicationDate":"04/10/2022","authors":"Zhijing Jin,Sydney Levine,Fernando Gonzalez,Ojasv Kamal,Maarten Sap,Mrinmaya Sachan,Rada Mihalcea,J. Tenenbaum,B. Schölkopf","id":"1c1ca2392155ddf30408a442e6b504b5d60d4f2a","summary":"This paper presents a novel challenge set consisting of rule-breaking question answering (RBQA) of cases that involve potentially permissible rule- Breaking – inspired by recent moral psychology studies and proposes a novel moral chain of thought prompting strategy that combines the strengths of LLMs with theories of moral reasoning developed in cognitive science to predict human moral judgments.","score":1},{"url":"https://www.semanticscholar.org/paper/a4c216d2ce9dd245c84771acc574722055967fd6","title":"Enhancing Code Classification by Mixup-Based Data Augmentation","venue":"ArXiv","year":2022,"referenceCount":79,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/10/2022","authors":"Zeming Dong,Qiang Hu,Yuejun Guo,Maxime Cordy,Mike Papadakis,Yves Le Traon,Jianjun Zhao","id":"a4c216d2ce9dd245c84771acc574722055967fd6","summary":"A Mixup-based data augmentation approach, MixCode, to enhance the source code classiﬁcation task, which employs multiple code refactoring methods to generate label-consistent code data.","score":1},{"url":"https://www.semanticscholar.org/paper/cb123f1afd67fb8bae15dc876709c842b626c49c","title":"SimSCOOD: Systematic Analysis of Out-of-Distribution Behavior of Source Code Models","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Hossein Hajipour,Ning Yu,Cristian-Alexandru Staicu,Mario Fritz","id":"cb123f1afd67fb8bae15dc876709c842b626c49c","summary":"This work contributes the first systematic approach that simulates various OOD scenarios along different dimensions of data properties and investigates the model behaviors in such scenarios and provides insights and sheds light for future research in terms of generalization, ro-bustness, and inductive biases of source code models.","score":1},{"url":"https://www.semanticscholar.org/paper/45a37f351bb275d22354b712c78df65715a37cc5","title":"CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code","venue":"International Conference on Automated Software Engineering","year":2022,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"A. Eghbali,Michael Pradel","id":"45a37f351bb275d22354b712c78df65715a37cc5","summary":"The metric preserves the desirable properties of BLEU, such as being language-agnostic, able to handle incomplete or partially incorrect code, and efficient, while reducing the noise caused by trivially shared n-grams.","score":1},{"url":"https://www.semanticscholar.org/paper/cdec75f901a93c75ee5386a98abbe44746286e80","title":"Delivering Fairness in Human Resources AI: Mutual Information to the Rescue","venue":"AACL","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"L'eo Hemamou,Willi Coleman","id":"cdec75f901a93c75ee5386a98abbe44746286e80","summary":"This paper proposes to minimize the MI between a candidate’s name and a latent representation of their CV or short biography to mitigate bias from sensitive variables without requiring the collection of these variables.","score":1},{"url":"https://www.semanticscholar.org/paper/82d9f1db6db43cb61fe4b0b26a489a2e72628675","title":"A Test for Evaluating Performance in Human-Computer Systems","venue":"ArXiv","year":2022,"referenceCount":79,"citationCount":2,"influentialCitationCount":0,"publicationDate":"24/06/2022","authors":"Andres Campero,Michelle Vaccaro,Jaeyoon Song,Haoran Wen,Abdullah Almaatouq,T. Malone","id":"82d9f1db6db43cb61fe4b0b26a489a2e72628675","summary":"This work shows how to perform a Turing test for comparing computer performance to that of humans using the ratio of means as a measure of effect size, and shows that 50 human non- programmers using GPT-3 can perform the task about as well as–and less expensively than–the human programmers.","score":1},{"url":"https://www.semanticscholar.org/paper/2cfd0dacfa267a64a23392332c358d4e3ec6fbd4","title":"The COVID That Wasn’t: Counterfactual Journalism Using GPT","venue":"LATECHCLFL","year":2022,"referenceCount":33,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"S. Hamilton,Andrew Piper","id":"2cfd0dacfa267a64a23392332c358d4e3ec6fbd4","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/8a9e437b2e2d813b402ac560c852ef0ab2f1cd3c","title":"Recognizing Families In the Wild (RFIW): The 4th Edition","venue":"IEEE International Conference on Automatic Face & Gesture Recognition","year":2020,"referenceCount":64,"citationCount":13,"influentialCitationCount":3,"publicationDate":"15/02/2020","authors":"Joseph P. Robinson,Yu Yin,Zaid Khan,Ming Shao,Siyu Xia,Michael Stopa,Samson Timoner,Matthew A. Turk,R. Chellappa,Y. Fu","id":"8a9e437b2e2d813b402ac560c852ef0ab2f1cd3c","summary":"The purpose of this paper is to describe the 2020 RFIW challenge, end-to-end, along with forecasts in promising future directions.","score":1},{"url":"https://www.semanticscholar.org/paper/4ff14a0580f3b36a9564ec18a51d2ce1d4eafebc","title":"Learning Methods for Solving Astronomy Course Problems","venue":"","year":2021,"referenceCount":34,"citationCount":2,"influentialCitationCount":0,"publicationDate":2021,"authors":"Avi Shporer,Brandon Kates","id":"4ff14a0580f3b36a9564ec18a51d2ce1d4eafebc","summary":"This work trains a specialized machine learning model to solve university undergraduate level Introduction to Astronomy course problems using a Transformer trained on both text and code, namely OpenAI Codex, and introduces the concept of turning questions into programming tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/b874faa9c6cfb5d7e87e3d79650007ade1394958","title":"Creating new Program Proofs by Combining Abductive and Deductive Reasoning","venue":"International Conference on Innovative Computing and Cloud Computing","year":2021,"referenceCount":13,"citationCount":1,"influentialCitationCount":0,"publicationDate":2021,"authors":"Kuruvilla George Aiyankovil,D. O'Donoghue,Rosemary Monahan","id":"b874faa9c6cfb5d7e87e3d79650007ade1394958","summary":"The abduction system that creates new formal specifications by leveraging a small set of inspiring artefacts to augment a subset of candidate problems by employing knowledge graphs to represent the raw data, discovering latent similarities between graphs using a graph-matching process.","score":1},{"url":"https://www.semanticscholar.org/paper/d66e80224cda0c1d5a4c1be3798df6a6bfe3713c","title":"GPT-3 for Few-Shot Dialogue State Tracking","venue":"","year":2021,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Nicholas Pezzotti","id":"d66e80224cda0c1d5a4c1be3798df6a6bfe3713c","summary":"It is found that natural language instructions in the prompt have little impact on performance, larger language models do not always induce higher downstream performance and that GPT-3 is highly sensitive to the order and number of the in-context examples.","score":1},{"url":"https://www.semanticscholar.org/paper/a3c2b81d8bb5ac69771ed7830d009310d98ac9dc","title":"A First Approach to AGI-based Robot Task Planning","venue":"AIRO@AI*IA","year":2021,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Michele Thiella,E. Tosello,E. Pagello","id":"a3c2b81d8bb5ac69771ed7830d009310d98ac9dc","summary":"An existing proto-Artificial General Intelligence system, namely OpenCog, is extended and given the ability to effectively solve manipulation tasks whose domains contain four actions: pick, place, stack, and unstack.","score":1},{"url":"https://www.semanticscholar.org/paper/007153d786caa906255fba2ca265fd67994f8b44","title":"Tracking Blobs in the Turbulent Edge Plasma of Tokamak Fusion Reactors","venue":"ArXiv","year":2021,"referenceCount":13,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"W. Han,RA Pietersen,Rafael Villamor-Lora,Matthew Beveridge,N. Offeddu,T. Golfinopoulos,C. Theiler,J. Terry,E. Marmar,Iddo Drori","id":"007153d786caa906255fba2ca265fd67994f8b44","summary":"This work tracks the shape and the position of blobs in high frequency video data obtained from Gas Puff Imaging (GPI) diagnostics, by training a mask R-CNN model on synthetic data and testing on both synthetic and real data.","score":1},{"url":"https://www.semanticscholar.org/paper/4da830b6d84e117cb147ff71f205e71500ebbbb1","title":"Machines and Influence","venue":"","year":2021,"referenceCount":128,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Shashank Yadav","id":"4da830b6d84e117cb147ff71f205e71500ebbbb1","summary":"It is suggested that better regulation and management of information systems can more optimally offset the risks of AI and utilise the emerging capabilities which these systems have to offer to policymakers and political institutions across the world.","score":1},{"url":"https://www.semanticscholar.org/paper/021bbcefc993c389bad6c1daefd8ff92d0fc2441","title":"Contrastive Code Representation Learning","venue":"Conference on Empirical Methods in Natural Language Processing","year":2020,"referenceCount":94,"citationCount":65,"influentialCitationCount":13,"publicationDate":"09/07/2020","authors":"Paras Jain,Ajay Jain,Tianjun Zhang,P. Abbeel,Joseph Gonzalez,I. Stoica","id":"021bbcefc993c389bad6c1daefd8ff92d0fc2441","summary":"Contracode is proposed: a contrastive pre-training task that learns code functionality, not form, and improves summarization and TypeScript type inference accuracy by 2 to 13 percentage points over competitive baselines.","score":1},{"url":"https://www.semanticscholar.org/paper/09279dc8018a8131e11d527cebb06d0a43c67cff","title":"Creativity and Machine Learning: A Survey","venue":"ArXiv","year":2021,"referenceCount":285,"citationCount":13,"influentialCitationCount":1,"publicationDate":"06/04/2021","authors":"Giorgio Franceschelli,Mirco Musolesi","id":"09279dc8018a8131e11d527cebb06d0a43c67cff","summary":"An overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods is presented.","score":1},{"url":"https://www.semanticscholar.org/paper/26450917d41c828b470ec8818d49f59516a5b9c0","title":"Towards Universality in Multilingual Text Rewriting","venue":"ArXiv","year":2021,"referenceCount":36,"citationCount":5,"influentialCitationCount":0,"publicationDate":"30/07/2021","authors":"Xavier García,Noah Constant,Mandy Guo,Orhan Firat","id":"26450917d41c828b470ec8818d49f59516a5b9c0","summary":"This work takes the first steps towards building a universal rewriter: a model capable of rewriting text in any language to exhibit a wide variety of attributes, including styles and languages, while preserving as much of the original semantics as possible.","score":1},{"url":"https://www.semanticscholar.org/paper/70087677fd1a6309829b42968934575d05a95f92","title":"What do pre-trained code models know about code?","venue":"International Conference on Automated Software Engineering","year":2021,"referenceCount":34,"citationCount":22,"influentialCitationCount":2,"publicationDate":"25/08/2021","authors":"Anjan Karmakar,R. Robbes","id":"70087677fd1a6309829b42968934575d05a95f92","summary":"Four probing tasks are constructed (probing for surface-level, syntactic, structural, and semantic information) for pre-trained code models to identify whether models are deficient in (understanding) certain code properties, characterize different model layers, and get insight into the model sample-efficiency.","score":1},{"url":"https://www.semanticscholar.org/paper/a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":40,"citationCount":199,"influentialCitationCount":79,"publicationDate":"02/09/2021","authors":"Yue Wang,Weishi Wang,Shafiq R. Joty,S. Hoi","id":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","summary":"Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL.","score":1},{"url":"https://www.semanticscholar.org/paper/b8b21c2ddcd7d1c23d7ccfceabb63cb1a05bcfca","title":"HYDRA - Hyper Dependency Representation Attentions","venue":"ArXiv","year":2021,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/09/2021","authors":"Nguyen Ha Thanh,Vu D. Tran,Binh Dang,Minh Q. Bui,Minh Le Nguyen,Le-Minh Nguyen","id":"b8b21c2ddcd7d1c23d7ccfceabb63cb1a05bcfca","summary":"This paper proposes HYDRA heads, lightweight pretrained linguistic self-attention heads to inject knowledge into transformer models without pretraining them again, and empirically verify the framework on benchmark datasets to show the contribution of linguistic knowledge to a transformer model.","score":1},{"url":"https://www.semanticscholar.org/paper/24e775b20adf21e9b5b95c6a9b7a5c164d055849","title":"M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining","venue":"ArXiv","year":2021,"referenceCount":52,"citationCount":19,"influentialCitationCount":0,"publicationDate":"08/10/2021","authors":"Junyang Lin,An Yang,Jinze Bai,Chang Zhou,Le Jiang,Xianyan Jia,Ang Wang,J. Zhang,Yong Li,Wei Lin,Jingren Zhou,Hongxia Yang","id":"24e775b20adf21e9b5b95c6a9b7a5c164d055849","summary":"This paper demonstrates a practice of pretraining unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 GPUs within 10 days, and provides a technique, Granular CPU offloading, to manage CPU memory for training large model and maintain high GPU utilities.","score":1},{"url":"https://www.semanticscholar.org/paper/360e0197378799d890f473893cc0c773b8182b4e","title":"Searching for Replacement Classes","venue":"ArXiv","year":2021,"referenceCount":36,"citationCount":1,"influentialCitationCount":0,"publicationDate":"11/10/2021","authors":"Malavika Samak,J. Cambronero,M. Rinard","id":"360e0197378799d890f473893cc0c773b8182b4e","summary":"This work introduces ClassFinder, a system which given a query class Q, and a search corpus S, returns a ranked subset of classes that can replace Q and its functionality, and leverages the complementary strengths of a distributed embeddingsbased search and type-based analysis.","score":1},{"url":"https://www.semanticscholar.org/paper/9991bb2eb7e7d7e9d831e257ae77ba2eaeaba3dc","title":"Applying quantum approximate optimization to the heterogeneous vehicle routing problem","venue":"","year":2021,"referenceCount":177,"citationCount":7,"influentialCitationCount":0,"publicationDate":"13/10/2021","authors":"David Fitzek,Toheed Ghandriz,L. Laine,M. Granath,A. F. Kockum","id":"9991bb2eb7e7d7e9d831e257ae77ba2eaeaba3dc","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/21bc4ead8ea415579ab40e437fcbc274929f17c8","title":"Solving the Families In the Wild Kinship Verification Challenge by Program Synthesis","venue":"IEEE International Conference on Automatic Face & Gesture Recognition","year":2021,"referenceCount":25,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/10/2021","authors":"Junyi Huang,M. Strome,I. Jenkins,Parker Williams,Bo Feng,Yaning Wang,Roman Wang,Vaibhav Bagri,Newman Cheng,Iddo Drori","id":"21bc4ead8ea415579ab40e437fcbc274929f17c8","summary":"This work uses Codex to generate model variants, and also demonstrates its ability to generate entire running programs for kinship verification tasks of specific relationships, among the top 3 winning entries in the competition.","score":1},{"url":"https://www.semanticscholar.org/paper/8091e51ebbcd2424a1c5b50c036bae5295090525","title":"Top 3 in FG 2021 Families In the Wild Kinship Verification Challenge","venue":"ArXiv","year":2021,"referenceCount":11,"citationCount":3,"influentialCitationCount":0,"publicationDate":2021,"authors":"Junyi Huang,M. Strome,Ian Jenkins,Parker Williams,Bo Feng,Yaning Wang,Roman Wang,Vaibhav Bagri,Newman Cheng,Iddo Drori","id":"8091e51ebbcd2424a1c5b50c036bae5295090525","summary":"This work demonstrates high quality kinship verification by participating in the 2021 Recognizing Families in the Wild challenge which provides the largest publicly available dataset in the field.","score":1},{"url":"https://www.semanticscholar.org/paper/6a269b1abccdbf57e79b3f115a97bff14b435ad9","title":"Automated Support for Unit Test Generation: A Tutorial Book Chapter","venue":"ArXiv","year":2021,"referenceCount":32,"citationCount":1,"influentialCitationCount":0,"publicationDate":"26/10/2021","authors":"Afonso Fontes,Gregory Gay,F. G. O. Neto,R. Feldt","id":"6a269b1abccdbf57e79b3f115a97bff14b435ad9","summary":"This chapter introduces two algorithms that can generate pytest-formatted unit tests, tuned towards coverage of source code statements, and introduces the concept of search-based unit test generation.","score":1},{"url":"https://www.semanticscholar.org/paper/9f260bdd4030af5297a9c1cbb817c75701ac8c83","title":"The 5th Recognizing Families in the Wild Data Challenge: Predicting Kinship from Faces","venue":"IEEE International Conference on Automatic Face & Gesture Recognition","year":2021,"referenceCount":54,"citationCount":2,"influentialCitationCount":0,"publicationDate":"31/10/2021","authors":"Joseph P. Robinson,Can Qin,Ming Shao,Matthew A. Turk,R. Chellappa,Y. Fu","id":"9f260bdd4030af5297a9c1cbb817c75701ac8c83","summary":"Submissions for this year's RFIW are summarized, and the results for kinship verification, tri-subject verification, and family member search and retrieval are reviewed.","score":1},{"url":"https://www.semanticscholar.org/paper/c23d9d44e8bc68408cea9f305d1f24d915bc0d0d","title":"Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey","venue":"ArXiv","year":2021,"referenceCount":322,"citationCount":39,"influentialCitationCount":2,"publicationDate":"01/11/2021","authors":"Bonan Min,Hayley H. Ross,Elior Sulem,Amir Pouran Ben Veyseh,Thien Huu Nguyen,Oscar Sainz,Eneko Agirre,Ilana Heinz,D. Roth","id":"c23d9d44e8bc68408cea9f305d1f24d915bc0d0d","summary":"A survey of recent work that uses large, pre-trained transformer-based language models to solve NLP tasks via pre-training then fine-tuning, prompting, or text generation approaches.","score":1},{"url":"https://www.semanticscholar.org/paper/1444536496d8064f33e10b38b5820fecfab5b367","title":"Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs","venue":"ArXiv","year":2021,"referenceCount":22,"citationCount":14,"influentialCitationCount":1,"publicationDate":"06/11/2021","authors":"Julian Aron Prenner,R. Robbes","id":"1444536496d8064f33e10b38b5820fecfab5b367","summary":"This work investigates whether Codex is able to localize and fix bugs, a task of central interest in the field of automated program repair, and finds that, despite not being trained for APR, Codex is surprisingly effective, and competitive with recent state of the art techniques.","score":1},{"url":"https://www.semanticscholar.org/paper/4e40595d6ecba027cebb4f2e3b43ae44bcf51daf","title":"Solving Linear Algebra by Program Synthesis","venue":"ArXiv","year":2021,"referenceCount":17,"citationCount":10,"influentialCitationCount":0,"publicationDate":"16/11/2021","authors":"Iddo Drori,Nakul Verma","id":"4e40595d6ecba027cebb4f2e3b43ae44bcf51daf","summary":"This work uses OpenAI Codex with zero-shot learning to synthesize code from questions and quantifies the difference between the original question text and the transformed question text that yields a correct answer.","score":1},{"url":"https://www.semanticscholar.org/paper/f7987fa2aadc0b368c185dc4d2fdb1337a202c32","title":"Solving Probability and Statistics Problems by Program Synthesis","venue":"ArXiv","year":2021,"referenceCount":14,"citationCount":9,"influentialCitationCount":1,"publicationDate":"16/11/2021","authors":"Leonard Tang,Elizabeth Ke,Nikhil Singh,Nakul Verma,Iddo Drori","id":"f7987fa2aadc0b368c185dc4d2fdb1337a202c32","summary":"This work is the first to introduce a new dataset of university-level probability and statistics problems and solve these problems in a scalable fashion using the program synthesis capabilities of large language models.","score":1},{"url":"https://www.semanticscholar.org/paper/04db9b694280134f09af5fa787a306907edba29d","title":"How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN","venue":"ArXiv","year":2021,"referenceCount":81,"citationCount":18,"influentialCitationCount":2,"publicationDate":"18/11/2021","authors":"R. Thomas McCoy,P. Smolensky,Tal Linzen,Jianfeng Gao,Asli Celikyilmaz","id":"04db9b694280134f09af5fa787a306907edba29d","summary":"AVEN, a suite of analyses for assessing the novelty of generated text, focusing on sequential structure (n-grams) and syntactic structure, is introduced, showing that GPT-2's novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues.","score":1},{"url":"https://www.semanticscholar.org/paper/cecc913290736a5a368642c5b59a130eddd1fa7b","title":"Can Pre-trained Language Models be Used to Resolve Textual and Semantic Merge Conflicts?","venue":"ArXiv","year":2021,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/11/2021","authors":"Jialu Zhang,Todd Mytkowicz,Mike Kaufman,R. Piskac,Shuvendu K. Lahiri","id":"cecc913290736a5a368642c5b59a130eddd1fa7b","summary":"The feasibility of automatically repairing merge conflicts (both textual and semantic) using k-shot learning with large neural language models (LM) such as GPT-3 is explored and the results are mixed.","score":1},{"url":"https://www.semanticscholar.org/paper/21ab011a3adccbd912aea58f76b84b7873c41df3","title":"Machines&Influence: An Information Systems Lens","venue":"","year":2021,"referenceCount":130,"citationCount":1,"influentialCitationCount":0,"publicationDate":"26/11/2021","authors":"Shashank Yadav","id":"21ab011a3adccbd912aea58f76b84b7873c41df3","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/827a67bbb96c8ed34c0f79e2ea811c5b53a6896b","title":"Controllable Response Generation for Assistive Use-cases","venue":"ArXiv","year":2021,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/12/2021","authors":"Shachi H. Kumar,Hsuan Su,R. Manuvinakurike,Saurav Sahay,L. Nachman","id":"827a67bbb96c8ed34c0f79e2ea811c5b53a6896b","summary":"This study shows that keyword-control on end-to-end response generation models is powerful and can enable and empower users with degenerative disorders to carry out their dayto-day communication.","score":1},{"url":"https://www.semanticscholar.org/paper/ee042a3e299a32c413532e64603de8d3ddb6aa87","title":"Automap: Towards Ergonomic Automated Parallelism for ML Models","venue":"ArXiv","year":2021,"referenceCount":36,"citationCount":6,"influentialCitationCount":0,"publicationDate":"06/12/2021","authors":"Michael Schaarschmidt,Dominik Grewe,Dimitrios Vytiniotis,Adam Paszke,G. Schmid,Tamara Norman,James Molloy,Jonathan Godwin,Norman A. Rink,Vinod Nair,Dan Belov","id":"ee042a3e299a32c413532e64603de8d3ddb6aa87","summary":"This work presents the prototype of an automated partitioner that seamlessly integrates into existing compilers and existing user workflows and enables SPMD-style parallelism that encompasses data parallelism and parameter/activation sharding.","score":1},{"url":"https://www.semanticscholar.org/paper/6ccc0ca964ddab19705e4832758e6a2447325348","title":"End to End Software Engineering Research","venue":"ArXiv","year":2021,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/12/2021","authors":"Idan Amit","id":"6ccc0ca964ddab19705e4832758e6a2447325348","summary":"The dataset is constructed in a way that enables not only predicting concepts but also investigating their causes, and improves over features based machine learning by not requiring domain experts and being able to extract new knowledge.","score":1},{"url":"https://www.semanticscholar.org/paper/58dd9a3da16c10f5c3cdbb9760a9ff378847bf76","title":"Self-supervision of wearable sensors time-series data for influenza detection","venue":"ArXiv","year":2021,"referenceCount":14,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/12/2021","authors":"Arinbjörn Kolbeinsson,Piyusha S. Gade,R. Kainkaryam,Filip Jankovic,L. Foschini","id":"58dd9a3da16c10f5c3cdbb9760a9ff378847bf76","summary":"The results show that predicting the next day’s resting heart rate or time-in-bed during sleep provides better representations for ILI prediction, adding to previous work demonstrating the practical application of self-supervised learning from activity data to improve health predictions.","score":1},{"url":"https://www.semanticscholar.org/paper/1b94afca9d6688cc584a744734126473283cbc93","title":"Can Transformers be Strong Treatment Effect Estimators?","venue":"ArXiv","year":2022,"referenceCount":58,"citationCount":8,"influentialCitationCount":4,"publicationDate":2022,"authors":"Yi-Fan Zhang,Hanlin Zhang,Zachary Chase Lipton,Li Erran Li,Eric Xing","id":"1b94afca9d6688cc584a744734126473283cbc93","summary":"A general framework based on the Transformer architecture is developed to address a variety of challenging treatment effect estimation (TEE) problems and a propensity score network is proposed that is trained with TransTEE in an adversarial manner to promote independence between covariates and treatments to further address selection bias.","score":1},{"url":"https://www.semanticscholar.org/paper/856d2c0f7b3f80dcf1f68d1dc0dcbf5c6fe5679a","title":"Interpreting docstrings without using common sense: the private science of very large language models∗","venue":"","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Darren Abramson,Ali Emami","id":"856d2c0f7b3f80dcf1f68d1dc0dcbf5c6fe5679a","summary":"GPT-3, the natural language model on which Codex is built, and that services such as Copilot ultimately depend on, suffers from scientific deficiencies, and critical remarks on Copilot’s structure and underlying language model are presented.","score":1},{"url":"https://www.semanticscholar.org/paper/78fd8185c5cd55830c31aa718a9909827e20774e","title":"A Research Agenda for Assessing the Economic Impacts of Code Generation Models","venue":"","year":2022,"referenceCount":58,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Sam Manning,Pamela Mishkin,Gillian K. Hadfield,Tyna,Eloundou,E. Eisner","id":"78fd8185c5cd55830c31aa718a9909827e20774e","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/8a4dce5735a101ff8f64c2b676afb8c24950a5d8","title":"Zero-shot Mathematical Problem Solving via Generative Pre-trained Transformers","venue":"International Conference on Enterprise Information Systems","year":2022,"referenceCount":9,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Federico A. Galatolo,M. Cimino,G. Vaglini","id":"8a4dce5735a101ff8f64c2b676afb8c24950a5d8","summary":"The proposed approach shows that coding based problem-solving is more effective than the natural language reasoning based one, and by exploiting the Python as programming language, the proposed pipeline achieves 54.20% solve rate.","score":1},{"url":"https://www.semanticscholar.org/paper/bee0be592c314435048599281bcd9c72bf63b735","title":"CueBot: Cue-Controlled Response Generation for Assistive Interaction Usages","venue":"Workshop on Speech and Language Processing for Assistive Technologies","year":2022,"referenceCount":52,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Shachi H. Kumar,Hsuan Su,R. Manuvinakurike,Maximilian Pinaroc,Sai Prasad,Saurav Sahay,L. Nachman","id":"bee0be592c314435048599281bcd9c72bf63b735","summary":"This work builds a system that can represent people with disabilities, or speech and language disorders, in a social conversation and generate responses that can be controlled by the users using cues/keywords and introduces a keyword-loss to lexically constrain the model response output.","score":1},{"url":"https://www.semanticscholar.org/paper/57c31c709792949bfbb9d4aaee941048aa07cc4b","title":"How to Give Imperfect Automated Guidance to Learners: A Case-Study in Workplace Learning","venue":"International Conference on Artificial Intelligence in Education","year":2022,"referenceCount":18,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"J. Whitehill,Amitai Erfanian","id":"57c31c709792949bfbb9d4aaee941048aa07cc4b","summary":"There was tentative evidence that workers’ behaviors were impacted by the FP/FN trade-oﬀ of their assigned experimental condition even after the ML assistant was removed and evidence that learners modulate their behaviors based on the ﬁne-grained conﬁdence values conveyed by the assistant.","score":1},{"url":"https://www.semanticscholar.org/paper/2e5b29457ff45b8faba69bc2eaf05521584a7bec","title":"B UG F IX G ENERATION USING G RAPH T RANS","venue":"","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","id":"2e5b29457ff45b8faba69bc2eaf05521584a7bec","summary":"This work proposes FIXUR, a new architecture for generating bug fixing edits, by complementing graph neural networks with Transformer to encode the code graph as a graph that encapsulates rich syntactic and semantic dependencies.","score":1},{"url":"https://www.semanticscholar.org/paper/f01e316d3b28ccecda25b4d57926f496a9b17d3d","title":"How Robust are Neural Code Completion Models to Source Code Transformation?","venue":"","year":2022,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","id":"f01e316d3b28ccecda25b4d57926f496a9b17d3d","summary":"This work develops a methodology for systematically evaluating neural code completion models using common source code transformations and provides insights into the strengths and weaknesses of different models, and serves as a foundation for future work towards improving the accuracy and robustness of Neural code completion.","score":1},{"url":"https://www.semanticscholar.org/paper/24c6982a25c0114bc98805d368b06d1a4f6d8fd5","title":"Understanding AI alignment research: A Systematic Analysis","venue":"","year":2022,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Jan H. Kirchner","id":"24c6982a25c0114bc98805d368b06d1a4f6d8fd5","summary":"This project collected and analyzed existing AI alignment research and found that the dataset is growing quickly, with several sub-elds emerging in parallel, and a classiﬁer trained onAI alignment research articles can detect relevant articles that the authors did not originally include in the dataset.","score":1},{"url":"https://www.semanticscholar.org/paper/b562be15b076b494023b8ac24fc8c459f4fdf80a","title":"Craft an Iron Sword: Dynamically Generating Interactive Game Characters by Prompting Large Language Models Tuned on Code","venue":"WORDPLAY","year":2022,"referenceCount":21,"citationCount":3,"influentialCitationCount":1,"publicationDate":2022,"authors":"Ryan Volum,Sudha Rao,Michael Xu,Gabriel DesGarennes,Chris Brockett,Benjamin Van Durme,Olivia Deng,Akanksha Malhotra,Bill Dolan","id":"b562be15b076b494023b8ac24fc8c459f4fdf80a","summary":"It is demonstrated that use of a few example conversational prompts can power a conversational agent to generate both natural language and novel code, which can permit development of NPCs with which players can have grounded conversations that are free-form and less repetitive.","score":1},{"url":"https://www.semanticscholar.org/paper/15e767aa26a14455da95a2b2f11e3d59f2c250f6","title":"Autoformalization for Neural Theorem Proving","venue":"","year":2022,"referenceCount":11,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Yuhuai Wu,Albert Qiaochu Jiang,Wenda Li,Markus N. Rabe,Charles Staats,M. Jamnik,Christian Szegedy","id":"15e767aa26a14455da95a2b2f11e3d59f2c250f6","summary":"This work demonstrates the feasibility and usefulness of autoformalization in the context of the newly introduced MiniF2F benchmark, and finds that transformer-based language models trained on a large amount of web data are capable of formalizing mathematical competition problem statements with a relatively high success rate.","score":1},{"url":"https://www.semanticscholar.org/paper/b69b84706fe84c4c614e4473760c57dffbfeb9a0","title":"Waveformer: Linear-Time Attention with Forward and Backward Wavelet Transform","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Yufan Zhuang,Zihan Wang,Fangbo Tao,Jingbo Shang","id":"b69b84706fe84c4c614e4473760c57dffbfeb9a0","summary":"Extensive experiments on seven long-range understanding datasets from the Long Range Arena benchmark and code understanding tasks demonstrate that Waveformer achieves competitive and even better accuracy than a number of state-of-the-art Transformer variants and WISE can boost accuracies of various attention approximation methods without increasing the time complexity.","score":1},{"url":"https://www.semanticscholar.org/paper/cd155729180ea707dea251f8e9654db241ffd808","title":"Is GPT-3 all you need for machine learning for chemistry?","venue":"","year":2022,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"K. Jablonka","id":"cd155729180ea707dea251f8e9654db241ffd808","summary":"This work analyzes whether one of the largest pre-trained LLMs, GPT-3, can be directly used for chemistry applications by fine-tuning on only a few data points from a chemistry dataset, i.e., without pre-training on a chemistry-specific dataset.","score":1},{"url":"https://www.semanticscholar.org/paper/0180d35b85dd4daead90e0652b64b1339e754684","title":"Assistance with large language models","venue":"","year":2022,"referenceCount":30,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Dmitrii Krasheninnikov","id":"0180d35b85dd4daead90e0652b64b1339e754684","summary":"A behavioral cloning approach is applied to GPT-3 such that it can respond to clear input questions directly, clarify the intent behind vague input questions, and respond based on the clarification it receives, and this approach leads to quantitative improvements in answer accuracy compared to a baseline that cannot ask for clarifications.","score":1},{"url":"https://www.semanticscholar.org/paper/e002bb8dae5a18a5ea1e7e1aafa16e19ad545662","title":"Solving Math Word Problems with Process-based and Outcome-based Feedback","venue":"","year":2022,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"J. Uesato,Nate Kushman,Ramana Kumar,Francis Song,Noah Siegel,L. Wang,Antonia Creswell,Geoffery Irving,I. Higgins","id":"e002bb8dae5a18a5ea1e7e1aafa16e19ad545662","summary":"This work runs the first comprehensive comparison between process- and outcome- based approaches trained on a natural language task, GSM8K, and finds that pure outcome-based supervision produces similar final-answer error rates with less label supervision.","score":1},{"url":"https://www.semanticscholar.org/paper/f9d38e03c97562b5f5942f3a0c43bdb751b9dc1c","title":"Wordplay 2022 The 3rd Wordplay: When Language Meets Games Workshop Proceedings of the Workshop","venue":"","year":2022,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Shrimai Prabhumoye","id":"f9d38e03c97562b5f5942f3a0c43bdb751b9dc1c","summary":"Novel techniques to generate text in a particular style are described, providing an approach of generating engaging naturalistic conversation responses using knowledge generated by pre-trained language models, considering their recent success in a multitude of NLP tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/a85c5d7272371345e28a9910080224cad799972e","title":"Schema Matching using Pre-Trained Language Models","venue":"","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Yunjia Zhang","id":"a85c5d7272371345e28a9910080224cad799972e","summary":"The Learned Schema Mapper (LSM) is proposed, a novel linguistic schema matching system that leverages the natural language understanding capabilities of pre-trained language models to improve the overall accuracy and significantly reduce the overall human labeling cost.","score":1},{"url":"https://www.semanticscholar.org/paper/95a2ee5aeccf2883f904ee3fcd7369adeb176359","title":"Probing Pretrained Models of Source Codes","venue":"BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Sergey Troshin,N. Chirkova","id":"95a2ee5aeccf2883f904ee3fcd7369adeb176359","summary":"This work shows that pretrained models of code indeed contain information about code syntactic structure, the notions of identifiers, and namespaces, but they may fail to recognize more complex code properties such as semantic equivalence, and investigates how probing results are affected by using code-specific pretraining objectives, varying the model size, or finetuning.","score":1},{"url":"https://www.semanticscholar.org/paper/51256ee5425d5c425b84e7fac011775d8eff0d1c","title":"An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Fatemehsadat Mireshghallah,Archit Uniyal,Tianhao Wang,David Evans,Taylor Berg-Kirkpatrick","id":"51256ee5425d5c425b84e7fac011775d8eff0d1c","summary":"This paper empirically study memorization of fine-tuning methods using membership inference and extraction attacks, and shows that their susceptibility to attacks is very different.","score":1},{"url":"https://www.semanticscholar.org/paper/9cd0cb3af7c2215eae9afdcf500a2bcd5330aae3","title":"20Q: Overlap-Free World Knowledge Benchmark for Language Models","venue":"IEEE Games Entertainment Media Conference","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Maxime De Bruyn,Ehsan Lotfi,Jeska Buhmann,Walter Daelemans","id":"9cd0cb3af7c2215eae9afdcf500a2bcd5330aae3","summary":"20Q is introduced, a novel benchmark using the Twenty Questions game to evaluate world knowledge and common sense of language models and shows that in-context learning is inefficient for evaluating language models’ world knowledge — fine-tuning is necessary to show their true capabilities.","score":1},{"url":"https://www.semanticscholar.org/paper/ae10c4b220a0bc0999bf169d5c219086d1f1aeed","title":"Edinburgh Research Explorer Taxonomy of risks posed by language models","venue":"","year":2022,"referenceCount":218,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Laura Weidinger,J. Uesato,M. Rauh,C. Griffin,P. Huang,John F. J. Mellor,A. Glaese,M. Cheng,B. Balle,A. Kasirzadeh,C. Biles,S. Brown,Z. Kenton,W. Hawkins,T. Stepleton,A. Birhane,L. Hendricks,Rimell,Laura Weidinger,J. Uesato,M. Rauh,C. Griffin,John F. J. Mellor,A. Glaese,M. Cheng,B. Balle,A. Kasirzadeh,C. Biles,S. Brown,Z. Kenton,Tom,Stepleton,A. Birhane,Lisa Anne Hendricks,Laura Rimell","id":"ae10c4b220a0bc0999bf169d5c219086d1f1aeed","summary":"A comprehensive taxonomy of ethical and social risks associated with LMs is developed, drawing on expertise and literature from computer science, linguistics, and the social sciences to ensure that language models are developed responsibly.","score":1},{"url":"https://www.semanticscholar.org/paper/60043104ca33a1fc905af57ead32768e52c69103","title":"C3PO: A Lightweight Copying Mechanism for Translating Pseudocode to Code","venue":"AACL","year":2022,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Vishruth Veerendranath,Vibha Masti,Prajwal Anagani,Mamatha Hr","id":"60043104ca33a1fc905af57ead32768e52c69103","summary":"This work proposes a lightweight alternative to LLMs that exploits the property of code wherein most tokens can be simply copied from the pseudocode, and achieves similar performance to non-C3PO models while reducing the computational cost of training as well as the vocabulary sizes.","score":1},{"url":"https://www.semanticscholar.org/paper/8a4299a61cc44b60e5be32fef35341c3bc7b2a0d","title":"The Hole Story: Type-Directed Synthesis and Repair","venue":"","year":2022,"referenceCount":106,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Matthías Páll Gissurarson","id":"8a4299a61cc44b60e5be32fef35341c3bc7b2a0d","summary":"This thesis explores the integration of program synthesis into GHC compiler error messages using typed-hole suggestions to aid the completion of partial programs during development and presents PropR, a tool based on type-driven synthesis aided by propertybased testing and fault-localization in conjunction with genetic algorithms to automatically repair buggy programs.","score":1},{"url":"https://www.semanticscholar.org/paper/25c402db512d327f1da143de3b8e797ad6fbfe5b","title":"P ROG P ROMPT : Generating Situated Robot Task Plans using Large Language Models","venue":"","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Llm Gpt","id":"25c402db512d327f1da143de3b8e797ad6fbfe5b","summary":"This work presents a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks, and makes concrete recommendations about prompt structure and generation constraints through ablation experiments.","score":1},{"url":"https://www.semanticscholar.org/paper/75e36bb95023e55f7dec95d1af557e219ba3d349","title":"CORAL: COde RepresentAtion learning with weakly-supervised transformers for analyzing data analysis","venue":"EPJ Data Science","year":2020,"referenceCount":71,"citationCount":7,"influentialCitationCount":2,"publicationDate":"28/08/2020","authors":"Ashley Ge Zhang,Michael Merrill,Yang Liu,Jeffrey Heer,Tim Althoff","id":"75e36bb95023e55f7dec95d1af557e219ba3d349","summary":"This work proposes a novel weakly supervised transformer-based architecture for computing joint representations of code from both abstract syntax trees and surrounding natural language comments and achieves a 38% increase in accuracy over expert-supplied heuristics and outperforms a suite of baselines.","score":1},{"url":"https://www.semanticscholar.org/paper/a63535ebbf90d0c51408252c23b85ffaf87f09ae","title":"Towards an AI Assistant for Power Grid Operators","venue":"HHAI","year":2020,"referenceCount":111,"citationCount":2,"influentialCitationCount":0,"publicationDate":"03/12/2020","authors":"Antoine Marot,Alexandre Rozier,Matthieu Dussartre,Laure Crochepierre,Benjamin Donnot","id":"a63535ebbf90d0c51408252c23b85ffaf87f09ae","summary":"The vision of a new assistant framework rely- ing on an hypervision interface and greater bidirectional interaction is exposed, and the known principles of decision-making driving the assistant design alongside with its supporting assistance functions are reviewed.","score":1},{"url":"https://www.semanticscholar.org/paper/8cf3a454556060d6e9aa86dbabf221bd10bf9759","title":"On the Effectiveness of Transfer Learning for Code Search","venue":"IEEE Transactions on Software Engineering","year":2021,"referenceCount":86,"citationCount":10,"influentialCitationCount":3,"publicationDate":"12/08/2021","authors":"P. Salza,Christoph Schwizer,Jian Gu,H. Gall","id":"8cf3a454556060d6e9aa86dbabf221bd10bf9759","summary":"It is demonstrated that natural language processing models based on the Transformer architecture can be directly applied to source code analysis tasks, such as code search, and the combined use of an information retrieval-based approach followed by a Transformer leads to the best results overall.","score":1},{"url":"https://www.semanticscholar.org/paper/a1e1297fb132d7769dda3f7917e57757e6e22605","title":"Impact of Evaluation Methodologies on Code Summarization","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":56,"citationCount":4,"influentialCitationCount":0,"publicationDate":"22/08/2021","authors":"Pengyu Nie,Jiyang Zhang,Junyi Jessy Li,R. Mooney,Miloš Gligorić","id":"a1e1297fb132d7769dda3f7917e57757e6e22605","summary":"The time-segmented evaluation methodology is introduced, which is novel to the code summarization research community, and compared with the mixed-project and cross-project methodologies that have been commonly used and shows that different methodologies lead to conflicting evaluation results.","score":1},{"url":"https://www.semanticscholar.org/paper/77d956cdab4508d569ae5741549b78e715fd0749","title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":70,"citationCount":74,"influentialCitationCount":13,"publicationDate":"08/09/2021","authors":"Stephanie C. Lin,Jacob Hilton,Owain Evans","id":"77d956cdab4508d569ae5741549b78e715fd0749","summary":"It is suggested that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.","score":1},{"url":"https://www.semanticscholar.org/paper/2672777d25562c9df6fc13b653181db62d39bece","title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA","venue":"AAAI Conference on Artificial Intelligence","year":2021,"referenceCount":42,"citationCount":58,"influentialCitationCount":21,"publicationDate":"10/09/2021","authors":"Zhengyuan Yang,Zhe Gan,Jianfeng Wang,Xiaowei Hu,Yumao Lu,Zicheng Liu,Lijuan Wang","id":"2672777d25562c9df6fc13b653181db62d39bece","summary":"This work proposes PICa, a simple yet effective method that Prompts GPT3 via the use of Image Captions, for knowledge-based VQA, and treats GPT-3 as an implicit and unstructured KB that can jointly acquire and process relevant knowledge.","score":1},{"url":"https://www.semanticscholar.org/paper/c6bb04f3d8000b7e800f6359082de39548c7da79","title":"Capturing Structural Locality in Non-parametric Language Models","venue":"International Conference on Learning Representations","year":2021,"referenceCount":47,"citationCount":5,"influentialCitationCount":0,"publicationDate":"06/10/2021","authors":"Frank F. Xu,Junxian He,Graham Neubig,V. Hellendoorn","id":"c6bb04f3d8000b7e800f6359082de39548c7da79","summary":"This paper proposes a simple yet effective approach for adding locality information into non-parametric language models by adding learned parameters that improve the likelihood of retrieving examples from local neighborhoods.","score":1},{"url":"https://www.semanticscholar.org/paper/a421ba0a9150cd35e231dddc323bdd9a59b3af93","title":"Coherence boosting: When your pretrained language model is not paying enough attention","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":64,"citationCount":4,"influentialCitationCount":1,"publicationDate":"15/10/2021","authors":"Nikolay Malkin,Zhen Wang,N. Jojic","id":"a421ba0a9150cd35e231dddc323bdd9a59b3af93","summary":"It is found that coherence boosting with state-of-the-art models for various zero-shot NLP tasks yields performance gains with no additional training.","score":1},{"url":"https://www.semanticscholar.org/paper/1cbb3d96242c3f47c3f40aada33616d0f5c07737","title":"Inductive Biases and Variable Creation in Self-Attention Mechanisms","venue":"International Conference on Machine Learning","year":2021,"referenceCount":74,"citationCount":12,"influentialCitationCount":4,"publicationDate":"19/10/2021","authors":"Benjamin Edelman,Surbhi Goel,S. Kakade,Cyril Zhang","id":"1cbb3d96242c3f47c3f40aada33616d0f5c07737","summary":"The main result shows that bounded-norm Transformer networks “cre-ate sparse variables”: a single self-attention head can represent a sparse function of the input sequence, with sample complexity scaling only logarithmically with the context length.","score":1},{"url":"https://www.semanticscholar.org/paper/231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models","venue":"NAACL-HLT","year":2021,"referenceCount":38,"citationCount":13,"influentialCitationCount":1,"publicationDate":"26/10/2021","authors":"Piotr Nawrot,Szymon Tworkowski,Michal Tyrolski,Lukasz Kaiser,Yuhuai Wu,Christian Szegedy,H. Michalewski","id":"231e768f0cd280faa0f725bb353262cb4fed08d1","summary":"Hourglass is a hierarchical Transformer language model that sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efﬁciency on the widely studied enwik8 benchmark.","score":1}]}