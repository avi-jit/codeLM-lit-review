{"papers":[{"url":"https://www.semanticscholar.org/paper/9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey","venue":"ArXiv","year":2022,"referenceCount":156,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Daoguang Zan,Bei Chen,Fengji Zhang,Di Lu,Bingchao Wu,Bei Guan,Yongji Wang,Jian-Guang Lou","id":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","summary":"This survey focuses on how does neural network (NN) solves NL2Code and proposes a comprehensive framework, which is able to cover all studies in this task, and in-depth parse the existing studies into this framework.","score":4},{"url":"https://www.semanticscholar.org/paper/3d5463a16d85d9d1d0d8ebb4117a31aca3c240ff","title":"Fooling MOSS Detection with Pretrained Language Models","venue":"International Conference on Information and Knowledge Management","year":2022,"referenceCount":70,"citationCount":2,"influentialCitationCount":0,"publicationDate":"19/01/2022","authors":"Stella Rose Biderman,Edward Raff","id":"3d5463a16d85d9d1d0d8ebb4117a31aca3c240ff","summary":"It is found that a student using GPT-J can complete introductory level programming assignments without triggering suspicion from MOSS, a widely used software similarity and plagiarism detection tool.","score":3},{"url":"https://www.semanticscholar.org/paper/075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":6,"influentialCitationCount":2,"publicationDate":"04/06/2022","authors":"J. Inala,Chenglong Wang,Mei Yang,Andrés Codas,Mark Encarnaci'on,Shuvendu K. Lahiri,M. Musuvathi,Jianfeng Gao","id":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","summary":"C ODE R ANKER is a neural ranker that can predict the correctness of a sampled program without executing it and can signiﬁcantly increase the pass@1 accuracy of various code generation models on APPS, HumanEval, and MBPP datasets.","score":3},{"url":"https://www.semanticscholar.org/paper/a08a3b08a5a1de6462a7da2906b1cd81691d6c18","title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation","venue":"International Joint Conference on Artificial Intelligence","year":2022,"referenceCount":27,"citationCount":4,"influentialCitationCount":0,"publicationDate":"14/06/2022","authors":"Daoguang Zan,Bei Chen,Dejian Yang,Zeqi Lin,Minsu Kim,Bei Guan,Yongji Wang,Weizhu Chen,Jian-Guang Lou","id":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","summary":"This paper investigates how to leverage an unlabelled code corpus to train a model for library-oriented code generation, and observes that library- oriented code snippets are more likely to share similar code sketches.","score":3},{"url":"https://www.semanticscholar.org/paper/876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":10,"influentialCitationCount":3,"publicationDate":"21/07/2022","authors":"Bei Chen,Fengji Zhang,A. Nguyen,Daoguang Zan,Zeqi Lin,Jian-Guang Lou,Weizhu Chen","id":"876eb375cb7b365475040046df669c039ad54202","summary":"A novel method, C ODE T, leverages the same pre-trained language models to test cases for the code samples, thus reducing the human effort and increasing the coverage of the test scenarios, achieving remarkable and consistent gains across different models and benchmarks.","score":3},{"url":"https://www.semanticscholar.org/paper/780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation","venue":"","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/08/2022","authors":"Federico Cassano,John Gouwar,Daniel Nguyen,S. Nguyen,Luna Phipps-Costin,Donald Pinckney,Ming-Ho Yee,Yangtian Zi,Carolyn Jane Anderson,Molly Q. Feldman,Arjun Guha,M. Greenberg,Abhinav Jangda","id":"780f7eebde16b1ae5843df3a79a7772899ef6a71","summary":"This work creates the first massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages and evaluates the multi-language performance of three state-of-the-art code generation models.","score":3},{"url":"https://www.semanticscholar.org/paper/1b4c19168410fb2690d285b205ab2281793db81a","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages","venue":"ArXiv","year":2022,"referenceCount":27,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Federico Cassano,John Gouwar,Daniel Nguyen,S. Nguyen,Luna Phipps-Costin,Donald Pinckney,Ming-Ho Yee,Yangtian Zi,Carolyn Jane Anderson,Molly Q. Feldman,Arjun Guha,M. Greenberg,Abhinav Jangda","id":"1b4c19168410fb2690d285b205ab2281793db81a","summary":"It is shown that on several languages, Codex matches and even exceeds its performance on Python, and a general approach is described for easily adding support for new benchmarks and languages to MultiPL-E, the first multi-language parallel benchmark for natural-language-to-code-generation.","score":3},{"url":"https://www.semanticscholar.org/paper/6032212d5790b6a580d68d469a9895aad6238c89","title":"Diverse Title Generation for Stack Overflow Posts with Multiple Sampling Enhanced Transformer","venue":"ArXiv","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/08/2022","authors":"Fengji Zhang,Jin Liu,Yao Wan,Xiao Yu,Xiao Liu,J. Keung","id":"6032212d5790b6a580d68d469a9895aad6238c89","summary":"A novel approach to automatically generate multiple post titles from the given code snippets, using the maximal marginal multiple nucleus sampling strategy to generate multiple high-quality and diverse title candidates at a time for the developers to choose from.","score":3},{"url":"https://www.semanticscholar.org/paper/0b340dd78fd04bbde2807d5efedb796d319355e3","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Jean-Baptiste Döderlein,M. Acher,D. Khelladi,B. Combemale","id":"0b340dd78fd04bbde2807d5efedb796d319355e3","summary":"Investigation of the various input parameters of two language models shows that varying the input parameters can improve the performance of language models, but there is a tight dependency when varying the temperature, the prompt and the number of generated solutions, making potentially hard to properly control the parameters to obtain an optimal result.","score":3},{"url":"https://www.semanticscholar.org/paper/cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":1,"influentialCitationCount":0,"publicationDate":"29/10/2022","authors":"Victor C. Dibia,Adam Fourney,Gagan Bansal,Forough Poursabzi-Sangdeh,Han Liu,Saleema Amershi","id":"cba98048f3e85a974c287b271692bf6c197db940","summary":"A simple hybrid metric is proposed, which combines functional correctness and similarity- based metrics to capture different dimensions of what programmers might value and shows that this hybrid metric more accurately captures effort.","score":3},{"url":"https://www.semanticscholar.org/paper/8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation","venue":"ArXiv","year":2022,"referenceCount":30,"citationCount":5,"influentialCitationCount":2,"publicationDate":"18/11/2022","authors":"Yuhang Lai,Chengxi Li,Yiming Wang,Tianyi Zhang,Ruiqi Zhong,Luke Zettlemoyer,S. Yih,Daniel Fried,Si-yi Wang,Tao Yu","id":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","summary":"This work introduces DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas, and proactively defends against memorization by slightly modifying the problems to be different from the original StackOverﬂow source.","score":3},{"url":"https://www.semanticscholar.org/paper/815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks","venue":"ArXiv","year":2022,"referenceCount":60,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Pengcheng Yin,Wen-Ding Li,Kefan Xiao,A. Rao,Yeming Wen,Kensen Shi,Joshua Howland,Paige Bailey,Michele Catasta,H. Michalewski,Alex Polozov,Charles Sutton","id":"815c6ca281536d18ec0eb408b6e46e72a0826163","summary":"P A C H - I NC O, a 62B code language model for Python computational notebooks, which outperforms public code LMs and explores few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions.","score":3},{"url":"https://www.semanticscholar.org/paper/433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Yangruibo Ding,Zijian Wang,Wasi Uddin Ahmad,M. Ramanathan,Ramesh Nallapati,Parminder Bhatia,D. Roth,Bing Xiang","id":"433def684b5a9de5a9163f50b9004a44a11128b1","summary":"A framework that incorporates cross-file context to learn the in-file and cross- file context jointly on top of pretrained code LMs is proposed, COCOMIC, which successfully improves the existing code LM with a 19.30% relative increase in exact match and a 15.41%relative increase in identifier matching for code completion when the cross-line context is provided.","score":3},{"url":"https://www.semanticscholar.org/paper/239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning","venue":"ArXiv","year":2022,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"E. Zelikman,Qian Huang,Gabriel Poesia,Noah D. Goodman,N. Haber","id":"239b5649b12f28fd610de036afba41b9246db6c9","summary":"This work introduces Parsel 2, a framework enabling automatic implementation and validation of complex algorithms with code LLMs, based on hierarchical function descriptions in natural language, which can be used across domains requiring hierarchical reasoning, e.g. code synthesis, theorem proving, and robotic planning.","score":3},{"url":"https://www.semanticscholar.org/paper/a3e355b5de868f34fdfa2500415c5f74c69d2091","title":"Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models","venue":"ArXiv","year":2023,"referenceCount":63,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/01/2023","authors":"Toufique Ahmed,Supriyo Ghosh,Chetan Bansal,T. Zimmermann,Xuchao Zhang,S. Rajmohan","id":"a3e355b5de868f34fdfa2500415c5f74c69d2091","summary":"A rigorous study on the effectiveness of large language models for helping engineers root cause and mitigate production incidents, and a human evaluation with actual incident owners show the future potential of using artiﬁcial intelligence for resolving cloud incidents.","score":3},{"url":"https://www.semanticscholar.org/paper/317208b423d24d52ba04221cfb46956962364e22","title":"Extracting Meaningful Attention on Source Code: An Empirical Study of Developer and Neural Model Code Exploration","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/10/2022","authors":"Matteo Paltenghi,Rahul Pandita,Austin Z. Henley,Albert Ziegler","id":"317208b423d24d52ba04221cfb46956962364e22","summary":"This work empirically evaluates attention-agnostic heuris-tics and ten attention-based post processing approaches of the attention signal against the ground truth of developers exploring code, including the novel concept of follow-up attention which exhibits the highest agreement.","score":2},{"url":"https://www.semanticscholar.org/paper/20e4ca6368a0731d3cbbd2e68aa90cef7384dc72","title":"Multi-lingual Evaluation of Code Generation Models","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":4,"influentialCitationCount":2,"publicationDate":"26/10/2022","authors":"Ben Athiwaratkun,Sanjay Krishna Gouda,Zijian Wang,Xiaopeng Li,Yuchen Tian,Ming Tan,Wasi Uddin Ahmad,Shiqi Wang,Qing Sun,Mingyue Shang,Sujan Kumar Gonugondla,Hantian Ding,Varun Kumar,Nathan Fulton,A. Farahani,Siddharth Jain,Robert Giaquinto,Haifeng Qian,M. Ramanathan,Ramesh Nallapati,Baishakhi Ray,Parminder Bhatia,Sudipta Sengupta,D. Roth,Bing Xiang","id":"20e4ca6368a0731d3cbbd2e68aa90cef7384dc72","summary":"This work presents MBXP, an execution-based code completion benchmark in 10+ programming languages that is able to evaluate code generation models in a multi-lingual fashion, and discovers generalization ability of language models on out-of-domain languages, advantages of large multi-lingsual models over mono-lingUAL, benefits of few-shot prompting, and zero-shot translation abilities.","score":2},{"url":"https://www.semanticscholar.org/paper/7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":45,"citationCount":65,"influentialCitationCount":21,"publicationDate":"25/05/2021","authors":"Ruchi Puri,David S. Kung,G. Janssen,Wei Zhang,Giacomo Domeniconi,Vladmir Zolotov,Julian Dolby,Jie Chen,M. Choudhury,Lindsey Decker,Veronika Thost,Luca Buratti,Saurabh Pujar,Ulrich Finkler","id":"7547680408358916e66917d03436fca7540a7528","summary":"Project CodeNet is a first-of-its-kind, very large scale, diverse, and high-quality dataset to accelerate the algorithmic advancements in AI for Code, which consists of 14M code samples and about 500M lines of code in 55 different programming languages.","score":2},{"url":"https://www.semanticscholar.org/paper/f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":83,"citationCount":12,"influentialCitationCount":0,"publicationDate":"10/06/2021","authors":"Tal Schuster,A. Kalyan,Oleksandr Polozov,A. Kalai","id":"f7664102a451332ed7e1286561b2f621eaff128d","summary":"A positive correlation between puzzlesolving performance and coding experience, and between the puzzle difficulty for humans and AI solvers are found, and further improvements on P3 could have a significant impact on many program synthesis areas.","score":2},{"url":"https://www.semanticscholar.org/paper/58a6ca2ae28a618126f71a07262cb958a8c37904","title":"Latent Execution for Neural Program Synthesis","venue":"","year":2021,"referenceCount":60,"citationCount":2,"influentialCitationCount":0,"publicationDate":"29/06/2021","authors":"Xinyun Chen,D. Song,Yuandong Tian","id":"58a6ca2ae28a618126f71a07262cb958a8c37904","summary":"LaSynth learns the latent representation to approximate the execution of partially generated programs, even if they are incomplete in syntax, and significantly improves the performance of next token prediction over existing approaches, facilitating search.","score":2},{"url":"https://www.semanticscholar.org/paper/a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models","venue":"ArXiv","year":2021,"referenceCount":102,"citationCount":151,"influentialCitationCount":35,"publicationDate":"16/08/2021","authors":"Jacob Austin,Augustus Odena,Maxwell Nye,Maarten Bosma,H. Michalewski,David Dohan,Ellen Jiang,Carrie J. Cai,Michael Terry,Quoc V. Le,Charles Sutton","id":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","summary":"The limits of the current generation of large language models for program synthesis in general purpose programming languages are explored, finding that even the best models are generally unable to predict the output of a program given a speciﬁc input.","score":2},{"url":"https://www.semanticscholar.org/paper/a8863de15a5ee8eed98107f423138a1a8f5a2ba8","title":"Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis","venue":"Proc. ACM Program. Lang.","year":2021,"referenceCount":77,"citationCount":16,"influentialCitationCount":0,"publicationDate":"03/09/2021","authors":"Kia Rahmani,Mohammad Raza,Sumit Gulwani,Vu Le,Daniel Morris,Arjun Radhakrishna,Gustavo Soares,A. Tiwari","id":"a8863de15a5ee8eed98107f423138a1a8f5a2ba8","summary":"This work presents an approach that combines PTMs with component-based synthesis (CBS): PTMs are used to generate candidates programs from the natural language description of the task, which are then used to guide the CBS procedure to find the program that matches the precise examples-based specification.","score":2},{"url":"https://www.semanticscholar.org/paper/570a6a5b8ec2827c3f33bb1b1bd027190a0d3e07","title":"Neural Program Generation Modulo Static Analysis","venue":"Neural Information Processing Systems","year":2021,"referenceCount":56,"citationCount":9,"influentialCitationCount":0,"publicationDate":"26/10/2021","authors":"Rohan Mukherjee,Yeming Wen,Dipak Chaudhari,T. Reps,Swarat Chaudhuri,C. Jermaine","id":"570a6a5b8ec2827c3f33bb1b1bd027190a0d3e07","summary":"The neurosymbolic method allows a deep generative model to symbolically compute, using calls to a static-analysis tool, long-distance semantic relationships in the code that it has already generated, and learns to generate programs conditioned on them.","score":2},{"url":"https://www.semanticscholar.org/paper/a5731122200fbb8b37f048010a1e1ca4474aa606","title":"Examining Zero-Shot Vulnerability Repair with Large Language Models","venue":"","year":2021,"referenceCount":56,"citationCount":1,"influentialCitationCount":0,"publicationDate":"03/12/2021","authors":"H. Pearce,B. Tan,Baleegh Ahmad,R. Karri,Brendan Dolan-Gavitt","id":"a5731122200fbb8b37f048010a1e1ca4474aa606","summary":"This work examines the use of large language models for code (such as OpenAI’s Codex and AI21's Jurassic J-1) for zero-shot vulnerability repair, and investigates challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code.","score":2},{"url":"https://www.semanticscholar.org/paper/5ff9032d0f7f246d01ae7b2c231ab03469a7344a","title":"Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?","venue":"ArXiv","year":2021,"referenceCount":56,"citationCount":20,"influentialCitationCount":1,"publicationDate":2021,"authors":"H. Pearce,B. Tan,Baleegh Ahmad,R. Karri,Brendan Dolan-Gavitt","id":"5ff9032d0f7f246d01ae7b2c231ab03469a7344a","summary":"This work examines the use of large language models for code (such as OpenAI's Codex and AI21's Jurassic J-1) for zero-shot vulnerability repair, and investigates challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code.","score":2},{"url":"https://www.semanticscholar.org/paper/a3564f3cf954c05844c757505325a50b4d858e22","title":"Generating Infrastructure-as-Code from Natural Language: Evaluating Fine-Tuned GPT-3 Models for Cloud Infrastructure Provisioning","venue":"","year":2022,"referenceCount":8,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","id":"a3564f3cf954c05844c757505325a50b4d858e22","summary":"A transformer-based model to generate Infrastructure-as-Code from natural language is introduced, which allows both technical and nontechnical users to dynamically generate IaC artifacts, enabling them to request and receive cloud resources using conversational interfaces such as chat bots, SMS, etc.","score":2},{"url":"https://www.semanticscholar.org/paper/6f84d0cc33c7c58f74b28ddcc1cbda91ea608c9f","title":"C ODE S UMMARIZATION : D O T RANSFORMERS R E ALLY U C","venue":"","year":2022,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Manasi S. Patwardhan,L. Vig,Raveendra Kumar Medicherla,Ravindra Naik,Gautam M. Shroff","id":"6f84d0cc33c7c58f74b28ddcc1cbda91ea608c9f","summary":"Overall, the quality of the generated summaries even from state-of-the-art (SOTA) models is quite poor, raising questions about the utility of current approaches and datasets.","score":2},{"url":"https://www.semanticscholar.org/paper/3b0cf543a730e674d4213d344ebc857fada76ead","title":"Understanding High-Level Properties of Low-Level Programs Through Transformers","venue":"","year":2022,"referenceCount":63,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"William S. Moses","id":"3b0cf543a730e674d4213d344ebc857fada76ead","summary":"It is shown that Transformer models can translate C to LLVM-IR with high accuracy, by training on a parallel corpus of functions extract from 1 million compilable, open-sourced C programs and its corresponding LL VM-IR after compiling with Clang.","score":2},{"url":"https://www.semanticscholar.org/paper/a2b6e1f7d8a7963d321f29fca7c01eeb1ebd7f0f","title":"P ATCH G ENERATION WITH L ANGUAGE M ODELS : F EASIBILITY AND S CALING B EHAVIOR","venue":"","year":2022,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Sophia Kolak,Ruben Martins,Claire Le Goues,V. Hellendoorn","id":"a2b6e1f7d8a7963d321f29fca7c01eeb1ebd7f0f","summary":"This work highlights a noticeable correlation of model size with test-passing accuracy and patch ranking quality, and the propensity for especially the largest models to generate candidate patches that closely resemble (if not exactly match), the original developer patch.","score":2},{"url":"https://www.semanticscholar.org/paper/b2c4fdb49bdb23e6ad0ac3272029324157046ea7","title":"Automated Repair of Code from Language Models","venue":"","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Zhiyu Fan,Xiang Gao,M. Mirchev,Abhik Roychoudhury,Shin Hwei Tan","id":"b2c4fdb49bdb23e6ad0ac3272029324157046ea7","summary":"The study revealed that automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to auto-generated code, and given bug location information provided by a statistical fault localization approach, Codex edit mode is similar to or better than existing Java repair tools TBar and Recoder in correcting incorrect solutions.","score":2},{"url":"https://www.semanticscholar.org/paper/78863000eb70945cc8d791d45d4a3fe8a6521cb6","title":"Open-Ended Knowledge Tracing for Computer Science Education","venue":"","year":2022,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Naiming Liu,Zichao Wang","id":"78863000eb70945cc8d791d45d4a3fe8a6521cb6","summary":"This paper develops an initial solution to the OKT problem, a student knowledge-guided code generation approach that combines program synthesis methods using language models with student knowledge tracing methods and conducts a series of quantitative and qualitative experiments to validate OKT and demonstrate its promise in educational applications.","score":2},{"url":"https://www.semanticscholar.org/paper/1aed58bd07026492194672adec494dc37c894a28","title":"Leveraging Automated Unit Tests for Unsupervised Code Translation","venue":"International Conference on Learning Representations","year":2021,"referenceCount":60,"citationCount":16,"influentialCitationCount":1,"publicationDate":"13/10/2021","authors":"Baptiste Rozière,J Zhang,François Charton,M. Harman,Gabriel Synnaeve,Guillaume Lample","id":"1aed58bd07026492194672adec494dc37c894a28","summary":"This work proposes to leverage an automated unit-testing system to filter out invalid translations, thereby creating a fully tested parallel corpus, and finds that fine-tuning an unsupervised model with this filtered data set significantly reduces the noise in the translations so-generated, comfortably outperforming the state of the art for all language pairs studied.","score":2},{"url":"https://www.semanticscholar.org/paper/52db8674337e5d86dcb96d013734befc8c3d4581","title":"Large Language Models are not Models of Natural Language: they are Corpus Models.","venue":"IEEE Access","year":2021,"referenceCount":63,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/12/2021","authors":"C. Veres","id":"52db8674337e5d86dcb96d013734befc8c3d4581","summary":"It is argued that the term language model is misleading because deep learning models are not theoretical models of language and proposed the adoption of corpus model instead, which better reflects the genesis and contents of the model.","score":2},{"url":"https://www.semanticscholar.org/paper/92a8f7f09f3705cb5a6009a42220a6f01ea084e8","title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents","venue":"International Conference on Machine Learning","year":2022,"referenceCount":55,"citationCount":81,"influentialCitationCount":13,"publicationDate":"18/01/2022","authors":"Wenlong Huang,P. Abbeel,Deepak Pathak,Igor Mordatch","id":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","summary":"This paper investigates the possibility of grounding high-level tasks, expressed in natural language, to a chosen set of actionable steps and proposes a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions.","score":2},{"url":"https://www.semanticscholar.org/paper/55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant","venue":"ArXiv","year":2022,"referenceCount":22,"citationCount":7,"influentialCitationCount":2,"publicationDate":"30/01/2022","authors":"Shubham Chandel,Colin B. Clement,Guillermo Serrato,Neel Sundaresan","id":"55ad5e818cfed72317576027fb33a9609210d592","summary":"This work studies the feasibility of a Data Science assistant powered by a sequence-to-sequence transformer by training a new model JuPyT5 on all publicly available Jupyter Notebook GitHub repositories and developing a new metric: Data Science Problems (DSP).","score":2},{"url":"https://www.semanticscholar.org/paper/5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode","venue":"Science","year":2022,"referenceCount":81,"citationCount":125,"influentialCitationCount":28,"publicationDate":"08/02/2022","authors":"Yujia Li,David H. Choi,Junyoung Chung,Nate Kushman,Julian Schrittwieser,Rémi Leblond,Tom,Eccles,James Keeling,Felix Gimeno,Agustin Dal Lago,T. Hubert,Peter Choy,Cyprien de,Masson d’Autume,I. Babuschkin,Xinyun Chen,Po-Sen Huang,Johannes Welbl,Sven Gowal,Alexey,Cherepanov,James Molloy,D. Mankowitz,Esme Sutherland Robson,Pushmeet Kohli,Nando de,Freitas,K. Kavukcuoglu,Oriol Vinyals","id":"5cbe278b65a81602a864184bbca37de91448a5f5","summary":"AlphaCode is introduced, a system for code generation that achieved an average ranking in the top 54.3% in simulated evaluations on recent programming competitions on the Codeforces platform, marking the first time an artificial intelligence system has performed competitively in programming competitions.","score":2},{"url":"https://www.semanticscholar.org/paper/76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","title":"Investigating Explainability of Generative AI for Code through Scenario-based Design","venue":"International Conference on Intelligent User Interfaces","year":2022,"referenceCount":102,"citationCount":8,"influentialCitationCount":0,"publicationDate":"10/02/2022","authors":"Jiao Sun,Q. Liao,Michael J. Muller,Mayank Agarwal,Stephanie Houde,Kartik Talamadupula,Justin D. Weisz","id":"76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","summary":"This work explores explainability needs for GenAI for code and demonstrates how human-centered approaches can drive the technical development of XAI in novel domains.","score":2},{"url":"https://www.semanticscholar.org/paper/7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective","venue":"ArXiv","year":2022,"referenceCount":265,"citationCount":3,"influentialCitationCount":0,"publicationDate":"10/02/2022","authors":"Erfan Al-Hossami,Samira Shaikh","id":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","summary":"This survey paper overviews major deep learning methods used in Natural Language Processing (NLP) and source code over the last 35 years and presents a software-engineering centered taxonomy for CI placing each of the works into one category describing how it best assists the software development cycle.","score":2},{"url":"https://www.semanticscholar.org/paper/7b5aa186ca8abc585607c5ec91562e127a398601","title":"Open-Ended Knowledge Tracing","venue":"ArXiv","year":2022,"referenceCount":64,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/02/2022","authors":"Naiming Liu,Zichao Wang,Richard Baraniuk,Andrew S. Lan","id":"7b5aa186ca8abc585607c5ec91562e127a398601","summary":"This paper develops an initial solution to the OKT problem, a student knowledge-guided code generation approach that combines program synthesis methods using language models with student knowledge tracing methods and conducts a series of quantitative and qualitative experiments on a real-world student code dataset to validate OKT and demonstrate its promise in educational applications.","score":2},{"url":"https://www.semanticscholar.org/paper/76f023c3a819fc58989a064a1b50825b11fce95d","title":"Capturing Failures of Large Language Models via Human Cognitive Biases","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":7,"influentialCitationCount":0,"publicationDate":"24/02/2022","authors":"Erik Jones,J. Steinhardt","id":"76f023c3a819fc58989a064a1b50825b11fce95d","summary":"The results indicate that experimental methodology from cognitive science can help characterize how machine learning systems behave, and draw inspiration from human cognitive biases as motivation to generate hypotheses for problems that models may have and develop experiments that elicit these problems.","score":2},{"url":"https://www.semanticscholar.org/paper/1f11601f9eb8c0bee1d1d2cc7f4fa187e8c5e525","title":"The impact of lexical and grammatical processing on generating code from natural language","venue":"Findings","year":2022,"referenceCount":19,"citationCount":4,"influentialCitationCount":2,"publicationDate":"28/02/2022","authors":"Nathanael Beau,Benoit Crabb'e","id":"1f11601f9eb8c0bee1d1d2cc7f4fa187e8c5e525","summary":"The paper highlights the importance of the lexical substitution component in the current natural language to code systems with a state-of-the-art architecture that relies on BERT encoder and a grammar-based decoder for which a formalization is provided.","score":2},{"url":"https://www.semanticscholar.org/paper/c96363c42bc8c465902c22b8c33c8704233f519e","title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages","venue":"ArXiv","year":2022,"referenceCount":50,"citationCount":6,"influentialCitationCount":2,"publicationDate":"16/03/2022","authors":"Zhiruo Wang,Grace Cuenca,Shuyan Zhou,Frank F. Xu,Graham Neubig","id":"c96363c42bc8c465902c22b8c33c8704233f519e","summary":"A multilingual dataset, MCoNaLa, is proposed to benchmark code generation from natural language commands extending beyond English, and a quantitative evaluation of performance on the M coNaLa dataset is presented by testing with state-of-theart code generation systems.","score":2},{"url":"https://www.semanticscholar.org/paper/590f6817b42407f96b079e82c935fae298196359","title":"Less is More: Summary of Long Instructions is Better for Program Synthesis","venue":"ArXiv","year":2022,"referenceCount":28,"citationCount":4,"influentialCitationCount":0,"publicationDate":"16/03/2022","authors":"Kirby Kuznia,Swaroop Mishra,Mihir Parmar,Chitta Baral","id":"590f6817b42407f96b079e82c935fae298196359","summary":"A meta-dataset consisting of human and synthesized summaries of the long and complicated programming questions shows that summaries improve performance for introductory and interview programming questions and shows improvement by a small margin for competitive programming questions, implying scope for future research in this direction.","score":2},{"url":"https://www.semanticscholar.org/paper/1a903282f7c19dbdb2714b852fb42dbb4675422b","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis","venue":"","year":2022,"referenceCount":47,"citationCount":6,"influentialCitationCount":2,"publicationDate":"25/03/2022","authors":"Erik Nijkamp,Bo Pang,Hiroaki Hayashi,Lifu Tu,Haiquan Wang,Yingbo Zhou,S. Savarese,Caiming Xiong","id":"1a903282f7c19dbdb2714b852fb42dbb4675422b","summary":"The utility of the trained model is shown by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval, and experiments show that the multi-step program synthesis capacity scales as a function of the model size and data size.","score":2},{"url":"https://www.semanticscholar.org/paper/771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":52,"influentialCitationCount":19,"publicationDate":2022,"authors":"Erik Nijkamp,Bo Pang,Hiroaki Hayashi,Lifu Tu,Haiquan Wang,Yingbo Zhou,S. Savarese,Caiming Xiong","id":"771371fb288da26a9812f5808535847a0a9c9a80","summary":"This work proposes and trains C ODE G EN, an interactive code generation model for program synthesis, and suggests that the capacity of conversational program synthesis scales as a function of the model size and data size.","score":2},{"url":"https://www.semanticscholar.org/paper/a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis","venue":"ArXiv","year":2022,"referenceCount":77,"citationCount":47,"influentialCitationCount":17,"publicationDate":"12/04/2022","authors":"Daniel Fried,Armen Aghajanyan,Jessy Lin,Sida I. Wang,Eric Wallace,Freda Shi,Ruiqi Zhong,Wen-tau Yih,Luke Zettlemoyer,M. Lewis","id":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","summary":"INCODER is introduced, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling) and the ability to condition on bidirectional context substantially improves performance on challenging tasks such as type inference, comment generation, and variable re-naming.","score":2},{"url":"https://www.semanticscholar.org/paper/47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":12,"influentialCitationCount":4,"publicationDate":"25/04/2022","authors":"Freda Shi,Daniel Fried,Marjan Ghazvininejad,Luke Zettlemoyer,Sida I. Wang","id":"47e15941c8b157873c8264e4bf50318d1ba5cd18","summary":"This work introduces execution result– based minimum Bayes risk decoding (MBR-EXEC) for program selection and shows that it improves the few-shot performance of pretrained code models on natural-language-to-code tasks, suggesting it as an effective approach for natural language to code translation.","score":2},{"url":"https://www.semanticscholar.org/paper/6050454e0446a3068617f73b0301453f3f67844d","title":"Stylette: Styling the Web with Natural Language","venue":"International Conference on Human Factors in Computing Systems","year":2022,"referenceCount":65,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/04/2022","authors":"Tae Soo Kim,Yoonseo Choi,D. Choi,Juho Kim","id":"6050454e0446a3068617f73b0301453f3f67844d","summary":"Stylette is a browser extension that enables users to change the style of websites by expressing goals in natural language, and shows that Stylette lowered the learning curve, helping participants perform styling changes 35% faster than those using developer tools.","score":2},{"url":"https://www.semanticscholar.org/paper/3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion","venue":"MAPS@PLDI","year":2022,"referenceCount":23,"citationCount":15,"influentialCitationCount":2,"publicationDate":"13/05/2022","authors":"Albert Ziegler,Eirini Kalliamvakou,Shawn Simister,Ganesh Sittampalam,X. A. Li,A. Rice,Devon Rifkin,E. Aftandilian","id":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","summary":"It is found that the rate with which shown suggestions are accepted, rather than more specific metrics regarding the persistence of completions in the code over time, drives developers’ perception of productivity.","score":2},{"url":"https://www.semanticscholar.org/paper/58aacb967cc7fc25cfc9d51b7ad3e57ac00d119b","title":"Can Foundation Models Wrangle Your Data?","venue":"Proceedings of the VLDB Endowment","year":2022,"referenceCount":91,"citationCount":6,"influentialCitationCount":1,"publicationDate":"20/05/2022","authors":"A. Narayan,Ines Chami,Laurel J. Orr,Christopher R'e","id":"58aacb967cc7fc25cfc9d51b7ad3e57ac00d119b","summary":"It is found that large FMs generalize and achieve SoTA performance on data cleaning and integration tasks, even though they are not trained for these data tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/0bcd59da541fdae66884afba8d25475a54a9da1a","title":"Automated Repair of Programs from Large Language Models","venue":"","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/05/2022","authors":"Zhiyu Fan,Xiang Gao,M. Mirchev,Abhik Roychoudhury,Shin Hwei Tan","id":"0bcd59da541fdae66884afba8d25475a54a9da1a","summary":"The study revealed that automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to auto-generated code, and given bug location information provided by a statistical fault localization approach, Codex edit mode is similar to or better than existing Java repair tools TBar and Recoder in correcting incorrect solutions.","score":2},{"url":"https://www.semanticscholar.org/paper/6074c7b75f27ca9adb6d74b080c07d5d079c3ea0","title":"Improving automatically generated code from Codex via Automated Program Repair","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":4,"influentialCitationCount":0,"publicationDate":2022,"authors":"Zhiyu Fan,Xiang Gao,Abhik Roychoudhury,Shin Hwei Tan","id":"6074c7b75f27ca9adb6d74b080c07d5d079c3ea0","summary":"This study systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests, revealing that automatically generated codes share some common programming mistakes with human-crafted solutions, indicating existing APR tools have the potential to fix auto-generated code.","score":2},{"url":"https://www.semanticscholar.org/paper/35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs","venue":"ArXiv","year":2022,"referenceCount":28,"citationCount":4,"influentialCitationCount":3,"publicationDate":"28/05/2022","authors":"Ansong Ni,J. Inala,Chenglong Wang,Oleksandr Polozov,Christopher Meek,Dragomir R. Radev,Jianfeng Gao","id":"35afb74de9660962ebac2843d26de22a6fac2ef6","summary":"This work proposes to let the model perform sampling during training and learn from both self-sampled fully-Correct programs, which yield the gold execution results, as well as partially-correct programs, whose intermediate execution state matches another correct program.","score":2},{"url":"https://www.semanticscholar.org/paper/1100dee3fd78655cddc4b7bfaef1161351d4fab5","title":"Automated Feedback Generation for Competition-Level Code","venue":"International Conference on Automated Software Engineering","year":2022,"referenceCount":48,"citationCount":1,"influentialCitationCount":0,"publicationDate":"03/06/2022","authors":"Jialu Zhang,De Li,John C. Kolesar,Hanyuan Shi,R. Piskac","id":"1100dee3fd78655cddc4b7bfaef1161351d4fab5","summary":"This work presents Clef, the first data-driven tool that can generate feedback on competition-level code automatically by repairing programmers’ incorrect submissions, and introduces a new data structure, merge trees, to capture the changes between submissions.","score":2},{"url":"https://www.semanticscholar.org/paper/2edc8efcda27c944a46f367acf6a5280b8f65525","title":"FixEval: Execution-based Evaluation of Program Fixes for Competitive Programming Problems","venue":"ArXiv","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Md. Mahim Anjum Haque,W. Ahmad,Ismini Lourentzou,Chris Brown","id":"2edc8efcda27c944a46f367acf6a5280b8f65525","summary":"This work introduces F IX E VAL, a benchmark comprising of buggy code submissions to competitive programming problems and their respective ﬁxes, and believes it provides a step towards real-world automatic bugﬁxing and model-generated code evaluation.","score":2},{"url":"https://www.semanticscholar.org/paper/1d160123cbbef972ea151a641dd435d57c727de8","title":"AixBench: A Code Generation Benchmark Dataset","venue":"ArXiv","year":2022,"referenceCount":4,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/06/2022","authors":"Yiyang Hao,Ge Li,Yongqiang Liu,Xiaowei Miao,He Zong,Siyuan Jiang,Yang Liu,He Wei","id":"1d160123cbbef972ea151a641dd435d57c727de8","summary":"A benchmark dataset for evaluating method-level code generation task and a new metric for automatically evaluating the correctness of the generated code, and a set of criteria to manually evaluating the overall quality of thegenerated code are presented.","score":2},{"url":"https://www.semanticscholar.org/paper/6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":77,"citationCount":9,"influentialCitationCount":1,"publicationDate":"05/07/2022","authors":"Hung Le,Yue Wang,Akhilesh Deepak Gotmare,S. Savarese,S. Hoi","id":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","summary":"This work proposes “CodeRL”, a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL), and treats the code-generating LM as an actor network, and introduces a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor.","score":2},{"url":"https://www.semanticscholar.org/paper/b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs","venue":"ArXiv","year":2022,"referenceCount":33,"citationCount":3,"influentialCitationCount":1,"publicationDate":2022,"authors":"Shuyan Zhou,Uri Alon,Frank F. Xu,Zhengbao Jiang,Graham Neubig","id":"b31b21d0750e849badfe76000e8170482f32b9be","summary":"DocCoder is introduced : an approach that explicitly leverages code manuals and documentation by retrieving the relevant documentation given the natural language intent, and generating the code based on the NL intent and the retrieved documentation.","score":2},{"url":"https://www.semanticscholar.org/paper/06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling","venue":"ArXiv","year":2022,"referenceCount":72,"citationCount":6,"influentialCitationCount":0,"publicationDate":"22/07/2022","authors":"Fenia Christopoulou,Gerasimos Lampouras,Milan Gritta,Guchun Zhang,Yinpeng Guo,Zhong-Yi Li,Qi Zhang,M. Xiao,Bo Shen,Lin Li,Hao Yu,Li-yu Yan,Pingyi Zhou,Xin Wang,Yu Ma,Ignacio Iacobacci,Yasheng Wang,Guangtai Liang,Jia Wei,Xin Jiang,Qianxiang Wang,Qun Liu","id":"06ea568379211ffa07d9605f66f26f6f736ea5e0","summary":"A pretrained decoder-only language model adopting the P AN G U - α architecture for text-to-code generation, i.e. the synthesis of programming language solutions given a natural language problem description is presented.","score":2},{"url":"https://www.semanticscholar.org/paper/63c670ba8018da0a7e33c34ffd84c2a3ca54b894","title":"Language Models Can Teach Themselves to Program Better","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":2,"influentialCitationCount":1,"publicationDate":"29/07/2022","authors":"Patrick M. Haluptzok,Matthew Bowers,A. Kalai","id":"63c670ba8018da0a7e33c34ffd84c2a3ca54b894","summary":"This work shows how generating synthetic programming puzzles and solutions, veriﬁed for correctness by a Python interpreter, can be used to improve performance in solving test puzzles from P3, a public benchmark set of Python Programming Puzzles.","score":2},{"url":"https://www.semanticscholar.org/paper/618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?","venue":"ArXiv","year":2022,"referenceCount":97,"citationCount":4,"influentialCitationCount":2,"publicationDate":"12/08/2022","authors":"Advait Sarkar,A. Gordon,C. Negreanu,Christian Poelitz,Sruti Srinivasa Ragavan,B. Zorn","id":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","summary":"This paper explores how programming with large language models (LLM-assisted programming) is similar to, and differs from, prior conceptualisations of programmer assistance, and draws upon publicly available experience reports of LLM- assisted programming, as well as prior usability and design studies.","score":2},{"url":"https://www.semanticscholar.org/paper/453a8fac3be9282be53908f0735160d0d21e0f48","title":"Repair Is Nearly Generation: Multilingual Program Repair with LLMs","venue":"ArXiv","year":2022,"referenceCount":48,"citationCount":4,"influentialCitationCount":0,"publicationDate":"24/08/2022","authors":"Harshit Joshi,J. Cambronero,Sumit Gulwani,Vu Le,Ivan Radicek,Gust Verbruggen","id":"453a8fac3be9282be53908f0735160d0d21e0f48","summary":"This work introduces RING, a multilingual repair engine powered by a large language model trained on code (LLMC) such as Codex that enables a ﬂipped model for programming assistance, one where the programmer writes code and the AI assistance suggests code, compared to traditional code suggestion technology.","score":2},{"url":"https://www.semanticscholar.org/paper/a7435722d8ab595da5a9c70ac9160f57d0dcd75a","title":"Enabling Transformers to Understand Low-Level Programs","venue":"IEEE Conference on High Performance Extreme Computing","year":2022,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/09/2022","authors":"Zifan Carl Guo,William S. Moses","id":"a7435722d8ab595da5a9c70ac9160f57d0dcd75a","summary":"This work applies transfer learning to low-level (LLVM) programs and study how low- level programs can be made more amenable to Transformer models through various techniques, including preprocessing, infix/prefix operators, and information deduplication.","score":2},{"url":"https://www.semanticscholar.org/paper/8a854331c593c6a766fa3b8037fb2ad1b95a6f06","title":"An Empirical Study of Code Smells in Transformer-based Code Generation Techniques","venue":"IEEE Working Conference on Source Code Analysis and Manipulation","year":2022,"referenceCount":57,"citationCount":3,"influentialCitationCount":1,"publicationDate":"01/10/2022","authors":"Mohammed Latif Siddiq,Shafayat H. Majumder,Maisha R. Mim,Sourov Jajodia,Joanna C. S. Santos","id":"8a854331c593c6a766fa3b8037fb2ad1b95a6f06","summary":"This study investigates to what extent code smells are present in the datasets of coding generation techniques and verify whether they leak into the output of these techniques.","score":2},{"url":"https://www.semanticscholar.org/paper/0c78a473e33a81246d5c0fbbda7e7de168814c18","title":"FlexType: A Plug-and-Play Framework for Type Inference Models","venue":"International Conference on Automated Software Engineering","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Sivani Voruganti,Kevin Jesse,Prem Devanbu","id":"0c78a473e33a81246d5c0fbbda7e7de168814c18","summary":"This work introduces FlexType, an IDE extension that can be used on both JavaScript and TypeScript to infer types in an interactive or automatic fashion and believes the interactive Visual Studio Code extension is inherently useful in both TypeScript and JavaScript especially when resolving types is taxing for the developer.","score":2},{"url":"https://www.semanticscholar.org/paper/39e40821b7207125e54e6ed7112e55cd38c6f0c3","title":"Language Models of Code are Few-Shot Commonsense Learners","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":9,"influentialCitationCount":1,"publicationDate":"13/10/2022","authors":"Aman Madaan,Shuyan Zhou,Uri Alon,Yiming Yang,Graham Neubig","id":"39e40821b7207125e54e6ed7112e55cd38c6f0c3","summary":"This paper shows that when this task is frame as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than L Ms of natural language, even when the downstream task does not involve source code at all.","score":2},{"url":"https://www.semanticscholar.org/paper/5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","title":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/10/2022","authors":"Hussein Mozannar,Gagan Bansal,Adam Fourney,E. Horvitz","id":"5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","summary":"This work studied GitHub Copilot, developed CUPS– a taxonomy of 12 programmer activities common to AI code completion systems, and conducted a study with 21 programmers who completed coding tasks and used the labeling tool to retrospectively label their sessions with CUPS.","score":2},{"url":"https://www.semanticscholar.org/paper/41f5e1ad7793593befc0b9c38f756836e8b07c98","title":"Code4ML: a Large-scale Dataset of annotated Machine Learning Code","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/10/2022","authors":"Anastasia Drozdova,P. Guseva,E. Trofimova,Anna Scherbakova,Andrey Ustyuzhanin","id":"41f5e1ad7793593befc0b9c38f756836e8b07c98","summary":"The Code4ML corpus, which contains code snippets, task summaries, competitions and dataset descriptions publicly available from Kaggle, can potentially help address a number of software engineering or data science challenges through a data-driven approach.","score":2},{"url":"https://www.semanticscholar.org/paper/4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":1,"influentialCitationCount":0,"publicationDate":"31/10/2022","authors":"Daoguang Zan,Bei Chen,Zeqi Lin,Bei Guan,Yongji Wang,Jian-Guang Lou","id":"4fbe0cb0777b228e39243692bf29e2829060b8de","summary":"This paper investigates how to equip pre-trained language models with the ability of code generation for private libraries, and proposes a novel framework with two modules: the APIRetriever and the APICoder, which generates code using these APIs.","score":2},{"url":"https://www.semanticscholar.org/paper/20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":1,"influentialCitationCount":0,"publicationDate":"31/10/2022","authors":"Spyridon Mouselinos,Mateusz Malinowski,H. Michalewski","id":"20fae749e3d469c331731ffa2f811079db792fdc","summary":"This work shows that current code generation systems exhibit biases inherited from large language model backbones, which might leak into generated code under specific circumstances, and proposes a framework that automatically removes hints and exposes various biases that these code generation models use.","score":2},{"url":"https://www.semanticscholar.org/paper/ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","title":"Evaluating How Fine-tuning on Bimodal Data Effects Code Generation","venue":"ArXiv","year":2022,"referenceCount":31,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Gabriel Orlanski,Seonhye Yang,Michael Healy","id":"ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","summary":"It is found that at higher temperatures, there are decreases to the model’s ability to generate runnable programs despite higher pass @ k scores, underscoring the need for better methods of incorporating such data that mitigate these side effects.","score":2},{"url":"https://www.semanticscholar.org/paper/e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models","venue":"ArXiv","year":2022,"referenceCount":35,"citationCount":2,"influentialCitationCount":2,"publicationDate":"17/11/2022","authors":"Junjie Huang,Chenglong Wang,Jipeng Zhang,Cong Yan,Haotian Cui,J. Inala,Colin B. Clement,Nan Duan,Jianfeng Gao","id":"e402dd77eba504ea93bc38e2a052398bb95db351","summary":"ExeDS is introduced, an evaluation dataset for execution evaluation for data science code generation tasks that contains a set of 534 problems from Jupyter Notebooks, each consisting of code context, task description, reference program, and the desired execution output.","score":2},{"url":"https://www.semanticscholar.org/paper/f3a6115e5fb2237df938976e005468f0b18da797","title":"The Stack: 3 TB of permissively licensed source code","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":2,"influentialCitationCount":0,"publicationDate":"20/11/2022","authors":"Denis Kocetkov,Raymond Li,Loubna Ben Allal,Jia Li,Chenghao Mou,Carlos Muñoz Ferrandis,Yacine Jernite,Margaret Mitchell,Sean Hughes,Thomas Wolf,Dzmitry Bahdanau,Leandro von Werra,Harm de Vries","id":"f3a6115e5fb2237df938976e005468f0b18da797","summary":"This work introduces The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages, and describes how to collect the full dataset, construct a permissically licensed subset, and present a data governance plan.","score":2},{"url":"https://www.semanticscholar.org/paper/20b60fb3993d2e9a5af04611f7bdf248e5a3a736","title":"Programming by Example and Text-to-Code Translation for Conversational Code Generation","venue":"ArXiv","year":2022,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Eli Whitehouse,William Gerard,Yauhen Klimovich,Marc Franco-Salvador","id":"20b60fb3993d2e9a5af04611f7bdf248e5a3a736","summary":"Modular Programs for Text-guided Hierarchical Synthesis (MPaTHS), a method for integrating Programming by Example and text-to-code systems which uses an accessible natural language interface for synthesizing general programs, is proposed.","score":2},{"url":"https://www.semanticscholar.org/paper/27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation","venue":"ArXiv","year":2022,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/11/2022","authors":"Tianyi Zhang,Tao Yu,Tatsunori Hashimoto,M. Lewis,Wen-tau Yih,Daniel Fried,Sida I. Wang","id":"27961ae80ad008bd4006704b1b8fa82664137d69","summary":"Experimental results show that Coder-Reviewer reranking leads to consistent and signiﬁcant improvement (up to 17 % absolute accuracy gain) over reranking with the Coder model only, and when combined with executability ﬁltering,Coder- reviewer reranking can often outperform the minimum Bayes risk method.","score":2},{"url":"https://www.semanticscholar.org/paper/32b58766a1bfcef7ebba07070a272687aa518206","title":"Explicit Knowledge Transfer for Weakly-Supervised Code Generation","venue":"ArXiv","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/11/2022","authors":"Zhangir Azerbayev,Ansong Ni,Hailey Schoelkopf,Dragomir R. Radev","id":"32b58766a1bfcef7ebba07070a272687aa518206","summary":"It is shown that EKT not only yields better performance than training with expert iteration, but also outperforms knowledge distillation, another form of knowledge transfer, and it is possible for a student model to outperform the teacher using EKT.","score":2},{"url":"https://www.semanticscholar.org/paper/9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming","venue":"ArXiv","year":2022,"referenceCount":79,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/12/2022","authors":"Qingfu Zhu,Xianzhen Luo,Fang Liu,Cuiyun Gao,Wanxiang Che","id":"9b4055674cd9849f8595240695bed69cd02492bc","summary":"This paper comprehensively investigates existing work in natural language processing for programming, rang-ing from early deductive models to the latest competition-level models.","score":2},{"url":"https://www.semanticscholar.org/paper/6bc87e51018d6de55011e95a0d43c588dd44a1e8","title":"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/12/2022","authors":"Yekun Chai,Shuohuan Wang,Chao Pang,Yu Sun,Hao Tian,Hua Wu","id":"6bc87e51018d6de55011e95a0d43c588dd44a1e8","summary":"This work releases ERNIE-Code, a uniﬁed pre-trained language model for 116 NLs and 6 PLs, and employs two methods for universal cross-lingual pre-training: span-corruption language modeling that learns patterns from monolingual NL or PL; and pivot-based translationlanguage modeling that re-lies on parallel data of manyNLs and PLs.","score":2},{"url":"https://www.semanticscholar.org/paper/ae441f7305dc2cd58c708528b3ecee3501cc5c46","title":"Plansformer: Generating Symbolic Plans using Transformers","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"Vishal Pallagani,Bharath Muppasani,K. Murugesan,F. Rossi,L. Horesh,Biplav Srivastava,F. Fabiano,Andrea Loreggia","id":"ae441f7305dc2cd58c708528b3ecee3501cc5c46","summary":"The use of LLMs for automated planning - a branch of AI concerned with the realization of action sequences (plans) to achieve a goal, typically executed by intelligent agents, autonomous robots, and unmanned vehicles are explored.","score":2},{"url":"https://www.semanticscholar.org/paper/6756fcd998caeb7b23702e08559e63710179334c","title":"Reasoning with Language Model Prompting: A Survey","venue":"ArXiv","year":2022,"referenceCount":150,"citationCount":3,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng,Chuanqi Tan,Fei Huang,Huajun Chen","id":"6756fcd998caeb7b23702e08559e63710179334c","summary":"This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting and introduces research works with comparisons and summaries and provides systematic resources to help beginners.","score":2},{"url":"https://www.semanticscholar.org/paper/690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation","venue":"ArXiv","year":2022,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Zhiruo Wang,Shuyan Zhou,Daniel Fried,Graham Neubig","id":"690c210564226c9307b3bab977cdc07a6a45863a","summary":"ODEX corroborates the mer-its of execution-based evaluation over metrics without execution but also unveils their complementary effects, and is released to facilitate research into open-domain problems for the code generation community.","score":2},{"url":"https://www.semanticscholar.org/paper/ab8a53be2ff8a756776aa0245ad8da41189d60d6","title":"Fuzzing Deep-Learning Libraries via Large Language Models","venue":"ArXiv","year":2022,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/12/2022","authors":"Yinlin Deng,Chun Xia,Haoran Peng,Chenyuan Yang,Lingming Zhang","id":"ab8a53be2ff8a756776aa0245ad8da41189d60d6","summary":"LLMFuzz is the first automated approach to directly leveraging Large Pre-trained Language Models (LLMs) to generate input programs for fuzzing DL libraries, and is able to detect 65 bugs, with 41 already confirmed as previously unknown bugs.","score":2},{"url":"https://www.semanticscholar.org/paper/69e330037afd18e5546fdcacbc9a9f7deb69bba9","title":"Memorization and Generalization in Neural Code Intelligence Models","venue":"Information and Software Technology","year":2021,"referenceCount":93,"citationCount":8,"influentialCitationCount":0,"publicationDate":"16/06/2021","authors":"Md Rafiqul Islam Rabin,Aftab Hussain,V. Hellendoorn,Mohammad Amin Alipour","id":"69e330037afd18e5546fdcacbc9a9f7deb69bba9","summary":"This work evaluates the memorization and generalization tendencies in neural code intelligence models through a case study across several benchmarks and model families by leveraging established approaches from other fields that use DNNs, such as introducing targeted noise into the training dataset.","score":2},{"url":"https://www.semanticscholar.org/paper/907a77639069bb7dd270f017068745706133cffc","title":"Inaccessible Neural Language Models Could Reinvigorate Linguistic Nativism","venue":"ArXiv","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/01/2023","authors":"Patrick Perrine","id":"907a77639069bb7dd270f017068745706133cffc","summary":"This work argues that this lack of accessibility could instill a nativist bias in researchers new to computational linguistics, and calls upon researchers to open source their LLM code wherever possible to allow both empircist and hybrid approaches to remain accessible.","score":2},{"url":"https://www.semanticscholar.org/paper/642e280df732665249315d6c144871f0e2ceeae6","title":"NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash Commands","venue":"NeurIPS","year":2021,"referenceCount":41,"citationCount":7,"influentialCitationCount":3,"publicationDate":"03/03/2021","authors":"Mayank Agarwal,T. Chakraborti,Quchen Fu,David Gros,Xi Victoria Lin,Jaron Maene,Kartik Talamadupula,Zhongwei Teng,Jules White","id":"642e280df732665249315d6c144871f0e2ceeae6","summary":"The NLC2CMD Competition hosted at NeurIPS 2020 aimed to bring the power of natural language processing to the command line by building models that can transform descriptions of command line tasks in English to their Bash syntax.","score":1},{"url":"https://www.semanticscholar.org/paper/f5eb526492798dd7a53fe78f28431f5f489192da","title":"A Survey on Semantic Parsing for Machine Programming","venue":"","year":2021,"referenceCount":89,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Celine Lee,Justin Emile Gottschlich,D. Roth","id":"f5eb526492798dd7a53fe78f28431f5f489192da","summary":"An overview of the growing body of research in natural language semantic parsing techniques and extracting lessons from the evolution of semantic parsing is provided, drawing parallels between modern efforts in neural semantic parsing and program synthesis.","score":1},{"url":"https://www.semanticscholar.org/paper/27e3ca4fd7b8290a0e12ca4fd2b7ad5bcd5900f2","title":"Are Transformers All That Karel Needs?","venue":"","year":2021,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Abhay Garg,Anand Sriraman,Kunal Pagarey,S. Karande","id":"27e3ca4fd7b8290a0e12ca4fd2b7ad5bcd5900f2","summary":"By changing the base architecture to a transformer based one, speciﬁcally GPT2, this work is able to apply simple execution guidance on top to achieve a generalization accurary of 89.64%, which is within 2.36 percentage points of the current state-of-the-art on Karel which uses ensembling.","score":1},{"url":"https://www.semanticscholar.org/paper/6a1b25f7a67395ad1e676027322913acbb0a0635","title":"CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":26,"citationCount":38,"influentialCitationCount":10,"publicationDate":"10/03/2021","authors":"Dan Hendrycks,Collin Burns,Anya Chen,Spencer Ball","id":"6a1b25f7a67395ad1e676027322913acbb0a0635","summary":"It is found that Transformer models have nascent performance, but that this performance is strongly influenced by model design and training dataset size, so there is still substantial room for improvement.","score":1},{"url":"https://www.semanticscholar.org/paper/27d16a7f2ce8f2b787b34ff1f9b4fece079700c3","title":"Figuring out Figures: Using Textual References to Caption Scientific Figures","venue":"","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Stanley Cao,Kevin Liu","id":"27d16a7f2ce8f2b787b34ff1f9b4fece079700c3","summary":"This work uses the S CI C AP datasets curated by Hsu et al. and uses a variant of a CLIP+GPT-2 encoder-decoder model with cross-attention to generate captions conditioned on the image, and uses SciBERT to encode the textual metadata and uses this encoding alongside the figure embedding.","score":1},{"url":"https://www.semanticscholar.org/paper/7497360b0f411a44aa6afbd8b830050c40ec8aed","title":"Dataset of Student Solutions to Algorithm and Data Structure Programming Assignments","venue":"International Conference on Language Resources and Evaluation","year":2022,"referenceCount":18,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Fynn Petersen-Frey,Marcus Soll,Louis Kobras,Melf Johannsen,Peter Kling,Chris Biemann","id":"7497360b0f411a44aa6afbd8b830050c40ec8aed","summary":"This paper presents a dataset containing source code solutions to algorithmic programming exercises solved by hundreds of Bachelor-level students at the University of Hamburg, and plans to extend the dataset with tasks and solutions from upcoming courses.","score":1},{"url":"https://www.semanticscholar.org/paper/407b9e9478ba6bff43ce4b20e8b6cb2b303477d2","title":"P LANNING WITH L ARGE L ANGUAGE M ODELS FOR C ODE G ENERATION","venue":"","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","id":"407b9e9478ba6bff43ce4b20e8b6cb2b303477d2","summary":"A novel Transformer decoding algorithm that uses a planning algorithm to do lookahead search and guide the Transformer to generate better programs, and enables controllable code generation, such as concise codes and highly-commented codes by optimizing modified objectives.","score":1},{"url":"https://www.semanticscholar.org/paper/04ff95e0edc3759fc5d18a1b929b3ccf79b032b2","title":"Deconstructing Distributions: A Pointwise Framework of Learning","venue":"ArXiv","year":2022,"referenceCount":90,"citationCount":6,"influentialCitationCount":0,"publicationDate":"20/02/2022","authors":"Gal Kaplun,Nikhil Ghosh,S. Garg,B. Barak,Preetum Nakkiran","id":"04ff95e0edc3759fc5d18a1b929b3ccf79b032b2","summary":"This work studies a point’s profile : the relationship between models’ average performance on the test distribution and their pointwise performance on this individual point, and finds that profiles can yield new insights into the structure of both models and data—in and out-of-distribution.","score":1},{"url":"https://www.semanticscholar.org/paper/1c1ca2392155ddf30408a442e6b504b5d60d4f2a","title":"When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment","venue":"ArXiv","year":2022,"referenceCount":78,"citationCount":1,"influentialCitationCount":0,"publicationDate":"04/10/2022","authors":"Zhijing Jin,Sydney Levine,Fernando Gonzalez,Ojasv Kamal,Maarten Sap,Mrinmaya Sachan,Rada Mihalcea,J. Tenenbaum,B. Schölkopf","id":"1c1ca2392155ddf30408a442e6b504b5d60d4f2a","summary":"This paper presents a novel challenge set consisting of rule-breaking question answering (RBQA) of cases that involve potentially permissible rule- Breaking – inspired by recent moral psychology studies and proposes a novel moral chain of thought prompting strategy that combines the strengths of LLMs with theories of moral reasoning developed in cognitive science to predict human moral judgments.","score":1},{"url":"https://www.semanticscholar.org/paper/a4c216d2ce9dd245c84771acc574722055967fd6","title":"Enhancing Code Classification by Mixup-Based Data Augmentation","venue":"ArXiv","year":2022,"referenceCount":79,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/10/2022","authors":"Zeming Dong,Qiang Hu,Yuejun Guo,Maxime Cordy,Mike Papadakis,Yves Le Traon,Jianjun Zhao","id":"a4c216d2ce9dd245c84771acc574722055967fd6","summary":"A Mixup-based data augmentation approach, MixCode, to enhance the source code classiﬁcation task, which employs multiple code refactoring methods to generate label-consistent code data.","score":1},{"url":"https://www.semanticscholar.org/paper/cb123f1afd67fb8bae15dc876709c842b626c49c","title":"SimSCOOD: Systematic Analysis of Out-of-Distribution Behavior of Source Code Models","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Hossein Hajipour,Ning Yu,Cristian-Alexandru Staicu,Mario Fritz","id":"cb123f1afd67fb8bae15dc876709c842b626c49c","summary":"This work contributes the first systematic approach that simulates various OOD scenarios along different dimensions of data properties and investigates the model behaviors in such scenarios and provides insights and sheds light for future research in terms of generalization, ro-bustness, and inductive biases of source code models.","score":1},{"url":"https://www.semanticscholar.org/paper/205ac1373eb7981aca2d08f2ab651871a001271e","title":"CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code","venue":"2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)","year":2022,"referenceCount":55,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/05/2022","authors":"A. Eghbali,Michael Pradel","id":"205ac1373eb7981aca2d08f2ab651871a001271e","summary":"The results show that CrystalBLEU differentiates similar and unrelated programs better than the original BLEU score and also a variant designed specifically for source code, CodeBLEU.","score":1},{"url":"https://www.semanticscholar.org/paper/45a37f351bb275d22354b712c78df65715a37cc5","title":"CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code","venue":"International Conference on Automated Software Engineering","year":2022,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"A. Eghbali,Michael Pradel","id":"45a37f351bb275d22354b712c78df65715a37cc5","summary":"The metric preserves the desirable properties of BLEU, such as being language-agnostic, able to handle incomplete or partially incorrect code, and efficient, while reducing the noise caused by trivially shared n-grams.","score":1},{"url":"https://www.semanticscholar.org/paper/cdec75f901a93c75ee5386a98abbe44746286e80","title":"Delivering Fairness in Human Resources AI: Mutual Information to the Rescue","venue":"AACL","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"L'eo Hemamou,Willi Coleman","id":"cdec75f901a93c75ee5386a98abbe44746286e80","summary":"This paper proposes to minimize the MI between a candidate’s name and a latent representation of their CV or short biography to mitigate bias from sensitive variables without requiring the collection of these variables.","score":1},{"url":"https://www.semanticscholar.org/paper/82d9f1db6db43cb61fe4b0b26a489a2e72628675","title":"A Test for Evaluating Performance in Human-Computer Systems","venue":"ArXiv","year":2022,"referenceCount":79,"citationCount":1,"influentialCitationCount":0,"publicationDate":"24/06/2022","authors":"Andres Campero,Michelle Vaccaro,Jaeyoon Song,Haoran Wen,Abdullah Almaatouq,T. Malone","id":"82d9f1db6db43cb61fe4b0b26a489a2e72628675","summary":"This work shows how to perform a Turing test for comparing computer performance to that of humans using the ratio of means as a measure of effect size, and shows that 50 human non- programmers using GPT-3 can perform the task about as well as–and less expensively than–the human programmers.","score":1},{"url":"https://www.semanticscholar.org/paper/1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization","venue":"ArXiv","year":2022,"referenceCount":27,"citationCount":2,"influentialCitationCount":0,"publicationDate":"11/08/2022","authors":"Shuvendu K. Lahiri,Aaditya Naik,Georgios Sakkas,Piali Choudhury,Curtis von Veh,M. Musuvathi,J. Inala,Chenglong Wang,Jianfeng Gao","id":"1c336c18e53ad878bf4688c864acd99f137ae29f","summary":"This paper proposes the workflow of test-driven user-intent formalization (TDUIF), which leverages lightweight user feedback to jointly formalize the user intent as tests (a partial specification), and generates code that meets the formal user intent.","score":1},{"url":"https://www.semanticscholar.org/paper/e5993b3afe6384b5e6f90093989773ad1f868f71","title":"Towards Top-Down Deep Code Generation in Limited Scopes","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/09/2022","authors":"Jian Gu,H. Gall","id":"e5993b3afe6384b5e6f90093989773ad1f868f71","summary":"A semantic pyramid framework (SPF) is proposed as the approach, focusing on softwares of high modularity and low complexity, and introduces a three-layer semantic pyramid (SP) to associate text data and code data.","score":1},{"url":"https://www.semanticscholar.org/paper/2cfd0dacfa267a64a23392332c358d4e3ec6fbd4","title":"The COVID That Wasn’t: Counterfactual Journalism Using GPT","venue":"LATECHCLFL","year":2022,"referenceCount":33,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"S. Hamilton,Andrew Piper","id":"2cfd0dacfa267a64a23392332c358d4e3ec6fbd4","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/8a9e437b2e2d813b402ac560c852ef0ab2f1cd3c","title":"Recognizing Families In the Wild (RFIW): The 4th Edition","venue":"IEEE International Conference on Automatic Face & Gesture Recognition","year":2020,"referenceCount":64,"citationCount":13,"influentialCitationCount":3,"publicationDate":"15/02/2020","authors":"Joseph P. Robinson,Yu Yin,Zaid Khan,Ming Shao,Siyu Xia,Michael Stopa,Samson Timoner,Matthew A. Turk,R. Chellappa,Y. Fu","id":"8a9e437b2e2d813b402ac560c852ef0ab2f1cd3c","summary":"The purpose of this paper is to describe the 2020 RFIW challenge, end-to-end, along with forecasts in promising future directions.","score":1},{"url":"https://www.semanticscholar.org/paper/a63535ebbf90d0c51408252c23b85ffaf87f09ae","title":"Towards an AI Assistant for Power Grid Operators","venue":"HHAI","year":2020,"referenceCount":111,"citationCount":2,"influentialCitationCount":0,"publicationDate":"03/12/2020","authors":"Antoine Marot,Alexandre Rozier,Matthieu Dussartre,Laure Crochepierre,Benjamin Donnot","id":"a63535ebbf90d0c51408252c23b85ffaf87f09ae","summary":"The vision of a new assistant framework rely- ing on an hypervision interface and greater bidirectional interaction is exposed, and the known principles of decision-making driving the assistant design alongside with its supporting assistance functions are reviewed.","score":1},{"url":"https://www.semanticscholar.org/paper/4ff14a0580f3b36a9564ec18a51d2ce1d4eafebc","title":"Learning Methods for Solving Astronomy Course Problems","venue":"","year":2021,"referenceCount":34,"citationCount":2,"influentialCitationCount":0,"publicationDate":2021,"authors":"Avi Shporer,Brandon Kates","id":"4ff14a0580f3b36a9564ec18a51d2ce1d4eafebc","summary":"This work trains a specialized machine learning model to solve university undergraduate level Introduction to Astronomy course problems using a Transformer trained on both text and code, namely OpenAI Codex, and introduces the concept of turning questions into programming tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/bab6893ee48d168d27c227c3b0867f6d471fbea8","title":"Language Models are not Models of Language","venue":"ArXiv","year":2021,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"C. Veres","id":"bab6893ee48d168d27c227c3b0867f6d471fbea8","summary":"It is argued that the term language model is misleading because deep learning models are not theoretical models of language and proposed the adoption of corpus model instead, which better reflects the genesis and contents of the model.","score":1},{"url":"https://www.semanticscholar.org/paper/b874faa9c6cfb5d7e87e3d79650007ade1394958","title":"Creating new Program Proofs by Combining Abductive and Deductive Reasoning","venue":"International Conference on Innovative Computing and Cloud Computing","year":2021,"referenceCount":13,"citationCount":1,"influentialCitationCount":0,"publicationDate":2021,"authors":"Kuruvilla George Aiyankovil,D. O'Donoghue,Rosemary Monahan","id":"b874faa9c6cfb5d7e87e3d79650007ade1394958","summary":"The abduction system that creates new formal specifications by leveraging a small set of inspiring artefacts to augment a subset of candidate problems by employing knowledge graphs to represent the raw data, discovering latent similarities between graphs using a graph-matching process.","score":1},{"url":"https://www.semanticscholar.org/paper/d66e80224cda0c1d5a4c1be3798df6a6bfe3713c","title":"GPT-3 for Few-Shot Dialogue State Tracking","venue":"","year":2021,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Nicholas Pezzotti","id":"d66e80224cda0c1d5a4c1be3798df6a6bfe3713c","summary":"It is found that natural language instructions in the prompt have little impact on performance, larger language models do not always induce higher downstream performance and that GPT-3 is highly sensitive to the order and number of the in-context examples.","score":1},{"url":"https://www.semanticscholar.org/paper/a3c2b81d8bb5ac69771ed7830d009310d98ac9dc","title":"A First Approach to AGI-based Robot Task Planning","venue":"AIRO@AI*IA","year":2021,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Michele Thiella,E. Tosello,E. Pagello","id":"a3c2b81d8bb5ac69771ed7830d009310d98ac9dc","summary":"An existing proto-Artificial General Intelligence system, namely OpenCog, is extended and given the ability to effectively solve manipulation tasks whose domains contain four actions: pick, place, stack, and unstack.","score":1},{"url":"https://www.semanticscholar.org/paper/007153d786caa906255fba2ca265fd67994f8b44","title":"Tracking Blobs in the Turbulent Edge Plasma of Tokamak Fusion Reactors","venue":"ArXiv","year":2021,"referenceCount":13,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"W. Han,RA Pietersen,Rafael Villamor-Lora,Matthew Beveridge,N. Offeddu,T. Golfinopoulos,C. Theiler,J. Terry,E. Marmar,Iddo Drori","id":"007153d786caa906255fba2ca265fd67994f8b44","summary":"This work tracks the shape and the position of blobs in high frequency video data obtained from Gas Puff Imaging (GPI) diagnostics, by training a mask R-CNN model on synthetic data and testing on both synthetic and real data.","score":1},{"url":"https://www.semanticscholar.org/paper/4da830b6d84e117cb147ff71f205e71500ebbbb1","title":"Machines and Influence","venue":"","year":2021,"referenceCount":128,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Shashank Yadav","id":"4da830b6d84e117cb147ff71f205e71500ebbbb1","summary":"It is suggested that better regulation and management of information systems can more optimally offset the risks of AI and utilise the emerging capabilities which these systems have to offer to policymakers and political institutions across the world.","score":1},{"url":"https://www.semanticscholar.org/paper/021bbcefc993c389bad6c1daefd8ff92d0fc2441","title":"Contrastive Code Representation Learning","venue":"Conference on Empirical Methods in Natural Language Processing","year":2020,"referenceCount":94,"citationCount":60,"influentialCitationCount":10,"publicationDate":"09/07/2020","authors":"Paras Jain,Ajay Jain,Tianjun Zhang,P. Abbeel,Joseph Gonzalez,I. Stoica","id":"021bbcefc993c389bad6c1daefd8ff92d0fc2441","summary":"Contracode is proposed: a contrastive pre-training task that learns code functionality, not form, and improves summarization and TypeScript type inference accuracy by 2 to 13 percentage points over competitive baselines.","score":1},{"url":"https://www.semanticscholar.org/paper/4f278ab5ad629267e06196e273252262854c1c57","title":"BF++: a language for general-purpose program synthesis","venue":"","year":2021,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/01/2021","authors":"Vadim Liventsev,A. Harma,M. Petkovi'c","id":"4f278ab5ad629267e06196e273252262854c1c57","summary":"A new programming language, BF ++ is proposed, designed speciﬁcally for automatic programming of agents in a Partially Observable Markov Decision Process (POMDP) setting and apply neural program synthesis to solve standard OpenAI Gym benchmarks.","score":1},{"url":"https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":65,"citationCount":85,"influentialCitationCount":19,"publicationDate":"05/03/2021","authors":"Dan Hendrycks,Collin Burns,Saurav Kadavath,Akul Arora,Steven Basart,Eric Tang,D. Song,J. Steinhardt","id":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","summary":"This work introduces MATH, a new dataset of 12, 500 challenging competition mathematics problems which can be used to teach models to generate answer derivations and explanations, and shows that accuracy remains relatively low, even with enormous Transformer models.","score":1},{"url":"https://www.semanticscholar.org/paper/09279dc8018a8131e11d527cebb06d0a43c67cff","title":"Creativity and Machine Learning: A Survey","venue":"ArXiv","year":2021,"referenceCount":285,"citationCount":13,"influentialCitationCount":1,"publicationDate":"06/04/2021","authors":"Giorgio Franceschelli,Mirco Musolesi","id":"09279dc8018a8131e11d527cebb06d0a43c67cff","summary":"An overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods is presented.","score":1},{"url":"https://www.semanticscholar.org/paper/98485ce6532d69f34a8ec67de6b09a39532bd221","title":"Communicating Natural Programs to Humans and Machines","venue":"ArXiv","year":2021,"referenceCount":87,"citationCount":7,"influentialCitationCount":0,"publicationDate":"15/06/2021","authors":"Samuel Acquaviva,Yewen Pu,Marta Kryven,Catherine Wong,Gabrielle Ecanow,Maxwell Nye,Theo Sechopoulos,Michael Henry Tessler,J. Tenenbaum","id":"98485ce6532d69f34a8ec67de6b09a39532bd221","summary":"LARC, the Language-complete ARC is presented, a collection of natural language descriptions by a group of human participants who instruct each other on how to solve ARC tasks using language alone, which contains successful instructions for 88% of the ARC tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/26450917d41c828b470ec8818d49f59516a5b9c0","title":"Towards Universality in Multilingual Text Rewriting","venue":"ArXiv","year":2021,"referenceCount":36,"citationCount":4,"influentialCitationCount":0,"publicationDate":"30/07/2021","authors":"Xavier García,Noah Constant,Mandy Guo,Orhan Firat","id":"26450917d41c828b470ec8818d49f59516a5b9c0","summary":"This work takes the first steps towards building a universal rewriter: a model capable of rewriting text in any language to exhibit a wide variety of attributes, including styles and languages, while preserving as much of the original semantics as possible.","score":1},{"url":"https://www.semanticscholar.org/paper/5436193122dff271796bca07df7cecb7a8d6dea6","title":"Natural language-guided programming","venue":"SIGPLAN symposium on New ideas, new paradigms, and reflections on programming and software","year":2021,"referenceCount":53,"citationCount":5,"influentialCitationCount":1,"publicationDate":"11/08/2021","authors":"Geert Heyman,Rafael Huysegems,P. Justen,Tom Van Cutsem","id":"5436193122dff271796bca07df7cecb7a8d6dea6","summary":"The key idea is to adapt code autocompletion tools such that they take into account not only the developer’s already-written code but also the intent of the task the developer is trying to achieve next, formulated in plain natural language.","score":1},{"url":"https://www.semanticscholar.org/paper/4885e616e85d420576196b2578525cbc501137ec","title":"Programming and execution models for next generation code intelligence systems (keynote)","venue":"ESEC/SIGSOFT FSE","year":2021,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/08/2021","authors":"M. Mezini","id":"4885e616e85d420576196b2578525cbc501137ec","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/3f97c2067cde9377e50b3160bbd7982c94abd88a","title":"An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions","venue":"ArXiv","year":2021,"referenceCount":29,"citationCount":19,"influentialCitationCount":1,"publicationDate":2021,"authors":"H. Pearce,Baleegh Ahmad,Benjamin Tan,Brendan Dolan-Gavitt,R. Karri","id":"3f97c2067cde9377e50b3160bbd7982c94abd88a","summary":"This work systematically investigates the prevalence and conditions that can cause GitHub Copilot to recommend insecure code, and explores Copilot’s performance on three distinct code generation axes—examining how it performs given diversity of weaknesses, diversity of prompts, and diversity of domains.","score":1},{"url":"https://www.semanticscholar.org/paper/70087677fd1a6309829b42968934575d05a95f92","title":"What do pre-trained code models know about code?","venue":"International Conference on Automated Software Engineering","year":2021,"referenceCount":34,"citationCount":19,"influentialCitationCount":2,"publicationDate":"25/08/2021","authors":"Anjan Karmakar,R. Robbes","id":"70087677fd1a6309829b42968934575d05a95f92","summary":"Four probing tasks are constructed (probing for surface-level, syntactic, structural, and semantic information) for pre-trained code models to identify whether models are deficient in (understanding) certain code properties, characterize different model layers, and get insight into the model sample-efficiency.","score":1},{"url":"https://www.semanticscholar.org/paper/a176b0de62840f7118006277d94bbc1547162a4d","title":"Learning to Synthesize Programs as Interpretable and Generalizable Policies","venue":"Neural Information Processing Systems","year":2021,"referenceCount":131,"citationCount":13,"influentialCitationCount":2,"publicationDate":"31/08/2021","authors":"Dweep Trivedi,Jesse Zhang,Shao-Hua Sun,Joseph J. Lim","id":"a176b0de62840f7118006277d94bbc1547162a4d","summary":"Experimental results demonstrate that the proposed framework not only learns to reliably synthesize task-solving programs but also outperforms DRL and program synthesis baselines while producing interpretable and more generalizable policies.","score":1},{"url":"https://www.semanticscholar.org/paper/a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":40,"citationCount":170,"influentialCitationCount":66,"publicationDate":"02/09/2021","authors":"Yue Wang,Weishi Wang,Shafiq R. Joty,S. Hoi","id":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","summary":"Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL.","score":1},{"url":"https://www.semanticscholar.org/paper/b8b21c2ddcd7d1c23d7ccfceabb63cb1a05bcfca","title":"HYDRA - Hyper Dependency Representation Attentions","venue":"ArXiv","year":2021,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/09/2021","authors":"Nguyen Ha Thanh,Vu D. Tran,Binh Dang,Minh Q. Bui,Minh Le Nguyen,Le-Minh Nguyen","id":"b8b21c2ddcd7d1c23d7ccfceabb63cb1a05bcfca","summary":"This paper proposes HYDRA heads, lightweight pretrained linguistic self-attention heads to inject knowledge into transformer models without pretraining them again, and empirically verify the framework on benchmark datasets to show the contribution of linguistic knowledge to a transformer model.","score":1},{"url":"https://www.semanticscholar.org/paper/bc9598dc4ed0472d8b59b87ed3a139f8347d40ee","title":"Towards A Measure Of General Machine Intelligence","venue":"ArXiv","year":2021,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/09/2021","authors":"Gautham Venkatasubramanian,Sibesh Kar,Abhimanyu Singh,Shubham Mishra,Dushyant Yadav,Shreyansh Chandak","id":"bc9598dc4ed0472d8b59b87ed3a139f8347d40ee","summary":"A common language of instruction is proposed, a programming language that allows the expression of programs in the form of directed acyclic graphs across a wide variety of real-world domains and computing platforms and evaluates the suitability of some well-known models as general intelligence systems by calculating their g-index scores.","score":1},{"url":"https://www.semanticscholar.org/paper/05c2e1ee203be217f100d2da05bdcc52004f00b6","title":"Unsolved Problems in ML Safety","venue":"ArXiv","year":2021,"referenceCount":217,"citationCount":59,"influentialCitationCount":4,"publicationDate":"28/09/2021","authors":"Dan Hendrycks,Nicholas Carlini,J. Schulman,J. Steinhardt","id":"05c2e1ee203be217f100d2da05bdcc52004f00b6","summary":"This work provides a new roadmap for ML Safety and presents four problems ready for research, namely withstanding hazards, identifying hazards, steering ML systems, and reducing deployment hazards.","score":1},{"url":"https://www.semanticscholar.org/paper/6c2d43e71e240e354b5790a38da78a291ceffe7c","title":"Learning to Superoptimize Real-world Programs","venue":"ArXiv","year":2021,"referenceCount":37,"citationCount":2,"influentialCitationCount":0,"publicationDate":"28/09/2021","authors":"Alex Shypula,P. Yin,Jeremy Lacomis,Claire Le Goues,E. Schwartz,Graham Neubig","id":"6c2d43e71e240e354b5790a38da78a291ceffe7c","summary":"A framework to learn to superoptimize real-world programs by using neural sequence-to-sequence models, and an approach to implement and outperforms a standard policy gradient learning approach on this dataset.","score":1},{"url":"https://www.semanticscholar.org/paper/24e775b20adf21e9b5b95c6a9b7a5c164d055849","title":"M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining","venue":"ArXiv","year":2021,"referenceCount":52,"citationCount":19,"influentialCitationCount":0,"publicationDate":"08/10/2021","authors":"Junyang Lin,An Yang,Jinze Bai,Chang Zhou,Le Jiang,Xianyan Jia,Ang Wang,J. Zhang,Yong Li,Wei Lin,Jingren Zhou,Hongxia Yang","id":"24e775b20adf21e9b5b95c6a9b7a5c164d055849","summary":"This paper demonstrates a practice of pretraining unprecedented 10-trillion-parameter model, an order of magnitude larger than the state-of-the-art, on solely 512 GPUs within 10 days, and provides a technique, Granular CPU offloading, to manage CPU memory for training large model and maintain high GPU utilities.","score":1},{"url":"https://www.semanticscholar.org/paper/360e0197378799d890f473893cc0c773b8182b4e","title":"Searching for Replacement Classes","venue":"ArXiv","year":2021,"referenceCount":36,"citationCount":1,"influentialCitationCount":0,"publicationDate":"11/10/2021","authors":"Malavika Samak,J. Cambronero,M. Rinard","id":"360e0197378799d890f473893cc0c773b8182b4e","summary":"This work introduces ClassFinder, a system which given a query class Q, and a search corpus S, returns a ranked subset of classes that can replace Q and its functionality, and leverages the complementary strengths of a distributed embeddingsbased search and type-based analysis.","score":1},{"url":"https://www.semanticscholar.org/paper/9991bb2eb7e7d7e9d831e257ae77ba2eaeaba3dc","title":"Applying quantum approximate optimization to the heterogeneous vehicle routing problem","venue":"","year":2021,"referenceCount":177,"citationCount":6,"influentialCitationCount":0,"publicationDate":"13/10/2021","authors":"David Fitzek,Toheed Ghandriz,L. Laine,M. Granath,A. F. Kockum","id":"9991bb2eb7e7d7e9d831e257ae77ba2eaeaba3dc","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/21bc4ead8ea415579ab40e437fcbc274929f17c8","title":"Solving the Families In the Wild Kinship Verification Challenge by Program Synthesis","venue":"2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)","year":2021,"referenceCount":25,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/10/2021","authors":"Junyi Huang,M. Strome,I. Jenkins,Parker Williams,Bo Feng,Yaning Wang,Roman Wang,Vaibhav Bagri,Newman Cheng,Iddo Drori","id":"21bc4ead8ea415579ab40e437fcbc274929f17c8","summary":"This work uses Codex to generate model variants, and also demonstrates its ability to generate entire running programs for kinship verification tasks of specific relationships, among the top 3 winning entries in the competition.","score":1},{"url":"https://www.semanticscholar.org/paper/8091e51ebbcd2424a1c5b50c036bae5295090525","title":"Top 3 in FG 2021 Families In the Wild Kinship Verification Challenge","venue":"ArXiv","year":2021,"referenceCount":11,"citationCount":3,"influentialCitationCount":0,"publicationDate":2021,"authors":"Junyi Huang,M. Strome,Ian Jenkins,Parker Williams,Bo Feng,Yaning Wang,Roman Wang,Vaibhav Bagri,Newman Cheng,Iddo Drori","id":"8091e51ebbcd2424a1c5b50c036bae5295090525","summary":"This work demonstrates high quality kinship verification by participating in the 2021 Recognizing Families in the Wild challenge which provides the largest publicly available dataset in the field.","score":1},{"url":"https://www.semanticscholar.org/paper/dace03e57056d736f9e24937bdf486e894f8e866","title":"Is neural machine translation approach accurate enough for coding assistance?","venue":"BCNC@SPLASH","year":2021,"referenceCount":20,"citationCount":2,"influentialCitationCount":1,"publicationDate":"15/10/2021","authors":"Yuka Akinobu,Momoka Obara,Teruno Kajiura,Shiho Takano,Miyu Tamura,Mayu Tomioka,Kimio Kuramitsu","id":"dace03e57056d736f9e24937bdf486e894f8e866","summary":"A transcompiler-based back-translation, a data augmentation method that generates parallel corpora from numerous source code repositories and the resulting BLEU indicates that the proposed model is accurate enough to allow coding assistance in the future.","score":1},{"url":"https://www.semanticscholar.org/paper/21e8e76386aaaa00e0971af70ce84a8a544e1aa1","title":"Cascaded Fast and Slow Models for Efficient Semantic Code Search","venue":"ArXiv","year":2021,"referenceCount":24,"citationCount":2,"influentialCitationCount":0,"publicationDate":"15/10/2021","authors":"Akhilesh Deepak Gotmare,Junnan Li,Shafiq R. Joty,S. Hoi","id":"21e8e76386aaaa00e0971af70ce84a8a544e1aa1","summary":"An efficient and accurate semantic code search framework with cascaded fast and slow models, in which a fast transformer encoder model is learned to optimize a scalable index for fast retrieval followed by learning a slow classification-based re-ranking model to improve the performance of the top K results from the fast retrieval.","score":1},{"url":"https://www.semanticscholar.org/paper/6a269b1abccdbf57e79b3f115a97bff14b435ad9","title":"Automated Support for Unit Test Generation: A Tutorial Book Chapter","venue":"ArXiv","year":2021,"referenceCount":32,"citationCount":1,"influentialCitationCount":0,"publicationDate":"26/10/2021","authors":"Afonso Fontes,Gregory Gay,F. G. O. Neto,R. Feldt","id":"6a269b1abccdbf57e79b3f115a97bff14b435ad9","summary":"This chapter introduces two algorithms that can generate pytest-formatted unit tests, tuned towards coverage of source code statements, and introduces the concept of search-based unit test generation.","score":1},{"url":"https://www.semanticscholar.org/paper/9f260bdd4030af5297a9c1cbb817c75701ac8c83","title":"The 5th Recognizing Families in the Wild Data Challenge: Predicting Kinship from Faces","venue":"IEEE International Conference on Automatic Face & Gesture Recognition","year":2021,"referenceCount":54,"citationCount":2,"influentialCitationCount":0,"publicationDate":"31/10/2021","authors":"Joseph P. Robinson,Can Qin,Ming Shao,Matthew A. Turk,R. Chellappa,Y. Fu","id":"9f260bdd4030af5297a9c1cbb817c75701ac8c83","summary":"Submissions for this year's RFIW are summarized, and the results for kinship verification, tri-subject verification, and family member search and retrieval are reviewed.","score":1},{"url":"https://www.semanticscholar.org/paper/c23d9d44e8bc68408cea9f305d1f24d915bc0d0d","title":"Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey","venue":"ArXiv","year":2021,"referenceCount":322,"citationCount":34,"influentialCitationCount":1,"publicationDate":"01/11/2021","authors":"Bonan Min,Hayley H. Ross,Elior Sulem,Amir Pouran Ben Veyseh,Thien Huu Nguyen,Oscar Sainz,Eneko Agirre,Ilana Heinz,D. Roth","id":"c23d9d44e8bc68408cea9f305d1f24d915bc0d0d","summary":"A survey of recent work that uses large, pre-trained transformer-based language models to solve NLP tasks via pre-training then fine-tuning, prompting, or text generation approaches.","score":1},{"url":"https://www.semanticscholar.org/paper/1444536496d8064f33e10b38b5820fecfab5b367","title":"Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs","venue":"ArXiv","year":2021,"referenceCount":22,"citationCount":13,"influentialCitationCount":1,"publicationDate":"06/11/2021","authors":"Julian Aron Prenner,R. Robbes","id":"1444536496d8064f33e10b38b5820fecfab5b367","summary":"This work investigates whether Codex is able to localize and fix bugs, a task of central interest in the field of automated program repair, and finds that, despite not being trained for APR, Codex is surprisingly effective, and competitive with recent state of the art techniques.","score":1},{"url":"https://www.semanticscholar.org/paper/4e40595d6ecba027cebb4f2e3b43ae44bcf51daf","title":"Solving Linear Algebra by Program Synthesis","venue":"ArXiv","year":2021,"referenceCount":17,"citationCount":10,"influentialCitationCount":0,"publicationDate":"16/11/2021","authors":"Iddo Drori,Nakul Verma","id":"4e40595d6ecba027cebb4f2e3b43ae44bcf51daf","summary":"This work uses OpenAI Codex with zero-shot learning to synthesize code from questions and quantifies the difference between the original question text and the transformed question text that yields a correct answer.","score":1},{"url":"https://www.semanticscholar.org/paper/f7987fa2aadc0b368c185dc4d2fdb1337a202c32","title":"Solving Probability and Statistics Problems by Program Synthesis","venue":"ArXiv","year":2021,"referenceCount":14,"citationCount":8,"influentialCitationCount":1,"publicationDate":"16/11/2021","authors":"Leonard Tang,Elizabeth Ke,Nikhil Singh,Nakul Verma,Iddo Drori","id":"f7987fa2aadc0b368c185dc4d2fdb1337a202c32","summary":"This work is the first to introduce a new dataset of university-level probability and statistics problems and solve these problems in a scalable fashion using the program synthesis capabilities of large language models.","score":1},{"url":"https://www.semanticscholar.org/paper/04db9b694280134f09af5fa787a306907edba29d","title":"How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN","venue":"ArXiv","year":2021,"referenceCount":81,"citationCount":16,"influentialCitationCount":2,"publicationDate":"18/11/2021","authors":"R. Thomas McCoy,P. Smolensky,Tal Linzen,Jianfeng Gao,Asli Celikyilmaz","id":"04db9b694280134f09af5fa787a306907edba29d","summary":"AVEN, a suite of analyses for assessing the novelty of generated text, focusing on sequential structure (n-grams) and syntactic structure, is introduced, showing that GPT-2's novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues.","score":1},{"url":"https://www.semanticscholar.org/paper/cecc913290736a5a368642c5b59a130eddd1fa7b","title":"Can Pre-trained Language Models be Used to Resolve Textual and Semantic Merge Conflicts?","venue":"ArXiv","year":2021,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/11/2021","authors":"Jialu Zhang,Todd Mytkowicz,Mike Kaufman,R. Piskac,Shuvendu K. Lahiri","id":"cecc913290736a5a368642c5b59a130eddd1fa7b","summary":"The feasibility of automatically repairing merge conflicts (both textual and semantic) using k-shot learning with large neural language models (LM) such as GPT-3 is explored and the results are mixed.","score":1},{"url":"https://www.semanticscholar.org/paper/21ab011a3adccbd912aea58f76b84b7873c41df3","title":"Machines&Influence: An Information Systems Lens","venue":"","year":2021,"referenceCount":130,"citationCount":1,"influentialCitationCount":0,"publicationDate":"26/11/2021","authors":"Shashank Yadav","id":"21ab011a3adccbd912aea58f76b84b7873c41df3","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models","venue":"ArXiv","year":2021,"referenceCount":30,"citationCount":102,"influentialCitationCount":10,"publicationDate":"30/11/2021","authors":"Maxwell Nye,Anders Andreassen,Guy Gur-Ari,H. Michalewski,Jacob Austin,David Bieber,David Dohan,Aitor Lewkowycz,Maarten Bosma,D. Luan,Charles Sutton,Augustus Odena","id":"92173d081b15824d22a9ef070e118744ceee8052","summary":"Surprisingly, large pre-trained language models are able to perform complex multistep computations—even in the few-shot regime—when asked to perform the operation “step by step”, showing the results of intermediate computations.","score":1},{"url":"https://www.semanticscholar.org/paper/827a67bbb96c8ed34c0f79e2ea811c5b53a6896b","title":"Controllable Response Generation for Assistive Use-cases","venue":"ArXiv","year":2021,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/12/2021","authors":"Shachi H. Kumar,Hsuan Su,R. Manuvinakurike,Saurav Sahay,L. Nachman","id":"827a67bbb96c8ed34c0f79e2ea811c5b53a6896b","summary":"This study shows that keyword-control on end-to-end response generation models is powerful and can enable and empower users with degenerative disorders to carry out their dayto-day communication.","score":1},{"url":"https://www.semanticscholar.org/paper/ee042a3e299a32c413532e64603de8d3ddb6aa87","title":"Automap: Towards Ergonomic Automated Parallelism for ML Models","venue":"ArXiv","year":2021,"referenceCount":36,"citationCount":5,"influentialCitationCount":0,"publicationDate":"06/12/2021","authors":"Michael Schaarschmidt,Dominik Grewe,Dimitrios Vytiniotis,Adam Paszke,G. Schmid,Tamara Norman,James Molloy,Jonathan Godwin,Norman A. Rink,Vinod Nair,Dan Belov","id":"ee042a3e299a32c413532e64603de8d3ddb6aa87","summary":"This work presents the prototype of an automated partitioner that seamlessly integrates into existing compilers and existing user workflows and enables SPMD-style parallelism that encompasses data parallelism and parameter/activation sharding.","score":1},{"url":"https://www.semanticscholar.org/paper/091fa84bdc07dcb22a34060c3996d8c58d71cd20","title":"Towards Neural Functional Program Evaluation","venue":"ArXiv","year":2021,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/12/2021","authors":"Torsten Scholak,Jonathan Pilault,Joey Velez-Ginorio","id":"091fa84bdc07dcb22a34060c3996d8c58d71cd20","summary":"A new program generation mechanism is introduced that allows control over syntactic sugar for semantically equivalent programs in transformer-based language models for program evaluation of simple functional programming languages.","score":1},{"url":"https://www.semanticscholar.org/paper/6ccc0ca964ddab19705e4832758e6a2447325348","title":"End to End Software Engineering Research","venue":"ArXiv","year":2021,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/12/2021","authors":"Idan Amit","id":"6ccc0ca964ddab19705e4832758e6a2447325348","summary":"The dataset is constructed in a way that enables not only predicting concepts but also investigating their causes, and improves over features based machine learning by not requiring domain experts and being able to extract new knowledge.","score":1},{"url":"https://www.semanticscholar.org/paper/58dd9a3da16c10f5c3cdbb9760a9ff378847bf76","title":"Self-supervision of wearable sensors time-series data for influenza detection","venue":"ArXiv","year":2021,"referenceCount":14,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/12/2021","authors":"Arinbjörn Kolbeinsson,Piyusha S. Gade,R. Kainkaryam,Filip Jankovic,L. Foschini","id":"58dd9a3da16c10f5c3cdbb9760a9ff378847bf76","summary":"The results show that predicting the next day’s resting heart rate or time-in-bed during sleep provides better representations for ILI prediction, adding to previous work demonstrating the practical application of self-supervised learning from activity data to improve health predictions.","score":1},{"url":"https://www.semanticscholar.org/paper/1b94afca9d6688cc584a744734126473283cbc93","title":"Can Transformers be Strong Treatment Effect Estimators?","venue":"ArXiv","year":2022,"referenceCount":58,"citationCount":8,"influentialCitationCount":4,"publicationDate":2022,"authors":"Yi-Fan Zhang,Hanlin Zhang,Zachary Chase Lipton,Li Erran Li,Eric Xing","id":"1b94afca9d6688cc584a744734126473283cbc93","summary":"A general framework based on the Transformer architecture is developed to address a variety of challenging treatment effect estimation (TEE) problems and a propensity score network is proposed that is trained with TransTEE in an adversarial manner to promote independence between covariates and treatments to further address selection bias.","score":1},{"url":"https://www.semanticscholar.org/paper/856d2c0f7b3f80dcf1f68d1dc0dcbf5c6fe5679a","title":"Interpreting docstrings without using common sense: the private science of very large language models∗","venue":"","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Darren Abramson,Ali Emami","id":"856d2c0f7b3f80dcf1f68d1dc0dcbf5c6fe5679a","summary":"GPT-3, the natural language model on which Codex is built, and that services such as Copilot ultimately depend on, suffers from scientific deficiencies, and critical remarks on Copilot’s structure and underlying language model are presented.","score":1},{"url":"https://www.semanticscholar.org/paper/78fd8185c5cd55830c31aa718a9909827e20774e","title":"A Research Agenda for Assessing the Economic Impacts of Code Generation Models","venue":"","year":2022,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Sam Manning,Pamela Mishkin,Gillian K. Hadfield,Tyna,Eloundou,E. Eisner","id":"78fd8185c5cd55830c31aa718a9909827e20774e","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"C OMPOSITIONAL G ENERALIZATION AND D ECOMPOSITION IN N EURAL P ROGRAM S YNTHESIS","venue":"","year":null,"referenceCount":0,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"","id":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","summary":"A suite of generalization tasks, which measure different types of compositional generalization that are desirable for program synthesis and are particularly difﬁcult for current sequence to sequence models, are proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/75c8f2916a2067f7549cf58ea8c9061565eb1dab","title":"C ROSS B EAM : L EARNING TO S EARCH IN B OTTOM -U P P ROGRAM S YNTHESIS","venue":"","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"H. Dai,Kevin Ellis,Charles Sutton","id":"75c8f2916a2067f7549cf58ea8c9061565eb1dab","summary":"This work uses a neural model to learn a hands-on search policy for bottom-up synthesis, instead of relying on a combinatorial search algorithm, and observes that CROSSBEAM learns to search efficiently, exploring much smaller portions of the program space compared to the state-of-the-art.","score":1},{"url":"https://www.semanticscholar.org/paper/8a4dce5735a101ff8f64c2b676afb8c24950a5d8","title":"Zero-shot Mathematical Problem Solving via Generative Pre-trained Transformers","venue":"International Conference on Enterprise Information Systems","year":2022,"referenceCount":9,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Federico A. Galatolo,M. Cimino,G. Vaglini","id":"8a4dce5735a101ff8f64c2b676afb8c24950a5d8","summary":"The proposed approach shows that coding based problem-solving is more effective than the natural language reasoning based one, and by exploiting the Python as programming language, the proposed pipeline achieves 54.20% solve rate.","score":1},{"url":"https://www.semanticscholar.org/paper/bee0be592c314435048599281bcd9c72bf63b735","title":"CueBot: Cue-Controlled Response Generation for Assistive Interaction Usages","venue":"Workshop on Speech and Language Processing for Assistive Technologies","year":2022,"referenceCount":52,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Shachi H. Kumar,Hsuan Su,R. Manuvinakurike,Maximilian Pinaroc,Sai Prasad,Saurav Sahay,L. Nachman","id":"bee0be592c314435048599281bcd9c72bf63b735","summary":"This work builds a system that can represent people with disabilities, or speech and language disorders, in a social conversation and generate responses that can be controlled by the users using cues/keywords and introduces a keyword-loss to lexically constrain the model response output.","score":1},{"url":"https://www.semanticscholar.org/paper/57c31c709792949bfbb9d4aaee941048aa07cc4b","title":"How to Give Imperfect Automated Guidance to Learners: A Case-Study in Workplace Learning","venue":"International Conference on Artificial Intelligence in Education","year":2022,"referenceCount":18,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"J. Whitehill,Amitai Erfanian","id":"57c31c709792949bfbb9d4aaee941048aa07cc4b","summary":"There was tentative evidence that workers’ behaviors were impacted by the FP/FN trade-oﬀ of their assigned experimental condition even after the ML assistant was removed and evidence that learners modulate their behaviors based on the ﬁne-grained conﬁdence values conveyed by the assistant.","score":1},{"url":"https://www.semanticscholar.org/paper/2e5b29457ff45b8faba69bc2eaf05521584a7bec","title":"B UG F IX G ENERATION USING G RAPH T RANS","venue":"","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","id":"2e5b29457ff45b8faba69bc2eaf05521584a7bec","summary":"This work proposes FIXUR, a new architecture for generating bug fixing edits, by complementing graph neural networks with Transformer to encode the code graph as a graph that encapsulates rich syntactic and semantic dependencies.","score":1},{"url":"https://www.semanticscholar.org/paper/f01e316d3b28ccecda25b4d57926f496a9b17d3d","title":"How Robust are Neural Code Completion Models to Source Code Transformation?","venue":"","year":2022,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","id":"f01e316d3b28ccecda25b4d57926f496a9b17d3d","summary":"This work develops a methodology for systematically evaluating neural code completion models using common source code transformations and provides insights into the strengths and weaknesses of different models, and serves as a foundation for future work towards improving the accuracy and robustness of Neural code completion.","score":1},{"url":"https://www.semanticscholar.org/paper/24c6982a25c0114bc98805d368b06d1a4f6d8fd5","title":"Researching Alignment Research: Unsupervised Analysis","venue":"ArXiv","year":2022,"referenceCount":0,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/06/2022","authors":"Jan H. Kirchner,Logan Smith,Jacques Thibodeau,Kyle McDonell,Laria Reynolds","id":"24c6982a25c0114bc98805d368b06d1a4f6d8fd5","summary":"The field is growing quickly, with several subfields emerging in parallel, and a classifier trained on AI alignment research articles can detect relevant articles that were not originally included in the dataset.","score":1},{"url":"https://www.semanticscholar.org/paper/b562be15b076b494023b8ac24fc8c459f4fdf80a","title":"Craft an Iron Sword: Dynamically Generating Interactive Game Characters by Prompting Large Language Models Tuned on Code","venue":"WORDPLAY","year":2022,"referenceCount":21,"citationCount":3,"influentialCitationCount":1,"publicationDate":2022,"authors":"Ryan Volum,Sudha Rao,Michael Xu,Gabriel DesGarennes,Chris Brockett,Benjamin Van Durme,Olivia Deng,Akanksha Malhotra,Bill Dolan","id":"b562be15b076b494023b8ac24fc8c459f4fdf80a","summary":"It is demonstrated that use of a few example conversational prompts can power a conversational agent to generate both natural language and novel code, which can permit development of NPCs with which players can have grounded conversations that are free-form and less repetitive.","score":1},{"url":"https://www.semanticscholar.org/paper/09e14c4c80e20e80c052e0adb0d49df51aff718d","title":"Yes, You Can Make an App Too: A Systematic Study of Prompt Engineering in the Automatic Generation of Mobile Applications from User Queries","venue":"","year":2022,"referenceCount":16,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Jasmine Shone","id":"09e14c4c80e20e80c052e0adb0d49df51aff718d","summary":"One of the first systematic studies of prompt engineering for the Codex model, a LLM that produces code from a natural-language input, is embarked on, improving the pipeline’s performance from baseline for complex apps using example selection mechanisms and 43% for simple apps.","score":1},{"url":"https://www.semanticscholar.org/paper/91260f73dd179487fb16713deb8267634ae14716","title":"CodexDB: Synthesizing Code for Query Processing from Natural Language Instructions using GPT-3 Codex","venue":"Proceedings of the VLDB Endowment","year":2022,"referenceCount":39,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"Immanuel Trummer","id":"91260f73dd179487fb16713deb8267634ae14716","summary":"The CodexDB framework is a framework on top of GPT-3 Codex that decomposes complex SQL queries into a series of simple processing steps, described in natural language, that enables users to customize SQL query processing via natural language instructions.","score":1},{"url":"https://www.semanticscholar.org/paper/9bf75110ea0923bbed49256b5491f1ec284019ec","title":"From BERT to GPT-3 Codex: Harnessing the Potential of Very Large Language Models for Data Management","venue":"Proc. VLDB Endow.","year":2022,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Immanuel Trummer","id":"9bf75110ea0923bbed49256b5491f1ec284019ec","summary":"The goal of the tutorial is to introduce database researchers to the latest generation of language models, and to their use cases in the domain of data management.","score":1},{"url":"https://www.semanticscholar.org/paper/ddab94478a7647ee136b1f6b5076417db3074d0f","title":"Machine Programming: Turning Data into Programmer Productivity","venue":"Proc. VLDB Endow.","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"A. Wasay,Nesime Tatbul,Justin Emile Gottschlich","id":"ddab94478a7647ee136b1f6b5076417db3074d0f","summary":"An introduction to machine programming is introduced introducing its three pillars: intention, invention, and adaptation, and an overview of the data ecosystem central to all machine programming systems is provided, highlighting challenges and novel opportunities relevant to the data systems community.","score":1},{"url":"https://www.semanticscholar.org/paper/2443179d421e1faf7474add557b45add554723c7","title":"Formal Premise Selection With Language Models","venue":"","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Szymon Tworkowski,Maciej Miku la,Tomasz Odrzygóźdź,K. Czechowski,Szymon Antoniak,Albert Qiaochu Jiang,Christian Szegedy,Lukasz Kucinski,Piotr Mi loś,Yuhuai Wu","id":"2443179d421e1faf7474add557b45add554723c7","summary":"This work provides a solution to the problem of selecting a useful premise to prove a new theorem by combining a premise selection model with a language model, and shows that this retrieval-augmented prover achieves significant improvements in proof rates compared to the language model alone.","score":1},{"url":"https://www.semanticscholar.org/paper/15e767aa26a14455da95a2b2f11e3d59f2c250f6","title":"Autoformalization for Neural Theorem Proving","venue":"","year":2022,"referenceCount":11,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Yuhuai Wu,Albert Qiaochu Jiang,Wenda Li,Markus N. Rabe,Charles Staats,M. Jamnik,Christian Szegedy","id":"15e767aa26a14455da95a2b2f11e3d59f2c250f6","summary":"This work demonstrates the feasibility and usefulness of autoformalization in the context of the newly introduced MiniF2F benchmark, and finds that transformer-based language models trained on a large amount of web data are capable of formalizing mathematical competition problem statements with a relatively high success rate.","score":1},{"url":"https://www.semanticscholar.org/paper/a1ef81e17a9ca41e09aba802040a2eca2744716f","title":"Generation Probabilities are Not Enough: Improving Error Highlighting for AI Code Suggestions","venue":"","year":2022,"referenceCount":15,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Helena Vasconcelos","id":"a1ef81e17a9ca41e09aba802040a2eca2744716f","summary":"It is unclear how best to convey the uncertainty of generative models to human operators or if doing so will positively impact human-AI collaboration.","score":1},{"url":"https://www.semanticscholar.org/paper/cd155729180ea707dea251f8e9654db241ffd808","title":"Is GPT-3 all you need for machine learning for chemistry?","venue":"","year":2022,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"K. Jablonka","id":"cd155729180ea707dea251f8e9654db241ffd808","summary":"This work analyzes whether one of the largest pre-trained LLMs, GPT-3, can be directly used for chemistry applications by fine-tuning on only a few data points from a chemistry dataset, i.e., without pre-training on a chemistry-specific dataset.","score":1},{"url":"https://www.semanticscholar.org/paper/0180d35b85dd4daead90e0652b64b1339e754684","title":"Assistance with large language models","venue":"","year":2022,"referenceCount":30,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Dmitrii Krasheninnikov","id":"0180d35b85dd4daead90e0652b64b1339e754684","summary":"A behavioral cloning approach is applied to GPT-3 such that it can respond to clear input questions directly, clarify the intent behind vague input questions, and respond based on the clarification it receives, and this approach leads to quantitative improvements in answer accuracy compared to a baseline that cannot ask for clarifications.","score":1},{"url":"https://www.semanticscholar.org/paper/e002bb8dae5a18a5ea1e7e1aafa16e19ad545662","title":"Solving Math Word Problems with Process-based and Outcome-based Feedback","venue":"","year":2022,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"J. Uesato,Nate Kushman,Ramana Kumar,Francis Song,Noah Siegel,L. Wang,Antonia Creswell,Geoffery Irving,I. Higgins","id":"e002bb8dae5a18a5ea1e7e1aafa16e19ad545662","summary":"This work runs the first comprehensive comparison between process- and outcome- based approaches trained on a natural language task, GSM8K, and finds that pure outcome-based supervision produces similar final-answer error rates with less label supervision.","score":1},{"url":"https://www.semanticscholar.org/paper/b98d6fe8f0ef02ec0d1bb2bcfb924c8f01feb7d4","title":"Convergent Representations of Computer Programs in Human and Artificial Neural Networks","venue":"","year":2022,"referenceCount":79,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Shashank Srikant,Benjamin Lipkin,Anna A. Ivanova,Evelina Fedorenko,Una-May O’Reilly","id":"b98d6fe8f0ef02ec0d1bb2bcfb924c8f01feb7d4","summary":"Analysis of brain recordings derived from functional magnetic resonance imaging studies of programmers comprehending Python code suggests at least two distinct neural mechanisms mediating computer program comprehension and evaluation, prompting the design of code model objectives that go beyond static language modeling.","score":1},{"url":"https://www.semanticscholar.org/paper/f9d38e03c97562b5f5942f3a0c43bdb751b9dc1c","title":"Wordplay 2022 The 3rd Wordplay: When Language Meets Games Workshop Proceedings of the Workshop","venue":"","year":2022,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Shrimai Prabhumoye","id":"f9d38e03c97562b5f5942f3a0c43bdb751b9dc1c","summary":"Novel techniques to generate text in a particular style are described, providing an approach of generating engaging naturalistic conversation responses using knowledge generated by pre-trained language models, considering their recent success in a multitude of NLP tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/a85c5d7272371345e28a9910080224cad799972e","title":"Schema Matching using Pre-Trained Language Models","venue":"","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Yunjia Zhang","id":"a85c5d7272371345e28a9910080224cad799972e","summary":"The Learned Schema Mapper (LSM) is proposed, a novel linguistic schema matching system that leverages the natural language understanding capabilities of pre-trained language models to improve the overall accuracy and significantly reduce the overall human labeling cost.","score":1},{"url":"https://www.semanticscholar.org/paper/ae10c4b220a0bc0999bf169d5c219086d1f1aeed","title":"Edinburgh Research Explorer Taxonomy of risks posed by language models","venue":"","year":2022,"referenceCount":218,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Laura Weidinger,J. Uesato,M. Rauh,C. Griffin,P. Huang,John F. J. Mellor,A. Glaese,M. Cheng,B. Balle,A. Kasirzadeh,C. Biles,S. Brown,Z. Kenton,W. Hawkins,T. Stepleton,A. Birhane,L. Hendricks,Rimell,Laura Weidinger,J. Uesato,M. Rauh,C. Griffin,John F. J. Mellor,A. Glaese,M. Cheng,B. Balle,A. Kasirzadeh,C. Biles,S. Brown,Z. Kenton,Tom,Stepleton,A. Birhane,Lisa Anne Hendricks,Laura Rimell","id":"ae10c4b220a0bc0999bf169d5c219086d1f1aeed","summary":"A comprehensive taxonomy of ethical and social risks associated with LMs is developed, drawing on expertise and literature from computer science, linguistics, and the social sciences to ensure that language models are developed responsibly.","score":1},{"url":"https://www.semanticscholar.org/paper/60043104ca33a1fc905af57ead32768e52c69103","title":"C3PO: A Lightweight Copying Mechanism for Translating Pseudocode to Code","venue":"AACL","year":2022,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Vishruth Veerendranath,Vibha Masti,Prajwal Anagani,Mamatha Hr","id":"60043104ca33a1fc905af57ead32768e52c69103","summary":"This work proposes a lightweight alternative to LLMs that exploits the property of code wherein most tokens can be simply copied from the pseudocode, and achieves similar performance to non-C3PO models while reducing the computational cost of training as well as the vocabulary sizes.","score":1},{"url":"https://www.semanticscholar.org/paper/8a4299a61cc44b60e5be32fef35341c3bc7b2a0d","title":"The Hole Story: Type-Directed Synthesis and Repair","venue":"","year":2022,"referenceCount":106,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Matthías Páll Gissurarson","id":"8a4299a61cc44b60e5be32fef35341c3bc7b2a0d","summary":"This thesis explores the integration of program synthesis into GHC compiler error messages using typed-hole suggestions to aid the completion of partial programs during development and presents PropR, a tool based on type-driven synthesis aided by propertybased testing and fault-localization in conjunction with genetic algorithms to automatically repair buggy programs.","score":1},{"url":"https://www.semanticscholar.org/paper/15ef2d1b88f54fa32a32927463a7116219b89529","title":"L EARNING TO S UPEROPTIMIZE R EAL - WORLD P ROGRAMS","venue":"","year":null,"referenceCount":0,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"","id":"15ef2d1b88f54fa32a32927463a7116219b89529","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/4797e960f8e7a2b47ae0d95c7071ef84fa5d4b5b","title":"Code Generation Using Machine Learning: A Systematic Review","venue":"IEEE Access","year":2022,"referenceCount":117,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Enrique Dehaerne,Bappaditya Dey,Sandip Halder,S. De Gendt,Wannes Meert","id":"4797e960f8e7a2b47ae0d95c7071ef84fa5d4b5b","summary":"This review provides a broad and detailed overview of studies for code generation using ML, and summarizes the applications, models, datasets, results, limitations, and future work of 37 publications.","score":1},{"url":"https://www.semanticscholar.org/paper/660ca9e15e19409903a0605f0584d0f263c35c67","title":"S YNCHROMESH : R ELIABLE C ODE G ENERATION FROM P RE - TRAINED L ANGUAGE M ODELS","venue":"","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Gabriel Poesia,A. Tiwari,Gustavo Soares,Christopher Meek","id":"660ca9e15e19409903a0605f0584d0f263c35c67","summary":"A framework for substantially improving the reliability of pre-trained models for code generation and observing substantial complementary gains from CSD and TST in prediction accuracy and in effectively preventing run-time errors is proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/25c402db512d327f1da143de3b8e797ad6fbfe5b","title":"P ROG P ROMPT : Generating Situated Robot Task Plans using Large Language Models","venue":"","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Llm Gpt","id":"25c402db512d327f1da143de3b8e797ad6fbfe5b","summary":"This work presents a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks, and makes concrete recommendations about prompt structure and generation constraints through ablation experiments.","score":1},{"url":"https://www.semanticscholar.org/paper/ba5d21b7c65c6598c7bd39a5d992308c205df374","title":"A S YSTEMATIC E VALUATION OF L ARGE L ANGUAGE M ODELS OF C ODE","venue":"","year":2022,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Graham Neubig,V. Hellendoorn","id":"ba5d21b7c65c6598c7bd39a5d992308c205df374","summary":"It is found that existing open-source models do achieve close results in some programming languages, although targeted mainly for natural language modeling, and a new model, PolyCoder, is released that was trained on 249GB of code across 12 programming languages on a single machine.","score":1},{"url":"https://www.semanticscholar.org/paper/56e6d62c638a24411f12d15cdc8821a31fc495c8","title":"Source Code Generation from Descriptions in a Natural Language","venue":"","year":2022,"referenceCount":70,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Bc. Jan Pašek","id":"56e6d62c638a24411f12d15cdc8821a31fc495c8","summary":"This work introduces CodeFormer, a Python source code generator pretrained on a massive GitHub crawl consisting of 230M Python functions, and releases the resulting model, built on BART architecture, which generates Python functions based on descriptions in English.","score":1},{"url":"https://www.semanticscholar.org/paper/75e36bb95023e55f7dec95d1af557e219ba3d349","title":"CORAL: COde RepresentAtion learning with weakly-supervised transformers for analyzing data analysis","venue":"EPJ Data Science","year":2020,"referenceCount":71,"citationCount":7,"influentialCitationCount":2,"publicationDate":"28/08/2020","authors":"Ashley Ge Zhang,Michael Merrill,Yang Liu,Jeffrey Heer,Tim Althoff","id":"75e36bb95023e55f7dec95d1af557e219ba3d349","summary":"This work proposes a novel weakly supervised transformer-based architecture for computing joint representations of code from both abstract syntax trees and surrounding natural language comments and achieves a 38% increase in accuracy over expert-supplied heuristics and outperforms a suite of baselines.","score":1},{"url":"https://www.semanticscholar.org/paper/8cf3a454556060d6e9aa86dbabf221bd10bf9759","title":"On the Effectiveness of Transfer Learning for Code Search","venue":"IEEE Transactions on Software Engineering","year":2021,"referenceCount":86,"citationCount":9,"influentialCitationCount":3,"publicationDate":"12/08/2021","authors":"P. Salza,Christoph Schwizer,Jian Gu,H. Gall","id":"8cf3a454556060d6e9aa86dbabf221bd10bf9759","summary":"It is demonstrated that natural language processing models based on the Transformer architecture can be directly applied to source code analysis tasks, such as code search, and the combined use of an information retrieval-based approach followed by a Transformer leads to the best results overall.","score":1},{"url":"https://www.semanticscholar.org/paper/6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions","venue":"IEEE Symposium on Security and Privacy","year":2021,"referenceCount":35,"citationCount":26,"influentialCitationCount":4,"publicationDate":"20/08/2021","authors":"H. Pearce,Baleegh Ahmad,Benjamin Tan,Brendan Dolan-Gavitt,R. Karri","id":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","summary":"This work systematically investigates the prevalence and conditions that can cause GitHub Copilot to recommend insecure code, and explores Copilot’s performance on three distinct code generation axes—examining how it performs given diversity of weaknesses, diversity of prompts, and diversity of domains.","score":1},{"url":"https://www.semanticscholar.org/paper/a1e1297fb132d7769dda3f7917e57757e6e22605","title":"Impact of Evaluation Methodologies on Code Summarization","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":56,"citationCount":3,"influentialCitationCount":0,"publicationDate":"22/08/2021","authors":"Pengyu Nie,Jiyang Zhang,Junyi Jessy Li,R. Mooney,Miloš Gligorić","id":"a1e1297fb132d7769dda3f7917e57757e6e22605","summary":"The time-segmented evaluation methodology is introduced, which is novel to the code summarization research community, and compared with the mixed-project and cross-project methodologies that have been commonly used and shows that different methodologies lead to conflicting evaluation results.","score":1},{"url":"https://www.semanticscholar.org/paper/ff0b2681d7b05e16c46dfb71d980cc2f605907cd","title":"Finetuned Language Models Are Zero-Shot Learners","venue":"International Conference on Learning Representations","year":2021,"referenceCount":167,"citationCount":326,"influentialCitationCount":74,"publicationDate":"03/09/2021","authors":"Jason Wei,Maarten Bosma,Vincent Zhao,Kelvin Guu,A. Yu,Brian Lester,Nan Du,Andrew M. Dai,Quoc V. Le","id":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","summary":"It is shown that instruction tuning —ﬁnetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks and outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze.","score":1},{"url":"https://www.semanticscholar.org/paper/77d956cdab4508d569ae5741549b78e715fd0749","title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":70,"citationCount":69,"influentialCitationCount":12,"publicationDate":"08/09/2021","authors":"Stephanie C. Lin,Jacob Hilton,Owain Evans","id":"77d956cdab4508d569ae5741549b78e715fd0749","summary":"It is suggested that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.","score":1},{"url":"https://www.semanticscholar.org/paper/2672777d25562c9df6fc13b653181db62d39bece","title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA","venue":"AAAI Conference on Artificial Intelligence","year":2021,"referenceCount":42,"citationCount":54,"influentialCitationCount":20,"publicationDate":"10/09/2021","authors":"Zhengyuan Yang,Zhe Gan,Jianfeng Wang,Xiaowei Hu,Yumao Lu,Zicheng Liu,Lijuan Wang","id":"2672777d25562c9df6fc13b653181db62d39bece","summary":"This work proposes PICa, a simple yet effective method that Prompts GPT3 via the use of Image Captions, for knowledge-based VQA, and treats GPT-3 as an implicit and unstructured KB that can jointly acquire and process relevant knowledge.","score":1},{"url":"https://www.semanticscholar.org/paper/c6bb04f3d8000b7e800f6359082de39548c7da79","title":"Capturing Structural Locality in Non-parametric Language Models","venue":"International Conference on Learning Representations","year":2021,"referenceCount":47,"citationCount":5,"influentialCitationCount":0,"publicationDate":"06/10/2021","authors":"Frank F. Xu,Junxian He,Graham Neubig,V. Hellendoorn","id":"c6bb04f3d8000b7e800f6359082de39548c7da79","summary":"This paper proposes a simple yet effective approach for adding locality information into non-parametric language models by adding learned parameters that improve the likelihood of retrieving examples from local neighborhoods.","score":1},{"url":"https://www.semanticscholar.org/paper/a421ba0a9150cd35e231dddc323bdd9a59b3af93","title":"Coherence boosting: When your pretrained language model is not paying enough attention","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":64,"citationCount":4,"influentialCitationCount":1,"publicationDate":"15/10/2021","authors":"Nikolay Malkin,Zhen Wang,N. Jojic","id":"a421ba0a9150cd35e231dddc323bdd9a59b3af93","summary":"It is found that coherence boosting with state-of-the-art models for various zero-shot NLP tasks yields performance gains with no additional training.","score":1},{"url":"https://www.semanticscholar.org/paper/1cbb3d96242c3f47c3f40aada33616d0f5c07737","title":"Inductive Biases and Variable Creation in Self-Attention Mechanisms","venue":"International Conference on Machine Learning","year":2021,"referenceCount":74,"citationCount":11,"influentialCitationCount":3,"publicationDate":"19/10/2021","authors":"Benjamin Edelman,Surbhi Goel,S. Kakade,Cyril Zhang","id":"1cbb3d96242c3f47c3f40aada33616d0f5c07737","summary":"The main result shows that bounded-norm Transformer networks “cre-ate sparse variables”: a single self-attention head can represent a sparse function of the input sequence, with sample complexity scaling only logarithmically with the context length.","score":1},{"url":"https://www.semanticscholar.org/paper/231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models","venue":"NAACL-HLT","year":2021,"referenceCount":38,"citationCount":13,"influentialCitationCount":1,"publicationDate":"26/10/2021","authors":"Piotr Nawrot,Szymon Tworkowski,Michal Tyrolski,Lukasz Kaiser,Yuhuai Wu,Christian Szegedy,H. Michalewski","id":"231e768f0cd280faa0f725bb353262cb4fed08d1","summary":"Hourglass is a hierarchical Transformer language model that sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efﬁciency on the widely studied enwik8 benchmark.","score":1},{"url":"https://www.semanticscholar.org/paper/8417d83e74a63b9d3874cd7695477ebad8b98f8a","title":"Choose your programming copilot: a comparison of the program synthesis performance of github copilot and genetic programming","venue":"Annual Conference on Genetic and Evolutionary Computation","year":2021,"referenceCount":38,"citationCount":14,"influentialCitationCount":0,"publicationDate":"15/11/2021","authors":"D. Sobania,Martin Briesch,Franz Rothlauf","id":"8417d83e74a63b9d3874cd7695477ebad8b98f8a","summary":"It is found that the performance of the two approaches on the benchmark problems is quite similar, however, in comparison to GitHub Copilot, the program synthesis approaches based on genetic programming are not yet mature enough to support programmers in practical software development.","score":1},{"url":"https://www.semanticscholar.org/paper/ecb5a6fe2f5261e4e717ece1e82c464c63cb4862","title":"Controlling Conditional Language Models without Catastrophic Forgetting","venue":"ICML","year":2021,"referenceCount":37,"citationCount":6,"influentialCitationCount":0,"publicationDate":"01/12/2021","authors":"Tomasz Korbak,Hady ElSahar,Germán Kruszewski,Marc Dymetman","id":"ecb5a6fe2f5261e4e717ece1e82c464c63cb4862","summary":"DPG is extended to conditional tasks by proposing Conditional DPG (CDPG), and results show thatne-tuning using CDPG robustly moves these pretrained models closer towards meeting control objectives and — in contrast with baseline approaches — does not result in catastrophic forgetting.","score":1},{"url":"https://www.semanticscholar.org/paper/d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis","venue":"International Conference on Software Engineering","year":2021,"referenceCount":42,"citationCount":21,"influentialCitationCount":1,"publicationDate":"06/12/2021","authors":"Naman Jain,Skanda Vaidyanath,Arun Shankar Iyer,Nagarajan Natarajan,Suresh Parthasarathy,S. Rajamani,Rahul Sharma","id":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","summary":"This paper presents an approach to augment these large language models with post-processing steps based on program analysis and synthesis techniques, that understand the syntax and semantics of programs, and presents the experiences from building and evaluating such a tool Jigsaw.","score":1},{"url":"https://www.semanticscholar.org/paper/6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a","title":"Few-Shot Semantic Parsing with Language Models Trained on Code","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":18,"citationCount":16,"influentialCitationCount":3,"publicationDate":"16/12/2021","authors":"Richard Shin,Benjamin Van Durme","id":"6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a","summary":"This paper evaluates OpenAI Codex on Overnight and SMCalFlow and finds that unlike GPT-3, Codex performs similarly when targeting meaning representations directly, perhaps because meaning representations are structured similar to code in these datasets.","score":1}]}