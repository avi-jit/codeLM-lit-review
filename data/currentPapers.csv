"id","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount","url"
"270b015093073d3ba254928b6d736a59870d3fb1","Chatbots As Fluent Polyglots: Revisiting Breakthrough Code Snippets","The research applies AI-driven code assistants to analyze a selection of influential computer code that has shaped modern technology, including email, internet browsing, robotics, and malicious software to provide insights into obfuscated code or software lacking explanatory commentary.","ArXiv",2023,"David Noever,Kevin Williams",0,19,0,"https://www.semanticscholar.org/paper/270b015093073d3ba254928b6d736a59870d3fb1"
"bae76e1d13abe54f66dc140be53538b864578ba8","A Survey on Pretrained Language Models for Neural Code Intelligence","This paper presents a comprehensive survey of the NCI domain, including a thorough review of pretraining techniques, tasks, datasets, and model architectures, and hopes it will serve as a bridge between the natural language and programming language communities.","ArXiv",2022,"Yichen Xu,Yanqiao Zhu",0,77,0,"https://www.semanticscholar.org/paper/bae76e1d13abe54f66dc140be53538b864578ba8"
"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","Evaluating Large Language Models Trained on Code","It is found that repeated sampling from the GPT language model is a surprisingly effective strategy for producing working solutions to difﬁcult prompts, and the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics are discussed.","ArXiv",2021,"Mark Chen,Jerry Tworek,Heewoo Jun,Qiming Yuan,Henrique Ponde,Jared Kaplan,Harrison Edwards,Yura Burda,Nicholas Joseph,Greg Brockman,Alex Ray,Raul Puri,Gretchen Krueger,Michael Petrov,Heidy Khlaaf,Girish Sastry,Pamela Mishkin,Brooke Chan,Scott Gray,Nick Ryder,Mikhail Pavlov,Alethea Power,Lukasz Kaiser,Mohammad Bavarian,Clemens Winter,Philippe Tillet,F. Such,D. Cummings,Matthias Plappert,Fotios Chantzis,Elizabeth Barnes,Ariel Herbert-Voss,William H. Guss,Alex Nichol,I. Babuschkin,S. Balaji,Shantanu Jain,A. Carr,J. Leike,Joshua Achiam,Vedant Misra,Evan Morikawa,Alec Radford,M. Knight,Miles Brundage,Mira Murati,Katie Mayer,P. Welinder,Bob McGrew,Dario Amodei,Sam McCandlish,Ilya Sutskever,Wojciech Zaremba",484,119,139,"https://www.semanticscholar.org/paper/acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269"
"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","A systematic evaluation of large language models of code","This work finds that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language modeling, and identifies an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code.","MAPS@PLDI",2022,"Frank F. Xu,Uri Alon,Graham Neubig,V. Hellendoorn",47,37,8,"https://www.semanticscholar.org/paper/b32a6f6ef7dd775e0f876b4713ceccebc56e651e"
"7ffb212356df9980347b3d3b9910dfba75a5d0c7","Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code","This paper studies to what extent a state-of-the-art, pre-trained language model of code, Codex, may serve this purpose, and concludes that few-shot language models are surprisingly effective.","ArXiv",2022,"Patrick Bareiss,Beatriz Souza,Marcelo d’Amorim,Michael Pradel",6,54,0,"https://www.semanticscholar.org/paper/7ffb212356df9980347b3d3b9910dfba75a5d0c7"
"8210cef990b8e5cddbc95000e46309bdd25337f7","Asking Clarification Questions for Code Generation in General-Purpose Programming Language","The empirical results support the hypothesis that clariﬁcations result in more precise generated code, as shown by an improve-ment of 17.52 in BLEU, 12.72 in CodeBLEU, and 7.7% in the exact match.","ArXiv",2022,"Haau-Sing Li,Mohsen Mesgar,André F. T. Martins,Iryna Gurevych",0,43,0,"https://www.semanticscholar.org/paper/8210cef990b8e5cddbc95000e46309bdd25337f7"
"9bfda45ef365466cba27e47a9fdb98d15d78aa15","Repository-Level Prompt Generation for Large Language Models of Code","This work proposes a framework called Repo-Level Prompt Generator that learns to generate example-speciﬁc prompts using prompt proposals, and demonstrates that an oracle constructed from these prompt proposals gives a remarkably high relative improvement over Codex, showing the quality of these proposals.","ArXiv",2022,"Disha Shrivastava,H. Larochelle,Daniel Tarlow",3,54,0,"https://www.semanticscholar.org/paper/9bfda45ef365466cba27e47a9fdb98d15d78aa15"
"b3a54332a0791751fcf234f6264f242c42ac00d2","DocPrompting: Generating Code by Retrieving the Docs","DocPrompting is a natural-language-to-code generation approach that explicitly leverages code documentation by retrieving the relevant documentation pieces given a natural language (NL) intent, and generating code based on the NL intent and the retrieved documentation.","",2022,"Shuyan Zhou,Uri Alon,Frank F. Xu,Zhiruo Wang,Zhengbao Jiang,Graham Neubig",1,38,0,"https://www.semanticscholar.org/paper/b3a54332a0791751fcf234f6264f242c42ac00d2"
"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","Measuring Coding Challenge Competence With APPS","APPS is introduced, a benchmark for code generation that measures the ability of models to take an arbitrary natural language speciﬁcation and generate satisfactory Python code and shows that machine learning models are now beginning to learn how to code.","NeurIPS Datasets and Benchmarks",2021,"Dan Hendrycks,Steven Basart,Saurav Kadavath,Mantas Mazeika,Akul Arora,Ethan Guo,Collin Burns,Samir Puranik,Horace He,D. Song,J. Steinhardt",74,46,20,"https://www.semanticscholar.org/paper/1ccd031f28dccfb226f6c0c588c93a97a50bf95f"