"id","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount","url"
"270b015093073d3ba254928b6d736a59870d3fb1","Chatbots As Fluent Polyglots: Revisiting Breakthrough Code Snippets","The research applies AI-driven code assistants to analyze a selection of influential computer code that has shaped modern technology, including email, internet browsing, robotics, and malicious software to provide insights into obfuscated code or software lacking explanatory commentary.","ArXiv",2023,"David Noever,Kevin Williams",2,19,0,"https://www.semanticscholar.org/paper/270b015093073d3ba254928b6d736a59870d3fb1"
"bae76e1d13abe54f66dc140be53538b864578ba8","A Survey on Pretrained Language Models for Neural Code Intelligence","This paper presents a comprehensive survey of the NCI domain, including a thorough review of pretraining techniques, tasks, datasets, and model architectures, and hopes it will serve as a bridge between the natural language and programming language communities.","ArXiv",2022,"Yichen Xu,Yanqiao Zhu",5,77,0,"https://www.semanticscholar.org/paper/bae76e1d13abe54f66dc140be53538b864578ba8"
"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","A systematic evaluation of large language models of code","This work finds that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language modeling, and identifies an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code.","MAPS@PLDI",2022,"Frank F. Xu,Uri Alon,Graham Neubig,V. Hellendoorn",136,37,8,"https://www.semanticscholar.org/paper/b32a6f6ef7dd775e0f876b4713ceccebc56e651e"
"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","Evaluating Large Language Models Trained on Code","It is found that repeated sampling from the GPT language model is a surprisingly effective strategy for producing working solutions to difﬁcult prompts, and the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics are discussed.","ArXiv",2021,"Mark Chen,Jerry Tworek,Heewoo Jun,Qiming Yuan,Henrique Ponde,Jared Kaplan,Harrison Edwards,Yura Burda,Nicholas Joseph,Greg Brockman,Alex Ray,Raul Puri,Gretchen Krueger,Michael Petrov,Heidy Khlaaf,Girish Sastry,Pamela Mishkin,Brooke Chan,Scott Gray,Nick Ryder,Mikhail Pavlov,Alethea Power,Lukasz Kaiser,Mohammad Bavarian,Clemens Winter,Philippe Tillet,F. Such,D. Cummings,Matthias Plappert,Fotios Chantzis,Elizabeth Barnes,Ariel Herbert-Voss,William H. Guss,Alex Nichol,I. Babuschkin,S. Balaji,Shantanu Jain,A. Carr,J. Leike,Joshua Achiam,Vedant Misra,Evan Morikawa,Alec Radford,M. Knight,Miles Brundage,Mira Murati,Katie Mayer,P. Welinder,Bob McGrew,Dario Amodei,Sam McCandlish,Ilya Sutskever,Wojciech Zaremba",1135,114,139,"https://www.semanticscholar.org/paper/acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269"
"7ffb212356df9980347b3d3b9910dfba75a5d0c7","Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code","This paper studies to what extent a state-of-the-art, pre-trained language model of code, Codex, may serve this purpose, and concludes that few-shot language models are surprisingly effective.","ArXiv",2022,"Patrick Bareiss,Beatriz Souza,Marcelo d’Amorim,Michael Pradel",23,54,0,"https://www.semanticscholar.org/paper/7ffb212356df9980347b3d3b9910dfba75a5d0c7"
"8210cef990b8e5cddbc95000e46309bdd25337f7","Asking Clarification Questions for Code Generation in General-Purpose Programming Language","The empirical results support the hypothesis that clariﬁcations result in more precise generated code, as shown by an improve-ment of 17.52 in BLEU, 12.72 in CodeBLEU, and 7.7% in the exact match.","ArXiv",2022,"Haau-Sing Li,Mohsen Mesgar,André F. T. Martins,Iryna Gurevych",2,43,0,"https://www.semanticscholar.org/paper/8210cef990b8e5cddbc95000e46309bdd25337f7"
"9bfda45ef365466cba27e47a9fdb98d15d78aa15","Repository-Level Prompt Generation for Large Language Models of Code","This work proposes a framework called Repo-Level Prompt Generator that learns to generate example-speciﬁc prompts using prompt proposals, and demonstrates that an oracle constructed from these prompt proposals gives a remarkably high relative improvement over Codex, showing the quality of these proposals.","ArXiv",2022,"Disha Shrivastava,H. Larochelle,Daniel Tarlow",14,65,0,"https://www.semanticscholar.org/paper/9bfda45ef365466cba27e47a9fdb98d15d78aa15"
"b3a54332a0791751fcf234f6264f242c42ac00d2","DocPrompting: Generating Code by Retrieving the Docs","DocPrompting is a natural-language-to-code generation approach that explicitly leverages code documentation by retrieving the relevant documentation pieces given a natural language (NL) intent, and generating code based on the NL intent and the retrieved documentation.","",2022,"Shuyan Zhou,Uri Alon,Frank F. Xu,Zhiruo Wang,Zhengbao Jiang,Graham Neubig",17,46,0,"https://www.semanticscholar.org/paper/b3a54332a0791751fcf234f6264f242c42ac00d2"
"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","Measuring Coding Challenge Competence With APPS","APPS is introduced, a benchmark for code generation that measures the ability of models to take an arbitrary natural language speciﬁcation and generate satisfactory Python code and shows that machine learning models are now beginning to learn how to code.","NeurIPS Datasets and Benchmarks",2021,"Dan Hendrycks,Steven Basart,Saurav Kadavath,Mantas Mazeika,Akul Arora,Ethan Guo,Collin Burns,Samir Puranik,Horace He,D. Song,J. Steinhardt",136,49,20,"https://www.semanticscholar.org/paper/1ccd031f28dccfb226f6c0c588c93a97a50bf95f"
"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation","This paper investigates how to leverage an unlabelled code corpus to train a model for library-oriented code generation, and observes that library- oriented code snippets are more likely to share similar code sketches.","International Joint Conference on Artificial Intelligence",2022,"Daoguang Zan,Bei Chen,Dejian Yang,Zeqi Lin,Minsu Kim,Bei Guan,Yongji Wang,Weizhu Chen,Jian-Guang Lou",17,27,0,"https://www.semanticscholar.org/paper/a08a3b08a5a1de6462a7da2906b1cd81691d6c18"
"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","When Neural Model Meets NL2Code: A Survey","This survey focuses on how does neural network (NN) solves NL2Code and proposes a comprehensive framework, which is able to cover all studies in this task, and in-depth parse the existing studies into this framework.","ArXiv",2022,"Daoguang Zan,Bei Chen,Fengji Zhang,Di Lu,Bingchao Wu,Bei Guan,Yongji Wang,Jian-Guang Lou",2,156,0,"https://www.semanticscholar.org/paper/9e3b52669d81dbf0d638a5f5d1d537c7087195d6"
"876eb375cb7b365475040046df669c039ad54202","CodeT: Code Generation with Generated Tests","A novel method, C ODE T, leverages the same pre-trained language models to test cases for the code samples, thus reducing the human effort and increasing the coverage of the test scenarios, achieving remarkable and consistent gains across different models and benchmarks.","ArXiv",2022,"Bei Chen,Fengji Zhang,A. Nguyen,Daoguang Zan,Zeqi Lin,Jian-Guang Lou,Weizhu Chen",60,41,3,"https://www.semanticscholar.org/paper/876eb375cb7b365475040046df669c039ad54202"
"780f7eebde16b1ae5843df3a79a7772899ef6a71","MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation","This work creates the first massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages and evaluates the multi-language performance of three state-of-the-art code generation models.","",2022,"Federico Cassano,John Gouwar,Daniel Nguyen,S. Nguyen,Luna Phipps-Costin,Donald Pinckney,Ming-Ho Yee,Yangtian Zi,Carolyn Jane Anderson,Molly Q. Feldman,Arjun Guha,M. Greenberg,Abhinav Jangda",3,42,0,"https://www.semanticscholar.org/paper/780f7eebde16b1ae5843df3a79a7772899ef6a71"
"0b340dd78fd04bbde2807d5efedb796d319355e3","Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?","Investigation of the various input parameters of two language models shows that varying the input parameters can improve the performance of language models, but there is a tight dependency when varying the temperature, the prompt and the number of generated solutions, making potentially hard to properly control the parameters to obtain an optimal result.","ArXiv",2022,"Jean-Baptiste Döderlein,M. Acher,D. Khelladi,B. Combemale",5,46,0,"https://www.semanticscholar.org/paper/0b340dd78fd04bbde2807d5efedb796d319355e3"
"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation","This work introduces DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas, and proactively defends against memorization by slightly modifying the problems to be different from the original StackOverﬂow source.","ArXiv",2022,"Yuhang Lai,Chengxi Li,Yiming Wang,Tianyi Zhang,Ruiqi Zhong,Luke Zettlemoyer,S. Yih,Daniel Fried,Si-yi Wang,Tao Yu",23,30,2,"https://www.semanticscholar.org/paper/8a4fc5f00cd4aca61e148e46a2125c3a406719f1"
"cba98048f3e85a974c287b271692bf6c197db940","Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers","A simple hybrid metric is proposed, which combines functional correctness and similarity- based metrics to capture different dimensions of what programmers might value and shows that this hybrid metric more accurately captures effort.","ArXiv",2022,"Victor C. Dibia,Adam Fourney,Gagan Bansal,Forough Poursabzi-Sangdeh,Han Liu,Saleema Amershi",4,32,0,"https://www.semanticscholar.org/paper/cba98048f3e85a974c287b271692bf6c197db940"
"815c6ca281536d18ec0eb408b6e46e72a0826163","Natural Language to Code Generation in Interactive Data Science Notebooks","P A C H - I NC O, a 62B code language model for Python computational notebooks, which outperforms public code LMs and explores few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions.","ArXiv",2022,"Pengcheng Yin,Wen-Ding Li,Kefan Xiao,A. Rao,Yeming Wen,Kensen Shi,Joshua Howland,Paige Bailey,Michele Catasta,H. Michalewski,Alex Polozov,Charles Sutton",8,78,0,"https://www.semanticscholar.org/paper/815c6ca281536d18ec0eb408b6e46e72a0826163"
"433def684b5a9de5a9163f50b9004a44a11128b1","CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context","A framework that incorporates cross-file context to learn the in-file and cross- file context jointly on top of pretrained code LMs is proposed, COCOMIC, which successfully improves the existing code LM with a 19.30% relative increase in exact match and a 15.41%relative increase in identifier matching for code completion when the cross-line context is provided.","ArXiv",2022,"Yangruibo Ding,Zijian Wang,Wasi Uddin Ahmad,M. Ramanathan,Ramesh Nallapati,Parminder Bhatia,D. Roth,Bing Xiang",6,44,0,"https://www.semanticscholar.org/paper/433def684b5a9de5a9163f50b9004a44a11128b1"
"20e4ca6368a0731d3cbbd2e68aa90cef7384dc72","Multi-lingual Evaluation of Code Generation Models","This work presents MBXP, an execution-based code completion benchmark in 10+ programming languages that is able to evaluate code generation models in a multi-lingual fashion, and discovers generalization ability of language models on out-of-domain languages, advantages of large multi-lingsual models over mono-lingUAL, benefits of few-shot prompting, and zero-shot translation abilities.","ArXiv",2022,"Ben Athiwaratkun,Sanjay Krishna Gouda,Zijian Wang,Xiaopeng Li,Yuchen Tian,Ming Tan,Wasi Uddin Ahmad,Shiqi Wang,Qing Sun,Mingyue Shang,Sujan Kumar Gonugondla,Hantian Ding,Varun Kumar,Nathan Fulton,A. Farahani,Siddharth Jain,Robert Giaquinto,Haifeng Qian,M. Ramanathan,Ramesh Nallapati,Baishakhi Ray,Parminder Bhatia,Sudipta Sengupta,D. Roth,Bing Xiang",29,73,2,"https://www.semanticscholar.org/paper/20e4ca6368a0731d3cbbd2e68aa90cef7384dc72"
"7547680408358916e66917d03436fca7540a7528","Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks","Project CodeNet is a first-of-its-kind, very large scale, diverse, and high-quality dataset to accelerate the algorithmic advancements in AI for Code, which consists of 14M code samples and about 500M lines of code in 55 different programming languages.","NeurIPS Datasets and Benchmarks",2021,"Ruchi Puri,David S. Kung,G. Janssen,Wei Zhang,Giacomo Domeniconi,Vladmir Zolotov,Julian Dolby,Jie Chen,M. Choudhury,Lindsey Decker,Veronika Thost,Luca Buratti,Saurabh Pujar,Ulrich Finkler",125,45,21,"https://www.semanticscholar.org/paper/7547680408358916e66917d03436fca7540a7528"
"a38e0f993e4805ba8a9beae4c275c91ffcec01df","Program Synthesis with Large Language Models","The limits of the current generation of large language models for program synthesis in general purpose programming languages are explored, finding that even the best models are generally unable to predict the output of a program given a speciﬁc input.","ArXiv",2021,"Jacob Austin,Augustus Odena,Maxwell Nye,Maarten Bosma,H. Michalewski,David Dohan,Ellen Jiang,Carrie J. Cai,Michael Terry,Quoc V. Le,Charles Sutton",291,102,35,"https://www.semanticscholar.org/paper/a38e0f993e4805ba8a9beae4c275c91ffcec01df"
"a3564f3cf954c05844c757505325a50b4d858e22","Generating Infrastructure-as-Code from Natural Language: Evaluating Fine-Tuned GPT-3 Models for Cloud Infrastructure Provisioning","A transformer-based model to generate Infrastructure-as-Code from natural language is introduced, which allows both technical and nontechnical users to dynamically generate IaC artifacts, enabling them to request and receive cloud resources using conversational interfaces such as chat bots, SMS, etc.","",2022,"",0,8,0,"https://www.semanticscholar.org/paper/a3564f3cf954c05844c757505325a50b4d858e22"
"55ad5e818cfed72317576027fb33a9609210d592","Training and Evaluating a Jupyter Notebook Data Science Assistant","This work studies the feasibility of a Data Science assistant powered by a sequence-to-sequence transformer by training a new model JuPyT5 on all publicly available Jupyter Notebook GitHub repositories and developing a new metric: Data Science Problems (DSP).","ArXiv",2022,"Shubham Chandel,Colin B. Clement,Guillermo Serrato,Neel Sundaresan",12,22,2,"https://www.semanticscholar.org/paper/55ad5e818cfed72317576027fb33a9609210d592"
"5cbe278b65a81602a864184bbca37de91448a5f5","Competition-level code generation with AlphaCode","AlphaCode is introduced, a system for code generation that achieved an average ranking in the top 54.3% in simulated evaluations on recent programming competitions on the Codeforces platform, marking the first time an artificial intelligence system has performed competitively in programming competitions.","Science",2022,"Yujia Li,David H. Choi,Junyoung Chung,Nate Kushman,Julian Schrittwieser,Rémi Leblond,Tom,Eccles,James Keeling,Felix Gimeno,Agustin Dal Lago,T. Hubert,Peter Choy,Cyprien de,Masson d’Autume,I. Babuschkin,Xinyun Chen,Po-Sen Huang,Johannes Welbl,Sven Gowal,Alexey,Cherepanov,James Molloy,D. Mankowitz,Esme Sutherland Robson,Pushmeet Kohli,Nando de,Freitas,K. Kavukcuoglu,Oriol Vinyals",337,82,28,"https://www.semanticscholar.org/paper/5cbe278b65a81602a864184bbca37de91448a5f5"
"7b46b9da287429d19a00ca3f9219c1020f7c9df8","A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective","This survey paper overviews major deep learning methods used in Natural Language Processing (NLP) and source code over the last 35 years and presents a software-engineering centered taxonomy for CI placing each of the works into one category describing how it best assists the software development cycle.","ArXiv",2022,"Erfan Al-Hossami,Samira Shaikh",4,265,0,"https://www.semanticscholar.org/paper/7b46b9da287429d19a00ca3f9219c1020f7c9df8"
"76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","Investigating Explainability of Generative AI for Code through Scenario-based Design","This work explores explainability needs for GenAI for code and demonstrates how human-centered approaches can drive the technical development of XAI in novel domains.","International Conference on Intelligent User Interfaces",2022,"Jiao Sun,Q. Liao,Michael J. Muller,Mayank Agarwal,Stephanie Houde,Kartik Talamadupula,Justin D. Weisz",32,109,0,"https://www.semanticscholar.org/paper/76ebf56d6ebb833d8c8c4124f7b2f15771a4997c"
"590f6817b42407f96b079e82c935fae298196359","Less is More: Summary of Long Instructions is Better for Program Synthesis","A meta-dataset consisting of human and synthesized summaries of the long and complicated programming questions shows that summaries improve performance for introductory and interview programming questions and shows improvement by a small margin for competitive programming questions, implying scope for future research in this direction.","ArXiv",2022,"Kirby Kuznia,Swaroop Mishra,Mihir Parmar,Chitta Baral",12,28,0,"https://www.semanticscholar.org/paper/590f6817b42407f96b079e82c935fae298196359"
"1a903282f7c19dbdb2714b852fb42dbb4675422b","CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis","The utility of the trained model is shown by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval, and experiments show that the multi-step program synthesis capacity scales as a function of the model size and data size.","",2022,"Erik Nijkamp,Bo Pang,Hiroaki Hayashi,Lifu Tu,Haiquan Wang,Yingbo Zhou,S. Savarese,Caiming Xiong",127,48,2,"https://www.semanticscholar.org/paper/1a903282f7c19dbdb2714b852fb42dbb4675422b"
"47e15941c8b157873c8264e4bf50318d1ba5cd18","Natural Language to Code Translation with Execution","This work introduces execution result– based minimum Bayes risk decoding (MBR-EXEC) for program selection and shows that it improves the few-shot performance of pretrained code models on natural-language-to-code tasks, suggesting it as an effective approach for natural language to code translation.","ArXiv",2022,"Freda Shi,Daniel Fried,Marjan Ghazvininejad,Luke Zettlemoyer,Sida I. Wang",34,47,4,"https://www.semanticscholar.org/paper/47e15941c8b157873c8264e4bf50318d1ba5cd18"
"a8fc183c089bd596ccc48b3d666f8814e1b41e55","InCoder: A Generative Model for Code Infilling and Synthesis","INCODER is introduced, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling) and the ability to condition on bidirectional context substantially improves performance on challenging tasks such as type inference, comment generation, and variable re-naming.","ArXiv",2022,"Daniel Fried,Armen Aghajanyan,Jessy Lin,Sida I. Wang,Eric Wallace,Freda Shi,Ruiqi Zhong,Wen-tau Yih,Luke Zettlemoyer,M. Lewis",157,81,17,"https://www.semanticscholar.org/paper/a8fc183c089bd596ccc48b3d666f8814e1b41e55"
"6074c7b75f27ca9adb6d74b080c07d5d079c3ea0","Improving automatically generated code from Codex via Automated Program Repair","This study systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests, revealing that automatically generated codes share some common programming mistakes with human-crafted solutions, indicating existing APR tools have the potential to fix auto-generated code.","ArXiv",2022,"Zhiyu Fan,Xiang Gao,Abhik Roychoudhury,Shin Hwei Tan",13,45,0,"https://www.semanticscholar.org/paper/6074c7b75f27ca9adb6d74b080c07d5d079c3ea0"
"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning","This work proposes “CodeRL”, a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL), and treats the code-generating LM as an actor network, and introduces a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor.","ArXiv",2022,"Hung Le,Yue Wang,Akhilesh Deepak Gotmare,S. Savarese,S. Hoi",55,77,1,"https://www.semanticscholar.org/paper/6d994b4f5a46cd14e8f09f1e9e49120546b15e31"
"63c670ba8018da0a7e33c34ffd84c2a3ca54b894","Language Models Can Teach Themselves to Program Better","This work shows how generating synthetic programming puzzles and solutions, veriﬁed for correctness by a Python interpreter, can be used to improve performance in solving test puzzles from P3, a public benchmark set of Python Programming Puzzles.","ArXiv",2022,"Patrick M. Haluptzok,Matthew Bowers,A. Kalai",9,47,1,"https://www.semanticscholar.org/paper/63c670ba8018da0a7e33c34ffd84c2a3ca54b894"
"b31b21d0750e849badfe76000e8170482f32b9be","DocCoder: Generating Code by Retrieving and Reading Docs","DocCoder is introduced : an approach that explicitly leverages code manuals and documentation by retrieving the relevant documentation given the natural language intent, and generating the code based on the NL intent and the retrieved documentation.","ArXiv",2022,"Shuyan Zhou,Uri Alon,Frank F. Xu,Zhengbao Jiang,Graham Neubig",7,33,1,"https://www.semanticscholar.org/paper/b31b21d0750e849badfe76000e8170482f32b9be"
"453a8fac3be9282be53908f0735160d0d21e0f48","Repair Is Nearly Generation: Multilingual Program Repair with LLMs","This work introduces RING, a multilingual repair engine powered by a large language model trained on code (LLMC) such as Codex that enables a ﬂipped model for programming assistance, one where the programmer writes code and the AI assistance suggests code, compared to traditional code suggestion technology.","ArXiv",2022,"Harshit Joshi,J. Cambronero,Sumit Gulwani,Vu Le,Ivan Radicek,Gust Verbruggen",17,48,0,"https://www.semanticscholar.org/paper/453a8fac3be9282be53908f0735160d0d21e0f48"
"41f5e1ad7793593befc0b9c38f756836e8b07c98","Code4ML: a Large-scale Dataset of annotated Machine Learning Code","The Code4ML corpus, which contains code snippets, task summaries, competitions and dataset descriptions publicly available from Kaggle, can potentially help address a number of software engineering or data science challenges through a data-driven approach.","ArXiv",2022,"Anastasia Drozdova,P. Guseva,E. Trofimova,Anna Scherbakova,Andrey Ustyuzhanin",0,35,0,"https://www.semanticscholar.org/paper/41f5e1ad7793593befc0b9c38f756836e8b07c98"
"4fbe0cb0777b228e39243692bf29e2829060b8de","When Language Model Meets Private Library","This paper investigates how to equip pre-trained language models with the ability of code generation for private libraries, and proposes a novel framework with two modules: the APIRetriever and the APICoder, which generates code using these APIs.","ArXiv",2022,"Daoguang Zan,Bei Chen,Zeqi Lin,Bei Guan,Yongji Wang,Jian-Guang Lou",8,40,0,"https://www.semanticscholar.org/paper/4fbe0cb0777b228e39243692bf29e2829060b8de"
"ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","Evaluating How Fine-tuning on Bimodal Data Effects Code Generation","It is found that at higher temperatures, there are decreases to the model’s ability to generate runnable programs despite higher pass @ k scores, underscoring the need for better methods of incorporating such data that mitigate these side effects.","ArXiv",2022,"Gabriel Orlanski,Seonhye Yang,Michael Healy",2,31,0,"https://www.semanticscholar.org/paper/ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4"
"20fae749e3d469c331731ffa2f811079db792fdc","A Simple, Yet Effective Approach to Finding Biases in Code Generation","This work shows that current code generation systems exhibit biases inherited from large language model backbones, which might leak into generated code under specific circumstances, and proposes a framework that automatically removes hints and exposes various biases that these code generation models use.","ArXiv",2022,"Spyridon Mouselinos,Mateusz Malinowski,H. Michalewski",3,48,0,"https://www.semanticscholar.org/paper/20fae749e3d469c331731ffa2f811079db792fdc"
"e402dd77eba504ea93bc38e2a052398bb95db351","Execution-based Evaluation for Data Science Code Generation Models","ExeDS is introduced, an evaluation dataset for execution evaluation for data science code generation tasks that contains a set of 534 problems from Jupyter Notebooks, each consisting of code context, task description, reference program, and the desired execution output.","ArXiv",2022,"Junjie Huang,Chenglong Wang,Jipeng Zhang,Cong Yan,Haotian Cui,J. Inala,Colin B. Clement,Nan Duan,Jianfeng Gao",7,36,2,"https://www.semanticscholar.org/paper/e402dd77eba504ea93bc38e2a052398bb95db351"
"20b60fb3993d2e9a5af04611f7bdf248e5a3a736","Programming by Example and Text-to-Code Translation for Conversational Code Generation","Modular Programs for Text-guided Hierarchical Synthesis (MPaTHS), a method for integrating Programming by Example and text-to-code systems which uses an accessible natural language interface for synthesizing general programs, is proposed.","ArXiv",2022,"Eli Whitehouse,William Gerard,Yauhen Klimovich,Marc Franco-Salvador",0,21,0,"https://www.semanticscholar.org/paper/20b60fb3993d2e9a5af04611f7bdf248e5a3a736"
"27961ae80ad008bd4006704b1b8fa82664137d69","Coder Reviewer Reranking for Code Generation","Experimental results show that Coder-Reviewer reranking leads to consistent and signiﬁcant improvement (up to 17 % absolute accuracy gain) over reranking with the Coder model only, and when combined with executability ﬁltering,Coder- reviewer reranking can often outperform the minimum Bayes risk method.","ArXiv",2022,"Tianyi Zhang,Tao Yu,Tatsunori Hashimoto,M. Lewis,Wen-tau Yih,Daniel Fried,Sida I. Wang",17,27,0,"https://www.semanticscholar.org/paper/27961ae80ad008bd4006704b1b8fa82664137d69"
"f3a6115e5fb2237df938976e005468f0b18da797","The Stack: 3 TB of permissively licensed source code","This work introduces The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages, and describes how to collect the full dataset, construct a permissically licensed subset, and present a data governance plan.","ArXiv",2022,"Denis Kocetkov,Raymond Li,Loubna Ben Allal,Jia Li,Chenghao Mou,Carlos Muñoz Ferrandis,Yacine Jernite,Margaret Mitchell,Sean Hughes,Thomas Wolf,Dzmitry Bahdanau,Leandro von Werra,Harm de Vries",23,48,0,"https://www.semanticscholar.org/paper/f3a6115e5fb2237df938976e005468f0b18da797"
"9b4055674cd9849f8595240695bed69cd02492bc","A Survey on Natural Language Processing for Programming","This paper comprehensively investigates existing work in natural language processing for programming, rang-ing from early deductive models to the latest competition-level models.","ArXiv",2022,"Qingfu Zhu,Xianzhen Luo,Fang Liu,Cuiyun Gao,Wanxiang Che",0,79,0,"https://www.semanticscholar.org/paper/9b4055674cd9849f8595240695bed69cd02492bc"
"ab8a53be2ff8a756776aa0245ad8da41189d60d6","Fuzzing Deep-Learning Libraries via Large Language Models","LLMFuzz is the first automated approach to directly leveraging Large Pre-trained Language Models (LLMs) to generate input programs for fuzzing DL libraries, and is able to detect 65 bugs, with 41 already confirmed as previously unknown bugs.","ArXiv",2022,"Yinlin Deng,Chun Xia,Haoran Peng,Chenyuan Yang,Lingming Zhang",9,79,0,"https://www.semanticscholar.org/paper/ab8a53be2ff8a756776aa0245ad8da41189d60d6"
"690c210564226c9307b3bab977cdc07a6a45863a","Execution-Based Evaluation for Open-Domain Code Generation","ODEX corroborates the mer-its of execution-based evaluation over metrics without execution but also unveils their complementary effects, and is released to facilitate research into open-domain problems for the code generation community.","ArXiv",2022,"Zhiruo Wang,Shuyan Zhou,Daniel Fried,Graham Neubig",6,35,0,"https://www.semanticscholar.org/paper/690c210564226c9307b3bab977cdc07a6a45863a"
"642e280df732665249315d6c144871f0e2ceeae6","NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash Commands","The NLC2CMD Competition hosted at NeurIPS 2020 aimed to bring the power of natural language processing to the command line by building models that can transform descriptions of command line tasks in English to their Bash syntax.","NeurIPS",2021,"Mayank Agarwal,T. Chakraborti,Quchen Fu,David Gros,Xi Victoria Lin,Jaron Maene,Kartik Talamadupula,Zhongwei Teng,Jules White",9,40,3,"https://www.semanticscholar.org/paper/642e280df732665249315d6c144871f0e2ceeae6"
"205ac1373eb7981aca2d08f2ab651871a001271e","CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code","The results show that CrystalBLEU differentiates similar and unrelated programs better than the original BLEU score and also a variant designed specifically for source code, CodeBLEU.","2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)",2022,"A. Eghbali,Michael Pradel",10,55,0,"https://www.semanticscholar.org/paper/205ac1373eb7981aca2d08f2ab651871a001271e"
"69e330037afd18e5546fdcacbc9a9f7deb69bba9","Memorization and Generalization in Neural Code Intelligence Models","This work evaluates the memorization and generalization tendencies in neural code intelligence models through a case study across several benchmarks and model families by leveraging established approaches from other fields that use DNNs, such as introducing targeted noise into the training dataset.","Information and Software Technology",2021,"Md Rafiqul Islam Rabin,Aftab Hussain,V. Hellendoorn,Mohammad Amin Alipour",15,93,0,"https://www.semanticscholar.org/paper/69e330037afd18e5546fdcacbc9a9f7deb69bba9"
"09e14c4c80e20e80c052e0adb0d49df51aff718d","Yes, You Can Make an App Too: A Systematic Study of Prompt Engineering in the Automatic Generation of Mobile Applications from User Queries","One of the first systematic studies of prompt engineering for the Codex model, a LLM that produces code from a natural-language input, is embarked on, improving the pipeline’s performance from baseline for complex apps using example selection mechanisms and 43% for simple apps.","",2022,"Jasmine Shone",0,16,0,"https://www.semanticscholar.org/paper/09e14c4c80e20e80c052e0adb0d49df51aff718d"
"91260f73dd179487fb16713deb8267634ae14716","CodexDB: Synthesizing Code for Query Processing from Natural Language Instructions using GPT-3 Codex","The CodexDB framework is a framework on top of GPT-3 Codex that decomposes complex SQL queries into a series of simple processing steps, described in natural language, that enables users to customize SQL query processing via natural language instructions.","Proceedings of the VLDB Endowment",2022,"Immanuel Trummer",11,39,0,"https://www.semanticscholar.org/paper/91260f73dd179487fb16713deb8267634ae14716"
"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","Fault-Aware Neural Code Rankers","C ODE R ANKER is a neural ranker that can predict the correctness of a sampled program without executing it and can signiﬁcantly increase the pass@1 accuracy of various code generation models on APPS, HumanEval, and MBPP datasets.","ArXiv",2022,"J. Inala,Chenglong Wang,Mei Yang,Andrés Codas,Mark Encarnaci'on,Shuvendu K. Lahiri,M. Musuvathi,Jianfeng Gao",16,42,2,"https://www.semanticscholar.org/paper/075b6fb7d3787953164eecc1bd2e13f97c9f3c44"
"239b5649b12f28fd610de036afba41b9246db6c9","Parsel: A Unified Natural Language Framework for Algorithmic Reasoning","This work introduces Parsel 2, a framework enabling automatic implementation and validation of complex algorithms with code LLMs, based on hierarchical function descriptions in natural language, which can be used across domains requiring hierarchical reasoning, e.g. code synthesis, theorem proving, and robotic planning.","ArXiv",2022,"E. Zelikman,Qian Huang,Gabriel Poesia,Noah D. Goodman,N. Haber",6,65,0,"https://www.semanticscholar.org/paper/239b5649b12f28fd610de036afba41b9246db6c9"
"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","Jigsaw: Large Language Models meet Program Synthesis","This paper presents an approach to augment these large language models with post-processing steps based on program analysis and synthesis techniques, that understand the syntax and semantics of programs, and presents the experiences from building and evaluating such a tool Jigsaw.","International Conference on Software Engineering",2021,"Naman Jain,Skanda Vaidyanath,Arun Shankar Iyer,Nagarajan Natarajan,Suresh Parthasarathy,S. Rajamani,Rahul Sharma",61,45,1,"https://www.semanticscholar.org/paper/d095f9ffcb5905bf0858ad1769d3d90e2e8737e2"
"35afb74de9660962ebac2843d26de22a6fac2ef6","Learning from Self-Sampled Correct and Partially-Correct Programs","This work proposes to let the model perform sampling during training and learn from both self-sampled fully-Correct programs, which yield the gold execution results, as well as partially-correct programs, whose intermediate execution state matches another correct program.","ArXiv",2022,"Ansong Ni,J. Inala,Chenglong Wang,Oleksandr Polozov,Christopher Meek,Dragomir R. Radev,Jianfeng Gao",8,28,3,"https://www.semanticscholar.org/paper/35afb74de9660962ebac2843d26de22a6fac2ef6"
"06ea568379211ffa07d9605f66f26f6f736ea5e0","PanGu-Coder: Program Synthesis with Function-Level Language Modeling","A pretrained decoder-only language model adopting the P AN G U - α architecture for text-to-code generation, i.e. the synthesis of programming language solutions given a natural language problem description is presented.","ArXiv",2022,"Fenia Christopoulou,Gerasimos Lampouras,Milan Gritta,Guchun Zhang,Yinpeng Guo,Zhong-Yi Li,Qi Zhang,M. Xiao,Bo Shen,Lin Li,Hao Yu,Li-yu Yan,Pingyi Zhou,Xin Wang,Yu Ma,Ignacio Iacobacci,Yasheng Wang,Guangtai Liang,Jia Wei,Xin Jiang,Qianxiang Wang,Qun Liu",15,72,0,"https://www.semanticscholar.org/paper/06ea568379211ffa07d9605f66f26f6f736ea5e0"
"3fbc8d04a1f3dba58bdaada1924ee132512e98be","Productivity assessment of neural code completion","It is found that the rate with which shown suggestions are accepted, rather than more specific metrics regarding the persistence of completions in the code over time, drives developers’ perception of productivity.","MAPS@PLDI",2022,"Albert Ziegler,Eirini Kalliamvakou,Shawn Simister,Ganesh Sittampalam,X. A. Li,A. Rice,Devon Rifkin,E. Aftandilian",54,21,2,"https://www.semanticscholar.org/paper/3fbc8d04a1f3dba58bdaada1924ee132512e98be"
"1100dee3fd78655cddc4b7bfaef1161351d4fab5","Automated Feedback Generation for Competition-Level Code","This work presents Clef, the first data-driven tool that can generate feedback on competition-level code automatically by repairing programmers’ incorrect submissions, and introduces a new data structure, merge trees, to capture the changes between submissions.","International Conference on Automated Software Engineering",2022,"Jialu Zhang,De Li,John C. Kolesar,Hanyuan Shi,R. Piskac",2,48,0,"https://www.semanticscholar.org/paper/1100dee3fd78655cddc4b7bfaef1161351d4fab5"
"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","What is it like to program with artificial intelligence?","This paper explores how programming with large language models (LLM-assisted programming) is similar to, and differs from, prior conceptualisations of programmer assistance, and draws upon publicly available experience reports of LLM- assisted programming, as well as prior usability and design studies.","ArXiv",2022,"Advait Sarkar,A. Gordon,C. Negreanu,Christian Poelitz,Sruti Srinivasa Ragavan,B. Zorn",19,97,2,"https://www.semanticscholar.org/paper/618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7"
"f7664102a451332ed7e1286561b2f621eaff128d","Programming Puzzles","A positive correlation between puzzlesolving performance and coding experience, and between the puzzle difficulty for humans and AI solvers are found, and further improvements on P3 could have a significant impact on many program synthesis areas.","NeurIPS Datasets and Benchmarks",2021,"Tal Schuster,A. Kalyan,Oleksandr Polozov,A. Kalai",17,83,0,"https://www.semanticscholar.org/paper/f7664102a451332ed7e1286561b2f621eaff128d"
"5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming","This work studied GitHub Copilot, developed CUPS– a taxonomy of 12 programmer activities common to AI code completion systems, and conducted a study with 21 programmers who completed coding tasks and used the labeling tool to retrospectively label their sessions with CUPS.","ArXiv",2022,"Hussein Mozannar,Gagan Bansal,Adam Fourney,E. Horvitz",15,33,0,"https://www.semanticscholar.org/paper/5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b"
"1c336c18e53ad878bf4688c864acd99f137ae29f","Interactive Code Generation via Test-Driven User-Intent Formalization","This paper proposes the workflow of test-driven user-intent formalization (TDUIF), which leverages lightweight user feedback to jointly formalize the user intent as tests (a partial specification), and generates code that meets the formal user intent.","ArXiv",2022,"Shuvendu K. Lahiri,Aaditya Naik,Georgios Sakkas,Piali Choudhury,Curtis von Veh,M. Musuvathi,J. Inala,Chenglong Wang,Jianfeng Gao",17,25,0,"https://www.semanticscholar.org/paper/1c336c18e53ad878bf4688c864acd99f137ae29f"
"ef29fb8cc6bdda4b011288f51da521d3c25fc53d","Learning to Prevent Profitless Neural Code Completion","An early-rejection mechanism to turn down low-return prompts by foretelling the completion qualities without sending them to the LCM is proposed and a lightweight Transformer-based estimator is proposed to demonstrate the feasibility of the mechanism.","ArXiv",2022,"Zhensu Sun,Xiaoning Du,Fu Song,Shangwen Wang,Mingze Ni,Li Li",1,51,0,"https://www.semanticscholar.org/paper/ef29fb8cc6bdda4b011288f51da521d3c25fc53d"
"407b9e9478ba6bff43ce4b20e8b6cb2b303477d2","P LANNING WITH L ARGE L ANGUAGE M ODELS FOR C ODE G ENERATION","A novel Transformer decoding algorithm that uses a planning algorithm to do lookahead search and guide the Transformer to generate better programs, and enables controllable code generation, such as concise codes and highly-commented codes by optimizing modified objectives.","",2022,"",0,43,0,"https://www.semanticscholar.org/paper/407b9e9478ba6bff43ce4b20e8b6cb2b303477d2"
"1a53e7446274016f737236bdd48e3ff05d966384","Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow","A novel method to mine high-quality aligned data from SO using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a probabilistic model to capture the correlation between NL and code using neural networks.","IEEE Working Conference on Mining Software Repositories",2018,"Pengcheng Yin,Bowen Deng,Edgar Chen,Bogdan Vasilescu,Graham Neubig",174,52,41,"https://www.semanticscholar.org/paper/1a53e7446274016f737236bdd48e3ff05d966384"
"0192638593c430e15e2804a74a3e8a7ecb52d435","On the Design of AI-powered Code Assistants for Notebooks","Challenges and opportunities for future systems in this space are identified, such as the value of disambiguation for tasks like data visualization, the potential of tightly scoped domain-specific tools (like linters), and the importance of polite assistants.","ArXiv",2023,"Andrew M. McNutt,Chenglong Wang,R. DeLine,S. Drucker",7,91,0,"https://www.semanticscholar.org/paper/0192638593c430e15e2804a74a3e8a7ecb52d435"
"3b0cf543a730e674d4213d344ebc857fada76ead","Understanding High-Level Properties of Low-Level Programs Through Transformers","It is shown that Transformer models can translate C to LLVM-IR with high accuracy, by training on a parallel corpus of functions extract from 1 million compilable, open-sourced C programs and its corresponding LL VM-IR after compiling with Clang.","",2022,"William S. Moses",0,63,0,"https://www.semanticscholar.org/paper/3b0cf543a730e674d4213d344ebc857fada76ead"
"6f84d0cc33c7c58f74b28ddcc1cbda91ea608c9f","C ODE S UMMARIZATION : D O T RANSFORMERS R E ALLY U C","Overall, the quality of the generated summaries even from state-of-the-art (SOTA) models is quite poor, raising questions about the utility of current approaches and datasets.","",2022,"Manasi S. Patwardhan,L. Vig,Raveendra Kumar Medicherla,Ravindra Naik,Gautam M. Shroff",0,40,0,"https://www.semanticscholar.org/paper/6f84d0cc33c7c58f74b28ddcc1cbda91ea608c9f"
"d6b7bc4e7968398101d8d9b4da7d4a0186763ff3","MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages","A multilingual dataset is proposed to benchmark code generation from natural language commands extending beyond English, modeled off of the methodology from the English Code/Natural Language Challenge (CoNaLa) dataset, and a quantitative evaluation of performance is presented by testing with state-of-the-art code generation systems.","ArXiv",2022,"Zhiruo Wang,Grace Cuenca,Shuyan Zhou,Frank F. Xu,Graham Neubig",15,60,1,"https://www.semanticscholar.org/paper/d6b7bc4e7968398101d8d9b4da7d4a0186763ff3"
"8a854331c593c6a766fa3b8037fb2ad1b95a6f06","An Empirical Study of Code Smells in Transformer-based Code Generation Techniques","This study investigates to what extent code smells are present in the datasets of coding generation techniques and verify whether they leak into the output of these techniques.","IEEE Working Conference on Source Code Analysis and Manipulation",2022,"Mohammed Latif Siddiq,Shafayat H. Majumder,Maisha R. Mim,Sourov Jajodia,Joanna C. S. Santos",10,57,1,"https://www.semanticscholar.org/paper/8a854331c593c6a766fa3b8037fb2ad1b95a6f06"
"6bc87e51018d6de55011e95a0d43c588dd44a1e8","ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages","This work releases ERNIE-Code, a uniﬁed pre-trained language model for 116 NLs and 6 PLs, and employs two methods for universal cross-lingual pre-training: span-corruption language modeling that learns patterns from monolingual NL or PL; and pivot-based translationlanguage modeling that re-lies on parallel data of manyNLs and PLs.","ArXiv",2022,"Yekun Chai,Shuohuan Wang,Chao Pang,Yu Sun,Hao Tian,Hua Wu",2,47,0,"https://www.semanticscholar.org/paper/6bc87e51018d6de55011e95a0d43c588dd44a1e8"
"4ddc26b3a5fe9044b97b408d163f7464d769ebbf","CODEP: Grammatical Seq2Seq Model for General-Purpose Code Generation","This paper proposes CODEP, a grammatical Seq2Seq code generation framework equipped with a Pushdown automaton (PDA) module, and constructs the DPA for the most popular GPL Python and conducts extensive experiments to evaluate the effectiveness.","ArXiv",2022,"Yihong Dong,Ge Li,Zhi Jin",5,49,0,"https://www.semanticscholar.org/paper/4ddc26b3a5fe9044b97b408d163f7464d769ebbf"
"3cba16fc46ac5b35c1cc72a822208aa0097384cc","CodePAD: Sequence-based Code Generation with Pushdown Automaton","This paper devise a pushdown automaton (PDA)-based methodology to address the problem of grammar constraints of programming language (PL), and proposes CodePAD, a sequence-based code generation framework equipped with a PDA module, to integrate the deduction of PDA into deep learning.","",2022,"Yihong Dong,Xue Jiang,Yuchen Liu,Ge Li,Zhi Jin",1,40,0,"https://www.semanticscholar.org/paper/3cba16fc46ac5b35c1cc72a822208aa0097384cc"
"5ff9032d0f7f246d01ae7b2c231ab03469a7344a","Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?","This work examines the use of large language models for code (such as OpenAI's Codex and AI21's Jurassic J-1) for zero-shot vulnerability repair, and investigates challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code.","ArXiv",2021,"H. Pearce,B. Tan,Baleegh Ahmad,R. Karri,Brendan Dolan-Gavitt",38,56,1,"https://www.semanticscholar.org/paper/5ff9032d0f7f246d01ae7b2c231ab03469a7344a"
"a1ef81e17a9ca41e09aba802040a2eca2744716f","Generation Probabilities are Not Enough: Improving Error Highlighting for AI Code Suggestions","It is unclear how best to convey the uncertainty of generative models to human operators or if doing so will positively impact human-AI collaboration.","",2022,"Helena Vasconcelos",4,15,0,"https://www.semanticscholar.org/paper/a1ef81e17a9ca41e09aba802040a2eca2744716f"
"bb7e46f316d319f9819c3554c99995ef8361ae9c","CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex","CodexDB is a framework on top of GPT-3 Codex that decomposes complex SQL queries into a series of simple processing steps, described in natural language, enriched with user-provided instructions and descriptions of database properties.","ArXiv",2022,"Immanuel Trummer",5,29,0,"https://www.semanticscholar.org/paper/bb7e46f316d319f9819c3554c99995ef8361ae9c"
"1d160123cbbef972ea151a641dd435d57c727de8","AixBench: A Code Generation Benchmark Dataset","A benchmark dataset for evaluating method-level code generation task and a new metric for automatically evaluating the correctness of the generated code, and a set of criteria to manually evaluating the overall quality of thegenerated code are presented.","ArXiv",2022,"Yiyang Hao,Ge Li,Yongqiang Liu,Xiaowei Miao,He Zong,Siyuan Jiang,Yang Liu,He Wei",7,4,1,"https://www.semanticscholar.org/paper/1d160123cbbef972ea151a641dd435d57c727de8"
"114eb5a35a3cd802cd1f46fff35c284e32ef6c54","GitHub Copilot AI pair programmer: Asset or Liability?","Comparing Copilot to humans, the results show that the correct ratio of human solutions is greater than Copilot’s correct ratio, while the buggy solutions generated by Copilot require less time to be repaired.","ArXiv",2022,"Arghavan Moradi Dakhel,Vahid Majdinasab,Amin Nikanjam,F. Khomh,M. Desmarais,Z. Jiang",38,61,1,"https://www.semanticscholar.org/paper/114eb5a35a3cd802cd1f46fff35c284e32ef6c54"
"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","Grounded Copilot: How Programmers Interact with Code-Generating Models","Interactions with programming assistants are bimodal : in acceleration mode , the programmer knows what to do next and uses Copilot to get there faster; in exploration mode, the programmer is unsure how to proceed and usesCopilot to explore their options.","ArXiv",2022,"Shraddha Barke,M. James,N. Polikarpova",41,66,2,"https://www.semanticscholar.org/paper/8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a"
"654cd297b307c0c6fa08732511ba852b8dce1977","Out of the BLEU: how should we assess quality of the Code Generation models?","A study on applicability of six metrics— BLEu, ROUGE-L, METEOR, ChrF, CodeBLEU, RUBY—for evaluation of the code generation models is presented and several recommendations on using metrics to estimate the model performance on the codegeneration tasks are derived.","SSRN Electronic Journal",2022,"Mikhail Evtikhiev,Egor Bogomolov,Yaroslav Sokolov,T. Bryksin",18,48,0,"https://www.semanticscholar.org/paper/654cd297b307c0c6fa08732511ba852b8dce1977"
"0509c25103939d59ed4e27b1393e74ad5734c453","How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot","The results suggest that model generated code is comparable in complexity and readability to code written by human pair programmers, and eye tracking data suggests, to a statistically significant level, that programmers direct less visual attention to model generate code.","International Conference on Automated Software Engineering",2022,"N. A. Madi",7,28,0,"https://www.semanticscholar.org/paper/0509c25103939d59ed4e27b1393e74ad5734c453"
"46d0a832fada6147bceb0bd4e39928e482733246","How Important are Good Method Names in Neural Code Generation? A Model Robustness Perspective","The potential of benefiting from method names to enhance the performance of PCGMs, from a model robustness perspective, is studied and a novel approach is proposed, named RADAR (neuRAl coDe generAtor Robustifier).","ArXiv",2022,"Guang Yang,Yu Zhou,Wenhua Yang,Tao Yue,Xiang Chen,Taolue Chen",3,75,0,"https://www.semanticscholar.org/paper/46d0a832fada6147bceb0bd4e39928e482733246"
"0f38267a8ba32789f5d3b1b19820f86940fea052","Generation-Augmented Query Expansion For Code Retrieval","This paper proposes a generation-augmented query expansion framework that leverages the code generation model to enhance the code retrieval task and achieves new state-of-the-art results on the CodeSearchNet benchmark and surpass the baselines signiﬁcantly.","ArXiv",2022,"Dong Li,Yelong Shen,Ruoming Jin,Yi Mao,Kuan Wang,Weizhu Chen",1,25,0,"https://www.semanticscholar.org/paper/0f38267a8ba32789f5d3b1b19820f86940fea052"
"269df328eec08b56b7b1f38a7555797fe2b999b6","ReCode: Robustness Evaluation of Code Generation Models","This paper proposes ReCode, a comprehensive robustness evaluation benchmark for code generation models, and customizable over 30 transformations for code on docstrings, function and variable names, code syntax, and code format, which provide multifaceted assessments of a model’s robustness performance.","ArXiv",2022,"Shiqi Wang,Zheng Li,Haifeng Qian,Cheng Yang,Zijian Wang,Mingyue Shang,Varun Kumar,Samson Tan,Baishakhi Ray,Parminder Bhatia,Ramesh Nallapati,M. Ramanathan,D. Roth,Bing Xiang",4,36,0,"https://www.semanticscholar.org/paper/269df328eec08b56b7b1f38a7555797fe2b999b6"
"51d253814e85249a84bbe634b4a80d306b74fbd0","""It would work for me too"": How Online Communities Shape Software Developers' Trust in AI-Powered Code Generation Tools","This study unpack how developers in online communities collectively make sense of AI code generation tools by developing proper expectation, understanding, strategies, and awareness of broader implications, as well as how they leverage community signals to evaluate AI suggestions.","ArXiv",2022,"Ruijia Cheng,Ruotong Wang,T. Zimmermann,Denae Ford",6,68,1,"https://www.semanticscholar.org/paper/51d253814e85249a84bbe634b4a80d306b74fbd0"
"0121c151c96b32cd7851e4bcda2a468b279c1e6f","CODE-MVP: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training","This paper proposes to integrate different views with the natural-language description of source code into a unified framework with Multi-View contrastive Pre-training, and names the model as CODE-MVP.","NAACL-HLT",2022,"Xin Wang,Yasheng Wang,Yao Wan,Jiawei Wang,Pingyi Zhou,Li Li,Hao Wu,Jin Liu",16,65,1,"https://www.semanticscholar.org/paper/0121c151c96b32cd7851e4bcda2a468b279c1e6f"
"13ea8689a4fe12e7443f5a3a0a25c5337a2f4cd8","MarianCG: a code generation transformer model inspired by machine translation","MarianCG, a code generation Transformer model used to tackle the code generation challenge of generating python code from natural language descriptions is presented, based on fine-tuning a machine translation pre-trained language model.","Journal of engineering and applied sciences",2022,"Ahmed S. Soliman,M. Hadhoud,S. Shaheen",1,41,0,"https://www.semanticscholar.org/paper/13ea8689a4fe12e7443f5a3a0a25c5337a2f4cd8"
"bf8d5f801237f4b9502efcc0528be3d27978bf59","CodeScore: Evaluating Code Generation by Learning Code Execution","CodeScore is proposed, an efﬁcient and effective CEM for code generation, which estimates test case PassRatio of generated code without executing code, and a framework named UniCE for training code evaluation models by learning code execution, i.e., learning Pass Ratio and Executability ofgenerated code.","ArXiv",2023,"Yihong Dong,J. Ding,Xue Jiang,Zhuo Li,Ge Li,Zhi Jin",6,41,0,"https://www.semanticscholar.org/paper/bf8d5f801237f4b9502efcc0528be3d27978bf59"
"dae74645479f7c1fa3671066f9e24ec6c20c17ec","TrojanPuzzle: Covertly Poisoning Code-Suggestion Models","Two novel data poisoning attacks are demonstrated, C OVERT and T ROJAN P UZZLE, that can bypass static analysis by planting malicious poisoning data in out-of-context regions such as docstrings and have implications for how practitioners should select code used to be coded.","ArXiv",2023,"H. Aghakhani,Wei Dai,Andre Manoel,Xavier Fernandes,Anant Kharkar,Christopher Kruegel,Giovanni Vigna,David Evans,B. Zorn,Robert Sim",4,57,0,"https://www.semanticscholar.org/paper/dae74645479f7c1fa3671066f9e24ec6c20c17ec"
"9afab8dc694269d205d769eaea549d8f7558d776","Exploring the Verifiability of Code Generated by GitHub Copilot","Evidence is found which corroborates the current consensus in the literature: Copilot is a powerful tool; however, it should not be “ﬂying the plane"" by itself.","ArXiv",2022,"Dakota Wong,Austin Kothig,Patrick Lam",2,14,0,"https://www.semanticscholar.org/paper/9afab8dc694269d205d769eaea549d8f7558d776"
"74f57a9ffa73e1bee99300b177904c06199840aa","CodeRetriever: A Large Scale Contrastive Pre-Training Method for Code Search","The proposed CodeRetriever model, which learns the function-level code semantic representations through large-scale code-text contrastive pre-training, achieves new state-of-the-art with significant improvement over existing code pre-trained models, on eleven domain/language-specific code search tasks with six programming languages in different code granularity.","Conference on Empirical Methods in Natural Language Processing",2022,"Xiaonan Li,Yeyun Gong,Yelong Shen,Xipeng Qiu,Hang Zhang,Bolun Yao,Weizhen Qi,Daxin Jiang,Weizhu Chen,Nan Duan",1,43,0,"https://www.semanticscholar.org/paper/74f57a9ffa73e1bee99300b177904c06199840aa"
"ebc08a933e053e57bc2102848efa1119c0ce2bd9","NS3: Neuro-Symbolic Semantic Code Search","This work proposes supplementing the query sentence with a layout of its semantic structure, using a Neural Module Network architecture to implement this idea, and demonstrates that this approach results in more precise code retrieval.","ArXiv",2022,"Shushan Arakelyan,Anna Hakhverdyan,Miltiadis Allamanis,Christophe Hauser,Luis Garcia,Xiang Ren",1,62,0,"https://www.semanticscholar.org/paper/ebc08a933e053e57bc2102848efa1119c0ce2bd9"
"2a0456b0408cd4c33f2ff4400374e7be2497a362","Repairing Bugs in Python Assignments Using Large Language Models","This work proposes to use a large language model trained on code, such as Codex, to build an APR system – MMAPR – for introductory Python programming assignments and finds that MM APR can produce more programs and produce smaller patches on average.","ArXiv",2022,"Jialu Zhang,J. Cambronero,Sumit Gulwani,Vu Le,R. Piskac,Gustavo Soares,Gust Verbruggen",10,49,1,"https://www.semanticscholar.org/paper/2a0456b0408cd4c33f2ff4400374e7be2497a362"
"bd5d3022dc395ca85f72e346022ed6175e13a278","A Transformer-based Approach for Translating Natural Language to Bash Commands","The approach presented in this paper is the best performing architecture on this problem to date and improves the current state-of-the-art accuracy on this translation task from 13.8% to 53.2%.","International Conference on Machine Learning and Applications",2021,"Quchen Fu,Zhongwei Teng,Jules White,Douglas C. Schmidt",8,50,0,"https://www.semanticscholar.org/paper/bd5d3022dc395ca85f72e346022ed6175e13a278"
"538288d24bdad73d831dfed44b706958287ed318","Generating Sequences by Learning to Self-Correct","SELF - CORRECTION is presented, an approach that decouples an imperfect base generator from a separate corrector that learns to iteratively correct imperfect generations and improves upon the base generator in three diverse generation tasks– mathematical program synthesis, lexically-constrained generation, and toxicity control.","ArXiv",2022,"S. Welleck,Ximing Lu,Peter West,Faeze Brahman,T. Shen,Daniel Khashabi,Yejin Choi",26,39,0,"https://www.semanticscholar.org/paper/538288d24bdad73d831dfed44b706958287ed318"
"535a3d95a743e1f4b591b5b2af3e778a6347158a","CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code","This paper proposes CodeBERTScore, an automatic evaluation metric for code generation, which builds on BERTScore and achieves a higher correlation with human preference and with functional correctness than all existing metrics.","",2023,"Shuyan Zhou,Uri Alon,Sumit Agarwal,Graham Neubig",5,38,0,"https://www.semanticscholar.org/paper/535a3d95a743e1f4b591b5b2af3e778a6347158a"
"86af60090d37ee820318910e1fea4f1784f82f41","Measuring The Impact Of Programming Language Distribution","The BabelCode framework for execution-based evaluation of any benchmark in any language and a new code translation dataset called Translating Python Programming Puzzles (TP3) from the Python Programming puzzles (Schuster et al. 2021) benchmark that involves translating expert-level python functions to any language are presented.","ArXiv",2023,"Gabriel Orlanski,Kefan Xiao,Xavier García,Jeffrey Hui,Joshua Howland,J. Malmaud,Jacob Austin,Rishah Singh,Michele Catasta",5,38,0,"https://www.semanticscholar.org/paper/86af60090d37ee820318910e1fea4f1784f82f41"
"7677e9ead35f8f2578acbf5ad15fb330c83762c4","Systematically Finding Security Vulnerabilities in Black-Box Code Generation Models","This work proposes a novel black-box inversion approach based on few-shot prompting that automatically and systematically finds 1000s of security vulnerabilities in various code generation models, including the commercial black- box model GitHub Copilot.","ArXiv",2023,"Hossein Hajipour,Thorsten Holz,Lea Schonherr,Mario Fritz",4,45,0,"https://www.semanticscholar.org/paper/7677e9ead35f8f2578acbf5ad15fb330c83762c4"
"a764d4f28612a7b5c4767ea6a2540b169fdb052c","CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models","A benchmark named CoderEval of pragmatic code generation with generative pre-trained models is proposed, compared with the widely-used HumanEval benchmark from OpenAI, which can be used to assess the performance of models against pragmatic codegeneration beyond just generating standalone functions.","ArXiv",2023,"Hao Yu,Bo Shen,Dezhi Ran,Jiaxin Zhang,Qi Zhang,Yu Ma,Guangtai Liang,Ying Li,Tao Xie,Qianxiang Wang",4,21,0,"https://www.semanticscholar.org/paper/a764d4f28612a7b5c4767ea6a2540b169fdb052c"
"f0ea9e2d3889d37f34743ed1dc64f11e8e0484de","Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models","The work examines whether next-token prediction succeeds from sentence completion into the realm of actual numerical understanding and showcases the model's capabilities to group by or pivot categorical sums, infer feature importance, derive correlations, and predict unseen test cases using linear regression.","ArXiv",2023,"David Noever,Forrest McKee",5,43,0,"https://www.semanticscholar.org/paper/f0ea9e2d3889d37f34743ed1dc64f11e8e0484de"
"782f3d43b37790a83c98d5fd3ef142b296f20616","CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code Models","A large-scale benchmark that includes 216 existing code-related tasks is proposed and it is demonstrated that the cross-task generalization of models can be largely improved by in-context learning methods such as few-shot learning and learning from task instructions, which shows the promising prospects of conducting cross- task learning research on this benchmark.","ArXiv",2023,"Changan Niu,Chuanyi Li,Vincent Ng,Bin Luo",1,76,0,"https://www.semanticscholar.org/paper/782f3d43b37790a83c98d5fd3ef142b296f20616"
"6248474933664013e5b0615dc474a7f6de5e97f4","LExecutor: Learning-Guided Execution","LExecutor is presented, a learning-guided approach for executing arbitrary code snippets in an underconstrained way to let a neural model predict missing values that otherwise would cause the program to get stuck, and to inject these values into the execution.","ArXiv",2023,"Beatriz Souza,Michael Pradel",1,92,0,"https://www.semanticscholar.org/paper/6248474933664013e5b0615dc474a7f6de5e97f4"
"1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a","SantaCoder: don't reach for the stars!",,"ArXiv",2023,"Loubna Ben Allal,Raymond Li,Denis Kocetkov,Chenghao Mou,Christopher Akiki,Carlos Muñoz Ferrandis,Niklas Muennighoff,Mayank Mishra,A. Gu,Manan Dey,Logesh Kumar Umapathi,Carolyn Jane Anderson,Yangtian Zi,J. Poirier,Hailey Schoelkopf,S. Troshin,Dmitry Abulkhanov,M. Romero,M. Lappert,F. Toni,Bernardo Garc'ia del R'io,Qian Liu,Shamik Bose,Urvashi Bhattacharyya,Terry Yue Zhuo,I. Yu,Paulo Villegas,M. Zocca,Sourab Mangrulkar,D. Lansky,Huu Nguyen,Danish Contractor,Luisa Villa,Jia Li,Dzmitry Bahdanau,Yacine Jernite,S. Hughes,Daniel Fried,Arjun Guha,Harm de Vries,Leandro von Werra",27,44,0,"https://www.semanticscholar.org/paper/1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a"
"0b077c9577f4297dcf3da835e253d21965bbc6e0","CoTexT: Multi-task Learning with Code-Text Transformer","CoTexT is a pre-trained, transformer-based encoder-decoder model that learns the representative context between natural language (NL) and programming language (PL) using self-supervision to learn a general understanding of language and code.","NLP4PROG",2021,"Long Phan,Hieu Tran,Daniel Le,H. Nguyen,J. Anibal,Alec Peltekian,Yanfang Ye",73,25,11,"https://www.semanticscholar.org/paper/0b077c9577f4297dcf3da835e253d21965bbc6e0"
"636f854b1a3a983e6803eae0277179596cc2cb95","Summarize and Generate to Back-translate: Unsupervised Translation of Programming Languages","This work proposes a method for performing back-translation via code summarization and generation of natural language summaries given code snippets that performs competitively with state-of-the-art methods.","ArXiv",2022,"Wasi Uddin Ahmad,Saikat Chakraborty,Baishakhi Ray,Kai-Wei Chang",7,53,0,"https://www.semanticscholar.org/paper/636f854b1a3a983e6803eae0277179596cc2cb95"