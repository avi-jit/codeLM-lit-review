{"papers":[{"url":"https://www.semanticscholar.org/paper/270b015093073d3ba254928b6d736a59870d3fb1","title":"Chatbots As Fluent Polyglots: Revisiting Breakthrough Code Snippets","venue":"ArXiv","year":2023,"referenceCount":19,"citationCount":2,"influentialCitationCount":0,"publicationDate":"05/01/2023","authors":"David Noever,Kevin Williams","citations":[{"paperId":"2424a94fcb5c498265333c5a1c591cf36fc29a01","title":"Grading Conversational Responses Of Chatbots"},{"paperId":"f0ea9e2d3889d37f34743ed1dc64f11e8e0484de","title":"Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models"}],"references":[{"paperId":"6756fcd998caeb7b23702e08559e63710179334c","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"3078a916b57e465614efea50628c889799e32289","title":"\"It would work for me too\": How Online Communities Shape Software Developers' Trust in AI-Powered Code Generation Tools"},{"paperId":"8175ce2cbb99b6a394bdac152ae39d413f4f1380","title":"Codex Hacks HackerRank: Memorization Issues and a Framework for Code Synthesis Evaluation"},{"paperId":"0d08ffccc982781e310bb184397bbe64b9aef157","title":"Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models"},{"paperId":"979eb5c97c49d7979447ed684500895a24d75ac4","title":"The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming"},{"paperId":"6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a","title":"Few-Shot Semantic Parsing with Language Models Trained on Code"},{"paperId":"91260f73dd179487fb16713deb8267634ae14716","title":"CodexDB: Synthesizing Code for Query Processing from Natural Language Instructions using GPT-3 Codex"},{"paperId":"1444536496d8064f33e10b38b5820fecfab5b367","title":"Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"5ff9032d0f7f246d01ae7b2c231ab03469a7344a","title":"Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?"},{"paperId":null,"title":"Can artificial intelligence make software development more productive"},{"paperId":null,"title":"GPT-2 Output Detector Demo"},{"paperId":null,"title":"OpenAI Codex"},{"paperId":"b72c89500dd57f1a4ceadb97f3dbf5015948a5e7","title":"A Brief History of Chatbots"},{"paperId":null,"title":"The Lines of Code That Changed Everything"},{"paperId":"e9431029d8c7da7f55c3496c9d8a3cec542a858f","title":"COBOL to Java and Newspapers Still Get Delivered"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":null,"title":"Towards Making Legacy HPC Codes Maintainable: Two- Way Fortran-Python Transpilation with Python Type Hints (Unrefereed Workshop Manuscript)"}],"id":"270b015093073d3ba254928b6d736a59870d3fb1","summary":"The research applies AI-driven code assistants to analyze a selection of influential computer code that has shaped modern technology, including email, internet browsing, robotics, and malicious software to provide insights into obfuscated code or software lacking explanatory commentary."},{"url":"https://www.semanticscholar.org/paper/bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence","venue":"ArXiv","year":2022,"referenceCount":77,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Yichen Xu,Yanqiao Zhu","citations":[],"references":[{"paperId":"4c26a6a777f1d46ffe79e3f41f8fd1ef6a6bd8c9","title":"Automatically Generating CS Learning Materials with Large Language Models"},{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"c03fa01fbb9c77fe3d10609ba5f1dee33a723867","title":"ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"},{"paperId":"703f79763d534dbf9674132cec890f432dcc19ec","title":"A Survey of Deep Learning Models for Structural Code Understanding"},{"paperId":"4b27f18bff43d605805c92696a979714ced0b805","title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"979eb5c97c49d7979447ed684500895a24d75ac4","title":"The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"916a06a6d51aa93de27aac2f3e14faed08dd6706","title":"Formal Mathematics Statement Curriculum Learning"},{"paperId":null,"title":"Natural Language Processing with Transformers: Building Language Applications with Hugging Face"},{"paperId":"f7987fa2aadc0b368c185dc4d2fdb1337a202c32","title":"Solving Probability and Statistics Problems by Program Synthesis"},{"paperId":"4e40595d6ecba027cebb4f2e3b43ae44bcf51daf","title":"Solving Linear Algebra by Program Synthesis"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"0b077c9577f4297dcf3da835e253d21965bbc6e0","title":"CoTexT: Multi-task Learning with Code-Text Transformer"},{"paperId":"1c0752fb3e9ab5c9392f196225075422f26b5110","title":"How could Neural Networks understand Programs?"},{"paperId":"3a863672b072a37ac8d2eb6b780234f6b748a2d9","title":"Generating bug-fixes using pretrained transformers"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"f389c09ad85263bdd1bb2518d61c90a8a558441d","title":"MulCode: A Multi-task Learning Approach for Source Code Understanding"},{"paperId":"20d37eb44ad65735f243938961fde9ba5b4d26b7","title":"D2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using Differential Analysis"},{"paperId":"3a1efac4a8aa2f40a8128e034c01f18660652dd5","title":"DOBF: A Deobfuscation Pre-Training Objective for Programming Languages"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"8b4c857311c001f6ed0cd790cce4af4dfcfb6533","title":"Deep Graph Matching and Searching for Semantic Code Retrieval"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"021bbcefc993c389bad6c1daefd8ff92d0fc2441","title":"Contrastive Code Representation Learning"},{"paperId":"2a405483796dfedf5d95483aa8880c57626e0e9f","title":"Integrating Tree Path in Transformer for Code Representation"},{"paperId":null,"title":"Fsf-funded call for white papers on philosophical and legal questions around copilot"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"49cf6a22a5dac5bc98b653534af65ffa0bc0e76d","title":"Multi-task Learning based Pre-trained Language Model for Code Completion"},{"paperId":"406afe68e789fcb0d7e3d24ebea65b53d206f740","title":"Exploring Software Naturalness through Neural Language Models"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"37a23c43ddf09ea97b82b38e2827a2229cfae545","title":"Novel positional encodings to enable tree-based transformers"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"00549af4bc3270e0f688acbf694f912d7ee39cad","title":"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks"},{"paperId":"e3ef11877bdd08140fcabf358dd9fc5bef6b15e0","title":"Recommendations for Datasets for Source Code Summarization"},{"paperId":"516497e7fc39ebb126bed28dcc5c6129e500acb8","title":"An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation"},{"paperId":"98d1307bed619b58b4a44acd8e65ac58495776c2","title":"code2seq: Generating Sequences from Structured Representations of Code"},{"paperId":"73bade9162bc4df6ca6c9ab8a7eb2d54d35acad2","title":"code2vec: learning distributed representations of code"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"e3deff41f9d3eb1e141207b69d6c7a4c007bd63e","title":"Program Synthesis Through Reinforcement Learning Guided Tree Search"},{"paperId":"41c8cabe475d85d7548c25935da19ea5b96dfe06","title":"Neural Code Comprehension: A Learnable Representation of Code Semantics"},{"paperId":"e3d772986d176057aca2f5e3eb783da53b559134","title":"Unsupervised Machine Translation Using Monolingual Corpora Only"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"32aa2517b03c871c11e521c2a3406f457833e2c3","title":"Summarizing Source Code using a Neural Attention Model"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"9277dc7dd5b348645e8fcf4a987e5b3fc9132c7c","title":"Towards a Big Data Curated Benchmark of Inter-project Code Clones"},{"paperId":"110b7719f16eae8d82af0f56fca6e802c842c181","title":"Mining source code repositories at massive scale using language modeling"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"9a56a9cea19b83bf46ab2d47b59bc1ea3020a2b1","title":"Experiences Using Static Analysis to Find Bugs"},{"paperId":"b7eceec1e8edfa769fdd095db16897a061b02a79","title":"Compilers: Principles, Techniques, and Tools (2nd Edition)"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"95d886a0d097f0a1a81db8f431e744996ecc3048","title":"Static Analysis versus Software Model Checking for Bug Finding"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"5a6dbe11f7bd7182ca008b0f94b75fe5cac57a08","title":"Types and programming languages"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"45510aef1c064bbd16361dc9413ddd2068bebe53","title":"Compiler Construction: Principles and Practice"},{"paperId":"f8d77bb8da085ec419866e0f87e4efc2577b6141","title":"Serial Order: A Parallel Distributed Processing Approach"},{"paperId":"111fd833a4ae576cfdbb27d87d2f8fc0640af355","title":"Learning internal representations by error propagation"},{"paperId":"fef8d6579c5ba743baf3aa4c8dda78516284c3d9","title":"The Cloze Procedure"},{"paperId":"e2e0dd827011bb9bcb09578efe95c973a1f413b5","title":"The Mythical Man-Month: Essays on Softw"},{"paperId":"766ce989b8b8b984f7a4691fd8c9af4bdb2b74cd","title":"“Cloze Procedure”: A New Tool for Measuring Readability"}],"id":"bae76e1d13abe54f66dc140be53538b864578ba8","summary":"This paper presents a comprehensive survey of the NCI domain, including a thorough review of pretraining techniques, tasks, datasets, and model architectures, and hopes it will serve as a bridge between the natural language and programming language communities."},{"url":"https://www.semanticscholar.org/paper/acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code","venue":"ArXiv","year":2021,"referenceCount":114,"citationCount":720,"influentialCitationCount":139,"publicationDate":"07/07/2021","authors":"Mark Chen,Jerry Tworek,Heewoo Jun,Qiming Yuan,Henrique Ponde,Jared Kaplan,Harrison Edwards,Yura Burda,Nicholas Joseph,Greg Brockman,Alex Ray,Raul Puri,Gretchen Krueger,Michael Petrov,Heidy Khlaaf,Girish Sastry,Pamela Mishkin,Brooke Chan,Scott Gray,Nick Ryder,Mikhail Pavlov,Alethea Power,Lukasz Kaiser,Mohammad Bavarian,Clemens Winter,Philippe Tillet,F. Such,D. Cummings,Matthias Plappert,Fotios Chantzis,Elizabeth Barnes,Ariel Herbert-Voss,William H. Guss,Alex Nichol,I. Babuschkin,S. Balaji,Shantanu Jain,A. Carr,J. Leike,Joshua Achiam,Vedant Misra,Evan Morikawa,Alec Radford,M. Knight,Miles Brundage,Mira Murati,Katie Mayer,P. Welinder,Bob McGrew,Dario Amodei,Sam McCandlish,Ilya Sutskever,Wojciech Zaremba","citations":[{"paperId":"bdd9f62abc0b548c05c8b9e6eea58319a80be29c","title":"Knowledge monopolies and the innovation divide: A governance perspective"},{"paperId":"699389de6f52efbf3efbc84fc731a6694dbe0639","title":"Learning to Program with Natural Language"},{"paperId":"fbb5cc9e8a46f61d1543bd17b6eca324fd5bf55c","title":"Fully Autonomous Programming with Large Language Models"},{"paperId":"9a6e7438138d5308ff92b8e8ee719a5dbabe9922","title":"GeneGPT: Teaching Large Language Models to Use NCBI Web APIs"},{"paperId":"0115e7945158195d3d159370dabd121e074d8142","title":"A Case Study on Scaffolding Exploratory Data Analysis for AI Pair Programmers"},{"paperId":"78602a7112b6ad1e6150ad04023a3b96636425e1","title":"GenAICHI 2023: Generative AI and HCI at CHI 2023"},{"paperId":"8f1e8a0245a7b95cf929f800fa04a79967ce4cd0","title":"Understanding the Benefits and Challenges of Deploying Conversational AI Leveraging Large Language Models for Public Health Intervention"},{"paperId":"4c746d8a8368dbea13c7eb7cccfd5d0cc15edb31","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"7fb7552dda3d092cfcfaaea852183cd9efaaf960","title":"Progressive-Hint Prompting Improves Reasoning in Large Language Models"},{"paperId":"fa0b17014e5c1b4c38ae5feb20f2047853728813","title":"Towards Objective-Tailored Genetic Improvement Through Large Language Models"},{"paperId":"c4e4b72da211dbf2ab2fd5263d453cf22ee0cf44","title":"CodeKGC: Code Language Model for Generative Knowledge Graph Construction"},{"paperId":"01f9b773408115a16fe872147348db175789e82f","title":"Tool Learning with Foundation Models"},{"paperId":"be2b0396de9431bae931642516a1d3e4906329f5","title":"Low-code LLM: Visual Programming over LLMs"},{"paperId":"57100e39d0413ee585b381ba9ab366e8a6cf2866","title":"Solving Math Word Problems by Combining Language Models With Symbolic Solvers"},{"paperId":"465d9a65a6e8dfd9976e303f7d5b9d091efe3ee9","title":"ArguGPT: evaluating, understanding and identifying argumentative essays generated by GPT models"},{"paperId":"3b32b827ab4d92310393faa4b04daf3457377ff0","title":"A Comprehensive Evaluation of the Copy Mechanism for Natural Language to SPARQL Query Generation"},{"paperId":"6298fb744cb31c0ca3db96833edd57a0c974424b","title":"API-Bank: A Benchmark for Tool-Augmented LLMs"},{"paperId":"b5009d25120434bd9fc400f4204a55d57f42cfba","title":"Stochastic Code Generation"},{"paperId":"8a2a2006e371ce07e0b29538cd11337a5c977caf","title":"“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models"},{"paperId":"1c00f88b69f1d4efe3e1695ca49aa38a1340bd98","title":"Improving Few-Shot Prompts with Relevant Static Analysis Products"},{"paperId":"f12ebcc9a0a7296cc6c85b243a003f7205c68b3d","title":"What does CLIP know about a red circle? Visual prompt engineering for VLMs"},{"paperId":"77645e3638201e352a404504543c8058df7bbd5d","title":"Evaluation of ChatGPT Model for Vulnerability Detection"},{"paperId":"dca6c3927ade6481a1ae080f5c24decbfeced1be","title":"Boosted Prompt Ensembles for Large Language Models"},{"paperId":"7eb07a629d42e2ab595555b018e61ccbce385aca","title":"Multi-step Jailbreaking Privacy Attacks on ChatGPT"},{"paperId":"2d51e79d541bb489b07aa4fa691a93a9d4a0498b","title":"Evaluating AIGC Detectors on Code Content"},{"paperId":"eb55442d940accc67d6790e66d3196e1941fc7a4","title":"Teaching Large Language Models to Self-Debug"},{"paperId":"5b37aa7ee3f517380f92fdac67753adbaa58514f","title":"Bayesian Optimization of Catalysts With In-context Learning"},{"paperId":"9395a2e773e5c04c4cfb056fba2b1b86077f346e","title":"Scallop: A Language for Neurosymbolic Programming"},{"paperId":"80e2eed05854d7c2a9d75c2fb136acb0421a8c77","title":"Automated Reading Passage Generation with OpenAI's Large Language Model"},{"paperId":"8ce0d339896e658b124aee63532f085c55fee225","title":"A Preliminary Evaluation of ChatGPT for Zero-shot Dialogue Understanding"},{"paperId":"5ef668ac636971c50c864e49949cfab2b965475a","title":"ChatGPT Empowered Long-Step Robot Control in Various Environments: A Case Application"},{"paperId":"326ff0157f655598ca9cb21d1e703654ccbde033","title":"Comparing Code Explanations Created by Students and Large Language Models"},{"paperId":"34d12432af63915caf14eab9a362f7e7d24e4c13","title":"Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions"},{"paperId":"94baee04669e51c689357747506aefa33341a559","title":"Approach Intelligent Writing Assistants Usability with Seven Stages of Action"},{"paperId":"cdf3033e629a8d1578c57f2b5383fc32616a3bda","title":"Document-Level Machine Translation with Large Language Models"},{"paperId":"f352a968c8735fac58912870a7bde57fcfc2e6bd","title":"\"It's Weird That it Knows What I Want\": Usability and Interactions with Copilot for Novice Programmers"},{"paperId":"470754e17de89081f63dde4719922fe9b63251d5","title":"Large Language Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via FuzzGPT"},{"paperId":"4de290467d903b9977e31b3d4084006789bd6ebd","title":"One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era"},{"paperId":"01713a5f38c0164151caa9b6cc740a8c864420cc","title":"Scientists' Perspectives on the Potential for Generative AI in their Fields"},{"paperId":"07f07d4d59fdbc3596284f51057cb006779d42c1","title":"A Bibliometric Review of Large Language Models Research from 2017 to 2023"},{"paperId":"64e20f2abc15d5cf04a682df5a1265bd45ba9fe7","title":"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"},{"paperId":"a845bce29105f55dab1e47d2f92d7338eb183a4e","title":"Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT"},{"paperId":"cbdc75e9c80870164662bb0359e23fd4736c5f0e","title":"A Survey of Large Language Models"},{"paperId":"3b01b5b497e1f359b11da45af029281ee6f64c2c","title":"Towards Enhancing In-Context Learning for Code Generation"},{"paperId":"975da5bb7fdd800ba577535d8c6ee5a5bc835d52","title":"Pair Programming with Large Language Models for Sampling and Estimation of Copulas"},{"paperId":"f24dabf3317abaa3d7393ab7d48115f353749bde","title":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X"},{"paperId":"762b4b11220d2d8a9c57f0a3af327840a67e7284","title":"Self-Refine: Iterative Refinement with Self-Feedback"},{"paperId":"70da4fb798a86cbe8cad96c27ced0415885bbd9d","title":"AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators"},{"paperId":"0a0d6a98bd246a82aaaa9d33ec0eadf4ceae69dc","title":"On Codex Prompt Engineering for OCL Generation: An Empirical Study"},{"paperId":"8aff5530f92684578f7d5a89e5fad7922f04b1e5","title":"Did You Mean...? Confidence-based Trade-offs in Semantic Parsing"},{"paperId":"ac7771c332da42b29a913b116bd6ef622cbf89cf","title":"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs"},{"paperId":"98e6e0b3b811193e89b1a033da6c0a454220877a","title":"Foundation Models and Fair Use"},{"paperId":"28dafb94a87daa71ad3edf9f04f3c8c32753398b","title":"Improving Code Generation by Training with Natural Language Feedback"},{"paperId":"8ed7c9ba7cdb33e816135381ca502ace649c7985","title":"Ecosystem Graphs: The Social Footprint of Foundation Models"},{"paperId":"bbddcfbef4aa59ab66736ba4ea58709dd4981f2b","title":"Natural Selection Favors AIs over Humans"},{"paperId":"ade1d35f7ced39df693837607d97112139a7d78c","title":"Unlocking the Potential of ChatGPT: A Comprehensive Exploration of its Applications, Advantages, Limitations, and Future Directions in Natural Language Processing"},{"paperId":"e92626244308479ac33ff855d610baae005ad07b","title":"The Programmer’s Assistant User Experience"},{"paperId":"c8f2aced926707fba8a0535a6df5b5823d394bac","title":"AI-Generated Content (AIGC): A Survey"},{"paperId":"645960e8bcb2a55bc7e8816c49e60e8f98303acb","title":"Combining Contexts from Multiple Sources for Documentation-Specific Code Example Generation"},{"paperId":"0cfdd655100055f234fd23ebecd915504b8e00e3","title":"Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System"},{"paperId":"2da3a84e72a2973504cd9cf1c0a377f5a5a91f09","title":"GPT is becoming a Turing machine: Here are some ways to program it"},{"paperId":"06e5828341aa3926e1d839039363b0673b9461cc","title":"Errors are Useful Prompts: Instruction Guided Task Programming with Verifier-Assisted Iterative Prompting"},{"paperId":"3c45fd32b56efaa009a3ecef963d233dc5814194","title":"NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations"},{"paperId":"805e1fa8250fd8aad356f6705f4957d412d2549a","title":"MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models"},{"paperId":"8d9367678667176710840a5f029d0474eec27d8a","title":"Towards Understanding the Generalization of Medical Text-to-SQL Models and Datasets"},{"paperId":"349ec9ca2ff17dbc1aa1caf27f98b7cd85a4747e","title":"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation"},{"paperId":"285dae5c2f2ef55c70971094a1ddd45afe720eee","title":"Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense"},{"paperId":"207b767bcc2d916afdd27e7c1b25ed261363548c","title":"Text2Motion: From Natural Language Instructions to Feasible Plans"},{"paperId":"af622206f1e5a6632600075897385cdabcae463d","title":"Large Language Models and Simple, Stupid Bugs"},{"paperId":"052a5e2bcc999810ee6f1eedcf758c528e4f125f","title":"Retrieving Multimodal Information for Augmented Generation: A Survey"},{"paperId":"24bcdce51edb8e1174fbabd072a0c07bf7362d57","title":"Language Model Behavior: A Comprehensive Survey"},{"paperId":"7857d80f516a943f96757b21053f7635ac1cf2af","title":"Revisiting the Plastic Surgery Hypothesis via Large Language Models"},{"paperId":"61b8aa990c358a93eecbe8fb73f1638ad05eaa72","title":"Requirement Formalisation Using Natural Language Processing and Machine Learning: A Systematic Review"},{"paperId":"02e80a115e98e6280c752ccd8820ddbac17db1f2","title":"The Role of Large Language Models in the Recognition of Territorial Sovereignty: An Analysis of the Construction of Legitimacy"},{"paperId":"538ea3b1f942f27d0db6eaf6c16711bc505c2c9e","title":"GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models"},{"paperId":"80b99ed94e991a941ffeabf74826a9ee8c995d6d","title":"Generate, Transform, Answer: Question Specific Tool Synthesis for Tabular Data"},{"paperId":"0d42221038c05cee8443c5b5af838505ee137dc3","title":"ART: Automatic multi-step reasoning and tool-use for large language models"},{"paperId":"8205612a6d15df52b8c3d26aabfd50abc10fdee3","title":"LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations"},{"paperId":"817e52b815560f95171d8fa60f78dd965e885a65","title":"How well do Large Language Models perform in Arithmetic tasks?"},{"paperId":"f59559bbc4640a63f5e3b9ede8df763acb82353f","title":"Towards the Scalable Evaluation of Cooperativeness in Language Models"},{"paperId":"9b1367167e45bac5e039ba0407cb3329125e9ff2","title":"Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?"},{"paperId":"59524ed1ed36a878e88af54ff631521dd8639a12","title":"Reclaiming the Digital Commons: A Public Data Trust for Training Data"},{"paperId":"2ce8331b0d85e8c336811a7692e8b105f6a9e373","title":"Can Large Language Models design a Robot?"},{"paperId":"8ca62fdf4c276ea3052dc96dcfd8ee96ca425a48","title":"GPT-4 Technical Report"},{"paperId":"b181c979a94a6cd019f2527da4c018de19c59371","title":"WikiCoder: Learning to Write Knowledge-Powered Code"},{"paperId":"8ed0ee821e28ff01d1e275a4728f248e3bbeff9a","title":"Mirror: A Natural Language Interface for Data Querying, Summarization, and Visualization"},{"paperId":"a0eb6078463a8fd524bfa092429a6ab595845ffa","title":"It Takes One to Tango but More Make Trouble? In-Context Training with Different Number of Demonstrations"},{"paperId":"6e754273d54a91371efbc928cd6b156364d517da","title":"ViperGPT: Visual Inference via Python Execution for Reasoning"},{"paperId":"68510c71410add818c63372d3af1898ca45ff9c4","title":"It Takes One to Tango but More Make Trouble? The Number of Demonstrations Needed for In-Context Learning"},{"paperId":"92f19090599910af1b1c9ed2b318abc0adea0527","title":"Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences"},{"paperId":"fbdd496c421e050a47c4fb2e0019635d2f4b97e7","title":"Meet in the Middle: A New Pre-training Paradigm"},{"paperId":"34d24b2d9f116f8f652c112d4ac924afcf11bd0d","title":"InferFix: End-to-End Program Repair with LLMs"},{"paperId":"acc7a277c6a9cb05b8bf4c3c25c7369aa9a42555","title":"Evaluating the use of large language model in identifying top research questions in gastroenterology"},{"paperId":"0c192ccbf9418f31cfd8672de2536da153b62f2e","title":"Task and Motion Planning with Large Language Models for Object Rearrangement"},{"paperId":"921dace8bf038a34cba5473a72abc8cf65d61e03","title":"Large Language Models (GPT) Struggle to Answer Multiple-Choice Questions about Code"},{"paperId":"eb0b9965732ce09b67e088efdbe0978aeafcdcc6","title":"Greener yet Powerful: Taming Large Code Generation Models with Quantization"},{"paperId":"36cd3d3a9e8a64f322476153513ece1fd617acfc","title":"nl2spec: Interactively Translating Unstructured Natural Language to Temporal Logics with Large Language Models"},{"paperId":"9b5de3f83649c347b2b2b9fdecb142fbca293f57","title":"Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference"},{"paperId":"b92b8a32f89f45cd771359e3351f6ddcad61450c","title":"ChatGPT Participates in a Computer Science Exam"},{"paperId":"1826dd7f745a8574018193b72fee94b7503180d9","title":"disco: a toolkit for Distributional Control of Generative Models"},{"paperId":"9f8ac6ee3760ab202e492c733362e5bfc6763934","title":"Baldur: Whole-Proof Generation and Repair with Large Language Models"},{"paperId":"2f94f03fdac62d05f0f416b7b3855d1f597afee9","title":"Automatically Auditing Large Language Models via Discrete Optimization"},{"paperId":"154493f69d7db3d49da0e51df0192c6ad5f1724a","title":"Larger language models do in-context learning differently"},{"paperId":"a7b3a868a80dbe97689135c99b1a6b6e10dcdfe5","title":"A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT"},{"paperId":"35afb57a646592c3a471a4f010d00e1b13dd3c43","title":"From Copilot to Pilot: Towards AI Supported Software Development"},{"paperId":"94ac118c522c065e24d9090b9f7827b83e8ff2f4","title":"Many bioinformatics programming tasks can be automated with ChatGPT"},{"paperId":"572b92972eff7501ca2b109b8998cdcb69aa1958","title":"Data Portraits: Recording Foundation Model Training Data"},{"paperId":"d2edaa84c13fd08735216ec85f2f3b2b5b53ff78","title":"On the Feasibility of Specialized Ability Extracting for Large Language Code Models"},{"paperId":"38fe8f324d2162e63a967a9ac6648974fc4c66f3","title":"PaLM-E: An Embodied Multimodal Language Model"},{"paperId":"196353ab8d340b31b465c22dff904de195fa9a60","title":"LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models"},{"paperId":"a55dfc1482c9fa32859d1e8e8c5813f5a22982cc","title":"OpenICL: An Open-Source Framework for In-context Learning"},{"paperId":"0888d909838f3b88d690b555bd34ea8a8d8dfb8b","title":"Ask and You Shall Receive (a Graph Drawing): Testing ChatGPT's Potential to Apply Graph Layout Algorithms"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"a8e0ba16346b72c3a04dd0b1da84bc5f28900174","title":"Using GitHub Copilot to Solve Simple Programming Problems"},{"paperId":"f857cf61c784225db92f5a500578b86e63cd73ee","title":"PatchZero: Zero-Shot Automatic Patch Correctness Assessment"},{"paperId":"809f3198289554fce309795219cdb42befede20e","title":"Almanac: Knowledge-Grounded Language Models for Clinical Medicine"},{"paperId":"57ffd52abfbba0eb76c8406c7331f1f8ca29aaff","title":"R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents"},{"paperId":"5e06c517a9883cdc8266a871d26183cf7aa954cf","title":"A Framework to Generate Neurosymbolic PDDL-compliant Planners"},{"paperId":"35d083ff4dc2805c477d0aedc8158fc7a1aefeda","title":"EvoPrompting: Language Models for Code-Level Neural Architecture Search"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"2ba85accd73c2e2069bbc2453807e51bacf132cd","title":"Responsible AI for Trusted AI-powered Enterprise Platforms"},{"paperId":"1358f90705b05cdb20ebe6799b02196205e7e9f0","title":"Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data"},{"paperId":"032962bfc563e2caf986d3b3746f3217303be1b0","title":"Extracting Victim Counts from Text"},{"paperId":"6173f30e0ca70ce8a6a8d3b7147dfee243953d67","title":"Active Prompting with Chain-of-Thought for Large Language Models"},{"paperId":"4403bb8e839e175142b877222ff736cf949f232e","title":"A Code Centric Evaluation of C/C++ Vulnerability Datasets for Deep Learning Based Vulnerability Detection Techniques"},{"paperId":"2edd00ed67c39d4166e5920d454885c3dc3e0a67","title":"Guiding Large Language Models via Directional Stimulus Prompting"},{"paperId":"7678d1862140d84f9c15d95e7d4d085857f332eb","title":"Conversational Text-to-SQL: An Odyssey into State-of-the-Art and Challenges Ahead"},{"paperId":"383157f9059061faba4364c7d2141d0ff78c87fa","title":"Learning Deep Semantics for Test Completion"},{"paperId":"3f83582c08a62e5bd02398fafc93f7eaf1e4b84e","title":"Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints"},{"paperId":"681cee58cf7e54199191cf9e0baf6851d8356704","title":"Complex QA and language models hybrid architectures, Survey"},{"paperId":"eb619cbfb34127f28f1cab03eb22234267f892ea","title":"PAC Prediction Sets for Large Language Models of Code"},{"paperId":"6d269364de402d4a72ac30b0c8d81324f6849807","title":"LEVER: Learning to Verify Language-to-Code Generation with Execution"},{"paperId":"79d083a742fe2c20f543f442f5324e63a4d4ae2d","title":"Auditing large language models: a three-layered approach"},{"paperId":"f03f39f735186c4359b719724be6e1c2eb912fef","title":"Studying the effect of AI Code Generators on Supporting Novice Learners in Introductory Programming"},{"paperId":"2029349c55c1dba3493c5b3bd25152f18ba21ae2","title":"Augmented Language Models: a Survey"},{"paperId":"f5d93cf5d81575aeee61faeddde4437fbcb74cd0","title":"The Programmer’s Assistant: Conversational Interaction with a Large Language Model for Software Development"},{"paperId":"7c817fa57edd087820d3b4d16e4d1f40d4cb5200","title":"Generation Probabilities Are Not Enough: Exploring the Effectiveness of Uncertainty Highlighting in AI-Powered Code Completions"},{"paperId":"390c71025beb7e7613640ecd331fa9a1179ca568","title":"Guiding Pretraining in Reinforcement Learning with Large Language Models"},{"paperId":"038f249ab708cebae2a58265b768b9b1cbadad3a","title":"The Impact of AI on Developer Productivity: Evidence from GitHub Copilot"},{"paperId":"6876fc47f4674fdf4f53208254b45dedc6de3755","title":"Adaptive Test Generation Using a Large Language Model"},{"paperId":"13a66fc8689724e295548ceac9e5425fc46cc093","title":"SkCoder: A Sketch-based Approach for Automatic Code Generation"},{"paperId":"bb276a8109c42eb1b7d531d84fe99f3c5fb5ea90","title":"Compositional Exemplars for In-context Learning"},{"paperId":"318c76d062bcc72320150a8e309e080bb4e175df","title":"Scaffolding Progress: How Structured Editors Shape Novice Errors When Transitioning from Blocks to Text"},{"paperId":"535a3d95a743e1f4b591b5b2af3e778a6347158a","title":"CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code"},{"paperId":"d3de0bac5703825796c240bfab8dc3c8e0a90222","title":"Impact of Code Language Models on Automated Program Repair"},{"paperId":"79680e20f0c8038039b243fb5fd385fbe967c799","title":"Controlling Large Language Models to Generate Secure and Vulnerable Code"},{"paperId":"d2170504c4ad9403bea118ae8debdfda95978546","title":"The Wisdom of Hindsight Makes Language Models Better Instruction Followers"},{"paperId":"3ad346ae7af5c30964c4916dbcee798f72e1bdb7","title":"Translating Natural Language to Planning Goals with Large-Language Models"},{"paperId":"09239dac5b1cded9414c946333eaf619dca9aaa7","title":"ChatGPT and Other Large Language Models as Evolutionary Engines for Online Interactive Collaborative Game Design"},{"paperId":"49293522fe3818769f5067e360c971bd7a82e0aa","title":"Explanation Selection Using Unlabeled Data for In-Context Learning"},{"paperId":"7677e9ead35f8f2578acbf5ad15fb330c83762c4","title":"Systematically Finding Security Vulnerabilities in Black-Box Code Generation Models"},{"paperId":"61c4abaef7564161ca39f40d3c511d0036a85f6f","title":"Fuzzing Automatic Differentiation in Deep-Learning Libraries"},{"paperId":"a2bcacc8fefb859c94c69d524b2368bb4792f9b1","title":"Adversarial Prompting for Black Box Foundation Models"},{"paperId":"782f3d43b37790a83c98d5fd3ef142b296f20616","title":"CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code Models"},{"paperId":"a6b129587f6f16b559d9f3e1742e67b59ffa8423","title":"ChatGPT for Clinical Vignette Generation, Revision, and Evaluation"},{"paperId":"98207fea68db75d3941577ef87d685944519e09c","title":"Reliable Natural Language Understanding with Large Language Models and Answer Set Programming"},{"paperId":"dccd176071e62d3c82af3710956256cbcf4df9b4","title":"Toward a Theory of Causation for Interpreting Neural Code Models"},{"paperId":"c1014f86c8d801c93b37eed58311da5f9c9da2e4","title":"ChatGPT and Software Testing Education: Promises & Perils"},{"paperId":"cd0988714ea326642d2b1bb18753e187fec71e42","title":"A Categorical Archive of ChatGPT Failures"},{"paperId":"fb243dfd1234b8f76dfda740a62402663da74085","title":"Exploring Data Augmentation for Code Generation Tasks"},{"paperId":"6248474933664013e5b0615dc474a7f6de5e97f4","title":"LExecutor: Learning-Guided Execution"},{"paperId":"9447aed4e4c3b8c0ad14ba43995924ac22dbb8d3","title":"Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents"},{"paperId":"27c2ddc921befc44f7b2cdb8bb39facdd8b0b188","title":"Measuring The Impact Of Programming Language Distribution"},{"paperId":"18a8a2a74b878aed15b36eb0848b5f543b8fc585","title":"KNOD: Domain Knowledge Distilled Tree Decoder for Automated Program Repair"},{"paperId":"ccfd107d18f2df1cc4e24dff3f34c0a7f6af2df3","title":"ANTM: An Aligned Neural Topic Model for Exploring Evolving Topics"},{"paperId":"a1f8082505c7e90b0a033e1b9da0a97d67aad66c","title":"Accelerating Large Language Model Decoding with Speculative Sampling"},{"paperId":"2af6a21a1b682ceb585165359d3605e89f4cf6b0","title":"Fixing Hardware Security Bugs with Large Language Models"},{"paperId":"861916af6428277a3ce2e18034e4b40dc6616eb9","title":"On the Robustness of Code Generation Techniques: An Empirical Study on GitHub Copilot"},{"paperId":"cd7e947c7b9f85ed88f79ef8eb07ee38854f6694","title":"Clinical Decision Transformer: Intended Treatment Recommendation through Goal Prompting"},{"paperId":"a764d4f28612a7b5c4767ea6a2540b169fdb052c","title":"CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models"},{"paperId":"458c1b8c8692250930ff6f27b388496782a9c01f","title":"A Comprehensive Survey on Program Synthesis With Evolutionary Algorithms"},{"paperId":"03fca0b32fa443ea5a343a2e8859f1e221d03d9c","title":"Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning"},{"paperId":"819f91874cfa43335401cddedd02b8cfc1c2b84f","title":"Mathematical Capabilities of ChatGPT"},{"paperId":"68edfd62d2619fb4af7c2469edb95b9e2fe4544a","title":"Execution-based Code Generation using Deep Reinforcement Learning"},{"paperId":"ea0688f9e7dfb0d3c2249486af65209c25809544","title":"Faithful Chain-of-Thought Reasoning"},{"paperId":"63396fceb84286b02796dc58e55c07ec1095c4dc","title":"FLAME: A small language model for spreadsheet formulas"},{"paperId":"1d34b6cffe67077cdd4df41950b1195f09ae0cb8","title":"Hierarchical Programmatic Reinforcement Learning via Learning to Compose Programs"},{"paperId":"236bba82c1f048b3aa1f661460ec462f2dc9257f","title":"REPLUG: Retrieval-Augmented Black-Box Language Models"},{"paperId":"1c91b23d78944f7f237cb512029c2165972ae9d5","title":"On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex"},{"paperId":"fbd49b25bdab98c171af49962a41139c73dacbde","title":"Specializing Smaller Language Models towards Multi-Step Reasoning"},{"paperId":"c532f1df90925e5c69789f0cd99248d8a2a2e5bc","title":"My AI Wants to Know if This Will Be on the Exam: Testing OpenAI’s Codex on CS2 Programming Exercises"},{"paperId":"0fbcb934048a0da9c12ccffe2417e48793884992","title":"Unifying Molecular and Textual Representations via Multi-task Language Modelling"},{"paperId":"074baf835aec44a100990178859b35451975f339","title":"Navigating Complexity in Software Engineering: A Prototype for Comparing GPT-n Solutions"},{"paperId":"658fbf9874c35b442ca67e3d0077682064808cbf","title":"Norm-based Generalization Bounds for Compositionally Sparse Neural Networks"},{"paperId":"2a9478d984d76658abcde82e6c6b5f64038ed540","title":"Case-Based Reasoning with Language Models for Classification of Logical Fallacies"},{"paperId":"22f0f4c65fe0c61ee76049f71576ed178252edc8","title":"ThoughtSource: A central hub for large language model reasoning data"},{"paperId":"13b79ebe697322c709fd26cc58ea650ad1c7e5a4","title":"Alien Coding"},{"paperId":"7d1ec348e56f3bb2df0f81aa08a61de99238d15a","title":"User-Customizable Transpilation of Scripting Languages"},{"paperId":"572e82dfdde0f72f9448caf72fdc68a233da6659","title":"Open Problems in Applied Deep Learning"},{"paperId":"0192638593c430e15e2804a74a3e8a7ecb52d435","title":"On the Design of AI-powered Code Assistants for Notebooks"},{"paperId":"c43a4a7b7ea4f4889de051321cb0073fd577f843","title":"Causal Reasoning of Entities and Events in Procedural Texts"},{"paperId":"0885556b71b24a641b4ffe139afd4d2712228cff","title":"Beware of the Unexpected: Bimodal Taint Analysis"},{"paperId":"288055fa8954c6ea6cdafee25cd523108b716d15","title":"Explicit or Implicit? On Feature Engineering for ML-based Variability-intensive Systems"},{"paperId":"c72ac81c4ac414314b52cf8f5b77370f8d0875d4","title":"Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models"},{"paperId":"1338b3771c27090dee722cc5b351ace179ebae76","title":"Mathematics, word problems, common sense, and artificial intelligence"},{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"},{"paperId":"7195ed3c7f11220f29634cecb68b1d39db2e36d9","title":"Dr.Spider: A Diagnostic Evaluation Benchmark towards Text-to-SQL Robustness"},{"paperId":"9f530ebf624bf58e91b2a1f20b0799a45ca48f9a","title":"An Analysis of the Automatic Bug Fixing Performance of ChatGPT"},{"paperId":"d8405996b4d08c304098636aedd9e1c1a1e262ee","title":"The Premature Obituary of Programming"},{"paperId":"e659fa1e79a2a151be331125c14339988542aac3","title":"Batch Prompting: Efficient Inference with Large Language Model APIs"},{"paperId":"cb29cf52f0f7d2e4324c68690a55b22890f2212d","title":"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection"},{"paperId":"a20875e70a38cb053cd34e170038c4746f85dac9","title":"A Case Study in Engineering a Conversational Programming Assistant's Persona 121-129"},{"paperId":"907a77639069bb7dd270f017068745706133cffc","title":"Inaccessible Neural Language Models Could Reinvigorate Linguistic Nativism"},{"paperId":"6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc","title":"See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning"},{"paperId":"1f22de83d912176cb8857efa1c6d65b14d6a2f5c","title":"ChatGPT is not all you need. A State of the Art Review of large Generative AI models"},{"paperId":"7803b2167a22c1db5729d65c41be774af15cb1b8","title":"EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata"},{"paperId":"c27bbdd8968c11513a68383145f7935293a57c25","title":"Exploring the Approximation Capabilities of Multiplicative Neural Networks for Smooth Functions"},{"paperId":"fba0b0817dbc8200b41a1de22654b54b778a11e9","title":"Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models"},{"paperId":"c9ad9d69d7568110dd5527598a92c7f8b335eef4","title":"Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations"},{"paperId":"468992bf970c37bd1fef58b78a6c2fcd8c018868","title":"Scaling Laws for Generative Mixed-Modal Language Models"},{"paperId":"5435ed7c26f0c250493f244acffb69dd929d116b","title":"Structured Case-based Reasoning for Inference-time Adaptation of Text-to-SQL parsers"},{"paperId":"4221f29aec5ce9feedddc1644f074af19f8d110e","title":"FlashFill++: Scaling Programming by Example by Cutting to the Chase"},{"paperId":"e028ba59aacec72a55164e274e1d64896fea0256","title":"Efficient Mutation Testing via Pre-Trained Language Models"},{"paperId":"dae74645479f7c1fa3671066f9e24ec6c20c17ec","title":"TrojanPuzzle: Covertly Poisoning Code-Suggestion Models"},{"paperId":"490f1e8ff352bded30bcde01d5b4769d6c2d2dd5","title":"Adversarial Attacks on Neural Models of Code via Code Difference Reduction"},{"paperId":"b2a8b21062718f930dbb1662a93ae1f13298fa1f","title":"Serenity: Library Based Python Code Analysis for Code Completion and Automated Machine Learning"},{"paperId":"270b015093073d3ba254928b6d736a59870d3fb1","title":"Chatbots As Fluent Polyglots: Revisiting Breakthrough Code Snippets"},{"paperId":"efb8d99c903328f64bfbcaa10b59f3b807798c55","title":"FastFlow: Accelerating Deep Learning Model Training with Smart Offloading of Input Data Pipeline"},{"paperId":"42630c03d3817b1153d245f20742ad4b30a80b75","title":"JEMMA: An extensible Java dataset for ML4Code applications"},{"paperId":"3b8ccc7ec80b8775de603e248ac1ca2b919d6b70","title":"Chatbots in a Botnet World"},{"paperId":"d6d90a28b2b4ceb9b81150b5bd498541d5d9aa89","title":"Robust and Explainable Identification of Logical Fallacies in Natural Language Arguments"},{"paperId":"c548b05a9696023a6eb5ff6d93a9a00e850b1ea8","title":"Automated variable renaming: are we there yet?"},{"paperId":"0bc9cedda48551847cc741b74c1fc299c5a9eed2","title":"Who Evaluates the Evaluators? On Automatic Metrics for Assessing AI-based Offensive Code Generators"},{"paperId":"dca3bc28a7d404b28780a813ea7072eda809e6c0","title":"Programming Is Hard - Or at Least It Used to Be: Educational Opportunities and Challenges of AI Code Generation"},{"paperId":"2abed82162c47a0cc32cd62afcf46b0745541017","title":"Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book"},{"paperId":"41f5e1ad7793593befc0b9c38f756836e8b07c98","title":"Code4ML: a Large-scale Dataset of annotated Machine Learning Code"},{"paperId":"0566c1c3eeeef5c968fced6d80b77fe22d02bbd9","title":"Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language"},{"paperId":"4431a39f677fe59b07b3f0cfde7b10f7208cf46c","title":"N-Best Hypotheses Reranking for Text-to-SQL Systems"},{"paperId":"ac2e15fbfe3ea338725f5d33d17a5a687609c431","title":"On The Computational Complexity of Self-Attention"},{"paperId":"6032212d5790b6a580d68d469a9895aad6238c89","title":"Diverse Title Generation for Stack Overflow Posts with Multiple Sampling Enhanced Transformer"},{"paperId":"0392d58335ce674a70f5e58ac8c438de296a0e6a","title":"Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models"},{"paperId":"0cffb48b8036d62ccc0ebd3bbbfb433860747c83","title":"Learning from flowsheets: A generative transformer model for autocompletion of flowsheets"},{"paperId":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"fa3609e00f9f422a309c621a35394c4a38f88687","title":"Using cognitive psychology to understand GPT-3"},{"paperId":"086bbf33c4593c3e73bdad297e7381d6be851331","title":"Learning to Parallelize in a Shared-Memory Environment with Transformers"},{"paperId":"b21db4f0a4da2159c0b17608270182d3887a1640","title":"Fold2Vec: Towards a Statement-Based Representation of Code for Code Comprehension"},{"paperId":"4c26a6a777f1d46ffe79e3f41f8fd1ef6a6bd8c9","title":"Automatically Generating CS Learning Materials with Large Language Models"},{"paperId":"c2a259bb42cedad59469797b2175f7ca062c2bd7","title":"User-Driven Support for Rapid Visualization Prototyping in D3"},{"paperId":"9bed7e0205963eaa332721b25f024cd1e876c30d","title":"User-Driven Support for Visualization Prototyping in D3"},{"paperId":"8cf3a454556060d6e9aa86dbabf221bd10bf9759","title":"On the Effectiveness of Transfer Learning for Code Search"},{"paperId":"69e330037afd18e5546fdcacbc9a9f7deb69bba9","title":"Memorization and Generalization in Neural Code Intelligence Models"},{"paperId":"a93dab80894abc5116b95782db5871e6288812b6","title":"NASA is launching a spacecraft to Mars in 2020 ... NLG model NASA launching spacecraft"},{"paperId":null,"title":"NLG model user input NASA is launching a spacecraft to the moon on Monday ... NLG model NLG model"},{"paperId":null,"title":"A Representative Sample of Major Generative AI Developments from 2020 to Jan 2023"},{"paperId":"40c7b3e24bc6d6fe518a8ff60e58bd4877813b1a","title":"Membership Inference Attacks With Token-Level Deduplication on Korean Language Models"},{"paperId":"66476832701361c9f9b2a7eb2354ee8cd9f72e67","title":"Large Language Models are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models"},{"paperId":"b8d06dd769f89d08bdd9997d7bd363c89ede845b","title":"ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models"},{"paperId":"4bea09d4c897fb201c032b9eb605a943b1e70435","title":"CORRPUS: Detecting Story Inconsistencies via Codex-Bootstrapped Neurosymbolic Reasoning"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"269df328eec08b56b7b1f38a7555797fe2b999b6","title":"ReCode: Robustness Evaluation of Code Generation Models"},{"paperId":"d3a7a4543d83f568f79d1febe8379465ff0140c9","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"0f38267a8ba32789f5d3b1b19820f86940fea052","title":"Generation-Augmented Query Expansion For Code Retrieval"},{"paperId":"3ee9c65366efbb17adf370c39f20dbef60d53670","title":"Towards Reasoning in Large Language Models: A Survey"},{"paperId":"5c32c653735b43a0a8923ca65ac191bd4bf15311","title":"Precise Zero-Shot Dense Retrieval without Relevance Labels"},{"paperId":"29be9045fb09f0c947fb24c76bd1136d47880d96","title":"Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions"},{"paperId":"3810345aef8d1146452196e26ac49bdc07b26d8b","title":"Can Current Task-oriented Dialogue Models Automate Real-world Scenarios in the Wild?"},{"paperId":"bf5fbd690f24f873df86d1b0a06579cf42f7dc36","title":"Dialog2API: Task-Oriented Dialogue with API Description and Example Programs"},{"paperId":"433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context"},{"paperId":"bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"8210cef990b8e5cddbc95000e46309bdd25337f7","title":"Asking Clarification Questions for Code Generation in General-Purpose Programming Language"},{"paperId":"6756fcd998caeb7b23702e08559e63710179334c","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"9e8f0d9ba4e7af673c8b214b3764e020706dd1f3","title":"Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"909c46441282736a0acb3df21b7c13a4e3da70ab","title":"Emergent Analogical Reasoning in Large Language Models"},{"paperId":"cef330bacf014d60daabbd489647b2006af130ca","title":"Discovering Language Model Behaviors with Model-Written Evaluations"},{"paperId":"0731c710fc09fe7c039aee3e9bff006bf94565aa","title":"XTest: A Parallel Multilingual Corpus with Test Cases for Code Translation and Its Evaluation*"},{"paperId":"9cc5c25517c3a78183a052e8e93a44e85bb17432","title":"Improving Cross-task Generalization of Unified Table-to-text Models with Compositional Task Configurations"},{"paperId":"ae441f7305dc2cd58c708528b3ecee3501cc5c46","title":"Plansformer: Generating Symbolic Plans using Transformers"},{"paperId":"4290a70025f29d7054c550c75ae6b24c38a79d12","title":"SE Factual Knowledge in Frozen Giant Code Model: A Study on FQN and its Retrieval"},{"paperId":"e4f8cb7bb933d95bec8d6eaeeb9d1815ed095f21","title":"Benchmarking Large Language Models for Automated Verilog RTL Code Generation"},{"paperId":"6bc87e51018d6de55011e95a0d43c588dd44a1e8","title":"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages"},{"paperId":"d98fd1dd218bf522722a42b7c56f53dd6b1d20b0","title":"Diverse Demonstrations Improve In-context Compositional Generalization"},{"paperId":"7932b714e2ae1def5828df52b97f1decb9bebd32","title":"Considerations for Differentially Private Learning with Large-Scale Public Pretraining"},{"paperId":"9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming"},{"paperId":"8ee45aeb7c97e3346cc62f216f673b91277ac718","title":"LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models"},{"paperId":"97148858b395c5122e09d6e8a20dd7016a111aa3","title":"Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning"},{"paperId":"8175ce2cbb99b6a394bdac152ae39d413f4f1380","title":"Codex Hacks HackerRank: Memorization Issues and a Framework for Code Synthesis Evaluation"},{"paperId":"fe3ead702e8e8948d00caef9bc9dd075dc560236","title":"I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification"},{"paperId":"04f87baf7d1b3eb303a52a8a66c8189f396dd114","title":"Application of Pretrained Large Language Models in Embodied Artificial Intelligence"},{"paperId":"720130cea1933e0d9ff4915e0a9a869bf1ab0221","title":"Automated Quantum Software Engineering: why? what? how?"},{"paperId":"c4607fdcc6bcbc214546147062104d9f50493810","title":"On Mixed-Initiative Content Creation for Video Games"},{"paperId":"be76b0f32e287d866bc7aefc700052f9825ed3ce","title":"BudgetLongformer: Can we Cheaply Pretrain a SotA Legal Language Model From Scratch?"},{"paperId":"ebd4bec684808aff360b5f255d15c0d112ba13d3","title":"Explicit Knowledge Transfer for Weakly-Supervised Code Generation"},{"paperId":"ea55ae4c82b45e3857f37406c94e9f642a2b3d84","title":"Genetic Programming with Local Scoring"},{"paperId":"6d4a12ea469ff08634eeb24c47b265a7dca2fce2","title":"PADL: Language-Directed Physics-Based Character Control"},{"paperId":"27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation"},{"paperId":"46d0a832fada6147bceb0bd4e39928e482733246","title":"How Important are Good Method Names in Neural Code Generation? A Model Robustness Perspective"},{"paperId":"6257d48954680fb4e7df9b663e0ac50db4404046","title":"On the Security Vulnerabilities of Text-to-SQL Models"},{"paperId":"34009b5d7a3ab44863c8dfa0d8dc4383c86c3115","title":"Deep-Learning-based Vulnerability Detection in Binary Executables"},{"paperId":"8aa23a86603f7dd4eceda3d2e0337ba90dff7f4f","title":"CodeExp: Explanatory Code Document Generation"},{"paperId":"6d7b8a478801bd9d21df82d5f33ae6eced90da5e","title":"Solving math word problems with process- and outcome-based feedback"},{"paperId":"edcd520e553dc58c728eceb8433e3d155955a89a","title":"Complementary Explanations for Effective In-Context Learning"},{"paperId":"ce9397eaa3de0f791bead5d16f14b5dd4a15052f","title":"GitHub Considered Harmful? Analyzing Open-Source Projects for the Automatic Generation of Cryptographic API Call Sequences"},{"paperId":"30624a18720bf93a85dc3efe570df271a8c9f4c3","title":"Program Repair"},{"paperId":"c192fb33f308f19ac8a5c4c2d623d56385b839be","title":"Systematic Literature Review on Solving Competitive Programming Problem with Artificial Intelligence (AI)"},{"paperId":"598d9b235f5ab148fc757240d9bc39a47b8eaf72","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"0908532a6d86bfaba8cacc42e1585f22128a0c38","title":"CLAWSAT: Towards Both Robust and Accurate Code Models"},{"paperId":"20b60fb3993d2e9a5af04611f7bdf248e5a3a736","title":"Programming by Example and Text-to-Code Translation for Conversational Code Generation"},{"paperId":"120f4e798d78c3f6dceed218bee1ca83a5855f55","title":"Exploring the Efficacy of Pre-trained Checkpoints in Text-to-Music Generation Task"},{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"0b871a9f12e5c2da1b291a8b166c671256ebe1cd","title":"A Copy Mechanism for Handling Knowledge Base Elements in SPARQL Neural Machine Translation"},{"paperId":"e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models"},{"paperId":"4c6d9161b9e7d7ce8b9ed7a45bb32ef2067f7b5c","title":"GAMMT: Generative Ambiguity Modeling Using Multiple Transformers"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"95915aa592fdfc73f039c13472a21d3e4220f129","title":"On the Compositional Generalization Gap of In-Context Learning"},{"paperId":"9dab4c20648cd4c7c6830e6274a95294b014aac9","title":"QAmeleon: Multilingual QA with Only 5 Examples"},{"paperId":"ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","title":"Evaluating How Fine-tuning on Bimodal Data Effects Code Generation"},{"paperId":"965e409a3e7b5670d609837fac9823b160d6639c","title":"Logical Tasks for Measuring Extrapolation and Rule Comprehension"},{"paperId":"b15cddd33b36d1f38a8e59412026f6dfde0ca38d","title":"Calibrated Interpretation: Confidence Estimation in Semantic Parsing"},{"paperId":"a4bdc300db297756f36bedee2859b62df8e268c2","title":"Follow the Wisdom of the Crowd: Effective Text Generation via Minimum Bayes Risk Decoding"},{"paperId":"fa4188bcc6728b580699f714a8a6fe5fc60d7dfe","title":"Rethinking data-driven networking with foundation models: challenges and opportunities"},{"paperId":"048ed70192de0232086eb32a95ffb3be8d336c76","title":"Metaphors We Learn By"},{"paperId":"632ab7663e6d64578ceda1d1df9ec525b503bacb","title":"Steps towards prompt-based creation of virtual worlds"},{"paperId":"eca35805d185374befe4da48c9f96ace6e962fad","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"ac3d7ae8b4acb137492d4a8d8bcff480b89fa000","title":"Programming Pedagogy and Assessment in the Era of AI/ML: A Position Paper"},{"paperId":"2037eb819d08446a057634a851ebf9afb8d7ae4b","title":"Assessing the quality of GitHub copilot’s code generation"},{"paperId":"fdca1d3a04b7f6483f26a9e3059d646dd114712e","title":"A New Era of Plagiarism the Danger of Cheating Using AI"},{"paperId":"87bb0bc9195751351761b21c3adfa7d055a824ea","title":"Neural language models for code quality identification"},{"paperId":"71280dba5bda65c162f9deaffed7d3d20692ca0a","title":"SecurityEval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques"},{"paperId":"61ddf932488405ab1c7b275460d2b3c5dfa274a0","title":"Fixing Model Bugs with Natural Language Patches"},{"paperId":"4610ffb1b016acaa82a2065ffd1a3adbae1ce722","title":"Large Language Models Are Human-Level Prompt Engineers"},{"paperId":"2701d5b55ddacbdc82bc33901451f593a92127f1","title":"MPCFormer: fast, performant and private Transformer inference with MPC"},{"paperId":"5f55a5dcb3be8ee70fc1b7d3743a6baee3496eda","title":"Contrastive Code-Comment Pre-training"},{"paperId":"621009f1c30951b7c952c65c45ef0064a204e91e","title":"Early Experience with Transformer-Based Similarity Analysis for DataRaceBench"},{"paperId":"3214fbf2a78c5d1bff4d2a2e67e3f22a341df460","title":"Hybrid Rule-based and Machine Learning System for Assertion Generation from Natural Language Specifications"},{"paperId":"afd834af31f043e7c1d348c6a51d299d029dafca","title":"SPEAK YOUR MIND: INTRODUCING APTLY, THE SOFTWARE PLATFORM THAT TURNS IDEAS INTO WORKING APPS"},{"paperId":"22071947891c505d7a4879aa172f8e0fcd9f9f44","title":"A methodology for refined evaluation of neural code completion approaches"},{"paperId":"20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"83d6d867abe63a2e382050339ddf20e687177b34","title":"Emergent Linguistic Structures in Neural Networks are Fragile"},{"paperId":"1819c5c8c556db205ec9a49d5ba4a8c2afb28a4a","title":"Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy"},{"paperId":"6b8f26678785ebd7b7b27984af3cb9a273b722b0","title":"Towards Zero-Shot and Few-Shot Table Question Answering using GPT-3"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"f5b3cb14e0947c62b470d2072483481f14258738","title":"A Solvable Model of Neural Scaling Laws"},{"paperId":"cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers"},{"paperId":"99323bd786ee5be1e1aa589858e14e89630f207b","title":"Exploring the Learnability of Program Synthesizers by Novice Programmers"},{"paperId":"aac533156c274dfc6afecc03a1ccdd64cd5708bc","title":"Multi-Viewpoint and Multi-Evaluation with Felicitous Inductive Bias Boost Machine Abstract Reasoning Ability"},{"paperId":"66c52c6ef45cdd77c086996bcaaf01470e82dbd2","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?"},{"paperId":"dcf3caaf797f4496241c7375c0e8ede8cc5616f1","title":"Learning to Configure Computer Networks with Neural Algorithmic Reasoning"},{"paperId":"a593ce5d2a93f3113be7717d08d1ba8e62fd7ddf","title":"Benchmarking Language Models for Code Syntax Understanding"},{"paperId":"472d87be3dc298102e058be55a814cc6d2085b39","title":"Towards Deceptive Defense in Software Security with Chaff Bugs"},{"paperId":"5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","title":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming"},{"paperId":"f031ba42cf82f106200bb03fbb91dd5671a59b9c","title":"Practical Program Repair in the Era of Large Pre-trained Language Models"},{"paperId":"38e1a9c5599fc7597b7c5ffd37951ba5f528094c","title":"XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Semantic Parsing"},{"paperId":"5697a0ede5425954d48daa6e1893dc87bd7d8be7","title":"Contrastive Search Is What You Need For Neural Text Generation"},{"paperId":"075f83cac2742dbb36ee49d30f7aee2a322f3127","title":"Towards Better Few-Shot and Finetuning Performance with Forgetful Causal Language Models"},{"paperId":"152dc3042f5d4fc5a2686b5f4e0904f1e12a9207","title":"Code4Struct: Code Generation for Few-Shot Structured Prediction from Natural Language"},{"paperId":"dae5c4660dab4c2b0a0be586f8537db980925d4a","title":"When Can Transformers Ground and Compose: Insights from Compositional Generalization Benchmarks"},{"paperId":"aacf8a1bf2349b29159ab884ea82465330c2e256","title":"Combining OCL and natural language: a call for a community effort"},{"paperId":"4634362d75606287955260ef1788171286efbeaa","title":"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs"},{"paperId":"1acb761ec624104124eda4a8c681e59adf1bf2b9","title":"Formalizing Chemical Theory using the Lean Theorem Prover"},{"paperId":"a6ef5bad716091fb1888bf365f6129628ab3a5ee","title":"ObSynth: An Interactive Synthesis System for Generating Object Models from Natural Language Specifications"},{"paperId":"5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"4bef9d46209ac8988ea5ab83547149760d4af65e","title":"Automatic Document Selection for Efficient Encoder Pretraining"},{"paperId":"4c2534f9b03ac2f3810c07abc398a11bcf47258e","title":"Transformers Learn Shortcuts to Automata"},{"paperId":"8ffe7b7eeddbd4c22d642b0a48379d17e61c3bab","title":"Soft-Labeled Contrastive Pre-training for Function-level Code Representation"},{"paperId":"663a41c866d49ce052801fbc88947d39764cad29","title":"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"},{"paperId":"711d5e8ddbb840ad31a9ffa3d38590603ba69a92","title":"Prompting GPT-3 To Be Reliable"},{"paperId":"706b1e0c7dcc58ac6ba80a0ca37efc2993e6e5ef","title":"CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling"},{"paperId":"e66f0f822d4c4853b39b27daaafa2993005fd55e","title":"Large Language Models are few(1)-shot Table Reasoners"},{"paperId":"ce4fe7497b7c32e9ab804bc5989f8a6858843fcf","title":"Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods"},{"paperId":"39e40821b7207125e54e6ed7112e55cd38c6f0c3","title":"Language Models of Code are Few-Shot Commonsense Learners"},{"paperId":"3f2fa77ce40d21a7cf5e9a4dbf594c85a31576d0","title":"Searching for Better Database Queries in the Outputs of Semantic Parsers"},{"paperId":"de7d334a543d077f4162ebcd8da7eee843b7b10a","title":"Predictive Querying for Autoregressive Neural Sequence Models"},{"paperId":"c305ab1bdba79442bec72ec7f5c5ee7c49c2a566","title":"Visual Language Maps for Robot Navigation"},{"paperId":"045a6f92b58f08d5b305dd5d661316a506ee4d43","title":"Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation"},{"paperId":"285dcaad8a5da5159bf2550239f3365b5282bf05","title":"Next Syntactic-Unit Code Completion and Applications"},{"paperId":"825333b7efe2cade106eaf36c7e731f757974806","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot"},{"paperId":"0c78a473e33a81246d5c0fbbda7e7de168814c18","title":"FlexType: A Plug-and-Play Framework for Type Inference Models"},{"paperId":"dcff38de0e5fb47bdb31d472c21b0c2d88cbc4fc","title":"AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models"},{"paperId":"259b7a01700c39d5669e88d1434873ea38a13528","title":"In-Context Policy Iteration"},{"paperId":"90350aa626bed47b02d0c162462e5b0ca82be6b2","title":"Automatic Chain of Thought Prompting in Large Language Models"},{"paperId":"d35a6adf01f35a18f655234198a4778d3307487b","title":"GNM: A General Navigation Model to Drive Any Robot"},{"paperId":"62f0db3a5ad5c795ec18fc7a6e7b01836809df57","title":"Language Models are Multilingual Chain-of-Thought Reasoners"},{"paperId":"f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab","title":"Binding Language Models in Symbolic Languages"},{"paperId":"1afe2799df238ca749534860552501eaf51c77eb","title":"TgDLF2.0: Theory-guided deep-learning for electrical load forecasting via Transformer and transfer learning"},{"paperId":"55e3fe05598be7c3dd357d51166869f6571b824f","title":"Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors"},{"paperId":"689a25e8f4c0704f6215466aca87ced2aac8d6c4","title":"WISE: Wavelet Transformation for Boosting Transformers' Long Sequence Learning Ability"},{"paperId":"8f84dcbad8cd3b5b4d9229c56bc95f24be859a35","title":"Grounding Language with Visual Affordances over Unstructured Data"},{"paperId":"04db926687fa9af8f7e9be04901f440bea135da0","title":"Guiding the PLMs with Semantic Anchors as Intermediate Supervision: Towards Interpretable Semantic Parsing"},{"paperId":"7d6f17706cbcfcca55f08485bcbf8c82e00c9279","title":"Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals"},{"paperId":"5c4bece777bbc7c52cdd4bbb6e222163f6a580dd","title":"Unveiling the Black Box of PLMs with Semantic Anchors: Towards Interpretable Neural Semantic Parsing"},{"paperId":"ed99a2572fb5f4240aa6068e3bf274832e831306","title":"Recitation-Augmented Language Models"},{"paperId":"0b8772b7790c69f40897b5eb7f8fd57f24138f3d","title":"ContraGen: Effective Contrastive Learning For Causal Language Model"},{"paperId":"c88cafa3e980765a64febe369ceb7c2aa7261d2a","title":"Complexity-Based Prompting for Multi-Step Reasoning"},{"paperId":"8a854331c593c6a766fa3b8037fb2ad1b95a6f06","title":"An Empirical Study of Code Smells in Transformer-based Code Generation Techniques"},{"paperId":"7002faf46875c84cb3ad148f362118b46946f163","title":"CodeDSI: Differentiable Code Search"},{"paperId":"2a0456b0408cd4c33f2ff4400374e7be2497a362","title":"Repairing Bugs in Python Assignments Using Large Language Models"},{"paperId":"95fa2b27ab7eb84738441ee16da97323538938f9","title":"I Speak, You Verify: Toward Trustworthy Neural Program Synthesis"},{"paperId":"cd6496bc404e18a24f634e3dded2ed1cdca03e0f","title":"Learning to Learn with Generative Models of Neural Network Checkpoints"},{"paperId":"363758e9e296adc9391ed731e834809cf5d4c19b","title":"Are machine programming systems using right source-code measures to select code repositories?"},{"paperId":"e16b2de59f7397fec8eb3c6717abba5519cc055c","title":"A Practical Three-phase Approach To Fully Automated Programming Using System Decomposition And Coding Copilots"},{"paperId":"971e875e28f26240987d2c9470d1ee74ad204205","title":"Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction"},{"paperId":"c03fa01fbb9c77fe3d10609ba5f1dee33a723867","title":"ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"},{"paperId":"885676b8a05437868f7b83c134a99f991190a1de","title":"T5QL: Taming language models for SQL generation"},{"paperId":"7b2be89668e8f18ed20dc51be7646bd31cc0aff3","title":"Assisted Specification of Code Using Search"},{"paperId":"83d879a830ac4286945e628e670c30fefb1493c6","title":"NL2INTERFACE: Interactive Visualization Interface Generation from Natural Language Queries"},{"paperId":"a7435722d8ab595da5a9c70ac9160f57d0dcd75a","title":"Enabling Transformers to Understand Low-Level Programs"},{"paperId":"2e72aaf89aea0ee494c6020ff537dd074586311e","title":"Code as Policies: Language Model Programs for Embodied Control"},{"paperId":"cd11837054a3396542daca1b9966b03a057fbe9f","title":"Malicious Source Code Detection Using Transformer"},{"paperId":"e54a6201ea31e3c11576a343ff04cab3cbba8ccb","title":"Multi-donor Neural Transfer Learning for Genetic Programming"},{"paperId":"ef29fb8cc6bdda4b011288f51da521d3c25fc53d","title":"Learning to Prevent Profitless Neural Code Completion"},{"paperId":"7d2c43a4a9e3198cd133bb0ab86ca69a5125f2c5","title":"Exploring Code Style Transfer with Neural Networks"},{"paperId":"88dd119dba5ee747851ade8f5d517b381614d918","title":"Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence"},{"paperId":"ace4d199aa72ab0808af0f30a61fc16727c95dec","title":"AudioLM: a Language Modeling Approach to Audio Generation"},{"paperId":"9360390b02b9a09ece9a2486055b17e18dc5d3f6","title":"Automatic Code Documentation Generation Using GPT-3"},{"paperId":"9afab8dc694269d205d769eaea549d8f7558d776","title":"Exploring the Verifiability of Code Generated by GitHub Copilot"},{"paperId":"6b5d1e50894b1f28e4798cf20e9ffa88b9ec011a","title":"How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models"},{"paperId":"5581bf85386737bd3378eec68189759a05280bea","title":"FOLIO: Natural Language Reasoning with First-Order Logic"},{"paperId":"c2efc25fa1a2b0192b95dc92d9dccf90e602c74c","title":"Efficient Methods for Natural Language Processing: A Survey"},{"paperId":"0509c25103939d59ed4e27b1393e74ad5734c453","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot"},{"paperId":"befde3e07ce97f02f12fe92ab27e99f23ccd17aa","title":"Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation"},{"paperId":"1aac692ca061feb846cf32cd61a1d422f89593a1","title":"A Survey on Text-to-SQL Parsing: Concepts, Methods, and Future Directions"},{"paperId":"453a8fac3be9282be53908f0735160d0d21e0f48","title":"Repair Is Nearly Generation: Multilingual Program Repair with LLMs"},{"paperId":"a889a606734cd47f802765866487c677497a70d6","title":"Lost at C: A User Study on the Security Implications of Large Language Model Code Assistants"},{"paperId":"9b61de7038290751377b64293baaf42f3e7cf441","title":"An Empirical Evaluation of Competitive Programming AI: A Case Study of AlphaCode"},{"paperId":"1b4c19168410fb2690d285b205ab2281793db81a","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"99f85119f113b5498517928eff74a904b69e37b7","title":"CCTEST: Testing and Repairing Code Completion Systems"},{"paperId":"916bfe3378b2b1828b181477720957e3a6395b26","title":"CommitBART: A Large Pre-trained Model for GitHub Commits"},{"paperId":"512e16b9aef6ca6cb973d734b4cc66661ea33498","title":"Targeted Honeyword Generation with Language Models"},{"paperId":"53b3b15e8cbe2baf82cd0de3709e0a1e6f677415","title":"Limits of an AI program for solving college math problems"},{"paperId":"568eb10d17f1643228303670fe0f1d6608bd6f4d","title":"Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models"},{"paperId":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?"},{"paperId":"8372a1a98825116a000f26d47d9e2812152fbab1","title":"CodeBERT-nt: Code Naturalness via CodeBERT"},{"paperId":"8c2d9d2aa891e3b53bbd9166c1b804a0741fc44a","title":"Finding Reusable Machine Learning Components to Build Programming Language Processing Pipelines"},{"paperId":"7018ff49ca3b81f2ed6228b097a471c2529986e4","title":"CoditT5: Pretraining for Source Code and Natural Language Editing"},{"paperId":"def2e28863338cb20782eb2015a39d32df697ed6","title":"Learning to Improve Code Efficiency"},{"paperId":"c067664a45dce31411b3052c635c044ad4587db4","title":"Generating Diverse Code Explanations using the GPT-3 Large Language Model"},{"paperId":"654cd297b307c0c6fa08732511ba852b8dce1977","title":"Out of the BLEU: how should we assess quality of the Code Generation models?"},{"paperId":"15bacb240e2598457af4ded3039b6988aa9706f0","title":"Few-shot Adaptation Works with UnpredicTable Data"},{"paperId":"c5b47a34fd7e0e595360683b4effa51c0261d1e3","title":"Language Models Can Teach Themselves to Program Better"},{"paperId":"ec4c8d99eb1c028c43af6d8bbf727392d351cb59","title":"Efficient Training of Language Models to Fill in the Middle"},{"paperId":"dd112d4dbd4656223770989778f39700de3052bc","title":"A Hazard Analysis Framework for Code Synthesis Large Language Models"},{"paperId":"1e3ec5709b2ca43233c566cd77dbabbd6892bfa6","title":"Neurosymbolic repair for low-code formula languages"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"4f4326e69a1a34f68f7b14556077571a1752c319","title":"Artificial Intelligence and Deep Learning for Rheumatologists"},{"paperId":"14a797972c3a7045c8449f20d524234ffc36bf24","title":"Mimetic Models: Ethical Implications of AI that Acts Like You"},{"paperId":"e9fc39f56abbc6b8aed1e05496d985e70345a95a","title":"An extensive study on pre-trained models for program understanding and generation"},{"paperId":"fc994f026a04e4cfc8e24ee68994836700166421","title":"COBE: A Natural Language Code Search Robustness Benchmark"},{"paperId":"2f2750b48a6f958ff12cba90e99695123d1e2f47","title":"Using pre-trained language models to resolve textual and semantic merge conflicts (experience paper)"},{"paperId":"e37155d21818513bd40d64ee212099aac82bd6f8","title":"Less training, more repairing please: revisiting automated program repair via zero-shot learning"},{"paperId":"5d142f8b1dc67ec6307050d34dbcd6dfd4c0218c","title":"Active Data Pattern Extraction Attacks on Generative Language Models"},{"paperId":"75cb3efeb69e044cc07613a3eba64504483e999d","title":"Combing for Credentials: Active Pattern Extraction from Smart Reply"},{"paperId":"0a39442979d6e678dd36bb443ad529c14e86a86e","title":"DocPrompting: Generating Code by Retrieving the Docs"},{"paperId":"b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs"},{"paperId":"f3cf71c51b882fe3111d71c4bf104297d38197f8","title":"Inner Monologue: Embodied Reasoning through Planning with Language Models"},{"paperId":"142ebbf4760145f591166bde2564ac70c001e927","title":"Language Models (Mostly) Know What They Know"},{"paperId":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models"},{"paperId":"cdf54c147434c83a4a380916b6c1279b0ca19fc2","title":"LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action"},{"paperId":"53661ff6fdbfb8557c5b19895fad151792c62da7","title":"Few-shot training LLMs for project-specific code-summarization"},{"paperId":"cec43785ac6ede33cff208b6b828dc440cf43b2b","title":"Big Learning: A Universal Machine Learning Paradigm?"},{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"10bf4c1ca1531a49dae14d1226e53095306506ff","title":"LaMPost: Design and Evaluation of an AI-assisted Email Writing Prototype for Adults with Dyslexia"},{"paperId":"b17cc18e4130505b939f7d527082eb6be2a7fd5b","title":"Rationale-Augmented Ensembles in Language Models"},{"paperId":"f220cd8740627a39d78ecae109f30dbceffff1ea","title":"Natural Language Interface for Data Visualization with Deep Learning Based Language Models"},{"paperId":"61dd84f069f0329b1f3a84059be925cf7391140d","title":"Machine Learning for Aggregate Computing: a Research Roadmap"},{"paperId":"3624c75561695cddbdb03dd11598572532309352","title":"Code Translation with Compiler Representations"},{"paperId":"d6954c43aa1ca197319c45d3988bc8fcec3de976","title":"GitHub Copilot AI pair programmer: Asset or Liability?"},{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"1d160123cbbef972ea151a641dd435d57c727de8","title":"AixBench: A Code Generation Benchmark Dataset"},{"paperId":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code"},{"paperId":"f8f2b17083c10f730b711a938e2bb5da992086e7","title":"AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models"},{"paperId":"374bbb716aa007a65ac03f0220d3027fa724874d","title":"AI Challenges for Society and Ethics"},{"paperId":"f2c17758e74707d379b87372528221656d14b697","title":"Taxonomy of Risks posed by Language Models"},{"paperId":"e98799e709dc93a8ea721dd6b3e1398104797050","title":"Cracking the code: Co-coding with AI in creative programming education"},{"paperId":"660fde2f51e025638b8c937bf228ecaa5c5b649c","title":"XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence"},{"paperId":"2edc8efcda27c944a46f367acf6a5280b8f65525","title":"FixEval: Execution-based Evaluation of Program Fixes for Competitive Programming Problems"},{"paperId":"6df72c1b2972703e04ca509dd277169ad8eee594","title":"FixEval: Execution-based Evaluation of Program Fixes for Programming Problems"},{"paperId":"45263786d07f5751f7494fdeee3c8764836d02c4","title":"NatGen: generative pre-training by “naturalizing” source code"},{"paperId":"64c36adaeb803ff1d49771ed6a7ae7271e17b35d","title":"Training Discrete Deep Generative Models via Gapped Straight-Through Estimator"},{"paperId":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation"},{"paperId":"3ba793e937cb90ea3e82b4a6903ee4a95f307ddf","title":"X-Risk Analysis for AI Research"},{"paperId":"9aed848be4e9e401b0e61a2e5d60dbdafa0c6cc1","title":"A Dataset and Benchmark for Automatically Answering and Generating Machine Learning Final Exams"},{"paperId":"647d81055d281f038b89a684db8d9c011e2a9bc0","title":"STable: Table Generation Framework for Encoder-Decoder Models"},{"paperId":"a64871352f8ac7f8aa226fda5cce70251a18a4fc","title":"Assessing Project-Level Fine-Tuning of ML4SE Models"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"40edfa97cd02268fccff75eb9c693b11c1a968b2","title":"Formal Specifications from Natural Language"},{"paperId":"0d08ffccc982781e310bb184397bbe64b9aef157","title":"Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models"},{"paperId":"1100dee3fd78655cddc4b7bfaef1161351d4fab5","title":"Automated Feedback Generation for Competition-Level Code"},{"paperId":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code"},{"paperId":"8c7fd98b6bc4f32772e76471afe8babc323f10d3","title":"CitySpec: An Intelligent Assistant System for Requirement Specification in Smart Cities"},{"paperId":"f1b6b34b4440a77ba86493f7062e8974062508c5","title":"Applying genetic programming to PSB2: the next generation program synthesis benchmark suite"},{"paperId":"4f161f3cf6a272061600c71cc2e8a325753a38f0","title":"Attention Flows for General Transformers"},{"paperId":"35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs"},{"paperId":"7107d06366b48b3593c8128ed2ca67e0b413628c","title":"Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions"},{"paperId":"196cc546041cb6db167784f632037f0a1dcf4a79","title":"Generating Natural Language Proofs with Verifier-Guided Search"},{"paperId":"0efa0441da820b1905572666ba1974a06a9663fb","title":"NaturalProver: Grounded Mathematical Proof Generation with Language Models"},{"paperId":"4a4581003f56e8cb581ad6f383c037964765d3d5","title":"Active Programming by Example with a Natural Language Prior"},{"paperId":"047a8344e3cfa49c8354fc244387d57ef9d2f01d","title":"Memorization in NLP Fine-tuning Methods"},{"paperId":"8b7c11e773dc6ec32560d247193c9dd4c5109644","title":"Análise de Performance dos Modelos Gerais de Aprendizado de Máquina Pré-Treinados: BERT vs DistilBERT"},{"paperId":"8c90bfe05c06fd47eaec0f5b1662e06862572afe","title":"Looking for a Handsome Carpenter! Debiasing GPT-3 Job Advertisements"},{"paperId":"636f854b1a3a983e6803eae0277179596cc2cb95","title":"Summarize and Generate to Back-translate: Unsupervised Translation of Programming Languages"},{"paperId":"415be47b17f5214a1710010c7c18f4fafd3ef524","title":"AdaptivePaste: Code Adaptation through Learning Semantics-aware Variable Usage Representations"},{"paperId":"6074c7b75f27ca9adb6d74b080c07d5d079c3ea0","title":"Improving automatically generated code from Codex via Automated Program Repair"},{"paperId":"0bcd59da541fdae66884afba8d25475a54a9da1a","title":"Automated Repair of Programs from Large Language Models"},{"paperId":"cbcd19395e4b5ad5e047e0476cb906ca6461df72","title":"Few-Shot Natural Language Inference Generation with PDD: Prompt and Dynamic Demonstration"},{"paperId":"d6c5aab433d9871cabc01ffb1e5e1ea89141155b","title":"KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation"},{"paperId":"58aacb967cc7fc25cfc9d51b7ad3e57ac00d119b","title":"Can Foundation Models Wrangle Your Data?"},{"paperId":"227966c9b4fe75271946d239507196408842a2f2","title":"Transformer-based Program Synthesis for Low-Data Environments"},{"paperId":"2ff6f2b4a175cbc533795b723d6f18d64cea3916","title":"A CLIP-Hitchhiker's Guide to Long Video Retrieval"},{"paperId":"c61ce808818308566124df2c8725c98d6bd38dc3","title":"A Precis of Language Models are not Models of Language"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"5922f437512158970c417f4413bface021df5f78","title":"A Generalist Agent"},{"paperId":"11d77a7a56381d9700befa86f5c93cf887108ecd","title":"Editorial: Theory of Mind in Humans and in Machines"},{"paperId":"7ef9aafc68511afab5b287e62b754576ea37b4ce","title":"Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks"},{"paperId":"9a7d4c2f4309a2a38ea8e5b23c6d616fa0952d44","title":"Neural language models for network configuration: Opportunities and reality check"},{"paperId":"d072b46a0504ac023d5035d8ec0c7876151245c4","title":"Playing Games with Ais: The Limits of GPT-3 and Similar Large Language Models"},{"paperId":"cdfe9580f63070f311151444f9df32818cc858bf","title":"An Empirical Evaluation of GitHub Copilot's Code Suggestions"},{"paperId":"3f400fff93d5ff352ee102d2c04d38f6214b4283","title":"CodePanorama: a language agnostic tool for visual code inspection"},{"paperId":"1f3a97d255a6d0b2d5535af75737ed43b1e96fc4","title":"Is GitHub Copilot a Substitute for Human Pair-programming? An Empirical Study"},{"paperId":"a8c072f6cbd4653a1f0888b3529898f66c0e6bb0","title":"Strategies for Reuse and Sharing among Data Scientists in Software Teams"},{"paperId":"06ce4c0d92ed51a2b5e6e363a6d3932b8f162e72","title":"Can Information Behaviour Inform Machine Learning?"},{"paperId":"fc32083203ce124375af2a225296a26576306f6c","title":"Language & Coding Creativity"},{"paperId":"779fa63ae731a1efaf0cb95cab8e47672a919581","title":"I Do Not Think It Means What You Think It Means: Artificial Intelligence, Cognitive Work & Scale"},{"paperId":"e5cbcb665486ae7077dc3a37a9fd6addeabca5cf","title":"PROPR: Property-Based Automatic Program Repair"},{"paperId":"f8aa0cab09bc0668276e2cb5690bae47fa50d350","title":"An Initial Look at Self-Reprogramming Artificial Intelligence"},{"paperId":"f7820b52e3cdff6625e6bd0430a8d48ca66cca3f","title":"Self-Programming Artificial Intelligence Using Code-Generating Language Models"},{"paperId":"4bc040835fbff57ce6612306d794b8c6c8226086","title":"Discovering the Syntax and Strategies of Natural Language Programming with Generative Language Models"},{"paperId":"1fafaccebc4a74898a74c606f846318c4c2c7536","title":"On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"6050454e0446a3068617f73b0301453f3f67844d","title":"Stylette: Styling the Web with Natural Language"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"1f87dc41bdf2c4c78e2dce9c5c8adfef5e25a70c","title":"Passport: Improving Automated Formal Verification Using Identifiers"},{"paperId":"2ba7104f7b93d77940312664f3467b8f090d6d16","title":"On Distribution Shift in Learning-based Bug Detectors"},{"paperId":"07cd498aacfb4d39fa2e0e8d8a9c8ad881257300","title":"Prompt Engineering for Text-Based Generative Art"},{"paperId":"1676160139ca59c6728472f34092db69460567a8","title":"A Taxonomy of Prompt Modifiers for Text-To-Image Generation"},{"paperId":"bb7e46f316d319f9819c3554c99995ef8361ae9c","title":"CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex"},{"paperId":"012378718c34f0b17b3fcd7316371f8f4e4fdde2","title":"Addressing Leakage in Self-Supervised Contextualized Code Retrieval"},{"paperId":"a6b9a934fe039a5636a26e94fb47f872263d702c","title":"To What Extent do Deep Learning-based Code Recommenders Generate Predictions by Cloning Code from the Training Set?"},{"paperId":"9fcf3424eee7a607813980e114f1d9d2d3657960","title":"GLAD: Neural Predicate Synthesis to Repair Omission Faults"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"89e78d6f76b70c30804ecd3592fa05fccdc49b64","title":"Fix Bugs with Transformer through a Neural-Symbolic Edit Grammar"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"0286b2736a114198b25fb5553c671c33aed5d477","title":"Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback"},{"paperId":"ec229b004ebaee27b9efa84b834825dbc2937538","title":"Is GitHub's Copilot as Bad As Humans at Introducing Vulnerabilities in Code?"},{"paperId":"6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis"},{"paperId":"a225d5d846ba5110232ed5bb32d54ea742b1c2d4","title":"KNN-Diffusion: Image Generation via Large-Scale Retrieval"},{"paperId":"e2baf81813d5a515f554ee60bcddcc57548f8c22","title":"Code Search: A Survey of Techniques for Finding Code"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"ada81a4de88a6ce474df2e2446ad11fea480616e","title":"Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language"},{"paperId":"d358ce242dacfd2ea738aa538779f06c79777f2b","title":"SELFIES and the future of molecular string representations"},{"paperId":"237f5ca6fcccef2b77a2212b34fb06a1dbd09b72","title":"Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting"},{"paperId":"af0899b397d5cf5974e939e5c1057abd7c54cdde","title":"MQDD: Pre-training of Multimodal Question Duplicity Detection for Software Engineering Domain"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"365d0313908871118df59c469c4db10f4e1b96bd","title":"Linearizing Transformer with Key-Value Memory"},{"paperId":"fb5c11bbf63884f75d2da615fbf37a3bcfa2bd20","title":"Wordcraft: Story Writing With Large Language Models"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"a522543180db6c5b21f47fe88abee44de158c85c","title":"Automatic Programming and Education"},{"paperId":"c347093e2dca530ce347526380b0b7aedf03a6b2","title":"CrossBeam: Learning to Search in Bottom-Up Program Synthesis"},{"paperId":"1d10023541f06701bf2f9ae8c91609e1055799ec","title":"Automating code review activities by large-scale pre-training"},{"paperId":"b951c0691a0d2ca65202a1eed2ccedf6e305d035","title":"CodeReviewer: Pre-Training for Automating Code Review Activities"},{"paperId":"590f6817b42407f96b079e82c935fae298196359","title":"Less is More: Summary of Long Instructions is Better for Program Synthesis"},{"paperId":"d6b7bc4e7968398101d8d9b4da7d4a0186763ff3","title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages"},{"paperId":"b645e706651391eca1f692e7f560051c21b3dea4","title":"In-Context Learning for Few-Shot Dialogue State Tracking"},{"paperId":"51000d9f79be0eefd7972fe94e3c71dddc90d2c6","title":"Evaluating the Text-to-SQL Capabilities of Large Language Models"},{"paperId":"2b556fcf2ac634f03c1fb0ace5e602e829418e65","title":"ReACC: A Retrieval-Augmented Code Completion Framework"},{"paperId":"5dd7bc394e032eb0e982699a5f0c781fab9e3111","title":"Contrastive Visual Semantic Pretraining Magnifies the Semantics of Natural Language Representations"},{"paperId":"463c6944ddcb43280e1c9765c2245bc7b764ab68","title":"AI-Driven Development Is Here: Should You Worry?"},{"paperId":"1f11601f9eb8c0bee1d1d2cc7f4fa187e8c5e525","title":"The impact of lexical and grammatical processing on generating code from natural language"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"75b8444e1d82ebb6c6a32d8ea86af858e5de93e3","title":"Iterative Genetic Improvement: Scaling Stochastic Program Synthesis"},{"paperId":"1b45949be1b203f096d6451d88cf91174c620be7","title":"From Natural Language to Simulations: Applying GPT-3 Codex to Automate Simulation Modeling of Logistics Systems"},{"paperId":"76f023c3a819fc58989a064a1b50825b11fce95d","title":"Capturing Failures of Large Language Models via Human Cognitive Biases"},{"paperId":"4a6a65968a8eb8c09ffb57a7774ddabb596565b1","title":"COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics"},{"paperId":"d1bcb86bab0906bd631c4eb5ab81ce342b5927d0","title":"Open-Ended Knowledge Tracing"},{"paperId":"004d6ce718b0edb5b999f26710c5ae80b04bc900","title":"GPT-based Open-Ended Knowledge Tracing"},{"paperId":"b96cb62450cb77926c712d5cd50d1764101de605","title":"On the Implicit Bias Towards Minimal Depth of Deep Neural Networks"},{"paperId":"44d6c201a4056e260e9844bdbb01461ea9b1a011","title":"A data-driven approach for learning to control computers"},{"paperId":"6aae3ddbe142f7ae28f0f18bb6248dc7b3f41c00","title":"Probing Pretrained Models of Source Code"},{"paperId":"9cbc044e315cdefe9a255119037ac7c23e9abdd5","title":"Predictability and Surprise in Large Generative Models"},{"paperId":"28c7e583d90ccfc5c3078dfc1d6b80a9ad90248d","title":"Quantifying Memorization Across Neural Language Models"},{"paperId":"979eb5c97c49d7979447ed684500895a24d75ac4","title":"The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming"},{"paperId":"62d17b6f6ad77fd71ef9954c7784700d5e316f1f","title":"What Does it Mean for a Language Model to Preserve Privacy?"},{"paperId":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective"},{"paperId":"76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","title":"Investigating Explainability of Generative AI for Code through Scenario-based Design"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"5d49c7401c5f2337c4cc88d243ae39ed659afe64","title":"Red Teaming Language Models with Language Models"},{"paperId":"9ac5e1ffa8f837671a89354d4e298b1adeb08d79","title":"Enabling Automatic Repair of Source Code Vulnerabilities Using Data-Driven Methods"},{"paperId":"763792c655e591c5d61f67d7ac9cbecbcb5f4508","title":"Pop Quiz! Can a Large Language Model Help With Reverse Engineering?"},{"paperId":"0109c662c101723aea99e561937e3aca58563537","title":"Exploring Transformer Backbones for Heterogeneous Treatment Effect Estimation"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"ab18a9168b93def3145e118b6686e84241b39f42","title":"DeepRNG: Towards Deep Reinforcement Learning-Assisted Generative Testing of Software"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"7428f9b16a82839e2cb6e6c7a77c1ffeab898813","title":"HEAT: Hyperedge Attention Networks"},{"paperId":"03eeff98d24383518ce0dacc0b3c4a38b6f1a514","title":"Recursive Decoding: A Situated Cognition Approach to Compositional Generation in Grounded Language Understanding"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"6d7d4fca9840504f630e9bea6acaa07322a6e889","title":"Text and Code Embeddings by Contrastive Pre-Training"},{"paperId":"3d5463a16d85d9d1d0d8ebb4117a31aca3c240ff","title":"Fooling MOSS Detection with Pretrained Language Models"},{"paperId":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"},{"paperId":"2b5d234efd26e7377698cf16c901601a3d3c4e56","title":"CoAuthor: Designing a Human-AI Collaborative Writing Dataset for Exploring Language Model Capabilities"},{"paperId":"53c0abe83fe9b4fdaf2208295d8504fcf5241694","title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models"},{"paperId":"49c3f85573a3204c5e66317289e4cecfed50f38a","title":"Assemble Foundation Models for Automatic Code Summarization"},{"paperId":"7a54c01824a725a03f99411c8646d0a5674b2777","title":"Predictive synthesis of API-centric code"},{"paperId":"c0a98fca99e5f2d34330a15ab8ad4b9d692146d0","title":"The growing cost of deep learning for source code"},{"paperId":"f9838a3be5c94bb2674a0e224de349b50e18f3c4","title":"Learning To Retrieve Prompts for In-Context Learning"},{"paperId":"6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a","title":"Few-Shot Semantic Parsing with Language Models Trained on Code"},{"paperId":"52db8674337e5d86dcb96d013734befc8c3d4581","title":"Large Language Models are not Models of Natural Language: they are Corpus Models."},{"paperId":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis"},{"paperId":"ecb5a6fe2f5261e4e717ece1e82c464c63cb4862","title":"Controlling Conditional Language Models without Catastrophic Forgetting"},{"paperId":"8417d83e74a63b9d3874cd7695477ebad8b98f8a","title":"Choose your programming copilot: a comparison of the program synthesis performance of github copilot and genetic programming"},{"paperId":"231e768f0cd280faa0f725bb353262cb4fed08d1","title":"Hierarchical Transformers Are More Efficient Language Models"},{"paperId":"1cbb3d96242c3f47c3f40aada33616d0f5c07737","title":"Inductive Biases and Variable Creation in Self-Attention Mechanisms"},{"paperId":"a421ba0a9150cd35e231dddc323bdd9a59b3af93","title":"Coherence boosting: When your pretrained language model is not paying enough attention"},{"paperId":"1aed58bd07026492194672adec494dc37c894a28","title":"Leveraging Automated Unit Tests for Unsupervised Code Translation"},{"paperId":"c6bb04f3d8000b7e800f6359082de39548c7da79","title":"Capturing Structural Locality in Non-parametric Language Models"},{"paperId":"2672777d25562c9df6fc13b653181db62d39bece","title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"},{"paperId":"77d956cdab4508d569ae5741549b78e715fd0749","title":"TruthfulQA: Measuring How Models Mimic Human Falsehoods"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"a1e1297fb132d7769dda3f7917e57757e6e22605","title":"Impact of Evaluation Methodologies on Code Summarization"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"},{"paperId":"a63535ebbf90d0c51408252c23b85ffaf87f09ae","title":"Towards an AI Assistant for Power Grid Operators"},{"paperId":"75e36bb95023e55f7dec95d1af557e219ba3d349","title":"CORAL: COde RepresentAtion learning with weakly-supervised transformers for analyzing data analysis"},{"paperId":"239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning"},{"paperId":"56e6d62c638a24411f12d15cdc8821a31fc495c8","title":"Source Code Generation from Descriptions in a Natural Language"},{"paperId":"78863000eb70945cc8d791d45d4a3fe8a6521cb6","title":"Open-ended Knowledge Tracing for Computer Science Education"},{"paperId":"1c8e15f15d67c5974445634bb971e2275e957aff","title":"Security Implications of Large Language Model Code Assistants: A User Study"},{"paperId":"ba5d21b7c65c6598c7bd39a5d992308c205df374","title":"A S YSTEMATIC E VALUATION OF L ARGE L ANGUAGE M ODELS OF C ODE"},{"paperId":"b2c4fdb49bdb23e6ad0ac3272029324157046ea7","title":"Automated Repair of Code from Language Models"},{"paperId":"25c402db512d327f1da143de3b8e797ad6fbfe5b","title":"P ROG P ROMPT : Generating Situated Robot Task Plans using Large Language Models"},{"paperId":"660ca9e15e19409903a0605f0584d0f263c35c67","title":"S YNCHROMESH : R ELIABLE C ODE G ENERATION FROM P RE - TRAINED L ANGUAGE M ODELS"},{"paperId":"4797e960f8e7a2b47ae0d95c7071ef84fa5d4b5b","title":"Code Generation Using Machine Learning: A Systematic Review"},{"paperId":"15ef2d1b88f54fa32a32927463a7116219b89529","title":"SUPEROPTIMIZE REAL-WORLD PROGRAMS"},{"paperId":"a2b6e1f7d8a7963d321f29fca7c01eeb1ebd7f0f","title":"P ATCH G ENERATION WITH L ANGUAGE M ODELS : F EASIBILITY AND S CALING B EHAVIOR"},{"paperId":"8a4299a61cc44b60e5be32fef35341c3bc7b2a0d","title":"The Hole Story: Type-Directed Synthesis and Repair"},{"paperId":"ab8a53be2ff8a756776aa0245ad8da41189d60d6","title":"Fuzzing Deep-Learning Libraries via Large Language Models"},{"paperId":"60043104ca33a1fc905af57ead32768e52c69103","title":"C3PO: A Lightweight Copying Mechanism for Translating Pseudocode to Code"},{"paperId":"ae10c4b220a0bc0999bf169d5c219086d1f1aeed","title":"Edinburgh Research Explorer Taxonomy of risks posed by language models"},{"paperId":"9cd0cb3af7c2215eae9afdcf500a2bcd5330aae3","title":"20Q: Overlap-Free World Knowledge Benchmark for Language Models"},{"paperId":"51256ee5425d5c425b84e7fac011775d8eff0d1c","title":"An Empirical Analysis of Memorization in Fine-tuned Autoregressive Language Models"},{"paperId":"95a2ee5aeccf2883f904ee3fcd7369adeb176359","title":"Probing Pretrained Models of Source Codes"},{"paperId":"a85c5d7272371345e28a9910080224cad799972e","title":"Schema Matching using Pre-Trained Language Models"},{"paperId":"f9d38e03c97562b5f5942f3a0c43bdb751b9dc1c","title":"Wordplay 2022 The 3rd Wordplay: When Language Meets Games Workshop Proceedings of the Workshop"},{"paperId":"b98d6fe8f0ef02ec0d1bb2bcfb924c8f01feb7d4","title":"Convergent Representations of Computer Programs in Human and Artificial Neural Networks"},{"paperId":"e002bb8dae5a18a5ea1e7e1aafa16e19ad545662","title":"Solving Math Word Problems with Process-based and Outcome-based Feedback"},{"paperId":"0180d35b85dd4daead90e0652b64b1339e754684","title":"Assistance with large language models"},{"paperId":"cd155729180ea707dea251f8e9654db241ffd808","title":"Is GPT-3 all you need for machine learning for chemistry?"},{"paperId":"a1ef81e17a9ca41e09aba802040a2eca2744716f","title":"Generation Probabilities are Not Enough: Improving Error Highlighting for AI Code Suggestions"},{"paperId":"b69b84706fe84c4c614e4473760c57dffbfeb9a0","title":"Waveformer: Linear-Time Attention with Forward and Backward Wavelet Transform"},{"paperId":"15e767aa26a14455da95a2b2f11e3d59f2c250f6","title":"Autoformalization for Neural Theorem Proving"},{"paperId":"2443179d421e1faf7474add557b45add554723c7","title":"Formal Premise Selection With Language Models"},{"paperId":"ddab94478a7647ee136b1f6b5076417db3074d0f","title":"Machine Programming: Turning Data into Programmer Productivity"},{"paperId":"9bf75110ea0923bbed49256b5491f1ec284019ec","title":"From BERT to GPT-3 Codex: Harnessing the Potential of Very Large Language Models for Data Management"},{"paperId":"91260f73dd179487fb16713deb8267634ae14716","title":"CodexDB: Synthesizing Code for Query Processing from Natural Language Instructions using GPT-3 Codex"},{"paperId":"3b0cf543a730e674d4213d344ebc857fada76ead","title":"Understanding High-Level Properties of Low-Level Programs Through Transformers"},{"paperId":"09e14c4c80e20e80c052e0adb0d49df51aff718d","title":"Yes, You Can Make an App Too: A Systematic Study of Prompt Engineering in the Automatic Generation of Mobile Applications from User Queries"},{"paperId":"b562be15b076b494023b8ac24fc8c459f4fdf80a","title":"Craft an Iron Sword: Dynamically Generating Interactive Game Characters by Prompting Large Language Models Tuned on Code"},{"paperId":null,"title":"Materials ExerciseTeacher Student Attempt Feedback"},{"paperId":"24c6982a25c0114bc98805d368b06d1a4f6d8fd5","title":"Understanding AI alignment research: A Systematic Analysis"},{"paperId":"f01e316d3b28ccecda25b4d57926f496a9b17d3d","title":"How Robust are Neural Code Completion Models to Source Code Transformation?"},{"paperId":"6f84d0cc33c7c58f74b28ddcc1cbda91ea608c9f","title":"C ODE S UMMARIZATION : D O T RANSFORMERS R E ALLY U C"},{"paperId":"2e5b29457ff45b8faba69bc2eaf05521584a7bec","title":"B UG F IX G ENERATION USING G RAPH T RANS"},{"paperId":"57c31c709792949bfbb9d4aaee941048aa07cc4b","title":"How to Give Imperfect Automated Guidance to Learners: A Case-Study in Workplace Learning"},{"paperId":"bee0be592c314435048599281bcd9c72bf63b735","title":"CueBot: Cue-Controlled Response Generation for Assistive Interaction Usages"},{"paperId":"8a4dce5735a101ff8f64c2b676afb8c24950a5d8","title":"Zero-shot Mathematical Problem Solving via Generative Pre-trained Transformers"},{"paperId":"75c8f2916a2067f7549cf58ea8c9061565eb1dab","title":"C ROSS B EAM : L EARNING TO S EARCH IN B OTTOM -U P P ROGRAM S YNTHESIS"},{"paperId":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"DECOMPOSITION IN NEURAL PROGRAM SYNTHESIS"},{"paperId":"a3564f3cf954c05844c757505325a50b4d858e22","title":"Generating Infrastructure-as-Code from Natural Language: Evaluating Fine-Tuned GPT-3 Models for Cloud Infrastructure Provisioning"},{"paperId":"78fd8185c5cd55830c31aa718a9909827e20774e","title":"A Research Agenda for Assessing the Economic Impacts of Code Generation Models"},{"paperId":"856d2c0f7b3f80dcf1f68d1dc0dcbf5c6fe5679a","title":"Interpreting docstrings without using common sense: the private science of very large language models∗"},{"paperId":"1b94afca9d6688cc584a744734126473283cbc93","title":"Can Transformers be Strong Treatment Effect Estimators?"},{"paperId":"58dd9a3da16c10f5c3cdbb9760a9ff378847bf76","title":"Self-supervision of wearable sensors time-series data for influenza detection"},{"paperId":"6ccc0ca964ddab19705e4832758e6a2447325348","title":"End to End Software Engineering Research"},{"paperId":"091fa84bdc07dcb22a34060c3996d8c58d71cd20","title":"Towards Neural Functional Program Evaluation"},{"paperId":"ee042a3e299a32c413532e64603de8d3ddb6aa87","title":"Automap: Towards Ergonomic Automated Parallelism for ML Models"},{"paperId":"827a67bbb96c8ed34c0f79e2ea811c5b53a6896b","title":"Controllable Response Generation for Assistive Use-cases"},{"paperId":"a5731122200fbb8b37f048010a1e1ca4474aa606","title":"Examining Zero-Shot Vulnerability Repair with Large Language Models"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"21ab011a3adccbd912aea58f76b84b7873c41df3","title":"Machines&Influence: An Information Systems Lens"},{"paperId":"cecc913290736a5a368642c5b59a130eddd1fa7b","title":"Can Pre-trained Language Models be Used to Resolve Textual and Semantic Merge Conflicts?"},{"paperId":"04db9b694280134f09af5fa787a306907edba29d","title":"How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN"},{"paperId":"f7987fa2aadc0b368c185dc4d2fdb1337a202c32","title":"Solving Probability and Statistics Problems by Program Synthesis"},{"paperId":"4e40595d6ecba027cebb4f2e3b43ae44bcf51daf","title":"Solving Linear Algebra by Program Synthesis"},{"paperId":"1444536496d8064f33e10b38b5820fecfab5b367","title":"Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs"},{"paperId":"c23d9d44e8bc68408cea9f305d1f24d915bc0d0d","title":"Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey"},{"paperId":"9f260bdd4030af5297a9c1cbb817c75701ac8c83","title":"The 5th Recognizing Families in the Wild Data Challenge: Predicting Kinship from Faces"},{"paperId":"570a6a5b8ec2827c3f33bb1b1bd027190a0d3e07","title":"Neural Program Generation Modulo Static Analysis"},{"paperId":"6a269b1abccdbf57e79b3f115a97bff14b435ad9","title":"Automated Support for Unit Test Generation: A Tutorial Book Chapter"},{"paperId":"21e8e76386aaaa00e0971af70ce84a8a544e1aa1","title":"Cascaded Fast and Slow Models for Efficient Semantic Code Search"},{"paperId":"dace03e57056d736f9e24937bdf486e894f8e866","title":"Is neural machine translation approach accurate enough for coding assistance?"},{"paperId":"8091e51ebbcd2424a1c5b50c036bae5295090525","title":"Top 3 in FG 2021 Families In the Wild Kinship Verification Challenge"},{"paperId":"21bc4ead8ea415579ab40e437fcbc274929f17c8","title":"Solving the Families In the Wild Kinship Verification Challenge by Program Synthesis"},{"paperId":"9991bb2eb7e7d7e9d831e257ae77ba2eaeaba3dc","title":"Applying quantum approximate optimization to the heterogeneous vehicle routing problem"},{"paperId":"360e0197378799d890f473893cc0c773b8182b4e","title":"Searching for Replacement Classes"},{"paperId":"24e775b20adf21e9b5b95c6a9b7a5c164d055849","title":"M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining"},{"paperId":"6c2d43e71e240e354b5790a38da78a291ceffe7c","title":"Learning to Superoptimize Real-world Programs"},{"paperId":"05c2e1ee203be217f100d2da05bdcc52004f00b6","title":"Unsolved Problems in ML Safety"},{"paperId":"bc9598dc4ed0472d8b59b87ed3a139f8347d40ee","title":"Towards A Measure Of General Machine Intelligence"},{"paperId":"b8b21c2ddcd7d1c23d7ccfceabb63cb1a05bcfca","title":"HYDRA - Hyper Dependency Representation Attentions"},{"paperId":"a8863de15a5ee8eed98107f423138a1a8f5a2ba8","title":"Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a176b0de62840f7118006277d94bbc1547162a4d","title":"Learning to Synthesize Programs as Interpretable and Generalizable Policies"},{"paperId":"70087677fd1a6309829b42968934575d05a95f92","title":"What do pre-trained code models know about code?"},{"paperId":"4885e616e85d420576196b2578525cbc501137ec","title":"Programming and execution models for next generation code intelligence systems (keynote)"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"5436193122dff271796bca07df7cecb7a8d6dea6","title":"Natural language-guided programming"},{"paperId":"26450917d41c828b470ec8818d49f59516a5b9c0","title":"Towards Universality in Multilingual Text Rewriting"},{"paperId":"58a6ca2ae28a618126f71a07262cb958a8c37904","title":"Latent Execution for Neural Program Synthesis"},{"paperId":"98485ce6532d69f34a8ec67de6b09a39532bd221","title":"Communicating Natural Programs to Humans and Machines"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"09279dc8018a8131e11d527cebb06d0a43c67cff","title":"Creativity and Machine Learning: A Survey"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"4f278ab5ad629267e06196e273252262854c1c57","title":"BF++: a language for general-purpose program synthesis"},{"paperId":"021bbcefc993c389bad6c1daefd8ff92d0fc2441","title":"Contrastive Code Representation Learning"},{"paperId":"5ff9032d0f7f246d01ae7b2c231ab03469a7344a","title":"Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?"},{"paperId":"3f97c2067cde9377e50b3160bbd7982c94abd88a","title":"An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions"},{"paperId":"4da830b6d84e117cb147ff71f205e71500ebbbb1","title":"Machines and Influence"},{"paperId":"007153d786caa906255fba2ca265fd67994f8b44","title":"Tracking Blobs in the Turbulent Edge Plasma of Tokamak Fusion Reactors"},{"paperId":"a3c2b81d8bb5ac69771ed7830d009310d98ac9dc","title":"A First Approach to AGI-based Robot Task Planning"},{"paperId":"d66e80224cda0c1d5a4c1be3798df6a6bfe3713c","title":"GPT-3 for Few-Shot Dialogue State Tracking"},{"paperId":"b874faa9c6cfb5d7e87e3d79650007ade1394958","title":"Creating new Program Proofs by Combining Abductive and Deductive Reasoning"},{"paperId":"bab6893ee48d168d27c227c3b0867f6d471fbea8","title":"Language Models are not Models of Language"},{"paperId":"4ff14a0580f3b36a9564ec18a51d2ce1d4eafebc","title":"Learning Methods for Solving Astronomy Course Problems"},{"paperId":"8a9e437b2e2d813b402ac560c852ef0ab2f1cd3c","title":"Recognizing Families In the Wild (RFIW): The 4th Edition"}],"references":[{"paperId":"113fba4c88e2eb74f49544a161ef70e9745e969a","title":"Dataset Security for Machine Learning: Data Poisoning, Backdoor Attacks, and Defenses"},{"paperId":"fc963363bcddfc1aeca07d44d7a9d0e53485662d","title":"Atlas of AI: power, politics, and the planetary costs of artificial intelligence."},{"paperId":"722ad6ac92286507437b31486f47987d6ece05c9","title":"BEiT: BERT Pre-Training of Image Transformers"},{"paperId":"06510ee15cabae97647ceb647dce6f5a1820d524","title":"Women’s Participation in Open Source Software: A Survey of the Literature"},{"paperId":"4a160efbe80c38cd5eb2f92c7c095b49b113397d","title":"In-IDE Code Generation from Natural Language: Promise and Challenges"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"79b8ef3905a42b771248719495a2117271906445","title":"Carbon Emissions and Large Neural Network Training"},{"paperId":"3a863672b072a37ac8d2eb6b780234f6b748a2d9","title":"Generating bug-fixes using pretrained transformers"},{"paperId":"49f905eb03958c7cfae52ac759ea8978b8b2a6ea","title":"Alignment of Language Agents"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","title":"Zero-Shot Text-to-Image Generation"},{"paperId":"56fa0b9cba4d9aee5ccc327365b3b3a721031c69","title":"Calibrate Before Use: Improving Few-Shot Performance of Language Models"},{"paperId":"4c2733d191e347753bb28afa46a1c55c65e085be","title":"Persistent Anti-Muslim Bias in Large Language Models"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"021bbcefc993c389bad6c1daefd8ff92d0fc2441","title":"Contrastive Code Representation Learning"},{"paperId":"1ec69f1a1a9d4ff5c5bc70db0e5087157b620570","title":"You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion"},{"paperId":"05f5f8b2065a520846d89771ebaea2bb1534e9c6","title":"DeBERTa: Decoding-enhanced BERT with Disentangled Attention"},{"paperId":"18a93dc1558bf9d7534d0b416633cebaf75c1145","title":"Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/ kingoflolz/mesh-transformer-jax"},{"paperId":null,"title":"A first look at rote learning in github copilot suggestions., Jun 2021. URL https://docs.github.com/en/ github/copilot/research-recitation"},{"paperId":"72730829313a9a907a875da6e8ab66af92ce62b2","title":"Multimodal Neural Script Knowledge Models"},{"paperId":null,"title":"Learning autocompletion from realworld datasets"},{"paperId":null,"title":"Supplemental Bias Analysis Generative models have been shown to encode bias in modalities such as natural language (Brown et al., 2020; Blodgett et al., 2020) and images (Radford et al., 2021)"},{"paperId":null,"title":"15-1252.00 -software developers"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"15a60130ea9cb4a00e0dac6d11a368894974cedf","title":"Unit Test Case Generation with Transformers and Focal Context"},{"paperId":"053b1d7b97eb2c91fc3921d589c160b0923c70b1","title":"Learning to summarize from human feedback"},{"paperId":"bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8","title":"Generative Pretraining From Pixels"},{"paperId":"49a049dc85e2380dde80501a984878341dd8efdf","title":"wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"d47a682723f710395454687319bb55635e653105","title":"Language (Technology) is Power: A Critical Survey of “Bias” in NLP"},{"paperId":"737189319865d7a363e38d957a8845c519953556","title":"SourceFinder: Finding Malware Source-Code from Publicly Available Repositories"},{"paperId":"4c7183fb2109271405e4a0fe23b5e827520a9f68","title":"Backstabber’s Knife Collection: A Review of Open Source Software Supply Chain Attacks"},{"paperId":"67dea28495cab71703993d0d52ca4733b9a66077","title":"Jukebox: A Generative Model for Music"},{"paperId":"70082b428b794d1d5e47c04ed4c3167a75aed22f","title":"Recalibrating global data center energy-use estimates"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"be9aa6c68d4df56a55444b15a83ff1e62c869bb5","title":"Robots and Jobs: Evidence from US Labor Markets"},{"paperId":null,"title":"Language models are few-shot"},{"paperId":null,"title":"formers is to fine-tune large pre-trained models with curated or human-generated datasets of the desired behavior (e.g., Raffel et al"},{"paperId":null,"title":"Working in public: the making and maintenance of open source software"},{"paperId":null,"title":"2020) for an analysis of conventional language models"},{"paperId":"210b796c904e5e262388bfb166bf7396a82d29bf","title":"What distinguishes great software engineers?"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"e8955a546425ef14a66da848f0bd174a33179373","title":"Automatic programming: The open issue?"},{"paperId":"75acc731bdd2b626edc74672a30da3bc51010ae8","title":"CTRL: A Conditional Transformer Language Model for Controllable Generation"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"433fcb584902e716ee25a44175e380267616b54e","title":"Learning Compositional Neural Programs with Recursive Tree Search and Planning"},{"paperId":"21da617a0f79aabf94272107184606cefe90ab75","title":"Generating Long Sequences with Sparse Transformers"},{"paperId":"79af328616d2440c77449d038f72d053c64d8f1f","title":"Unified rational protein engineering with sequence-based deep representation learning"},{"paperId":"e1799998d9e0932617aee2949167b040ebc8bec7","title":"The Wrong Kind of Ai? Artificial Intelligence and the Future of Labor Demand"},{"paperId":"516497e7fc39ebb126bed28dcc5c6129e500acb8","title":"An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation"},{"paperId":"7d3ab2a839b077a318022f7842225db55033b2c3","title":"Generating High Fidelity Images with Subscale Pixel Networks and Multidimensional Upscaling"},{"paperId":"98d1307bed619b58b4a44acd8e65ac58495776c2","title":"code2seq: Generating Sequences from Structured Representations of Code"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"Improving the standard risk matrix: Part 1. 2019"},{"paperId":null,"title":"Search-based pseudocode to code"},{"paperId":null,"title":"Comment regarding request for comments on intellectual property protection for artificial intelligence innovation. Before the United States Patent and Trademark Office Department of Commerce"},{"paperId":"b227f3e4c0dc96e5ac5426b85485a70f2175a205","title":"Representation Learning with Contrastive Predictive Coding"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"bec0b38f0083bd351d25daf8b31e6e049c376f94","title":"Improving Neural Program Synthesis with Inferred Execution Traces"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":null,"title":"Clarifying ”ai alignment"},{"paperId":null,"title":"Open-sourcing gvisor, a sandboxed container runtime, 2018"},{"paperId":null,"title":"Protecting applications with automated software diversity, Sep 2018"},{"paperId":"75f06defa60e069b863853afc6ac6f5feaca9450","title":"On the difficulty of benchmarking inductive program synthesis methods"},{"paperId":"3807a517403cfc7ed67fdfea64baf62b9948bcfe","title":"A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"3ff0af64279929a952ee340e645256b7e0580f65","title":"RobustFill: Neural Program Learning under Noisy I/O"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":null,"title":"A6:2017-security misconfiguration"},{"paperId":null,"title":"A syntactic neural model for generalpurpose code generation"},{"paperId":null,"title":"The trouble with bias"},{"paperId":"784ee73d5363c711118f784428d1ab89f019daa5","title":"Hybrid computing using a neural network with dynamic external memory"},{"paperId":"df0402517a7338ae28bc54acaac400de6b456a46","title":"WaveNet: A Generative Model for Raw Audio"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"41f1d50c85d3180476c4c7b3eea121278b0d8474","title":"Pixel Recurrent Neural Networks"},{"paperId":"b59d91e0699d4e1896a15bae13fd180bdaf77ea5","title":"Neural Programmer-Interpreters"},{"paperId":"4aa9f5150b46320f534de4747a2dd0cd7f3fe292","title":"Semi-supervised Sequence Learning"},{"paperId":"23f9a5ec68f4e947d168e2a8dfc99771ab3e81f2","title":"General Program Synthesis Benchmark Suite"},{"paperId":"193136b86539bd6df3f57a3685629c049a037418","title":"An analysis of patch plausibility and correctness for generate-and-validate patch generation systems"},{"paperId":"ac39029550eda5efbc6df754d74ec0e378eea4a8","title":"Representation Learning"},{"paperId":"0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a","title":"Learning to Execute"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"215999a0e155a21255e4655d4eac312858058a84","title":"Structured Generative Models of Natural Source Code"},{"paperId":"d1e8e4e952d6d9b49dd4c5d4fed9dedb22ed9946","title":"Temporal Logics for Hyperproperties"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"304f7fd4af9cc82355eaf912a8f120f7429362de","title":"Spreadsheet data manipulation using examples"},{"paperId":"546d3b90ef0e4ab5f4a61dcda59184bc951672ec","title":"A systematic study of automated program repair: Fixing 55 out of 105 bugs for $8 each"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"56dd0149688e9d196ddc4c833f864e6ee85e81ac","title":"The Economics of Software Quality"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"dd892c577eb45f3237c1e4d2513b1674cbc2043c","title":"BugFix: A learning-based tool to assist developers in fixing bugs"},{"paperId":null,"title":"Cwe-780: Use of rsa algorithm without oaep"},{"paperId":null,"title":"Use of a broken or risky cryptographic algorithm"},{"paperId":"bc876b7d314af5507fb52ffdb9279c83bae20035","title":"The technology trap"},{"paperId":"92b286e00a26f059c278a66d00e087c01306648f","title":"The economic impacts of inadequate infrastructure for software testing"},{"paperId":"3da678cf38575a81e0634d564919da3865b12fc6","title":"USENIX Association"},{"paperId":null,"title":"Online; accessed 29-June-2000"},{"paperId":"0361f0a1b710d4101d190b56c85e1d07cd96bae5","title":"Genetic Programming III - Darwinian Invention and Problem Solving"},{"paperId":"19b3b95f3dc0a3bf1b22e5f08df918bcc0079486","title":"Application of Dynamic Slicing in Program Debugging"},{"paperId":"83721103a6fd5535e943b1b575cf70862c2322a8","title":"Handbook of Applied Cryptography"},{"paperId":"028af662ebf7203017714e2d257faed94a566961","title":"Fault localization using execution slices and dataflow tests"},{"paperId":"94d9cbfc474cb6ce961de45a06233b2853ca6724","title":"Toward automatic program synthesis"},{"paperId":"eb8d64c0f34610597cc02d31ec10d5d6bcd6d39c","title":"Experiments with a Heuristic Compiler"},{"paperId":null,"title":"Fun and dystopia with ai-based code generation using gpt-j-6b"}],"id":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","summary":"It is found that repeated sampling from the GPT language model is a surprisingly effective strategy for producing working solutions to difﬁcult prompts, and the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics are discussed."},{"url":"https://www.semanticscholar.org/paper/b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code","venue":"MAPS@PLDI","year":2022,"referenceCount":36,"citationCount":75,"influentialCitationCount":8,"publicationDate":"26/02/2022","authors":"Frank F. Xu,Uri Alon,Graham Neubig,V. Hellendoorn","citations":[{"paperId":"1c00f88b69f1d4efe3e1695ca49aa38a1340bd98","title":"Improving Few-Shot Prompts with Relevant Static Analysis Products"},{"paperId":"8a2a2006e371ce07e0b29538cd11337a5c977caf","title":"“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models"},{"paperId":"eb55442d940accc67d6790e66d3196e1941fc7a4","title":"Teaching Large Language Models to Self-Debug"},{"paperId":"64e20f2abc15d5cf04a682df5a1265bd45ba9fe7","title":"Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling"},{"paperId":"312bb6f461095306e18a2b35fcc08af165d9b057","title":"DiverseVul: A New Vulnerable Source Code Dataset for Deep Learning Based Vulnerability Detection"},{"paperId":"a845bce29105f55dab1e47d2f92d7338eb183a4e","title":"Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT"},{"paperId":"3e7055980da1853d196fcee0d3b7f390fb518d62","title":"A Survey on Automated Program Repair Techniques"},{"paperId":"cbdc75e9c80870164662bb0359e23fd4736c5f0e","title":"A Survey of Large Language Models"},{"paperId":"f24dabf3317abaa3d7393ab7d48115f353749bde","title":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X"},{"paperId":"dca09e13fe9b4f46b1e10b966f3ede63f63f9489","title":"Understanding the Usability of AI Programming Assistants"},{"paperId":"28dafb94a87daa71ad3edf9f04f3c8c32753398b","title":"Improving Code Generation by Training with Natural Language Feedback"},{"paperId":"a938fcc877d181e3d9a64e5de0a1421ef461bb58","title":"Unified Text Structuralization with Instruction-tuned Language Models"},{"paperId":"645960e8bcb2a55bc7e8816c49e60e8f98303acb","title":"Combining Contexts from Multiple Sources for Documentation-Specific Code Example Generation"},{"paperId":"c3ba52534031d61cbb19f97cf2ae1fc010378bb6","title":"Generative AI Assistants in Software Development Education"},{"paperId":"443d898928eb8e32d2e6f8f287beaa63f5b00eb9","title":"JaCoText: A Pretrained Model for Java Code-Text Generation"},{"paperId":"af622206f1e5a6632600075897385cdabcae463d","title":"Large Language Models and Simple, Stupid Bugs"},{"paperId":"7857d80f516a943f96757b21053f7635ac1cf2af","title":"Revisiting the Plastic Surgery Hypothesis via Large Language Models"},{"paperId":"8205612a6d15df52b8c3d26aabfd50abc10fdee3","title":"LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations"},{"paperId":"35d083ff4dc2805c477d0aedc8158fc7a1aefeda","title":"EvoPrompting: Language Models for Code-Level Neural Architecture Search"},{"paperId":"1786a2f9140ed7211b21302977de64e948b92308","title":"Learning Performance-Improving Code Edits"},{"paperId":"535a3d95a743e1f4b591b5b2af3e778a6347158a","title":"CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code"},{"paperId":"79680e20f0c8038039b243fb5fd385fbe967c799","title":"Controlling Large Language Models to Generate Secure and Vulnerable Code"},{"paperId":"e77d4cdc8e2d3f1aea84aafcf427a19d1fdef04c","title":"Shrinking the Inductive Programming Search Space with Instruction Subsets"},{"paperId":"7677e9ead35f8f2578acbf5ad15fb330c83762c4","title":"Systematically Finding Security Vulnerabilities in Black-Box Code Generation Models"},{"paperId":"cd0988714ea326642d2b1bb18753e187fec71e42","title":"A Categorical Archive of ChatGPT Failures"},{"paperId":"6248474933664013e5b0615dc474a7f6de5e97f4","title":"LExecutor: Learning-Guided Execution"},{"paperId":"907a77639069bb7dd270f017068745706133cffc","title":"Inaccessible Neural Language Models Could Reinvigorate Linguistic Nativism"},{"paperId":"fba0b0817dbc8200b41a1de22654b54b778a11e9","title":"Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models"},{"paperId":"1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a","title":"SantaCoder: don't reach for the stars!"},{"paperId":"41f5e1ad7793593befc0b9c38f756836e8b07c98","title":"Code4ML: a Large-scale Dataset of annotated Machine Learning Code"},{"paperId":"6032212d5790b6a580d68d469a9895aad6238c89","title":"Diverse Title Generation for Stack Overflow Posts with Multiple Sampling Enhanced Transformer"},{"paperId":"69e330037afd18e5546fdcacbc9a9f7deb69bba9","title":"Memorization and Generalization in Neural Code Intelligence Models"},{"paperId":"66476832701361c9f9b2a7eb2354ee8cd9f72e67","title":"Large Language Models are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models"},{"paperId":"433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context"},{"paperId":"bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence"},{"paperId":"8210cef990b8e5cddbc95000e46309bdd25337f7","title":"Asking Clarification Questions for Code Generation in General-Purpose Programming Language"},{"paperId":"6756fcd998caeb7b23702e08559e63710179334c","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"6bc87e51018d6de55011e95a0d43c588dd44a1e8","title":"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages"},{"paperId":"ebd4bec684808aff360b5f255d15c0d112ba13d3","title":"Explicit Knowledge Transfer for Weakly-Supervised Code Generation"},{"paperId":"20b60fb3993d2e9a5af04611f7bdf248e5a3a736","title":"Programming by Example and Text-to-Code Translation for Conversational Code Generation"},{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","title":"Evaluating How Fine-tuning on Bimodal Data Effects Code Generation"},{"paperId":"cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers"},{"paperId":"2577d053f8aab912d29b424e1f09133d83740fd2","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"39e40821b7207125e54e6ed7112e55cd38c6f0c3","title":"Language Models of Code are Few-Shot Commonsense Learners"},{"paperId":"2cfd0dacfa267a64a23392332c358d4e3ec6fbd4","title":"The COVID That Wasn’t: Counterfactual Journalism Using GPT"},{"paperId":"0c78a473e33a81246d5c0fbbda7e7de168814c18","title":"FlexType: A Plug-and-Play Framework for Type Inference Models"},{"paperId":"2e72aaf89aea0ee494c6020ff537dd074586311e","title":"Code as Policies: Language Model Programs for Embodied Control"},{"paperId":"e5993b3afe6384b5e6f90093989773ad1f868f71","title":"Towards Top-Down Deep Code Generation in Limited Scopes"},{"paperId":"1b4c19168410fb2690d285b205ab2281793db81a","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?"},{"paperId":"1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs"},{"paperId":"0a39442979d6e678dd36bb443ad529c14e86a86e","title":"DocPrompting: Generating Code by Retrieving the Docs"},{"paperId":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code"},{"paperId":"82d9f1db6db43cb61fe4b0b26a489a2e72628675","title":"A Test for Evaluating Performance in Human-Computer Systems"},{"paperId":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code"},{"paperId":"35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs"},{"paperId":"58aacb967cc7fc25cfc9d51b7ad3e57ac00d119b","title":"Can Foundation Models Wrangle Your Data?"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"ec229b004ebaee27b9efa84b834825dbc2937538","title":"Is GitHub's Copilot as Bad As Humans at Introducing Vulnerabilities in Code?"},{"paperId":"3d5463a16d85d9d1d0d8ebb4117a31aca3c240ff","title":"Fooling MOSS Detection with Pretrained Language Models"},{"paperId":"ab8a53be2ff8a756776aa0245ad8da41189d60d6","title":"Fuzzing Deep-Learning Libraries via Large Language Models"},{"paperId":"239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning"},{"paperId":"cdec75f901a93c75ee5386a98abbe44746286e80","title":"Delivering Fairness in Human Resources AI: Mutual Information to the Rescue"},{"paperId":"a2b6e1f7d8a7963d321f29fca7c01eeb1ebd7f0f","title":"P ATCH G ENERATION WITH L ANGUAGE M ODELS : F EASIBILITY AND S CALING B EHAVIOR"},{"paperId":"a5731122200fbb8b37f048010a1e1ca4474aa606","title":"Examining Zero-Shot Vulnerability Repair with Large Language Models"},{"paperId":"5ff9032d0f7f246d01ae7b2c231ab03469a7344a","title":"Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?"}],"references":[{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"c0a98fca99e5f2d34330a15ab8ad4b9d692146d0","title":"The growing cost of deep learning for source code"},{"paperId":"92d211c65ae8c8006c07d34dfa0587e278d0ac00","title":"An Empirical Study of C++ Vulnerabilities in Crowd-Sourced Code Examples"},{"paperId":null,"title":"Natural Language Processing with Transformers"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"feba0c47bf12a02c3a725174bb53df78658a72a8","title":"Pre-Trained Models: Past, Present and Future"},{"paperId":"7571ed4cf1bbdcf891b576a0da12c910b1f0c72f","title":"Concealed Data Poisoning Attacks on NLP Models"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"05e396e79a2f88f0b8f8d99f5ab36ab3efa95c14","title":"Language-Agnostic Representation Learning of Source Code from Structure and Context"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"3944354c42ddfff7414ad06022f96c72858d5fa6","title":"Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"db392858262b17aa9c8ff8659738f68fbf832ebe","title":"Structural Language Models of Code"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"d170bd486e4c0fe82601e322b0e9e0dde63ab299","title":"Adaptive Input Representations for Neural Language Modeling"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"b4a26011eb7c26d71cd074f182c4f093da24308b","title":"Are deep neural networks the best choice for modeling source code?"},{"paperId":"022788ecf8685ed30e3f69f9121b5a3d242a22d6","title":"On the naturalness of software"},{"paperId":"2579e826f22da988fbc1b5d545aa59ab68bde4f3","title":"Program Synthesis Using Natural Language"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"6c2b28f9354f667cd5bd07afc0471d8334430da7","title":"A Neural Probabilistic Language Model"}],"id":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","summary":"This work finds that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language modeling, and identifies an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code."},{"url":"https://www.semanticscholar.org/paper/7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":14,"influentialCitationCount":0,"publicationDate":"02/06/2022","authors":"Patrick Bareiss,Beatriz Souza,Marcelo d’Amorim,Michael Pradel","citations":[{"paperId":"1c00f88b69f1d4efe3e1695ca49aa38a1340bd98","title":"Improving Few-Shot Prompts with Relevant Static Analysis Products"},{"paperId":"3b01b5b497e1f359b11da45af029281ee6f64c2c","title":"Towards Enhancing In-Context Learning for Code Generation"},{"paperId":"0fbdb0de7859fb04f88dcd7315aa36aba5bd1fb0","title":"Vulnerability Mimicking Mutants"},{"paperId":"383157f9059061faba4364c7d2141d0ff78c87fa","title":"Learning Deep Semantics for Test Completion"},{"paperId":"6876fc47f4674fdf4f53208254b45dedc6de3755","title":"Adaptive Test Generation Using a Large Language Model"},{"paperId":"7677e9ead35f8f2578acbf5ad15fb330c83762c4","title":"Systematically Finding Security Vulnerabilities in Black-Box Code Generation Models"},{"paperId":"6248474933664013e5b0615dc474a7f6de5e97f4","title":"LExecutor: Learning-Guided Execution"},{"paperId":"0885556b71b24a641b4ffe139afd4d2712228cff","title":"Beware of the Unexpected: Bimodal Taint Analysis"},{"paperId":"fba0b0817dbc8200b41a1de22654b54b778a11e9","title":"Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models"},{"paperId":"66c52c6ef45cdd77c086996bcaaf01470e82dbd2","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?"},{"paperId":"317208b423d24d52ba04221cfb46956962364e22","title":"Extracting Meaningful Attention on Source Code: An Empirical Study of Developer and Neural Model Code Exploration"},{"paperId":"45a37f351bb275d22354b712c78df65715a37cc5","title":"CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code"},{"paperId":"453a8fac3be9282be53908f0735160d0d21e0f48","title":"Repair Is Nearly Generation: Multilingual Program Repair with LLMs"},{"paperId":"205ac1373eb7981aca2d08f2ab651871a001271e","title":"CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code"}],"references":[{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"4413f763efcd34498ec1c4439a9c4c9395689024","title":"µBert: Mutation Testing using Pre-Trained Language Models"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"7cbc2a7843411a1768ab762930707af0a3c33a19","title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"},{"paperId":"c0a98fca99e5f2d34330a15ab8ad4b9d692146d0","title":"The growing cost of deep learning for source code"},{"paperId":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis"},{"paperId":"26f1c97c9e38ed0da329e3197ed554553b305d18","title":"Neural software analysis"},{"paperId":"209333bef25395800997a7411c905757baf38553","title":"MeMo: Automatically identifying metamorphic relations in Javadoc comments for test automation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"c2c7233293d55f201fe5b496234bed1914eea70e","title":"Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"1311adebe439ccd9e373d594a8df1d720a685190","title":"Code Prediction by Feeding Trees to Transformers"},{"paperId":"49cf6a22a5dac5bc98b653534af65ffa0bc0e76d","title":"Multi-task Learning based Pre-trained Language Model for Code Completion"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"27724bd19946d6a824d06cdca3cdfe5d40f71003","title":"A structural model for contextual code changes"},{"paperId":"9a4bfc410eb7efac0557025fdb00503eb5a2adc0","title":"Learning semantic program embeddings with graph interval neural network"},{"paperId":"08066c80919620397e8e4e5372ff84caf401e675","title":"Global Relational Models of Source Code"},{"paperId":"6acf0d7b24a93953269a4d52c6b2f0c7d11f9ae4","title":"Hoppity: Learning Graph Transformations to Detect and Fix Bugs in Programs"},{"paperId":"29fc7ff02e68537a86d32978bfd2e8e45258e77d","title":"LambdaNet: Probabilistic Type Inference using Graph Neural Networks"},{"paperId":"4ddb9135fc6fdf3f9e01fdfcd115fdbf6f7486b4","title":"Sequence Model Design for Code Completion in the Modern IDE"},{"paperId":"0dfe706526e5234338411489e1826b4060acc4e8","title":"CC2Vec: Distributed Representations of Code Changes"},{"paperId":"0a94b2f15b6f8730538e4b1550bbfe29ede63163","title":"Metamorphic Testing: A New Approach for Generating Next Test Cases"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"a429c6c8407b10a89c9610ab5e9682ec89772356","title":"TypeWriter: neural type prediction with search-based validation"},{"paperId":"deabe3945e100ec42c80582652aa149c2e1b66b0","title":"Automatic Software Repair: a Bibliography"},{"paperId":"b5abecf78c1bd975449e34350289a904e6ef35cc","title":"Automated program repair"},{"paperId":"1432c8378b1cafa3f91f09fa743082d154fdab92","title":"A Novel Neural Source Code Representation Based on Abstract Syntax Tree"},{"paperId":"d2c7cbd578878ff0064857240426e605c2e959d2","title":"On the Effectiveness of Manual and Automatic Unit Test Generation: Ten Years Later"},{"paperId":"8e71af3fbec666e67c0fcfefedb38881027a0254","title":"NL2Type: Inferring JavaScript Function Types from Natural Language Information"},{"paperId":"d02285385fd2b53a6100b8d62421bba6693b788c","title":"When Code Completion Fails: A Case Study on Real-World Completions"},{"paperId":"98d1307bed619b58b4a44acd8e65ac58495776c2","title":"code2seq: Generating Sequences from Structured Representations of Code"},{"paperId":"73bade9162bc4df6ca6c9ab8a7eb2d54d35acad2","title":"code2vec: learning distributed representations of code"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"d461ab9482b7fb5eadd7e6cd2c6dffced8ede8b8","title":"Chapter Six - Mutation Testing Advances: An Analysis and Survey"},{"paperId":"ea1b62a03ab16c4acca6f7041c6f096bedec244b","title":"Deep learning type inference"},{"paperId":"06fedb27a62aac78dda4f852e284d09d264844f8","title":"Translating code comments to procedure specifications"},{"paperId":"8b1aa4727eb2a83db1bd3ae54e078b0b7ce5eccb","title":"Retrieval on source code: a neural code search"},{"paperId":"e033a0b29af2939fd44e5765d03380c08897a9e8","title":"Deep Code Search"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"554c9b3aa8c3b7504810270b18b942a0d22156e7","title":"DeepFix: Fixing Common C Language Errors by Deep Learning"},{"paperId":"a60bbe22c11454661ab56dbbd1ceff1e83d94dea","title":"Automatic generation of oracles for exceptional behaviors"},{"paperId":"10d5cee2e9e3a5c7df8548f95eb7791f9d79d626","title":"The major mutation framework: efficient and scalable mutation analysis for Java"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"1fab8830baef8f353dce995c0269a32fd3105f64","title":"An orchestrated survey of methodologies for automated software test case generation"},{"paperId":"216b98bb3d9221d5f5d261864975612e4d0faaa6","title":"EvoSuite: automatic test suite generation for object-oriented software"},{"paperId":"494fbdd61eb5d9fc3b0d7c4b0e557d9b4996fae6","title":"An Analysis and Survey of the Development of Mutation Testing"},{"paperId":"ec316064d25c1f1dadc32a8570b56049aa109f78","title":"Inferring Resource Specifications from Natural Language API Documentation"},{"paperId":"af5a7be7bfc592e400e2f12a24ad3e6a5a44b58f","title":"Learning from examples to improve code completion systems"},{"paperId":"6dc7017e357767dbc210bcb27080260fb0816701","title":"Feedback-Directed Random Test Generation"},{"paperId":"208a35b667ac0b17333f593f9c68cc8d8603df1e","title":"JCrasher: an automatic robustness tester for Java"}],"id":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","summary":"This paper studies to what extent a state-of-the-art, pre-trained language model of code, Codex, may serve this purpose, and concludes that few-shot language models are surprisingly effective."},{"url":"https://www.semanticscholar.org/paper/8210cef990b8e5cddbc95000e46309bdd25337f7","title":"Asking Clarification Questions for Code Generation in General-Purpose Programming Language","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Haau-Sing Li,Mohsen Mesgar,André F. T. Martins,Iryna Gurevych","citations":[],"references":[{"paperId":"1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization"},{"paperId":"0a39442979d6e678dd36bb443ad529c14e86a86e","title":"DocPrompting: Generating Code by Retrieving the Docs"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"906f9b3a5cd1912158528c7c6bd2c94b76c791bf","title":"Generating Clarifying Questions for Query Refinement in Source Code Search"},{"paperId":"f2a5af2e2fd7fabaefbb21ca211a500bca7cf6b7","title":"Pseudo Ambiguous and Clarifying Questions Based on Sentence Structures Toward Clarifying Question Answering System"},{"paperId":null,"title":"Natural Language Processing with Transformers: Building Language Applications with Hugging Face"},{"paperId":null,"title":"Competition-level code generation"},{"paperId":"267d9a093ae7e8388fd1e25d2b5e4cfe91c71226","title":"Building and Evaluating Open-Domain Dialogue Corpora with Clarifying Questions"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"c56aced0f0c5cfebefadb530cb08d736c3ac5c05","title":"Retrieval Augmented Code Generation and Summarization"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"b255a1aeed73c236fc1ae209743e32e805795168","title":"Ask what’s missing and what’s useful: Improving Clarification Question Generation using Global Knowledge"},{"paperId":"6e2b1038682cd116b2e38bec19b5721196c41eea","title":"HAConvGNN: Hierarchical Attention Based Convolutional Graph Neural Network for Code Documentation Generation in Jupyter Notebooks"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"6714dff86284cbde6be351e3e1f8ddee1bfadb9c","title":"A Toolkit for Generating Code Knowledge Graphs"},{"paperId":"1af678b040ce638aedf8b582212937f0921ccc1d","title":"Abg-CoQA: Clarifying Ambiguity in Conversational Question Answering"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"77910e51a40d17157fc798325d06edfa6cff18d6","title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation"},{"paperId":"42e863f93203d37a2518da381beaf06e4c70fb3d","title":"Stanza: A Python Natural Language Processing Toolkit for Many Human Languages"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"9cb32bdd43f64b36cb447ba1307869c5d8bf675c","title":"YAKE! Keyword extraction from single documents using multiple local features"},{"paperId":"517f770a38c6f09b1d0ee03793887912a844e69e","title":"Asking Clarification Questions in Knowledge-Based Question Answering"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"4a75598125bd0be3600d840aec04de40bf03c33b","title":"Asking Clarifying Questions in Open-Domain Information-Seeking Conversations"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"SentenceBERT: Sentence embeddings using Siamese BERTnetworks"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"7aca31a3bf370ad63af2eefa8e1fb146f7988eb0","title":"Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"3ade4d3be53981a1678b1e3a736d01547f7d3b9e","title":"Dialog for Language to Code"},{"paperId":"3807a517403cfc7ed67fdfea64baf62b9948bcfe","title":"A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation"},{"paperId":null,"title":"Asian Federation of Natural Language Processing"},{"paperId":"b5defb8729294614d40e0247eabfdc483bf1a992","title":"Bimodal Modelling of Source Code and Natural Language"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"d2071c1e4a6030dc0005dbfeefdd196a8b293e84","title":"Okapi at TREC-3"},{"paperId":null,"title":"Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language model for code with multi"},{"paperId":null,"title":"Curtis von Veh, Madanlal Musuvathi"}],"id":"8210cef990b8e5cddbc95000e46309bdd25337f7","summary":"The empirical results support the hypothesis that clariﬁcations result in more precise generated code, as shown by an improve-ment of 17.52 in BLEU, 12.72 in CodeBLEU, and 7.7% in the exact match."},{"url":"https://www.semanticscholar.org/paper/9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":8,"influentialCitationCount":0,"publicationDate":"26/06/2022","authors":"Disha Shrivastava,H. Larochelle,Daniel Tarlow","citations":[{"paperId":"fbb5cc9e8a46f61d1543bd17b6eca324fd5bf55c","title":"Fully Autonomous Programming with Large Language Models"},{"paperId":"1c00f88b69f1d4efe3e1695ca49aa38a1340bd98","title":"Improving Few-Shot Prompts with Relevant Static Analysis Products"},{"paperId":"8a2a2006e371ce07e0b29538cd11337a5c977caf","title":"“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models"},{"paperId":"349ec9ca2ff17dbc1aa1caf27f98b7cd85a4747e","title":"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation"},{"paperId":"433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"317208b423d24d52ba04221cfb46956962364e22","title":"Extracting Meaningful Attention on Source Code: An Empirical Study of Developer and Neural Model Code Exploration"},{"paperId":"0a39442979d6e678dd36bb443ad529c14e86a86e","title":"DocPrompting: Generating Code by Retrieving the Docs"}],"references":[{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"4bc040835fbff57ce6612306d794b8c6c8226086","title":"Discovering the Syntax and Strategies of Natural Language Programming with Generative Language Models"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"0e802c0739771acf70e60d59c2df51cd7e8c50c0","title":"Memorizing Transformers"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","title":"Investigating Explainability of Generative AI for Code through Scenario-based Design"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"0271d1dbc01eda68c2f0291c62a956fca3092864","title":"PRIMERA: Pyramid-based Masked Sentence Pre-training for Multi-document Summarization"},{"paperId":"c6bb04f3d8000b7e800f6359082de39548c7da79","title":"Capturing Structural Locality in Non-parametric Language Models"},{"paperId":"59641c10ed7431a3cf841f308367dc2dc0281b74","title":"What Makes Good In-Context Examples for GPT-3?"},{"paperId":"7bbfe2586d10d56081915a9edc44be2d29bbf8dc","title":"CodeTrek: Flexible Modeling of Code using an Extensible Relational Representation"},{"paperId":null,"title":"Natural Language Processing with Transformers"},{"paperId":null,"title":"A generalist agent, 2022"},{"paperId":null,"title":"Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"2ee03e28208a9310a9be4032c2b04ebdddb83cc7","title":"FLEX: Unifying Evaluation for Few-Shot NLP"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"209f9bde2dee7cf1677801586562ffe56d435d38","title":"Learning How to Ask: Querying LMs with Mixtures of Soft Prompts"},{"paperId":"a711a240ccf37b68b86e97c46aaed3d2cd256f75","title":"Learning to Generate Code Comments from Class Hierarchies"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"ac3cdb50606f7770eef8e4cd951840a4f71287a0","title":"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"},{"paperId":"85e7d63f75c0916bd350a229e040c5fbb1472e7a","title":"Making Pre-trained Language Models Better Few-shot Learners"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"8ae9a17c87a4518b513e860683a0ef7824be994d","title":"Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference"},{"paperId":"18a93dc1558bf9d7534d0b416633cebaf75c1145","title":"Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"2934145c4ab42bfee022067b1b6b213acd836a2a","title":"On-the-Fly Adaptation of Source Code Models using Meta-Learning"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"7be8c119dbe065c52125ee7716601751f3116844","title":"Generalization through Memorization: Nearest Neighbor Language Models"},{"paperId":null,"title":"Auto- Prompt: Eliciting knowledge from language models with automatically generated prompts"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"b4a26011eb7c26d71cd074f182c4f093da24308b","title":"Are deep neural networks the best choice for modeling source code?"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"34f25a8704614163c4095b3ee2fc969b60de4698","title":"Dropout: a simple way to prevent neural networks from overfitting"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"},{"paperId":null,"title":"For the example shown in Figure 1 of the main paper"},{"paperId":null,"title":"Type Identifiers (TI): Type Identifiers define the type of an identifier. For example, in the code snippet class DPAffinityPropagation extends AffinityPropagation"},{"paperId":null,"title":"Type Identifiers (TI): Take all the type identifiers used in the rule context location"},{"paperId":null,"title":"Method Names and Bodies (MNB): Take all the method names along with their signatures and corresponding bodies used in the rule context location"},{"paperId":null,"title":"This was done by splitting the filenames based on either camel-case or underscore. If the sub-parts of two files match"},{"paperId":null,"title":"Identifiers (I): Identifiers are the names of variables used in the code. For example, for the rule context taken from the imported file shown in Figure 1 in the main paper"}],"id":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","summary":"This work proposes a framework called Repo-Level Prompt Generator that learns to generate example-speciﬁc prompts using prompt proposals, and demonstrates that an oracle constructed from these prompt proposals gives a remarkably high relative improvement over Codex, showing the quality of these proposals."},{"url":"https://www.semanticscholar.org/paper/b3a54332a0791751fcf234f6264f242c42ac00d2","title":"DocPrompting: Generating Code by Retrieving the Docs","venue":"","year":2022,"referenceCount":46,"citationCount":7,"influentialCitationCount":0,"publicationDate":"13/07/2022","authors":"Shuyan Zhou,Uri Alon,Frank F. Xu,Zhiruo Wang,Zhengbao Jiang,Graham Neubig","citations":[{"paperId":"01f9b773408115a16fe872147348db175789e82f","title":"Tool Learning with Foundation Models"},{"paperId":"645960e8bcb2a55bc7e8816c49e60e8f98303acb","title":"Combining Contexts from Multiple Sources for Documentation-Specific Code Example Generation"},{"paperId":"052a5e2bcc999810ee6f1eedcf758c528e4f125f","title":"Retrieving Multimodal Information for Augmented Generation: A Survey"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"8fdd34153d1035d09dd4a6efa9cb0c91d23d0045","title":"More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models"},{"paperId":"535a3d95a743e1f4b591b5b2af3e778a6347158a","title":"CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code"},{"paperId":"8210cef990b8e5cddbc95000e46309bdd25337f7","title":"Asking Clarification Questions for Code Generation in General-Purpose Programming Language"}],"references":[{"paperId":null,"title":"Human-annotated unit tests Following Chen et al"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"0e802c0739771acf70e60d59c2df51cd7e8c50c0","title":"Memorizing Transformers"},{"paperId":"1f11601f9eb8c0bee1d1d2cc7f4fa187e8c5e525","title":"The impact of lexical and grammatical processing on generating code from natural language"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"a0e92f6e9564b8c38b6649ae71b892ddfb988faa","title":"Controllable Semantic Parsing via Retrieval Augmentation"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"c56aced0f0c5cfebefadb530cb08d736c3ac5c05","title":"Retrieval Augmented Code Generation and Summarization"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"c26759e6c701201af2f62f7ee4eb68742b5bf085","title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"bde0c85ed3d61de2a8874ddad70497b3d68bc8ad","title":"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"},{"paperId":"2cbb8de53759e75411bc528518947a3094fbce3a","title":"Billion-Scale Similarity Search with GPUs"},{"paperId":"45793225c6593db60e9efd95bce1d70bf4844198","title":"Evaluation Paradigms in Question Answering"},{"paperId":"58ed1fbaabe027345f7bb3a6312d41c5aac63e22","title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"paperId":"77910e51a40d17157fc798325d06edfa6cff18d6","title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation"},{"paperId":"79cd9f77e5258f62c0e15d11534aea6393ef73fe","title":"Dense Passage Retrieval for Open-Domain Question Answering"},{"paperId":"34733eaf66007516347a40ad5d9bbe1cc9dacb6b","title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"832fff14d2ed50eb7969c4c4b976c35776548f56","title":"REALM: Retrieval-Augmented Language Model Pre-Training"},{"paperId":"3c9fa66674e1053eb9de12076c48f13e46422c29","title":"CLAI: A Platform for AI Skills on the Command Line"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2b88aefb70cd52a4d6899020f4be97c669a5edcb","title":"RTFM: Generalising to Novel Environment Dynamics via Reading"},{"paperId":"db392858262b17aa9c8ff8659738f68fbf832ebe","title":"Structural Language Models of Code"},{"paperId":"37ea01066a563c661587b7c3f50fbf64d1bf311a","title":"Accelerating Large-Scale Inference with Anisotropic Vector Quantization"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"75c0d369c7151b925155cfe1b3f01dd7d0503981","title":"Generative Code Modeling with Graphs"},{"paperId":"f6335a158a3f4fcfffe74f2df1d55d835bf95095","title":"A Retrieve-and-Edit Framework for Predicting Structured Outputs"},{"paperId":"d3e13d2514edaf74b863bfbe45a739c32a7689e1","title":"Retrieval-Based Neural Code Generation"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"429b65937d4922578a81e1f0ef5aeab7361ae36b","title":"NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System"},{"paperId":"cd93ed9bab97d8999a790e6d7e7cd46fff76ce36","title":"Data-Driven Program Completion"},{"paperId":"2c1e874c3b67510a3215e535f5646b362de5bc89","title":"Abstract Syntax Networks for Code Generation and Semantic Parsing"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"b5defb8729294614d40e0247eabfdc483bf1a992","title":"Bimodal Modelling of Source Code and Natural Language"},{"paperId":"3170bb1affffeddfe9d3262c9a0787c0b29cb18a","title":"How do professional developers comprehend software?"},{"paperId":"99f1921a8ba5ceecb18e5ca7ff19b7f95c4e250d","title":"Learning to Win by Reading Manuals in a Monte-Carlo Framework"},{"paperId":"d744c7cf1f5e9e77c7de55db8df5a918ee1f41d7","title":"How software engineers use documentation: the state of the practice"},{"paperId":"3046cf5c122149b80915ce5bb99153d9cbd0366e","title":"The relevance of software documentation, tools and technologies: a survey"},{"paperId":"660f2447218bdf884fb39f2802383824c575dc43","title":"What programmers really want: results of a needs assessment for SDK documentation"},{"paperId":"f6e3e57567e9803718623ec088cd7fea65cfbc9d","title":"Relevance weighting of search terms"}],"id":"b3a54332a0791751fcf234f6264f242c42ac00d2","summary":"DocPrompting is a natural-language-to-code generation approach that explicitly leverages code documentation by retrieving the relevant documentation pieces given a natural language (NL) intent, and generating code based on the NL intent and the retrieved documentation."},{"url":"https://www.semanticscholar.org/paper/1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":46,"citationCount":96,"influentialCitationCount":20,"publicationDate":"20/05/2021","authors":"Dan Hendrycks,Steven Basart,Saurav Kadavath,Mantas Mazeika,Akul Arora,Ethan Guo,Collin Burns,Samir Puranik,Horace He,D. Song,J. Steinhardt","citations":[{"paperId":"b5009d25120434bd9fc400f4204a55d57f42cfba","title":"Stochastic Code Generation"},{"paperId":"8a2a2006e371ce07e0b29538cd11337a5c977caf","title":"“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models"},{"paperId":"2d51e79d541bb489b07aa4fa691a93a9d4a0498b","title":"Evaluating AIGC Detectors on Code Content"},{"paperId":"eb55442d940accc67d6790e66d3196e1941fc7a4","title":"Teaching Large Language Models to Self-Debug"},{"paperId":"cbdc75e9c80870164662bb0359e23fd4736c5f0e","title":"A Survey of Large Language Models"},{"paperId":"f24dabf3317abaa3d7393ab7d48115f353749bde","title":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X"},{"paperId":"349ec9ca2ff17dbc1aa1caf27f98b7cd85a4747e","title":"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation"},{"paperId":"362cbfd0d05e139cd6cf049754098a6e1520b910","title":"PanGu-{\\Sigma}: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing"},{"paperId":"fbdd496c421e050a47c4fb2e0019635d2f4b97e7","title":"Meet in the Middle: A New Pre-training Paradigm"},{"paperId":"a7396ba28087cab3c4ad46928fe73e9760e605b0","title":"Boosting Source Code Learning with Data Augmentation: An Empirical Study"},{"paperId":"8d03ebc375b3d887056e769018467bf0a6ed99e3","title":"Planning with Large Language Models for Code Generation"},{"paperId":"9b5de3f83649c347b2b2b9fdecb142fbca293f57","title":"Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference"},{"paperId":"8475c1d1d98932111b194dc139c8f8b30818725d","title":"xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"eb619cbfb34127f28f1cab03eb22234267f892ea","title":"PAC Prediction Sets for Large Language Models of Code"},{"paperId":"3f83582c08a62e5bd02398fafc93f7eaf1e4b84e","title":"Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints"},{"paperId":"0cf694b8f85ab2e11d45595de211a15cfbadcd22","title":"Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks"},{"paperId":"68edfd62d2619fb4af7c2469edb95b9e2fe4544a","title":"Execution-based Code Generation using Deep Reinforcement Learning"},{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"},{"paperId":"1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a","title":"SantaCoder: don't reach for the stars!"},{"paperId":"6032212d5790b6a580d68d469a9895aad6238c89","title":"Diverse Title Generation for Stack Overflow Posts with Multiple Sampling Enhanced Transformer"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"ae441f7305dc2cd58c708528b3ecee3501cc5c46","title":"Plansformer: Generating Symbolic Plans using Transformers"},{"paperId":"9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming"},{"paperId":"27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation"},{"paperId":"c192fb33f308f19ac8a5c4c2d623d56385b839be","title":"Systematic Literature Review on Solving Competitive Programming Problem with Artificial Intelligence (AI)"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models"},{"paperId":"20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers"},{"paperId":"2577d053f8aab912d29b424e1f09133d83740fd2","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"66c52c6ef45cdd77c086996bcaaf01470e82dbd2","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?"},{"paperId":"5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","title":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming"},{"paperId":"cb123f1afd67fb8bae15dc876709c842b626c49c","title":"SimSCOOD: Systematic Analysis of Out-of-Distribution Behavior of Source Code Models"},{"paperId":"a4c216d2ce9dd245c84771acc574722055967fd6","title":"Enhancing Code Classification by Mixup-Based Data Augmentation"},{"paperId":"1c1ca2392155ddf30408a442e6b504b5d60d4f2a","title":"When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment"},{"paperId":"8a854331c593c6a766fa3b8037fb2ad1b95a6f06","title":"An Empirical Study of Code Smells in Transformer-based Code Generation Techniques"},{"paperId":"a7435722d8ab595da5a9c70ac9160f57d0dcd75a","title":"Enabling Transformers to Understand Low-Level Programs"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"1b4c19168410fb2690d285b205ab2281793db81a","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"},{"paperId":"c5b47a34fd7e0e595360683b4effa51c0261d1e3","title":"Language Models Can Teach Themselves to Program Better"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"1d160123cbbef972ea151a641dd435d57c727de8","title":"AixBench: A Code Generation Benchmark Dataset"},{"paperId":"2edc8efcda27c944a46f367acf6a5280b8f65525","title":"FixEval: Execution-based Evaluation of Program Fixes for Competitive Programming Problems"},{"paperId":"6df72c1b2972703e04ca509dd277169ad8eee594","title":"FixEval: Execution-based Evaluation of Program Fixes for Programming Problems"},{"paperId":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"1100dee3fd78655cddc4b7bfaef1161351d4fab5","title":"Automated Feedback Generation for Competition-Level Code"},{"paperId":"0bcd59da541fdae66884afba8d25475a54a9da1a","title":"Automated Repair of Programs from Large Language Models"},{"paperId":"6074c7b75f27ca9adb6d74b080c07d5d079c3ea0","title":"Improving automatically generated code from Codex via Automated Program Repair"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"6050454e0446a3068617f73b0301453f3f67844d","title":"Stylette: Styling the Web with Natural Language"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"590f6817b42407f96b079e82c935fae298196359","title":"Less is More: Summary of Long Instructions is Better for Program Synthesis"},{"paperId":"d6b7bc4e7968398101d8d9b4da7d4a0186763ff3","title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages"},{"paperId":"1f11601f9eb8c0bee1d1d2cc7f4fa187e8c5e525","title":"The impact of lexical and grammatical processing on generating code from natural language"},{"paperId":"76f023c3a819fc58989a064a1b50825b11fce95d","title":"Capturing Failures of Large Language Models via Human Cognitive Biases"},{"paperId":"d1bcb86bab0906bd631c4eb5ab81ce342b5927d0","title":"Open-Ended Knowledge Tracing"},{"paperId":"004d6ce718b0edb5b999f26710c5ae80b04bc900","title":"GPT-based Open-Ended Knowledge Tracing"},{"paperId":"04ff95e0edc3759fc5d18a1b929b3ccf79b032b2","title":"Deconstructing Distributions: A Pointwise Framework of Learning"},{"paperId":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective"},{"paperId":"76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","title":"Investigating Explainability of Generative AI for Code through Scenario-based Design"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"3d5463a16d85d9d1d0d8ebb4117a31aca3c240ff","title":"Fooling MOSS Detection with Pretrained Language Models"},{"paperId":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"},{"paperId":"52db8674337e5d86dcb96d013734befc8c3d4581","title":"Large Language Models are not Models of Natural Language: they are Corpus Models."},{"paperId":"1aed58bd07026492194672adec494dc37c894a28","title":"Leveraging Automated Unit Tests for Unsupervised Code Translation"},{"paperId":"407b9e9478ba6bff43ce4b20e8b6cb2b303477d2","title":"P LANNING WITH L ARGE L ANGUAGE M ODELS FOR C ODE G ENERATION"},{"paperId":"239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning"},{"paperId":"b2c4fdb49bdb23e6ad0ac3272029324157046ea7","title":"Automated Repair of Code from Language Models"},{"paperId":"78863000eb70945cc8d791d45d4a3fe8a6521cb6","title":"Open-ended Knowledge Tracing for Computer Science Education"},{"paperId":"3b0cf543a730e674d4213d344ebc857fada76ead","title":"Understanding High-Level Properties of Low-Level Programs Through Transformers"},{"paperId":"7497360b0f411a44aa6afbd8b830050c40ec8aed","title":"Dataset of Student Solutions to Algorithm and Data Structure Programming Assignments"},{"paperId":"6f84d0cc33c7c58f74b28ddcc1cbda91ea608c9f","title":"C ODE S UMMARIZATION : D O T RANSFORMERS R E ALLY U C"},{"paperId":"27d16a7f2ce8f2b787b34ff1f9b4fece079700c3","title":"Figuring out Figures: Using Textual References to Caption Scientific Figures"},{"paperId":"a3564f3cf954c05844c757505325a50b4d858e22","title":"Generating Infrastructure-as-Code from Natural Language: Evaluating Fine-Tuned GPT-3 Models for Cloud Infrastructure Provisioning"},{"paperId":"570a6a5b8ec2827c3f33bb1b1bd027190a0d3e07","title":"Neural Program Generation Modulo Static Analysis"},{"paperId":"a8863de15a5ee8eed98107f423138a1a8f5a2ba8","title":"Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"58a6ca2ae28a618126f71a07262cb958a8c37904","title":"Latent Execution for Neural Program Synthesis"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"6a1b25f7a67395ad1e676027322913acbb0a0635","title":"CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review"},{"paperId":"27e3ca4fd7b8290a0e12ca4fd2b7ad5bcd5900f2","title":"Are Transformers All That Karel Needs?"},{"paperId":"f5eb526492798dd7a53fe78f28431f5f489192da","title":"A Survey on Semantic Parsing for Machine Programming"},{"paperId":"642e280df732665249315d6c144871f0e2ceeae6","title":"NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash Commands"}],"references":[{"paperId":"c317f4d5ddeb11da9a5c8fea1e3cdec2f7de485d","title":"Deep Learning Based Program Generation From Requirements Text: Are We There Yet?"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"10bb7e2c54b947fa50e7bb65b0b5c700fe998044","title":"Measuring Massive Multitask Language Understanding"},{"paperId":"65906e6027246ae9e4ecd18d6e019a24505c842e","title":"Aligning AI With Shared Human Values"},{"paperId":"0df347f5e3118fac7c351917e3a497899b071d1e","title":"Datasheets for datasets"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"5fe0a4af3bd1479d5e39fbda2215c86bce54722b","title":"Generative Language Modeling for Automated Theorem Proving"},{"paperId":"725264948d7b6946259af5b8d966e996b9570f99","title":"DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"},{"paperId":"70f2c1567ef94fdf4581e1290bf7667cc9a4dcfc","title":"LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"0071ee55f1749e6be586645bab08f0385ee856b4","title":"Graph-based, Self-Supervised Program Repair from Diagnostic Feedback"},{"paperId":"04f4e55e14150b7c48b0287ba77c7443df76ed45","title":"PIQA: Reasoning about Physical Commonsense in Natural Language"},{"paperId":"0c5bc409e62e65f86838968a2a7cdae5fa0b288b","title":"RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers"},{"paperId":"00c957711b12468cb38424caccdf5291bb354033","title":"ZeRO: Memory optimizations Toward Training Trillion Parameter Models"},{"paperId":null,"title":"2020a) do a smaller-scale analysis of code generation but with their limited"},{"paperId":"66117f82def0c69a3b9cc77eb3e2694b0245ca86","title":"Cosmos QA: Machine Reading Comprehension with Contextual Commonsense Reasoning"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"d9f6ada77448664b71128bb19df15765336974a6","title":"SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems"},{"paperId":"8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad","title":"HellaSwag: Can a Machine Really Finish Your Sentence?"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"2016) introduce datasets based on Hearthstone and Magic the Gathering card games for code generation"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":null,"title":"2019c) use Spider"},{"paperId":null,"title":"2018) introduce the NAPS dataset for converting"},{"paperId":null,"title":"Gathering card games for code"},{"paperId":null,"title":"Natural program synthesis dataset"},{"paperId":null,"title":"Number of Programs’ refers to the number of human-written programs"},{"paperId":"6b024162f81e8ff7aa34c3a43d601a912d012c78","title":"Making Neural Programming Architectures Generalize via Recursion"},{"paperId":null,"title":"Attention is all you"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"2579e826f22da988fbc1b5d545aa59ab68bde4f3","title":"Program Synthesis Using Natural Language"},{"paperId":null,"title":"2018) by also facilitating the synthesis of database queries, though more recent program synthesis works such as Wang et al. (2019c) use Spider from Yu et al. (2018)"},{"paperId":null,"title":"A Additional Dataset Information Expanded Dataset Comparisons. We compared to several datasets in the (Kulal et al., 2019"},{"paperId":"27e1dbe9f7c71cd6cc1b0357f49aef497e572d09","title":"Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"},{"paperId":"e2d2099379af74d7be80a1964830f99edddaeb11","title":"Compositional Program Synthesis from Natural Language and Examples"},{"paperId":"110b7719f16eae8d82af0f56fca6e802c842c181","title":"Mining source code repositories at massive scale using language modeling"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"f60d8dd8ca3a7dfa7d0a14988af73084ad93619d","title":"Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing"},{"paperId":"b7c0e47f8b768258b7d536c21b218e6c46ab8791","title":"Learning to Parse Database Queries Using Inductive Logic Programming"}],"id":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","summary":"APPS is introduced, a benchmark for code generation that measures the ability of models to take an arbitrary natural language speciﬁcation and generate satisfactory Python code and shows that machine learning models are now beginning to learn how to code."},{"url":"https://www.semanticscholar.org/paper/9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey","venue":"ArXiv","year":2022,"referenceCount":156,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Daoguang Zan,Bei Chen,Fengji Zhang,Di Lu,Bingchao Wu,Bei Guan,Yongji Wang,Jian-Guang Lou","citations":[{"paperId":"68edfd62d2619fb4af7c2469edb95b9e2fe4544a","title":"Execution-based Code Generation using Deep Reinforcement Learning"}],"references":[{"paperId":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"4c26a6a777f1d46ffe79e3f41f8fd1ef6a6bd8c9","title":"Automatically Generating CS Learning Materials with Large Language Models"},{"paperId":"6bc87e51018d6de55011e95a0d43c588dd44a1e8","title":"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages"},{"paperId":"5fd0533ba1068af1cb075d4eb71ee551691a71ac","title":"ExploitGen: Template-augmented exploit code generation based on CodeBERT"},{"paperId":"598d9b235f5ab148fc757240d9bc39a47b8eaf72","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","title":"Evaluating How Fine-tuning on Bimodal Data Effects Code Generation"},{"paperId":"71280dba5bda65c162f9deaffed7d3d20692ca0a","title":"SecurityEval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"2577d053f8aab912d29b424e1f09133d83740fd2","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"152dc3042f5d4fc5a2686b5f4e0904f1e12a9207","title":"Code4Struct: Code Generation for Few-Shot Structured Prediction from Natural Language"},{"paperId":"39e40821b7207125e54e6ed7112e55cd38c6f0c3","title":"Language Models of Code are Few-Shot Commonsense Learners"},{"paperId":"2a0456b0408cd4c33f2ff4400374e7be2497a362","title":"Repairing Bugs in Python Assignments Using Large Language Models"},{"paperId":"453a8fac3be9282be53908f0735160d0d21e0f48","title":"Repair Is Nearly Generation: Multilingual Program Repair with LLMs"},{"paperId":"eed359a8a3ffdc45253e698b352443f0271a9666","title":"Compressing Pre-trained Models of Code into 3 MB"},{"paperId":"53b3b15e8cbe2baf82cd0de3709e0a1e6f677415","title":"Limits of an AI program for solving college math problems"},{"paperId":"c067664a45dce31411b3052c635c044ad4587db4","title":"Generating Diverse Code Explanations using the GPT-3 Large Language Model"},{"paperId":"654cd297b307c0c6fa08732511ba852b8dce1977","title":"Out of the BLEU: how should we assess quality of the Code Generation models?"},{"paperId":"ec4c8d99eb1c028c43af6d8bbf727392d351cb59","title":"Efficient Training of Language Models to Fill in the Middle"},{"paperId":"df1cc92fba512ce7d28d1d608ea19f18cda185ca","title":"No more fine-tuning? an experimental evaluation of prompt tuning in code intelligence"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"f0631f7928b99ee51f8164acd04889219b2bcdbb","title":"Diet code is healthy: simplifying programs for pre-trained models of code"},{"paperId":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code"},{"paperId":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation"},{"paperId":"9aed848be4e9e401b0e61a2e5d60dbdafa0c6cc1","title":"A Dataset and Benchmark for Automatically Answering and Generating Machine Learning Final Exams"},{"paperId":"0d08ffccc982781e310bb184397bbe64b9aef157","title":"Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models"},{"paperId":"cdfe9580f63070f311151444f9df32818cc858bf","title":"An Empirical Evaluation of GitHub Copilot's Code Suggestions"},{"paperId":"1f3a97d255a6d0b2d5535af75737ed43b1e96fc4","title":"Is GitHub Copilot a Substitute for Human Pair-programming? An Empirical Study"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"0c6c5e0fb38c28e50909cd5e165737636abf804b","title":"WhyGen: Explaining ML-powered Code Generation by Referring to Training Examples"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"d358ce242dacfd2ea738aa538779f06c79777f2b","title":"SELFIES and the future of molecular string representations"},{"paperId":"a522543180db6c5b21f47fe88abee44de158c85c","title":"Automatic Programming and Education"},{"paperId":"1d10023541f06701bf2f9ae8c91609e1055799ec","title":"Automating code review activities by large-scale pre-training"},{"paperId":"51000d9f79be0eefd7972fe94e3c71dddc90d2c6","title":"Evaluating the Text-to-SQL Capabilities of Large Language Models"},{"paperId":"2b556fcf2ac634f03c1fb0ace5e602e829418e65","title":"ReACC: A Retrieval-Augmented Code Completion Framework"},{"paperId":"bf81b2a50009fc7370d25f2ae6f8acc09c7da5d9","title":"GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models"},{"paperId":"7a1069dafaeb484e22f2473d5545f1e45ce30656","title":"Compilable Neural Code Generation with Compiler Feedback"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"463c6944ddcb43280e1c9765c2245bc7b764ab68","title":"AI-Driven Development Is Here: Should You Worry?"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"76f023c3a819fc58989a064a1b50825b11fce95d","title":"Capturing Failures of Large Language Models via Human Cognitive Biases"},{"paperId":"8c23e7627896b7571648896b7d61b6ef76ece982","title":"Locating and Editing Factual Knowledge in GPT"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"23e8ed7568454e11d9a6fecb8242e1d16b1828d5","title":"Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"6d7d4fca9840504f630e9bea6acaa07322a6e889","title":"Text and Code Embeddings by Contrastive Pre-Training"},{"paperId":"166b64f2ae8e52f5779682fab756cbd617a6e74b","title":"A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level"},{"paperId":"8417d83e74a63b9d3874cd7695477ebad8b98f8a","title":"Choose your programming copilot: a comparison of the program synthesis performance of github copilot and genetic programming"},{"paperId":"a341bf8acc0cb13c06f747db6543cac9f2120065","title":"Natural language processing models that automate programming will transform chemistry research and teaching"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"},{"paperId":"4a160efbe80c38cd5eb2f92c7c095b49b113397d","title":"In-IDE Code Generation from Natural Language: Promise and Challenges"},{"paperId":"37bf0bf34603145246c3311df19e2afdf6e0270a","title":"JAKET: Joint Pre-training of Knowledge Graph and Language Understanding"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs"},{"paperId":"737daa49c4234a8897b1f5b466c004db56241d83","title":"S2QL: Retrieval Augmented Zero-Shot Question Answering over Knowledge Graph"},{"paperId":"e11d61b669bdf256d4021e92fae980152591bf45","title":"Are NLP Metrics Suitable for Evaluating Generated Code?"},{"paperId":"91260f73dd179487fb16713deb8267634ae14716","title":"CodexDB: Synthesizing Code for Query Processing from Natural Language Instructions using GPT-3 Codex"},{"paperId":"1b4c19168410fb2690d285b205ab2281793db81a","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"},{"paperId":null,"title":"2022d. MCoNaLa: A"},{"paperId":null,"title":"2022a. A systematic evaluation"},{"paperId":"f7987fa2aadc0b368c185dc4d2fdb1337a202c32","title":"Solving Probability and Statistics Problems by Program Synthesis"},{"paperId":"4e40595d6ecba027cebb4f2e3b43ae44bcf51daf","title":"Solving Linear Algebra by Program Synthesis"},{"paperId":"1444536496d8064f33e10b38b5820fecfab5b367","title":"Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs"},{"paperId":"92add61e7c3dd745321fc8a6e113dae8199668ac","title":"Improving Text-to-Code Generation with Features of Code Graph on GPT-2"},{"paperId":"2016e814eed00b0c0a9358e193e29854e0ed526f","title":"Long-Range Modeling of Source Code Files with eWASH: Extended Window Access by Syntax Hierarchy"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"cc9ad384ec0d0176ce05865d3866b44d2519bd68","title":"Reading StackOverflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"0b077c9577f4297dcf3da835e253d21965bbc6e0","title":"CoTexT: Multi-task Learning with Code-Text Transformer"},{"paperId":"240b0caabb415578bdea4da7d0a32bdff2e8163f","title":"Editing Factual Knowledge in Language Models"},{"paperId":"969e8c2c7cdf26c35e6c3fc19a9a56b3e7fcd6f9","title":"Generating Code with the Help of Retrieved Template Functions and Stack Overflow Answers"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"bc4f195d2f0937dab16bf50a24bf385c10781dd7","title":"MLIR: Scaling Compiler Infrastructure for Domain Specific Computation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"10d5a71da740e4d709914c450fc70fee1959b196","title":"Self-Supervised Contrastive Learning for Code Retrieval and Summarization via Semantic-Preserving Transformations"},{"paperId":"021bbcefc993c389bad6c1daefd8ff92d0fc2441","title":"Contrastive Code Representation Learning"},{"paperId":"5d4cbdd2172039b84b8628f1a2f77b83ba1fa551","title":"Enriching contextualized language model from knowledge graph for biomedical information extraction"},{"paperId":"1728cb805a9573b59330890ba9723e73d6c3c974","title":"Knowledge Distillation: A Survey"},{"paperId":"375b9b36ef68678185f2b6e4dbbbe7bbfad6535a","title":"Large-Scale Relation Learning for Question Answering over Knowledge Bases with Pre-trained Language Models"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"49cf6a22a5dac5bc98b653534af65ffa0bc0e76d","title":"Multi-task Learning based Pre-trained Language Model for Code Completion"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"77910e51a40d17157fc798325d06edfa6cff18d6","title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation"},{"paperId":"21d9d58ab9537d10927b749eaa1c8b8c5a970579","title":"Leveraging Code Generation to Improve Code Retrieval and Summarization via Dual Learning"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"dfc08920427d7013e793fd54b6fdef6b76615b41","title":"GANCoder: An Automatic Natural Language-to-Programming Language Translation Approach Based on GAN"},{"paperId":"b7af363ff612f41e0c1071ae24ce68a836aaa678","title":"Code Generation as a Dual Task of Code Summarization"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"9e955da40c62b2543b963454b90928ad6cbd5f8a","title":"Does BLEU Score Work for Code Migration?"},{"paperId":"9c2ab4707e902f6174910847e3f94f3b85017ec3","title":"A Grammar-Based Structural CNN Decoder for Code Generation"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"c2a6a4b6be2dabfcae7e17f2b02b6b29e03a8277","title":"CodeAlchemist: Semantics-Aware Code Generation to Find Vulnerabilities in JavaScript Engines"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"077ea3a496bd76bd22d76c432b79b9fa40e136e8","title":"Improving Automatic Source Code Summarization via Deep Reinforcement Learning"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"d3e13d2514edaf74b863bfbe45a739c32a7689e1","title":"Retrieval-Based Neural Code Generation"},{"paperId":"d6b7abd5c17f9e7d83c5b709c27da3a82a9788f2","title":"NAPS: Natural Program Synthesis Dataset"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"f6a4bf043af1a9ec7f104a7b7ab56806b241ceda","title":"Model compression via distillation and quantization"},{"paperId":"429b65937d4922578a81e1f0ef5aeab7361ae36b","title":"NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System"},{"paperId":"4df7bbe3ca7806f39a490c99f17867a0ac299bc3","title":"Learning Explanatory Rules from Noisy Data"},{"paperId":"ebeb5026cc5c6cf496441887f8b5bd0e36ff987b","title":"Systematic mapping study of template-based code generation"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":null,"title":"and Illia Polosukhin"},{"paperId":"956c52bf738517a8827f63be976e1291750e0ebc","title":"Learning to select examples for program synthesis"},{"paperId":"b4a26011eb7c26d71cd074f182c4f093da24308b","title":"Are deep neural networks the best choice for modeling source code?"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2c1e874c3b67510a3215e535f5646b362de5bc89","title":"Abstract Syntax Networks for Code Generation and Semantic Parsing"},{"paperId":"de7e3537d974c2ca28e3ca130d531e5eb23eb79f","title":"Program Synthesis for Character Level Language Modeling"},{"paperId":"8ec6abfdc5009b4e490e975991c871dfeec05434","title":"Program Synthesis from Natural Language Using Recurrent Neural Networks"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"ecc5492d00b1c53ed30d830136c300d381ca2770","title":"Automatic code generation of convolutional neural networks in FPGA implementation"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"a1b6b07157442f9ca2b5af3533a42b4caa100307","title":"A deep language model for software code"},{"paperId":"32aa2517b03c871c11e521c2a3406f457833e2c3","title":"Summarizing Source Code using a Neural Attention Model"},{"paperId":"a62731619b2b09a7d166b9e805f4a1a9c4c7c2d3","title":"PHOG: Probabilistic Model for Code"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"db02cd07726371790a825208cec377ec15f5b5f1","title":"Tree-to-Sequence Attentional Neural Machine Translation"},{"paperId":"5a4cc911a2b3d50d974aea6f9e3dbdae76d9df98","title":"An analysis of tools for automatic software development and automatic code generation"},{"paperId":"27e1dbe9f7c71cd6cc1b0357f49aef497e572d09","title":"Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"},{"paperId":"285c165c81fc9275955147a892b9a039ec8b1052","title":"chrF: character n-gram F-score for automatic MT evaluation"},{"paperId":"cd7ee037c029b25e691650a26cec7538d255405c","title":"Suggesting accurate method and class names"},{"paperId":"b5defb8729294614d40e0247eabfdc483bf1a992","title":"Bimodal Modelling of Source Code and Natural Language"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"26adb749fc5d80502a6d889966e50b31391560d3","title":"Meteor Universal: Language Specific Translation Evaluation for Any Target Language"},{"paperId":"7f013f172a45824d907f68481e92a22e0188ea0b","title":"Mining idioms from source code"},{"paperId":"215999a0e155a21255e4655d4eac312858058a84","title":"Structured Generative Models of Natural Source Code"},{"paperId":"9769e24c45c87e0daa5cff39991e0313882213fd","title":"A statistical semantic language model for source code"},{"paperId":"2813afed488061326bc42d3d297dd5db37c9744f","title":"Complete completion using types and weights"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"18b8ef71bc01b8658b4ef2c8b9a9e4e6e5c2a07b","title":"Dimensions in program synthesis"},{"paperId":"fd07159f48b358e912d0bd6b81ca87710f463734","title":"Inducing Tree-Substitution Grammars"},{"paperId":"0228810a988f6b8f06337e14f564e2fd3f6e1056","title":"The Recurrent Temporal Restricted Boltzmann Machine"},{"paperId":"3960dda299e0f8615a7db675b8e6905b375ecf8a","title":"Z3: An Efficient SMT Solver"},{"paperId":"bd7d93193aad6c4b71cc8942e808753019e87706","title":"Three new graphical models for statistical language modelling"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"1459ed38b154648d1375b29f39891f94d459c64c","title":"A Formalism for Dependency Grammar Based on Tree Adjoining Grammar"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"e41498c05d4c68e4750fb84a380317a112d97b01","title":"Connectionist language modeling for large vocabulary continuous speech recognition"}],"id":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","summary":"This survey focuses on how does neural network (NN) solves NL2Code and proposes a comprehensive framework, which is able to cover all studies in this task, and in-depth parse the existing studies into this framework."},{"url":"https://www.semanticscholar.org/paper/a08a3b08a5a1de6462a7da2906b1cd81691d6c18","title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation","venue":"International Joint Conference on Artificial Intelligence","year":2022,"referenceCount":27,"citationCount":9,"influentialCitationCount":0,"publicationDate":"14/06/2022","authors":"Daoguang Zan,Bei Chen,Dejian Yang,Zeqi Lin,Minsu Kim,Bei Guan,Yongji Wang,Weizhu Chen,Jian-Guang Lou","citations":[{"paperId":"3b01b5b497e1f359b11da45af029281ee6f64c2c","title":"Towards Enhancing In-Context Learning for Code Generation"},{"paperId":"28dafb94a87daa71ad3edf9f04f3c8c32753398b","title":"Improving Code Generation by Training with Natural Language Feedback"},{"paperId":"13a66fc8689724e295548ceac9e5425fc46cc093","title":"SkCoder: A Sketch-based Approach for Automatic Code Generation"},{"paperId":"1d74875aa4f415cb2c60b17fd1eb3e4ae543bfe1","title":"Keeping Pace with Ever-Increasing Data: Towards Continual Learning of Code Intelligence Models"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"6e0087571449c002d412e283e2db6dbef7fc3b3f","title":"Code Question Answering via Task-Adaptive Sequence-to-Sequence Pre-training"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"045a6f92b58f08d5b305dd5d661316a506ee4d43","title":"Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation"},{"paperId":"1d160123cbbef972ea151a641dd435d57c727de8","title":"AixBench: A Code Generation Benchmark Dataset"}],"references":[{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax"},{"paperId":"ed2a639dd4a6b3abdcc0040733a264cb1dbcd7d4","title":"PlotCoder: Hierarchical Decoding for Synthesizing Visualization Code in Programmatic Context"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"1fa9ed2bea208511ae698a967875e943049f16b6","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"6c72cff44294e050f75b47f1027889f539e11347","title":"Learning to Infer Program Sketches"},{"paperId":"9c2ab4707e902f6174910847e3f94f3b85017ec3","title":"A Grammar-Based Structural CNN Decoder for Code Generation"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"fd04e31c25451f9103a0ac2220ac8d7e7884c343","title":"Coarse-to-Fine Decoding for Neural Semantic Parsing"},{"paperId":"d11777ca327c6d91de34d1d2ac50316b905578b2","title":"Neural Sketch Learning for Conditional Program Generation"},{"paperId":"ae271f8b7ae47e83bcd8af7d567df433a36c5dce","title":"API usage pattern recommendation for software development"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"1737bbf048409b19cfda6d0d18a4262dbb57a194","title":"Mining succinct and high-coverage API usage patterns from source code"},{"paperId":"fed132d312a9f618329238ac6542d0148a0ff157","title":"MAPO: Mining and Recommending API Usage Patterns"}],"id":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","summary":"This paper investigates how to leverage an unlabelled code corpus to train a model for library-oriented code generation, and observes that library- oriented code snippets are more likely to share similar code sketches."},{"url":"https://www.semanticscholar.org/paper/876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":26,"influentialCitationCount":3,"publicationDate":"21/07/2022","authors":"Bei Chen,Fengji Zhang,A. Nguyen,Daoguang Zan,Zeqi Lin,Jian-Guang Lou,Weizhu Chen","citations":[{"paperId":"dd0960a8ffb6a8baff69ec96c3f67deb2912b53a","title":"Self-collaboration Code Generation via ChatGPT"},{"paperId":"eb55442d940accc67d6790e66d3196e1941fc7a4","title":"Teaching Large Language Models to Self-Debug"},{"paperId":"2d51e79d541bb489b07aa4fa691a93a9d4a0498b","title":"Evaluating AIGC Detectors on Code Content"},{"paperId":"f352a968c8735fac58912870a7bde57fcfc2e6bd","title":"\"It's Weird That it Knows What I Want\": Usability and Interactions with Copilot for Novice Programmers"},{"paperId":"3b01b5b497e1f359b11da45af029281ee6f64c2c","title":"Towards Enhancing In-Context Learning for Code Generation"},{"paperId":"349ec9ca2ff17dbc1aa1caf27f98b7cd85a4747e","title":"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation"},{"paperId":"8ca62fdf4c276ea3052dc96dcfd8ee96ca425a48","title":"GPT-4 Technical Report"},{"paperId":"d39db76fb80c1f74455a0f6432c2242b727389da","title":"Self-planning Code Generation with Large Language Model"},{"paperId":"8d03ebc375b3d887056e769018467bf0a6ed99e3","title":"Planning with Large Language Models for Code Generation"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"6d269364de402d4a72ac30b0c8d81324f6849807","title":"LEVER: Learning to Verify Language-to-Code Generation with Execution"},{"paperId":"1786a2f9140ed7211b21302977de64e948b92308","title":"Learning Performance-Improving Code Edits"},{"paperId":"6876fc47f4674fdf4f53208254b45dedc6de3755","title":"Adaptive Test Generation Using a Large Language Model"},{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"46d0a832fada6147bceb0bd4e39928e482733246","title":"How Important are Good Method Names in Neural Code Generation? A Model Robustness Perspective"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"66c52c6ef45cdd77c086996bcaaf01470e82dbd2","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?"},{"paperId":"f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab","title":"Binding Language Models in Symbolic Languages"},{"paperId":"95fa2b27ab7eb84738441ee16da97323538938f9","title":"I Speak, You Verify: Toward Trustworthy Neural Program Synthesis"},{"paperId":"971e875e28f26240987d2c9470d1ee74ad204205","title":"Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization"},{"paperId":"7107d06366b48b3593c8128ed2ca67e0b413628c","title":"Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"}],"references":[{"paperId":"1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization"},{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"4288d44c2b8e6a89607780caf1272061028f6f97","title":"On the Advance of Making Language Models Better Reasoners"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"0d08ffccc982781e310bb184397bbe64b9aef157","title":"Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"eadcf2f484b2d326f4d32ba4a897b009e4de1784","title":"Pynguin: Automated Unit Test Generation for Python"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"c783e1fb3ce8514f981925ee590c00884660ee4e","title":"CM3: A Causal Masked Multimodal Model of the Internet"},{"paperId":"166b64f2ae8e52f5779682fab756cbd617a6e74b","title":"A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level"},{"paperId":"1aed58bd07026492194672adec494dc37c894a28","title":"Leveraging Automated Unit Tests for Unsupervised Code Translation"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":null,"title":"Natural language processing with transformers"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"4698fc4712f0212c8a3810fd67b41ee8b8896aba","title":"Generate & Rank: A Multi-task Framework for Math Word Problems"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"5ff9032d0f7f246d01ae7b2c231ab03469a7344a","title":"Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax"},{"paperId":"15a60130ea9cb4a00e0dac6d11a368894974cedf","title":"Unit Test Case Generation with Transformers and Focal Context"},{"paperId":"74998707fd7d2886ef53206bd3c3c7536d2f3a94","title":"BiRank: Fast and Flexible Ranking on Bipartite Networks with R and Python"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"1fa9ed2bea208511ae698a967875e943049f16b6","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"f669b5406320a83e94b4dd439806211c8b1e95fc","title":"Write, Execute, Assess: Program Synthesis with a REPL"},{"paperId":"6c41bedc4637f3fd504c68baa3b3d8881e056ac1","title":"Execution-Guided Neural Program Synthesis"},{"paperId":"0943d656221144bf063a538f6c667ac006c303cd","title":"Automated Test Case Generation as a Many-Objective Optimisation Problem with Dynamic Selection of the Targets"},{"paperId":"b28abc70eee3a5b628d20ba7ffa96dd54c29beb6","title":"Many Independent Objective (MIO) Algorithm for Test Suite Generation"},{"paperId":"728f470d67c5b2d6e3ea1fa13b3b3a23545b3f65","title":"BiRank: Towards Ranking on Bipartite Graphs"},{"paperId":"07315439e66dd8c6709a1834cb15a673ab9a19bc","title":"Reformulating Branch Coverage as a Many-Objective Optimization Problem"},{"paperId":"216b98bb3d9221d5f5d261864975612e4d0faaa6","title":"EvoSuite: automatic test suite generation for object-oriented software"},{"paperId":"6dc7017e357767dbc210bcb27080260fb0816701","title":"Feedback-Directed Random Test Generation"},{"paperId":"058861dd838ddf4a6a36860f54e82e11f8945b32","title":"Authoritative sources in a hyperlinked environment"},{"paperId":"4f37468a95ccc62debb9e5a4cb0d73489ca61190","title":"Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography"}],"id":"876eb375cb7b365475040046df669c039ad54202","summary":"A novel method, C ODE T, leverages the same pre-trained language models to test cases for the code samples, thus reducing the human effort and increasing the coverage of the test scenarios, achieving remarkable and consistent gains across different models and benchmarks."},{"url":"https://www.semanticscholar.org/paper/780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation","venue":"","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/08/2022","authors":"Federico Cassano,John Gouwar,Daniel Nguyen,S. Nguyen,Luna Phipps-Costin,Donald Pinckney,Ming-Ho Yee,Yangtian Zi,Carolyn Jane Anderson,Molly Q. Feldman,Arjun Guha,M. Greenberg,Abhinav Jangda","citations":[],"references":[{"paperId":"2577d053f8aab912d29b424e1f09133d83740fd2","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"d6b7bc4e7968398101d8d9b4da7d4a0186763ff3","title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"c783e1fb3ce8514f981925ee590c00884660ee4e","title":"CM3: A Causal Masked Multimodal Model of the Internet"},{"paperId":"166b64f2ae8e52f5779682fab756cbd617a6e74b","title":"A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level"},{"paperId":"775a9c722262c7b656876a5fef20f4577afd8981","title":"Multilingual training for Software Engineering"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":null,"title":"Natural Language Processing with Transformers"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"0df347f5e3118fac7c351917e3a497899b071d1e","title":"Datasheets for datasets"},{"paperId":"b48be6bdd1c59ae965144fc8e449cbde10941d48","title":"Neurosymbolic Programming"},{"paperId":null,"title":"Openai codex"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"15a60130ea9cb4a00e0dac6d11a368894974cedf","title":"Unit Test Case Generation with Transformers and Focal Context"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"29fc7ff02e68537a86d32978bfd2e8e45258e77d","title":"LambdaNet: Probabilistic Type Inference using Graph Neural Networks"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"a429c6c8407b10a89c9610ab5e9682ec89772356","title":"TypeWriter: neural type prediction with search-based validation"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"ea1b62a03ab16c4acca6f7041c6f096bedec244b","title":"Deep learning type inference"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"116a877c774969b53399b1ccaa34d51bfb492ee4","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"9b536c7b0f45d4b512f337d0acda09ca3e4cd953","title":"Fitting Linear Mixed-Effects Models Using lme4"},{"paperId":"f7ba81057c8ec4f1c039027232034143a5afe6bf","title":"Syntax-guided synthesis"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"2fb9729c0a3b046c390ad6748773527eae6cfc84","title":"What Is Program Synthesis?"}],"id":"780f7eebde16b1ae5843df3a79a7772899ef6a71","summary":"This work creates the first massively multilingual code generation benchmark by using MultiPL-E to translate two popular Python code generation benchmarks to 18 additional programming languages and evaluates the multi-language performance of three state-of-the-art code generation models."},{"url":"https://www.semanticscholar.org/paper/0b340dd78fd04bbde2807d5efedb796d319355e3","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":2,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Jean-Baptiste Döderlein,M. Acher,D. Khelladi,B. Combemale","citations":[{"paperId":"f0ea9e2d3889d37f34743ed1dc64f11e8e0484de","title":"Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models"},{"paperId":"a889a606734cd47f802765866487c677497a70d6","title":"Lost at C: A User Study on the Security Implications of Large Language Model Code Assistants"}],"references":[{"paperId":"4610ffb1b016acaa82a2065ffd1a3adbae1ce722","title":"Large Language Models Are Human-Level Prompt Engineers"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"34503c0b6a615124eaf82cb0e4a1dab2866e8980","title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"},{"paperId":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"cdfe9580f63070f311151444f9df32818cc858bf","title":"An Empirical Evaluation of GitHub Copilot's Code Suggestions"},{"paperId":"c5b2dc81baeaba0595c6e8a2ed84a42ce706cab5","title":"Improving Machine Translation Systems via Isotopic Replacement"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"a6b9a934fe039a5636a26e94fb47f872263d702c","title":"To What Extent do Deep Learning-based Code Recommenders Generate Predictions by Cloning Code from the Training Set?"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"80d0116d77beeded0c23cf48946d9d10d4faee14","title":"GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"},{"paperId":"8417d83e74a63b9d3874cd7695477ebad8b98f8a","title":"Choose your programming copilot: a comparison of the program synthesis performance of github copilot and genetic programming"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"},{"paperId":"0adec918885dff698acf359988ed79a543157f80","title":"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity"},{"paperId":"f7987fa2aadc0b368c185dc4d2fdb1337a202c32","title":"Solving Probability and Statistics Problems by Program Synthesis"},{"paperId":"4e40595d6ecba027cebb4f2e3b43ae44bcf51daf","title":"Solving Linear Algebra by Program Synthesis"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"ac3cdb50606f7770eef8e4cd951840a4f71287a0","title":"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"399e7d8129c60818ee208f236c8dda17e876d21f","title":"RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"3dd61d97827e3f380bf9304101149a3f865051fc","title":"Injecting Numerical Reasoning Skills into Language Models"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85","title":"How Can We Know What Language Models Know?"},{"paperId":"93d63ec754f29fa22572615320afe0521f7ec66d","title":"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"1ac30af5522c7a50ec4d1ee43fd2bd8652a9bd52","title":"A Neural Attention Model for Abstractive Sentence Summarization"},{"paperId":"80864f25239fa181e38c5293a7f1b76dc3396ea9","title":"A Survey on Document Clustering with Similarity Measures"},{"paperId":null,"title":"Auto- Prompt: Eliciting knowledge from language models with automatically generated prompts"},{"paperId":null,"title":"GitHub copilot · your AI pair programmer"},{"paperId":null,"title":"Natural language processing with transformers , revised edition [ book ]"},{"paperId":null,"title":"Copilot internals . [ Online ] Research recitation . [ Online ]"},{"paperId":null,"title":"IntelliSense in visual studio code"},{"paperId":null,"title":"Research recitation"}],"id":"0b340dd78fd04bbde2807d5efedb796d319355e3","summary":"Investigation of the various input parameters of two language models shows that varying the input parameters can improve the performance of language models, but there is a tight dependency when varying the temperature, the prompt and the number of generated solutions, making potentially hard to properly control the parameters to obtain an optimal result."},{"url":"https://www.semanticscholar.org/paper/cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":4,"influentialCitationCount":0,"publicationDate":"29/10/2022","authors":"Victor C. Dibia,Adam Fourney,Gagan Bansal,Forough Poursabzi-Sangdeh,Han Liu,Saleema Amershi","citations":[{"paperId":"01713a5f38c0164151caa9b6cc740a8c864420cc","title":"Scientists' Perspectives on the Potential for Generative AI in their Fields"},{"paperId":"dca09e13fe9b4f46b1e10b966f3ede63f63f9489","title":"Understanding the Usability of AI Programming Assistants"},{"paperId":"4e97303aeb299ee736b1b8c29cef046212690354","title":"Generation-based Code Review Automation: How Far Are We?"},{"paperId":"9431181f8115a2360621df5ed76e1a23b88e3b2f","title":"Evaluating Human-Language Model Interaction"}],"references":[{"paperId":"a38af38baeb1b5e25c089b699ab5072823ae6b4c","title":"The Fallacy of AI Functionality"},{"paperId":"4c59d10c24cd1948a6e88174564b5766b4e5c530","title":"Deconstructing NLG Evaluation: Evaluation Practices, Assumptions, and Their Implications"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"2fc583fbd1df9be20580db003d26d995b95f4eec","title":"Reliance on metrics is a fundamental challenge for AI"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":null,"title":"Quantifying GitHub Copilot’s impact on developer productivity and happiness"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"2a36402451c4fcab33c9e7f59fc027167c070aa6","title":"Perfection Not Required? Human-AI Partnerships in Code Translation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"681f4fbce872f138cbac9cdd92e8f6ed89ba6f8d","title":"Measurement and Fairness"},{"paperId":"28797c288403828f29d3faca6ab9d64d134a4ea3","title":"The SPACE of Developer Productivity: There's more to it than you think"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"868207797e69df5055f5c3fd4aa78a33e5a7ca45","title":"Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":null,"title":"Beyond accuracy: Grounding evaluation metrics for human-machine learning systems"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"d02285385fd2b53a6100b8d62421bba6693b788c","title":"When Code Completion Fails: A Case Study on Real-World Completions"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"71bb158c45d80da5b4408321f79a47789f9663ad","title":"Will They Like This? Evaluating Code Contributions with Language Models"},{"paperId":null,"title":"TensorFlow: Large-scale machine learning on heterogeneous systems"},{"paperId":"93ff001eb7ddd019c107879943126c74a973993b","title":"Learning natural coding conventions"},{"paperId":"d12fbc23cff452074a286b099cd475fdd3dcd91a","title":"Classifier Technology and the Illusion of Progress"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"cba98048f3e85a974c287b271692bf6c197db940","summary":"A simple hybrid metric is proposed, which combines functional correctness and similarity- based metrics to capture different dimensions of what programmers might value and shows that this hybrid metric more accurately captures effort."},{"url":"https://www.semanticscholar.org/paper/8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation","venue":"ArXiv","year":2022,"referenceCount":30,"citationCount":10,"influentialCitationCount":2,"publicationDate":"18/11/2022","authors":"Yuhang Lai,Chengxi Li,Yiming Wang,Tianyi Zhang,Ruiqi Zhong,Luke Zettlemoyer,S. Yih,Daniel Fried,Si-yi Wang,Tao Yu","citations":[{"paperId":"2ef1c2438c3a4552db9e7080e15d8c51bc071f58","title":"Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes"},{"paperId":"cbdc75e9c80870164662bb0359e23fd4736c5f0e","title":"A Survey of Large Language Models"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"535a3d95a743e1f4b591b5b2af3e778a6347158a","title":"CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code"},{"paperId":"27c2ddc921befc44f7b2cdb8bb39facdd8b0b188","title":"Measuring The Impact Of Programming Language Distribution"},{"paperId":"1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a","title":"SantaCoder: don't reach for the stars!"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation"}],"references":[{"paperId":"ec4c8d99eb1c028c43af6d8bbf727392d351cb59","title":"Efficient Training of Language Models to Fill in the Middle"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"c783e1fb3ce8514f981925ee590c00884660ee4e","title":"CM3: A Causal Masked Multimodal Model of the Internet"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"f40aeae3e522ada1f6a9f326841b01ef5c8657b6","title":"Unifying Language Learning Paradigms"},{"paperId":"5fbcfccd3736969d95ed660d8e6962c86b7a9113","title":"PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"56fa0b9cba4d9aee5ccc327365b3b3a721031c69","title":"Calibrate Before Use: Improving Few-Shot Performance of Language Models"},{"paperId":"ed2a639dd4a6b3abdcc0040733a264cb1dbcd7d4","title":"PlotCoder: Hierarchical Decoding for Synthesizing Visualization Code in Programmatic Context"},{"paperId":"e6e8a2c56243847b77b604259cde9e10a2daccb8","title":"Semantic Evaluation for Text-to-SQL with Distilled Test Suite"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"15a60130ea9cb4a00e0dac6d11a368894974cedf","title":"Unit Test Case Generation with Transformers and Focal Context"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"133bcd7488a3c07cb0f493a87564c30e5433768c","title":"Reproducible, interactive, scalable and extensible microbiome data science using QIIME 2"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"f3318491a55590e00dfe45d68708f515822e343a","title":"Cosette: An Automated Prover for SQL"},{"paperId":"9ba08d45d60130c7e5880f63a980b185a86e177c","title":"A Big Data Guide to Understanding Climate Change: The Case for Theory-Guided Data Science"},{"paperId":"b29447ba499507a259ae9d8f685d60cc1597d7d3","title":"Semantic Parsing on Freebase from Question-Answer Pairs"},{"paperId":"c73b0424e1a4ab2574cfce2e41c505f71f46940e","title":"Data mining in education"},{"paperId":null,"title":"Learning DependencyBased Compositional Semantics"},{"paperId":"774113732db34ce0b797fc3dcceded811fb6edbc","title":"Online Learning of Relaxed CCG Grammars for Parsing to Logical Form"},{"paperId":"b7c0e47f8b768258b7d536c21b218e6c46ab8791","title":"Learning to Parse Database Queries Using Inductive Logic Programming"},{"paperId":null,"title":"Reference SoluHon # df: pd.DataFrame as input result = df.replace(df"}],"id":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","summary":"This work introduces DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas, and proactively defends against memorization by slightly modifying the problems to be different from the original StackOverﬂow source."},{"url":"https://www.semanticscholar.org/paper/815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks","venue":"ArXiv","year":2022,"referenceCount":60,"citationCount":2,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Pengcheng Yin,Wen-Ding Li,Kefan Xiao,A. Rao,Yeming Wen,Kensen Shi,Joshua Howland,Paige Bailey,Michele Catasta,H. Michalewski,Alex Polozov,Charles Sutton","citations":[{"paperId":"8475c1d1d98932111b194dc139c8f8b30818725d","title":"xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval"},{"paperId":"239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning"}],"references":[{"paperId":"67c0b0f0b1efb2e25084d23ed310109fb3c451aa","title":"Neural-Symbolic Inference for Robust Autoregressive Graph Parsing via Compositional Uncertainty Quantification"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models"},{"paperId":"4af2891ce1aab624c4917e8a69fcee5c8a1f41db","title":"NL2Viz: natural language to visualization via constrained syntax-guided synthesis"},{"paperId":"f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab","title":"Binding Language Models in Symbolic Languages"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"51000d9f79be0eefd7972fe94e3c71dddc90d2c6","title":"Evaluating the Text-to-SQL Capabilities of Large Language Models"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"53c0abe83fe9b4fdaf2208295d8504fcf5241694","title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models"},{"paperId":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis"},{"paperId":"fe5cb4375f213abdbd34d25b02dc7e48794d286d","title":"AutoML to Date and Beyond: Challenges and Opportunities"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":null,"title":"We also report training data composition in Tab. 7. Motivation For what purpose was the dataset created? Who created the dataset? Who funded the creation"},{"paperId":null,"title":"Natural language processing with transformers"},{"paperId":null,"title":"2022a. Documentation matters: Human-centered ai system to assist data science code documentation"},{"paperId":"5436193122dff271796bca07df7cecb7a8d6dea6","title":"Natural language-guided programming"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"9cd3d6eef7c574830be410598c3024191ee974d4","title":"KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers"},{"paperId":"84b26030b648b6d79177bdafd3e896b1dda9f91e","title":"Towards Robustness of Text-to-SQL Models against Synonym Substitution"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"d377e9a167996d18eaaa35786cfcadecbc8b241e","title":"Reactive, reproducible, collaborative: computational notebooks evolve"},{"paperId":"66c10bf1f11bc1b2d92204d8f8391d087f6de1c4","title":"RoFormer: Enhanced Transformer with Rotary Position Embedding"},{"paperId":"7282f5c9d84cd47c516a6a66c5a6b8f1e2cf44b6","title":"AutoDS: Towards Human-Centered Automation of Data Science"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"5593676873d799a4727123a2cbffb231d3b4eb80","title":"NL4DV: A Toolkit for Generating Analytic Specifications for Data Visualization from Natural Language Queries"},{"paperId":"3a7fa673ff8ec4ec2f322473de005f3cd09ea820","title":"AutoML: A Survey of the State-of-the-Art"},{"paperId":"330b5844d170b6b77f5f9fa4c2024150cef2af18","title":"Benchmark and Survey of Automated Machine Learning Frameworks"},{"paperId":"ed2a639dd4a6b3abdcc0040733a264cb1dbcd7d4","title":"PlotCoder: Hierarchical Decoding for Synthesizing Visualization Code in Programmatic Context"},{"paperId":null,"title":"2021b. How much automation does a data scientist want? arXiv preprint arXiv:2101.03970"},{"paperId":null,"title":"GPT-J6B: A 6 Billion Parameter Autoregressive Language Model"},{"paperId":null,"title":"GPT-Neo: Large Scale"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"72e0344605bde4707d24d2f9b0b4e1e0aa953604","title":"Quda: Natural Language Queries for Visual Data Analytics"},{"paperId":"8f8532a193313b9b956a8df402dd7f879bbe1377","title":"Data Engineering for Data Analytics: A Classification of the Issues, and Case Studies"},{"paperId":"4e80a42d330331d7b7e88f39ccc802fe6656ac5a","title":"How can AI Automate End-to-End Data Science?"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"545e60873b0fad25407bb6d647f92c905bd2483d","title":"CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases"},{"paperId":"f79af5e9e96f9c777e8759d791e33e9da83ffa65","title":"SParC: Cross-Domain Semantic Parsing in Context"},{"paperId":"30228f5e3ecd19452a2a5388b23086569e6233f4","title":"A Large-Scale Study About Quality and Reproducibility of Jupyter Notebooks"},{"paperId":"2012c176d7b003eb57a282bfd8681190704fb965","title":"Learning to Map Context-Dependent Sentences to Executable Formal Queries"},{"paperId":"f56425ec56586dcfd2694ab83643e9e76f314e91","title":"50 Years of Data Science"},{"paperId":"8ff54aa8045b1e30c348cf2ca42259c946cd7a9e","title":"Search-based Neural Structured Learning for Sequential Question Answering"},{"paperId":"554c9b3aa8c3b7504810270b18b942a0d22156e7","title":"DeepFix: Fixing Common C Language Errors by Deep Learning"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"7306437b2145677fe7bf3b7711ac8aa25989f1e3","title":"Simpler Context-Dependent Logical Forms via Model Projections"},{"paperId":"722e01d5ba05083f7a091f3188cfdfcf183a325d","title":"Larger-Context Language Modelling with Recurrent Neural Network"},{"paperId":"775a4e375cc79b53b94e37fa3eedff481823e4a6","title":"Efficient and Robust Automated Machine Learning"},{"paperId":"b41e95c8c97846d5ca4c11ef79d7814499cc9663","title":"Compositional Semantic Parsing on Semi-Structured Tables"},{"paperId":"4c67851b77766ba4ad9f1ac0bd4c9491c327574e","title":"Wrangler: interactive visual specification of data transformation scripts"},{"paperId":"07216ee1119f61b351b69e94b2e7c3698d96b026","title":"Learning Context-Dependent Mappings from Sentences to Logical Form"},{"paperId":"180e548520d2091beb8bb039473bd542e7de5aec","title":"Low-level components of analytic activity in information visualization"}],"id":"815c6ca281536d18ec0eb408b6e46e72a0826163","summary":"P A C H - I NC O, a 62B code language model for Python computational notebooks, which outperforms public code LMs and explores few-shot prompting strategies to elicit better code with step-by-step decomposition and NL explanation, showing the potential to improve the diversity and explainability of model predictions."},{"url":"https://www.semanticscholar.org/paper/433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context","venue":"ArXiv","year":2022,"referenceCount":0,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Yangruibo Ding,Zijian Wang,Wasi Uddin Ahmad,M. Ramanathan,Ramesh Nallapati,Parminder Bhatia,D. Roth,Bing Xiang","citations":[{"paperId":"349ec9ca2ff17dbc1aa1caf27f98b7cd85a4747e","title":"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation"}],"references":[{"paperId":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"4b27f18bff43d605805c92696a979714ced0b805","title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"467306672c89d4a0a7c6bc733814605c53bbfa97","title":"Towards Learning (Dis)-Similarity of Source Code from Program Contrasts"},{"paperId":"5fde5c44197473ad2ac0645f643e577188b316c4","title":"Can Machines Read Coding Manuals Yet? - A Benchmark for Building Better Language Models for Code Understanding"},{"paperId":"b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"91670df8aae26546da4fac58599e6ecc14708776","title":"CoCoSum: Contextual Code Summarization with Multi-Relational Graph Neural Network"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"6714dff86284cbde6be351e3e1f8ddee1bfadb9c","title":"A Toolkit for Generating Code Knowledge Graphs"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"00549af4bc3270e0f688acbf694f912d7ee39cad","title":"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks"},{"paperId":"516497e7fc39ebb126bed28dcc5c6129e500acb8","title":"An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"07c4549be429a52274bc0ec083bf5598a3e5c365","title":"Modeling and Discovering Vulnerabilities with Code Property Graphs"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"346ccc20d5b8b8dec2a627af4d15a49db92d2f4e","title":"Using structural context to recommend source code examples"},{"paperId":"289baa69e259aee340352eb71831f1f81c3c47f2","title":"Automatic method completion"},{"paperId":"be51aaa0f8a5a12274082e9d554d736373405a08","title":"Component rank: relative significance rank for software component search"},{"paperId":"fc041feb5271efca959d0f54722338d419330cf0","title":"Hipikat: recommending pertinent software development artifacts"},{"paperId":"90e9139cc21dc074767e0f8927f73a597dd1fd3d","title":"Supporting reuse by delivering task-relevant and personalized information"},{"paperId":"877e314d3a9f9317c162309c9ee0c660878a4bdb","title":"On the criteria to be used in decomposing systems into modules"},{"paperId":"1bc13b40746ccc95967746b73e11d837e3425154","title":"The structure and value of modularity in software design"},{"paperId":"aaaacfab112e465baebb4648a6808fe821f67e88","title":"CodeWeb: data mining library reuse patterns"},{"paperId":"8ced2d5d52413c2104b808c7bbc751c611daa75a","title":"Integrating active information delivery and reuse repository systems"},{"paperId":"a9e14144a391131c849b5496f04698fd71646843","title":"The reuse of uses in Smalltalk programming"},{"paperId":"f7376b069da8e403f0f8ebd54d02a8c44ff89aad","title":"Retrieving software objects in an example-based programming environment"},{"paperId":"ece80f049a527954af1c153d61cafee5789c2afe","title":"The program dependence graph and its use in optimization"},{"paperId":"3dfa629816bae8a681cc47de6a737abf8858f4fe","title":"The Modular Structure of Complex Systems"}],"id":"433def684b5a9de5a9163f50b9004a44a11128b1","summary":"A framework that incorporates cross-file context to learn the in-file and cross- file context jointly on top of pretrained code LMs is proposed, COCOMIC, which successfully improves the existing code LM with a 19.30% relative increase in exact match and a 15.41%relative increase in identifier matching for code completion when the cross-line context is provided."},{"url":"https://www.semanticscholar.org/paper/20e4ca6368a0731d3cbbd2e68aa90cef7384dc72","title":"Multi-lingual Evaluation of Code Generation Models","venue":"ArXiv","year":2022,"referenceCount":73,"citationCount":9,"influentialCitationCount":2,"publicationDate":"26/10/2022","authors":"Ben Athiwaratkun,Sanjay Krishna Gouda,Zijian Wang,Xiaopeng Li,Yuchen Tian,Ming Tan,Wasi Uddin Ahmad,Shiqi Wang,Qing Sun,Mingyue Shang,Sujan Kumar Gonugondla,Hantian Ding,Varun Kumar,Nathan Fulton,A. Farahani,Siddharth Jain,Robert Giaquinto,Haifeng Qian,M. Ramanathan,Ramesh Nallapati,Baishakhi Ray,Parminder Bhatia,Sudipta Sengupta,D. Roth,Bing Xiang","citations":[{"paperId":"eb55442d940accc67d6790e66d3196e1941fc7a4","title":"Teaching Large Language Models to Self-Debug"},{"paperId":"3b01b5b497e1f359b11da45af029281ee6f64c2c","title":"Towards Enhancing In-Context Learning for Code Generation"},{"paperId":"fbdd496c421e050a47c4fb2e0019635d2f4b97e7","title":"Meet in the Middle: A New Pre-training Paradigm"},{"paperId":"535a3d95a743e1f4b591b5b2af3e778a6347158a","title":"CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code"},{"paperId":"27c2ddc921befc44f7b2cdb8bb39facdd8b0b188","title":"Measuring The Impact Of Programming Language Distribution"},{"paperId":"1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a","title":"SantaCoder: don't reach for the stars!"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning"}],"references":[{"paperId":"27c2ddc921befc44f7b2cdb8bb39facdd8b0b188","title":"Measuring The Impact Of Programming Language Distribution"},{"paperId":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"89daa253cfd707958b1539ec4d8ea9664e8ceb7d","title":"NL-Augmenter: A Framework for Task-Sensitive Natural Language Augmentation"},{"paperId":"99f85119f113b5498517928eff74a904b69e37b7","title":"CCTEST: Testing and Repairing Code Completion Systems"},{"paperId":"7018ff49ca3b81f2ed6228b097a471c2529986e4","title":"CoditT5: Pretraining for Source Code and Natural Language Editing"},{"paperId":"3624c75561695cddbdb03dd11598572532309352","title":"Code Translation with Compiler Representations"},{"paperId":"660fde2f51e025638b8c937bf228ecaa5c5b649c","title":"XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"8342b592fe238f3d230e4959b06fd10153c45db1","title":"Training Compute-Optimal Large Language Models"},{"paperId":"d6b7bc4e7968398101d8d9b4da7d4a0186763ff3","title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"6074c7b75f27ca9adb6d74b080c07d5d079c3ea0","title":"Improving automatically generated code from Codex via Automated Program Repair"},{"paperId":null,"title":"2022) extended the dataset in Go and Rust languages"},{"paperId":null,"title":"2021) improves upon CodeBERT by leveraging AST and data flow"},{"paperId":null,"title":"2022) introduce execution result–based minimum Bayes"},{"paperId":null,"title":"2022) introduce a new dataset which is parallel across 7 programming languages"},{"paperId":null,"title":"Jangda. A scalable and extensible approach to benchmarking nl2code for 18 programming languages, 2022"},{"paperId":null,"title":"2022), and CodeGen (Nijkamp et al., 2022)"},{"paperId":null,"title":"In addition, researchers proposed various ways of improving code generation models. For example, Poesia et al. (2022) propose Target Similarity Tuning for code retrieval augmentation and Con"},{"paperId":"f3a332ff1b73acda482e5d83696b2c701f487819","title":"P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks"},{"paperId":"2016e814eed00b0c0a9358e193e29854e0ed526f","title":"Long-Range Modeling of Source Code Files with eWASH: Extended Window Access by Syntax Hierarchy"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"c2b98ea55333895f736b9267414b4c9b63b9d04b","title":"AVATAR: A Parallel Corpus for Java-Python Program Translation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":null,"title":"MBPP: PYTHON Note that we convert the original MBPP dataset (Austin et al., 2021) which has a slightly different format into HumanEval format (Chen et al., 2021) with function signature and docstring"},{"paperId":null,"title":"2021) composed a token and line completion"},{"paperId":null,"title":"2021) presented a method generation dataset in Python based on"},{"paperId":"725264948d7b6946259af5b8d966e996b9570f99","title":"DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":null,"title":"2020a) collected a corpus of parallel functions in Java, Python"},{"paperId":null,"title":"In this setup, we use a complete function in Python as an input prompt. The transcoder model then generates a complete function in Java and C++"},{"paperId":"c6801d553a43530b192309ef4364a43e33e4067f","title":"Data augmentation using back-translation for context-aware neural machine translation"},{"paperId":"0d1f3f5a6bf3393c517f07f3fb2107e4ad5a7e85","title":"Improving Neural Machine Translation Robustness via Data Augmentation: Beyond Back-Translation"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"8323c591e119eb09b28b29fd6c7bc76bd889df7a","title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"e65c84e2778d7b13b7541e6b14ff790b624a24ec","title":"A Study of BFLOAT16 for Deep Learning Training"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":null,"title":"2019) (e.g., “create a function” to “write one function"},{"paperId":"7d41dbfa22eb5a425fc0ff27db41aaeb75b1201c","title":"Introducing MathQA - A Math-Aware Question Answering System"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"6c6170ffb39cdc8cfffbeda9c7a2259eda5875f2","title":"Tree-to-tree Neural Networks for Program Translation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"554c9b3aa8c3b7504810270b18b942a0d22156e7","title":"DeepFix: Fixing Common C Language Errors by Deep Learning"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"5539a60b8e30ca0e278829880cee2a1ec65fa677","title":"Using machine translation for converting Python 2 to Python 3 code"},{"paperId":"4755b856dc08ac024ae935e7a6f9df325b00ae53","title":"Phrase-Based Statistical Translation of Programming Languages"},{"paperId":"5447a3b8701d59f3a2f1a7f7af030f687ba495c3","title":"Lexical statistical machine translation for language migration"},{"paperId":"110b7719f16eae8d82af0f56fca6e802c842c181","title":"Mining source code repositories at massive scale using language modeling"},{"paperId":"68c03788224000794d5491ab459be0b2a2c38677","title":"WordNet: A Lexical Database for English"},{"paperId":null,"title":"28 var arg01 : Int = 2 29 var arg02 : Int = 2 30 var x0 : Int = minCost(cost : arg00, m : arg01, n : arg02) 31 var v0 : Int = 8 32 assert(x0 == v0"},{"paperId":null,"title":"You are an expert Perl programmer, and here is your task"},{"paperId":null,"title":"16)){} else { throw 'Error at 3th assert statement. Value = ' + JSON.stringify(x ) }"},{"paperId":null,"title":") 34 if(compare(x, 16)){} else { throw 'Error at 3th assert statement"},{"paperId":null,"title":"Exception --test case 1 did not pass"},{"paperId":null,"title":"# Write a function to find the minimum cost path to reach (m, n) from (0, 0) for the given cost matrix cost"},{"paperId":null,"title":"Error at test case 2\" 33 end 34 x = min_cost"},{"paperId":null,"title":"18 for (let j = 0; j <= n; j++) { 19 dp"},{"paperId":null,"title":"Error at 2th assert statement"}],"id":"20e4ca6368a0731d3cbbd2e68aa90cef7384dc72","summary":"This work presents MBXP, an execution-based code completion benchmark in 10+ programming languages that is able to evaluate code generation models in a multi-lingual fashion, and discovers generalization ability of language models on out-of-domain languages, advantages of large multi-lingsual models over mono-lingUAL, benefits of few-shot prompting, and zero-shot translation abilities."},{"url":"https://www.semanticscholar.org/paper/7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":45,"citationCount":80,"influentialCitationCount":21,"publicationDate":"25/05/2021","authors":"Ruchi Puri,David S. Kung,G. Janssen,Wei Zhang,Giacomo Domeniconi,Vladmir Zolotov,Julian Dolby,Jie Chen,M. Choudhury,Lindsey Decker,Veronika Thost,Luca Buratti,Saurabh Pujar,Ulrich Finkler","citations":[{"paperId":"95d9787911db07ae00c896c64f7b24a447302875","title":"Leveraging Static Analysis for Bug Repair"},{"paperId":"70ebcc3716583b7bc0d59e443c83d3ac522b79e9","title":"RunBugRun -- An Executable Dataset for Automated Program Repair"},{"paperId":"762b4b11220d2d8a9c57f0a3af327840a67e7284","title":"Self-Refine: Iterative Refinement with Self-Feedback"},{"paperId":"2eb8342c2d16f28c823e0b747fa71a00f1218ed1","title":"Knowledge Transfer for Pseudo-code Generation from Low Resource Programming Language"},{"paperId":"d260808661c7ee37873f9d24261217ab24370f5e","title":"Implant Global and Local Hierarchy Information to Sequence based Code Representation Models"},{"paperId":"a7396ba28087cab3c4ad46928fe73e9760e605b0","title":"Boosting Source Code Learning with Data Augmentation: An Empirical Study"},{"paperId":"fd585ba7c0caf1a55e3b879d987e789238a53c5a","title":"FalconCode: A Multiyear Dataset of Python Code Samples from an Introductory Computer Science Course"},{"paperId":"66699763017d1e7c5cd54179526e05e654806089","title":"A Game-Based Framework to Compare Program Classifiers and Evaders"},{"paperId":"d8405996b4d08c304098636aedd9e1c1a1e262ee","title":"The Premature Obituary of Programming"},{"paperId":"42630c03d3817b1153d245f20742ad4b30a80b75","title":"JEMMA: An extensible Java dataset for ML4Code applications"},{"paperId":"41f5e1ad7793593befc0b9c38f756836e8b07c98","title":"Code4ML: a Large-scale Dataset of annotated Machine Learning Code"},{"paperId":"bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence"},{"paperId":"0731c710fc09fe7c039aee3e9bff006bf94565aa","title":"XTest: A Parallel Multilingual Corpus with Test Cases for Code Translation and Its Evaluation*"},{"paperId":"d6d9c368aae753c886c0beef888ebff6f3d0dca0","title":"Unsupervised Translation of Programming Language - A Survey Paper"},{"paperId":"9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming"},{"paperId":"e052fd3e1480a501c3145f53ad5ddb4526efbb21","title":"Source Code Preprocessing Method Analysis in Unit Test Code Classification"},{"paperId":"621009f1c30951b7c952c65c45ef0064a204e91e","title":"Early Experience with Transformer-Based Similarity Analysis for DataRaceBench"},{"paperId":"00aacec39159bcd92a412aa314b376c3378c49cb","title":"Improvement of Vulnerable Code Dataset Based on Program Equivalence Transformation"},{"paperId":"20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"66c52c6ef45cdd77c086996bcaaf01470e82dbd2","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?"},{"paperId":"8ffe7b7eeddbd4c22d642b0a48379d17e61c3bab","title":"Soft-Labeled Contrastive Pre-training for Function-level Code Representation"},{"paperId":"8b58130ecb302a2f0e78e9ffb7115cb4906cb966","title":"Towards Robust Models of Code via Energy-Based Learning on Auxiliary Datasets"},{"paperId":"05f225085154e4326af07f7c8f273156f132aa70","title":"Geração Automática de Benchmarks para Compilação Preditiva"},{"paperId":"0b8772b7790c69f40897b5eb7f8fd57f24138f3d","title":"ContraGen: Effective Contrastive Learning For Causal Language Model"},{"paperId":"363758e9e296adc9391ed731e834809cf5d4c19b","title":"Are machine programming systems using right source-code measures to select code repositories?"},{"paperId":"939b4b1ff5a21108bb2f8c81117f1d5b230180a9","title":"Extreme Multi-Domain, Multi-Task Learning With Unified Text-to-Text Transfer Transformers"},{"paperId":"a7435722d8ab595da5a9c70ac9160f57d0dcd75a","title":"Enabling Transformers to Understand Low-Level Programs"},{"paperId":"89b58765614bd6c52baca0006d67f64985d2204e","title":"Topical: Learning Repository Embeddings from Source Code using Attention"},{"paperId":"da78bab10019e530a93584f60b9224e353d90f2a","title":"A Tree-structured Transformer for Program Representation Learning"},{"paperId":"1444ed03083523a4413d9f15f2200007447771db","title":"A Library for Representing Python Programs as Graphs for Machine Learning"},{"paperId":"713bd2971116098211ef06336dfbe91a69854404","title":"Probing Semantic Grounding in Language Models of Code with Representational Similarity Analysis"},{"paperId":"8bf89cf8f18f08a43fd3d058687987666996b995","title":"PST: Measuring Skill Proficiency in Programming Exercise Process via Programming Skill Tracing"},{"paperId":"3624c75561695cddbdb03dd11598572532309352","title":"Code Translation with Compiler Representations"},{"paperId":"62851a515ea0ee3a547d94e8a493d978c22d0be9","title":"Multilingual Code Snippets Training for Program Translation"},{"paperId":"2edc8efcda27c944a46f367acf6a5280b8f65525","title":"FixEval: Execution-based Evaluation of Program Fixes for Competitive Programming Problems"},{"paperId":"6df72c1b2972703e04ca509dd277169ad8eee594","title":"FixEval: Execution-based Evaluation of Program Fixes for Programming Problems"},{"paperId":"73b4f4c31852273b38868f2bd362abeafce40232","title":"CodeS: Towards Code Model Generalization Under Distribution Shift"},{"paperId":"c6e6cb19e7055f3d0616c3314a85b0914132ae40","title":"CodeS: A Distribution Shift Benchmark Dataset for Source Code Learning"},{"paperId":"1100dee3fd78655cddc4b7bfaef1161351d4fab5","title":"Automated Feedback Generation for Competition-Level Code"},{"paperId":"5514b87e34db2b34bd9a9b995894243f91435efc","title":"Learning to Represent Programs with Code Hierarchies"},{"paperId":"d18287d5ef8653aa1276a11957f2b3934c7c93e1","title":"CodeAttack: Code-based Adversarial Attacks for Pre-Trained Programming Language Models"},{"paperId":"636f854b1a3a983e6803eae0277179596cc2cb95","title":"Summarize and Generate to Back-translate: Unsupervised Translation of Programming Languages"},{"paperId":"6074c7b75f27ca9adb6d74b080c07d5d079c3ea0","title":"Improving automatically generated code from Codex via Automated Program Repair"},{"paperId":"0bcd59da541fdae66884afba8d25475a54a9da1a","title":"Automated Repair of Programs from Large Language Models"},{"paperId":"c1ddf0006e1aa0d5551e1ba1ad734ec0ecf27fd0","title":"CV4Code: Sourcecode Understanding via Visual Code Representations"},{"paperId":"0121c151c96b32cd7851e4bcda2a468b279c1e6f","title":"CODE-MVP: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training"},{"paperId":"703f79763d534dbf9674132cec890f432dcc19ec","title":"A Survey of Deep Learning Models for Structural Code Understanding"},{"paperId":"80b2b006ed2f26ec3ddc91e303dc9861fb456a26","title":"On The Cross-Modal Transfer from Natural Language to Code through Adapter Modules"},{"paperId":"c765091cb8bec8448669351f3662101c307c03c4","title":"Zero-Shot Program Representation Learning"},{"paperId":"1413acc991434ee36248b282b4cedac77ade1737","title":"Evaluating few shot and Contrastive learning Methods for Code Clone Detection"},{"paperId":"5e5a7f8423e0b990bbe1c85a999da86f16ee68a3","title":"LaF: Labeling-Free Model Selection for Automated Deep Neural Network Reusing"},{"paperId":"8c8bf30828bc789be679f29ee08cc6cdebd36600","title":"Characterizing and Understanding the Behavior of Quantized Models for Reliable Deployment"},{"paperId":"2b556fcf2ac634f03c1fb0ace5e602e829418e65","title":"ReACC: A Retrieval-Augmented Code Completion Framework"},{"paperId":"4b27f18bff43d605805c92696a979714ced0b805","title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation"},{"paperId":"c125b0be73c8493ebc27beb572f6c1b21d6b4ae4","title":"Static Prediction of Runtime Errors by Learning to Execute Programs with External Resource Descriptions"},{"paperId":"7438626a757c5442b9c0fb37b54ec0fe7e1889c3","title":"Better Together? An Evaluation of AI-Supported Code Translation"},{"paperId":"76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","title":"Investigating Explainability of Generative AI for Code through Scenario-based Design"},{"paperId":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"9f0852ce9338c00135fe39426d893a36a289e5d5","title":"GraphCode2Vec: Generic Code Embedding via Lexical and Program Dependence Analyses"},{"paperId":"b2c0e903b79835b6ee8fd553c2213ea8abbf7864","title":"Senatus - A Fast and Accurate Code-to-Code Recommendation Engine"},{"paperId":"cc80244d69c2c5bb7fe79fc6ca2561f2bed830ca","title":"Learning to Represent Programs with Heterogeneous Graphs"},{"paperId":"7d500d6bd3ae49fa3acb213fd25d5b11566e64fd","title":"Labeling-Free Comparison Testing of Deep Learning Models"},{"paperId":"6f84d0cc33c7c58f74b28ddcc1cbda91ea608c9f","title":"C ODE S UMMARIZATION : D O T RANSFORMERS R E ALLY U C"},{"paperId":"b98d6fe8f0ef02ec0d1bb2bcfb924c8f01feb7d4","title":"Convergent Representations of Computer Programs in Human and Artificial Neural Networks"},{"paperId":"b2c4fdb49bdb23e6ad0ac3272029324157046ea7","title":"Automated Repair of Code from Language Models"},{"paperId":"56e6d62c638a24411f12d15cdc8821a31fc495c8","title":"Source Code Generation from Descriptions in a Natural Language"},{"paperId":"ddab94478a7647ee136b1f6b5076417db3074d0f","title":"Machine Programming: Turning Data into Programmer Productivity"},{"paperId":"3b0cf543a730e674d4213d344ebc857fada76ead","title":"Understanding High-Level Properties of Low-Level Programs Through Transformers"},{"paperId":"d4f16dae4ab1d21db3c0b0bd7b54f4b62e2d1b85","title":"The Effectiveness of Transformer Models for Analyzing Low-Level Programs"},{"paperId":"eeadabea580953c14bb00ca99b41ee9b2cef6300","title":"Energy-bounded Learning for Robust Models of Code"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"7fba3100768fa3aac0fbf961d5e894b2f629e6e6","title":"Federated Data Science to Break Down Silos [Vision]"},{"paperId":"4e7de32c8da8c910285acdaf397347dc94ca3594","title":"Many Heads but One Brain: Fusion Brain -- a Competition and a Single Multimodal Multitask Architecture"},{"paperId":"58b142663367ef6ed67507e3d7591b6e384a6937","title":"Deep Distilling: automated code generation using explainable deep learning"},{"paperId":"21363c1ac138d8df80b20af4848b5113bd3bf6f8","title":"Clone-advisor: recommending code tokens and clone methods with deep learning and information retrieval"},{"paperId":"0c7cb854756f6b69f070a925cd497c2970b136f2","title":"DeSkew-LSH based Code-to-Code Recommendation Engine"},{"paperId":"4885e616e85d420576196b2578525cbc501137ec","title":"Programming and execution models for next generation code intelligence systems (keynote)"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"fd0bccf5e7c1fb5dadd75972e3212554fb255fe2","title":"MISIM: A Neural Code Semantics Similarity System Using the Context-Aware Semantics Structure"}],"references":[{"paperId":"bea6af010fc02f5ac29edfc17096be6078edab46","title":"A Survey on Deep Learning for Software Engineering"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"a989f5daa7c438d6b8433efbb1461f1b0af1aa8e","title":"Break-It-Fix-It: Unsupervised Learning for Program Repair"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"20d37eb44ad65735f243938961fde9ba5b4d26b7","title":"D2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using Differential Analysis"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"c6337dc83db09c9648ae850c71937eb8e5fd7a43","title":"Directed Acyclic Graph Neural Networks"},{"paperId":"59cc0871534a20ac98db0663539c6675ab63566f","title":"SequenceR: Sequence-to-Sequence Learning for End-to-End Program Repair"},{"paperId":"406afe68e789fcb0d7e3d24ebea65b53d206f740","title":"Exploring Software Naturalness through Neural Language Models"},{"paperId":"0d93d1943ebd0372d46ec1bdff4cb27c44a237a2","title":"MISIM: A Novel Code Similarity System."},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"597bd2e45427563cdf025e53a3239006aa364cfc","title":"Open Graph Benchmark: Datasets for Machine Learning on Graphs"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"00549af4bc3270e0f688acbf694f912d7ee39cad","title":"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks"},{"paperId":"311caedb65fff281470100585d90f2c672a9f97f","title":"Cyber Security Threats Detection in Internet of Things Using Deep Learning Approach"},{"paperId":"3b99ae042727b7bf679a8e4120b28999d6530ff6","title":"Graph Matching Networks for Learning the Similarity of Graph Structured Objects"},{"paperId":"63a513832f56addb67be81a2fa399b233f3030fc","title":"Fast Graph Representation Learning with PyTorch Geometric"},{"paperId":"516497e7fc39ebb126bed28dcc5c6129e500acb8","title":"An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"def6c6912c0120b40b7fefb24aa68708bb357d50","title":"Aroma: code recommendation via structural code search"},{"paperId":"62ed9bf1d83c8db1f9cbf92ea2f57ea90ef683d9","title":"How Powerful are Graph Neural Networks?"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"41c8cabe475d85d7548c25935da19ea5b96dfe06","title":"Neural Code Comprehension: A Learnable Representation of Code Semantics"},{"paperId":"6989e13df80edfc6e638e8d8502cb0739d494ca6","title":"Machine Learning in Compiler Optimization"},{"paperId":"6c6170ffb39cdc8cfffbeda9c7a2259eda5875f2","title":"Tree-to-tree Neural Networks for Program Translation"},{"paperId":"36652428740cd30d245d55889f01a7fb04a91c93","title":"Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning"},{"paperId":"e3d772986d176057aca2f5e3eb783da53b559134","title":"Unsupervised Machine Translation Using Monolingual Corpora Only"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"554c9b3aa8c3b7504810270b18b942a0d22156e7","title":"DeepFix: Fixing Common C Language Errors by Deep Learning"},{"paperId":"36eff562f65125511b5dfab68ce7f7a943c27478","title":"Semi-Supervised Classification with Graph Convolutional Networks"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"993d2f3c404f1f7af879ee1afddf09f943b01508","title":"Large-Scale Code Clone Detection"},{"paperId":"a85012088ae447fca09c0642cc89f6e0cc0619ac","title":"オンラインジャッジの開発と運用 -Aizu Online Judge-"},{"paperId":"3c902294cb3230f81c504b63afffbad41cc302a1","title":"TBCNN: A Tree-Based Convolutional Neural Network for Programming Language Processing"},{"paperId":"110b7719f16eae8d82af0f56fca6e802c842c181","title":"Mining source code repositories at massive scale using language modeling"},{"paperId":"4b2d92d6e04cbdfd0dc9a10fa6f134fd5aa477c3","title":"The Definitive ANTLR 4 Reference"},{"paperId":null,"title":"Belongie , and Ser - Nam Lim . A metric learning reality check"},{"paperId":null,"title":"A metric learning reality check. CoRR, abs"},{"paperId":null,"title":"Ankur Singh . \" end - to - end masked language modeling with bert \""},{"paperId":null,"title":"Each sample is pre-processed in the same way as the training samples and one token (never a padding) is arbitrarily replaced by the"},{"paperId":null,"title":"530 Clement , Dawn Drain , Daxin Jiang , Duyu Tang , Ge Li , Lidong Zhou , Linjun Shou , Long Zhou"},{"paperId":null,"title":"Women in data science"}],"id":"7547680408358916e66917d03436fca7540a7528","summary":"Project CodeNet is a first-of-its-kind, very large scale, diverse, and high-quality dataset to accelerate the algorithmic advancements in AI for Code, which consists of 14M code samples and about 500M lines of code in 55 different programming languages."},{"url":"https://www.semanticscholar.org/paper/a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models","venue":"ArXiv","year":2021,"referenceCount":102,"citationCount":197,"influentialCitationCount":35,"publicationDate":"16/08/2021","authors":"Jacob Austin,Augustus Odena,Maxwell Nye,Maarten Bosma,H. Michalewski,David Dohan,Ellen Jiang,Carrie J. Cai,Michael Terry,Quoc V. Le,Charles Sutton","citations":[{"paperId":"5ebb98f1f7edcb700266d7dd6ecb48428a70435a","title":"Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy to Fine-Tune and Hard to Detect with other LLMs"},{"paperId":"dd0960a8ffb6a8baff69ec96c3f67deb2912b53a","title":"Self-collaboration Code Generation via ChatGPT"},{"paperId":"34d12432af63915caf14eab9a362f7e7d24e4c13","title":"Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions"},{"paperId":"f352a968c8735fac58912870a7bde57fcfc2e6bd","title":"\"It's Weird That it Knows What I Want\": Usability and Interactions with Copilot for Novice Programmers"},{"paperId":"c715914c388fa64dd8686cd8755e5adfebbf2388","title":"REFINER: Reasoning Feedback on Intermediate Representations"},{"paperId":"a9a92a5c1d9c3b53bfe5f9a027baabbbb119cbc2","title":"Rolling the Dice: Imagining Generative AI as a Dungeons & Dragons Storytelling Companion"},{"paperId":"a845bce29105f55dab1e47d2f92d7338eb183a4e","title":"Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT"},{"paperId":"3b01b5b497e1f359b11da45af029281ee6f64c2c","title":"Towards Enhancing In-Context Learning for Code Generation"},{"paperId":"cbdc75e9c80870164662bb0359e23fd4736c5f0e","title":"A Survey of Large Language Models"},{"paperId":"f24dabf3317abaa3d7393ab7d48115f353749bde","title":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X"},{"paperId":"28dafb94a87daa71ad3edf9f04f3c8c32753398b","title":"Improving Code Generation by Training with Natural Language Feedback"},{"paperId":"c11810fa8887b678facea62da4607c4898360308","title":"Training Language Models with Language Feedback at Scale"},{"paperId":"c8f2aced926707fba8a0535a6df5b5823d394bac","title":"AI-Generated Content (AIGC): A Survey"},{"paperId":"645960e8bcb2a55bc7e8816c49e60e8f98303acb","title":"Combining Contexts from Multiple Sources for Documentation-Specific Code Example Generation"},{"paperId":"b4a53ce6ece0c76ab79239c30a319aa8cfc64a62","title":"Neural Interpretation of Generic Source Code"},{"paperId":"362cbfd0d05e139cd6cf049754098a6e1520b910","title":"PanGu-{\\Sigma}: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing"},{"paperId":"af622206f1e5a6632600075897385cdabcae463d","title":"Large Language Models and Simple, Stupid Bugs"},{"paperId":"7857d80f516a943f96757b21053f7635ac1cf2af","title":"Revisiting the Plastic Surgery Hypothesis via Large Language Models"},{"paperId":"8205612a6d15df52b8c3d26aabfd50abc10fdee3","title":"LLMSecEval: A Dataset of Natural Language Prompts for Security Evaluations"},{"paperId":"e1a17cb16742cf484aedc9ae106363d82454172e","title":"Practices and Challenges of Using GitHub Copilot: An Empirical Study"},{"paperId":"fbdd496c421e050a47c4fb2e0019635d2f4b97e7","title":"Meet in the Middle: A New Pre-training Paradigm"},{"paperId":"d39db76fb80c1f74455a0f6432c2242b727389da","title":"Self-planning Code Generation with Large Language Model"},{"paperId":"aae61ba5b629eba965b7f49a685b3d9f1bfb358c","title":"Model-Agnostic Syntactical Information for Pre-Trained Programming Language Models"},{"paperId":"eb0b9965732ce09b67e088efdbe0978aeafcdcc6","title":"Greener yet Powerful: Taming Large Code Generation Models with Quantization"},{"paperId":"8d03ebc375b3d887056e769018467bf0a6ed99e3","title":"Planning with Large Language Models for Code Generation"},{"paperId":"94ac118c522c065e24d9090b9f7827b83e8ff2f4","title":"Many bioinformatics programming tasks can be automated with ChatGPT"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"57ffd52abfbba0eb76c8406c7331f1f8ca29aaff","title":"R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents"},{"paperId":"35d083ff4dc2805c477d0aedc8158fc7a1aefeda","title":"EvoPrompting: Language Models for Code-Level Neural Architecture Search"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"6d269364de402d4a72ac30b0c8d81324f6849807","title":"LEVER: Learning to Verify Language-to-Code Generation with Execution"},{"paperId":"757ce0529971d496e3c17155b405747f73bc18c3","title":"GLUECons: A Generic Benchmark for Learning Under Constraints"},{"paperId":"1786a2f9140ed7211b21302977de64e948b92308","title":"Learning Performance-Improving Code Edits"},{"paperId":"535a3d95a743e1f4b591b5b2af3e778a6347158a","title":"CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code"},{"paperId":"79680e20f0c8038039b243fb5fd385fbe967c799","title":"Controlling Large Language Models to Generate Secure and Vulnerable Code"},{"paperId":"782f3d43b37790a83c98d5fd3ef142b296f20616","title":"CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code Models"},{"paperId":"27c2ddc921befc44f7b2cdb8bb39facdd8b0b188","title":"Measuring The Impact Of Programming Language Distribution"},{"paperId":"68edfd62d2619fb4af7c2469edb95b9e2fe4544a","title":"Execution-based Code Generation using Deep Reinforcement Learning"},{"paperId":"8bfc22de7fe66286ad9ae705d677246757fbf8a8","title":"Large Language Models Can Be Easily Distracted by Irrelevant Context"},{"paperId":"a23e0cf74f10b6a3061ab71497fe2c8c476fecd1","title":"Conversational Automated Program Repair"},{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"},{"paperId":"d8405996b4d08c304098636aedd9e1c1a1e262ee","title":"The Premature Obituary of Programming"},{"paperId":"9b5fb07df99b0dd65f3058701d7f017c3a70c144","title":"Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data"},{"paperId":"1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a","title":"SantaCoder: don't reach for the stars!"},{"paperId":"dae74645479f7c1fa3671066f9e24ec6c20c17ec","title":"TrojanPuzzle: Covertly Poisoning Code-Suggestion Models"},{"paperId":"2abed82162c47a0cc32cd62afcf46b0745541017","title":"Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book"},{"paperId":"41f5e1ad7793593befc0b9c38f756836e8b07c98","title":"Code4ML: a Large-scale Dataset of annotated Machine Learning Code"},{"paperId":"0566c1c3eeeef5c968fced6d80b77fe22d02bbd9","title":"Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language"},{"paperId":"66476832701361c9f9b2a7eb2354ee8cd9f72e67","title":"Large Language Models are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"d3a7a4543d83f568f79d1febe8379465ff0140c9","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"269df328eec08b56b7b1f38a7555797fe2b999b6","title":"ReCode: Robustness Evaluation of Code Generation Models"},{"paperId":"bf5fbd690f24f873df86d1b0a06579cf42f7dc36","title":"Dialog2API: Task-Oriented Dialogue with API Description and Example Programs"},{"paperId":"bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"9e8f0d9ba4e7af673c8b214b3764e020706dd1f3","title":"Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments"},{"paperId":"6bc87e51018d6de55011e95a0d43c588dd44a1e8","title":"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages"},{"paperId":"9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming"},{"paperId":"8175ce2cbb99b6a394bdac152ae39d413f4f1380","title":"Codex Hacks HackerRank: Memorization Issues and a Framework for Code Synthesis Evaluation"},{"paperId":"ebd4bec684808aff360b5f255d15c0d112ba13d3","title":"Explicit Knowledge Transfer for Weakly-Supervised Code Generation"},{"paperId":"27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation"},{"paperId":"c192fb33f308f19ac8a5c4c2d623d56385b839be","title":"Systematic Literature Review on Solving Competitive Programming Problem with Artificial Intelligence (AI)"},{"paperId":"20b60fb3993d2e9a5af04611f7bdf248e5a3a736","title":"Programming by Example and Text-to-Code Translation for Conversational Code Generation"},{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models"},{"paperId":"ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","title":"Evaluating How Fine-tuning on Bimodal Data Effects Code Generation"},{"paperId":"4610ffb1b016acaa82a2065ffd1a3adbae1ce722","title":"Large Language Models Are Human-Level Prompt Engineers"},{"paperId":"3cba16fc46ac5b35c1cc72a822208aa0097384cc","title":"CodePAD: Sequence-based Code Generation with Pushdown Automaton"},{"paperId":"4ddc26b3a5fe9044b97b408d163f7464d769ebbf","title":"CODEP: Grammatical Seq2Seq Model for General-Purpose Code Generation"},{"paperId":"20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"538288d24bdad73d831dfed44b706958287ed318","title":"Generating Sequences by Learning to Self-Correct"},{"paperId":"cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers"},{"paperId":"e7d0a8eb7e98863f37b51d89b5ca305b04aaba99","title":"SemanticOn: Specifying Content-Based Semantic Conditions for Web Automation Programs"},{"paperId":"2577d053f8aab912d29b424e1f09133d83740fd2","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"e8db669c8cb1c07557ede15e2771968f9370330b","title":"Large language models are not zero-shot communicators"},{"paperId":"66c52c6ef45cdd77c086996bcaaf01470e82dbd2","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?"},{"paperId":"472d87be3dc298102e058be55a814cc6d2085b39","title":"Towards Deceptive Defense in Software Security with Chaff Bugs"},{"paperId":"a6ef5bad716091fb1888bf365f6129628ab3a5ee","title":"ObSynth: An Interactive Synthesis System for Generating Object Models from Natural Language Specifications"},{"paperId":"663a41c866d49ce052801fbc88947d39764cad29","title":"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"},{"paperId":"13d3733e0dabbb4ccdd036a5f04fd5b3e39eecb0","title":"Vision Transformers provably learn spatial structure"},{"paperId":"39e40821b7207125e54e6ed7112e55cd38c6f0c3","title":"Language Models of Code are Few-Shot Commonsense Learners"},{"paperId":"045a6f92b58f08d5b305dd5d661316a506ee4d43","title":"Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation"},{"paperId":"0c78a473e33a81246d5c0fbbda7e7de168814c18","title":"FlexType: A Plug-and-Play Framework for Type Inference Models"},{"paperId":"3ee3f425482cf86989d809155cc8cf2bf8d8113e","title":"Understanding HTML with Large Language Models"},{"paperId":"62f0db3a5ad5c795ec18fc7a6e7b01836809df57","title":"Language Models are Multilingual Chain-of-Thought Reasoners"},{"paperId":"8a854331c593c6a766fa3b8037fb2ad1b95a6f06","title":"An Empirical Study of Code Smells in Transformer-based Code Generation Techniques"},{"paperId":"95fa2b27ab7eb84738441ee16da97323538938f9","title":"I Speak, You Verify: Toward Trustworthy Neural Program Synthesis"},{"paperId":"2e72aaf89aea0ee494c6020ff537dd074586311e","title":"Code as Policies: Language Model Programs for Embodied Control"},{"paperId":"6b5d1e50894b1f28e4798cf20e9ffa88b9ec011a","title":"How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models"},{"paperId":"18a831422a4b4c89bbf1cc4baaa2cfcbf29daaf1","title":"Exploring and evaluating personalized models for code generation"},{"paperId":"1b4c19168410fb2690d285b205ab2281793db81a","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?"},{"paperId":"1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization"},{"paperId":"def2e28863338cb20782eb2015a39d32df697ed6","title":"Learning to Improve Code Efficiency"},{"paperId":"c5b47a34fd7e0e595360683b4effa51c0261d1e3","title":"Language Models Can Teach Themselves to Program Better"},{"paperId":"1e3ec5709b2ca43233c566cd77dbabbd6892bfa6","title":"Neurosymbolic repair for low-code formula languages"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"e37155d21818513bd40d64ee212099aac82bd6f8","title":"Less training, more repairing please: revisiting automated program repair via zero-shot learning"},{"paperId":"713bd2971116098211ef06336dfbe91a69854404","title":"Probing Semantic Grounding in Language Models of Code with Representational Similarity Analysis"},{"paperId":"b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs"},{"paperId":"0a39442979d6e678dd36bb443ad529c14e86a86e","title":"DocPrompting: Generating Code by Retrieving the Docs"},{"paperId":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models"},{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code"},{"paperId":"0f86d5ae106a53f40f89b60dff24074f6c2cd127","title":"The Case for a Single Model that can Both Generate Continuations and Fill in the Blank"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code"},{"paperId":"f1b6b34b4440a77ba86493f7062e8974062508c5","title":"Applying genetic programming to PSB2: the next generation program synthesis benchmark suite"},{"paperId":"35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs"},{"paperId":"7107d06366b48b3593c8128ed2ca67e0b413628c","title":"Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions"},{"paperId":"0efa0441da820b1905572666ba1974a06a9663fb","title":"NaturalProver: Grounded Mathematical Proof Generation with Language Models"},{"paperId":"6e10343767ab09dde83cf99ea3442907402a9810","title":"Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing"},{"paperId":"6074c7b75f27ca9adb6d74b080c07d5d079c3ea0","title":"Improving automatically generated code from Codex via Automated Program Repair"},{"paperId":"0bcd59da541fdae66884afba8d25475a54a9da1a","title":"Automated Repair of Programs from Large Language Models"},{"paperId":"c61ce808818308566124df2c8725c98d6bd38dc3","title":"A Precis of Language Models are not Models of Language"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"16168520c7efbfa84bcb609a05362916b04022bb","title":"Context-Aware Abbreviation Expansion Using Large Language Models"},{"paperId":"9a7d4c2f4309a2a38ea8e5b23c6d616fa0952d44","title":"Neural language models for network configuration: Opportunities and reality check"},{"paperId":"cdfe9580f63070f311151444f9df32818cc858bf","title":"An Empirical Evaluation of GitHub Copilot's Code Suggestions"},{"paperId":"fd7c3c8fbe8cf88bd967ead02738b43081e306a7","title":"Training Language Models with Language Feedback"},{"paperId":"4bc040835fbff57ce6612306d794b8c6c8226086","title":"Discovering the Syntax and Strategies of Natural Language Programming with Generative Language Models"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"1f87dc41bdf2c4c78e2dce9c5c8adfef5e25a70c","title":"Passport: Improving Automated Formal Verification Using Identifiers"},{"paperId":"2ba7104f7b93d77940312664f3467b8f090d6d16","title":"On Distribution Shift in Learning-based Bug Detectors"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"9a6730534295335247eebdec59b7decdeb83d59a","title":"On the Transferability of Pre-trained Language Models for Low-Resource Programming Languages"},{"paperId":"237f5ca6fcccef2b77a2212b34fb06a1dbd09b72","title":"Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"fb5c11bbf63884f75d2da615fbf37a3bcfa2bd20","title":"Wordcraft: Story Writing With Large Language Models"},{"paperId":"c347093e2dca530ce347526380b0b7aedf03a6b2","title":"CrossBeam: Learning to Search in Bottom-Up Program Synthesis"},{"paperId":"0e802c0739771acf70e60d59c2df51cd7e8c50c0","title":"Memorizing Transformers"},{"paperId":"590f6817b42407f96b079e82c935fae298196359","title":"Less is More: Summary of Long Instructions is Better for Program Synthesis"},{"paperId":"d6b7bc4e7968398101d8d9b4da7d4a0186763ff3","title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages"},{"paperId":"c125b0be73c8493ebc27beb572f6c1b21d6b4ae4","title":"Static Prediction of Runtime Errors by Learning to Execute Programs with External Resource Descriptions"},{"paperId":"1f11601f9eb8c0bee1d1d2cc7f4fa187e8c5e525","title":"The impact of lexical and grammatical processing on generating code from natural language"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"9cbc044e315cdefe9a255119037ac7c23e9abdd5","title":"Predictability and Surprise in Large Generative Models"},{"paperId":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective"},{"paperId":"76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","title":"Investigating Explainability of Generative AI for Code through Scenario-based Design"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"7428f9b16a82839e2cb6e6c7a77c1ffeab898813","title":"HEAT: Hyperedge Attention Networks"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"3d5463a16d85d9d1d0d8ebb4117a31aca3c240ff","title":"Fooling MOSS Detection with Pretrained Language Models"},{"paperId":"53c0abe83fe9b4fdaf2208295d8504fcf5241694","title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models"},{"paperId":"edc07f490c1c1b773094f236157219677f6a2f71","title":"Better Modeling the Programming World with Code Concept Graphs-augmented Multi-modal Learning"},{"paperId":"6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a","title":"Few-Shot Semantic Parsing with Language Models Trained on Code"},{"paperId":"52db8674337e5d86dcb96d013734befc8c3d4581","title":"Large Language Models are not Models of Natural Language: they are Corpus Models."},{"paperId":"05af6c968ef8c7f9b07b0d67f138780179f29511","title":"Sparks: Inspiration for Science Writing using Language Models"},{"paperId":"1aed58bd07026492194672adec494dc37c894a28","title":"Leveraging Automated Unit Tests for Unsupervised Code Translation"},{"paperId":"d3640eb3b542eaf36fee2261f037a6bf0d8eac9c","title":"AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"},{"paperId":"660ca9e15e19409903a0605f0584d0f263c35c67","title":"S YNCHROMESH : R ELIABLE C ODE G ENERATION FROM P RE - TRAINED L ANGUAGE M ODELS"},{"paperId":"ab8a53be2ff8a756776aa0245ad8da41189d60d6","title":"Fuzzing Deep-Learning Libraries via Large Language Models"},{"paperId":"239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning"},{"paperId":"b2c4fdb49bdb23e6ad0ac3272029324157046ea7","title":"Automated Repair of Code from Language Models"},{"paperId":"407b9e9478ba6bff43ce4b20e8b6cb2b303477d2","title":"P LANNING WITH L ARGE L ANGUAGE M ODELS FOR C ODE G ENERATION"},{"paperId":"8d282bf295d655e38b63ddbc7cdc94b188df8bb2","title":"G ENERATING S EQUENCES BY L EARNING TO [S ELF -]C ORRECT"},{"paperId":"2443179d421e1faf7474add557b45add554723c7","title":"Formal Premise Selection With Language Models"},{"paperId":"6df98ac2300c6e9c232440147ba976b4f501ca67","title":"C ODEX HACKS H ACKER R ANK : B ENEFITS AND R ISKS OF L ARGE -S CALE S OURCE C ODE M ODELS"},{"paperId":"6f84d0cc33c7c58f74b28ddcc1cbda91ea608c9f","title":"C ODE S UMMARIZATION : D O T RANSFORMERS R E ALLY U C"},{"paperId":"ba5d21b7c65c6598c7bd39a5d992308c205df374","title":"A S YSTEMATIC E VALUATION OF L ARGE L ANGUAGE M ODELS OF C ODE"},{"paperId":"a2b6e1f7d8a7963d321f29fca7c01eeb1ebd7f0f","title":"P ATCH G ENERATION WITH L ANGUAGE M ODELS : F EASIBILITY AND S CALING B EHAVIOR"},{"paperId":"2aec574791fd33e9be32fd5191a66734f805a6a1","title":"Training Language Models with Natural Language Feedback"},{"paperId":"75c8f2916a2067f7549cf58ea8c9061565eb1dab","title":"C ROSS B EAM : L EARNING TO S EARCH IN B OTTOM -U P P ROGRAM S YNTHESIS"},{"paperId":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"DECOMPOSITION IN NEURAL PROGRAM SYNTHESIS"},{"paperId":"54d00fc330248b3b2f89193da31bb17851ebd2b7","title":"M EMORIZING T RANSFORMERS"},{"paperId":"091fa84bdc07dcb22a34060c3996d8c58d71cd20","title":"Towards Neural Functional Program Evaluation"},{"paperId":"a5731122200fbb8b37f048010a1e1ca4474aa606","title":"Examining Zero-Shot Vulnerability Repair with Large Language Models"},{"paperId":"4b70f356f50d3e8e1f72c4a10f0ce2a26da95b5a","title":"Controlling Conditional Language Models with Distributional Policy Gradients"},{"paperId":"3dc7dc1bea9a4f70c02b6759a0bda7aca0005a9e","title":"A General Language Assistant as a Laboratory for Alignment"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"21e8e76386aaaa00e0971af70ce84a8a544e1aa1","title":"Cascaded Fast and Slow Models for Efficient Semantic Code Search"},{"paperId":"1093adc2a47212ef599e4708fe3e41ff7c9ec6d0","title":"GenLine and GenForm: Two Tools for Interacting with Generative Language Models in a Code Editor"},{"paperId":"05c2e1ee203be217f100d2da05bdcc52004f00b6","title":"Unsolved Problems in ML Safety"},{"paperId":"bc9598dc4ed0472d8b59b87ed3a139f8347d40ee","title":"Towards A Measure Of General Machine Intelligence"},{"paperId":"a176b0de62840f7118006277d94bbc1547162a4d","title":"Learning to Synthesize Programs as Interpretable and Generalizable Policies"},{"paperId":"98485ce6532d69f34a8ec67de6b09a39532bd221","title":"Communicating Natural Programs to Humans and Machines"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"abd77509ef1cc739c0757ae657025fcee13c98dc","title":"Program Synthesis Guided Reinforcement Learning for Partially Observed Environments"},{"paperId":"be09ed6cd73654a23f78416433a1b23ea623ea79","title":"Symbolic Behaviour in Artificial Intelligence"},{"paperId":"2ad6b59ff14d6bec3f14d8b6480025bbebc50e46","title":"Optimal Neural Program Synthesis from Multimodal Specifications"},{"paperId":"27e3ca4fd7b8290a0e12ca4fd2b7ad5bcd5900f2","title":"Are Transformers All That Karel Needs?"},{"paperId":"bab6893ee48d168d27c227c3b0867f6d471fbea8","title":"Language Models are not Models of Language"},{"paperId":"5ff9032d0f7f246d01ae7b2c231ab03469a7344a","title":"Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?"},{"paperId":"3f97c2067cde9377e50b3160bbd7982c94abd88a","title":"An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions"}],"references":[{"paperId":"34503c0b6a615124eaf82cb0e4a1dab2866e8980","title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"},{"paperId":"1093adc2a47212ef599e4708fe3e41ff7c9ec6d0","title":"GenLine and GenForm: Two Tools for Interacting with Generative Language Models in a Code Editor"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"ac879df2cc36f3f824fa24149517622b6bc7bd09","title":"Implicit Representations of Meaning in Neural Language Models"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"c2c7233293d55f201fe5b496234bed1914eea70e","title":"Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks"},{"paperId":"62d1a3137b01a69443bebf4d92c1990ec512a6a1","title":"Extracting Training Data from Large Language Models"},{"paperId":"03b6e258168796f96f1c40d32411bd699b6de922","title":"Deep Just-In-Time Inconsistency Detection Between Comments and Source Code"},{"paperId":"30a156f17ca8f54aa14d01d32c2315c11fcbe723","title":"BUSTLE: Bottom-up program-Synthesis Through Learning-guided Exploration"},{"paperId":"1ec69f1a1a9d4ff5c5bc70db0e5087157b620570","title":"You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion"},{"paperId":"1311adebe439ccd9e373d594a8df1d720a685190","title":"Code Prediction by Feeding Trees to Transformers"},{"paperId":"59cc0871534a20ac98db0663539c6675ab63566f","title":"SequenceR: Sequence-to-Sequence Learning for End-to-End Program Repair"},{"paperId":"8990154f515dadf6d1bfda745e62a67dc3b0e709","title":"A large-scale benchmark for few-shot program induction and synthesis"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"5f818ecbfce3bc44325a4f8ef2d744bc94006d6c","title":"Learning to Execute Programs with Instruction Pointer Attention Graph Neural Networks"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"02eaaf87f9cae34cca398fed146079e6eeb1f868","title":"Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data"},{"paperId":"be188977f1791faa60128d5f8a85470cad93f0f9","title":"Where should I comment my code? A dataset and model for predicting locations that need comments"},{"paperId":"ef2bbcd928749978b4395460a96c9869833c9c89","title":"DreamCoder: Growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0071ee55f1749e6be586645bab08f0385ee856b4","title":"Graph-based, Self-Supervised Program Repair from Diagnostic Feedback"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"08066c80919620397e8e4e5372ff84caf401e675","title":"Global Relational Models of Source Code"},{"paperId":"6acf0d7b24a93953269a4d52c6b2f0c7d11f9ae4","title":"Hoppity: Learning Graph Transformations to Detect and Fix Bugs in Programs"},{"paperId":"29fc7ff02e68537a86d32978bfd2e8e45258e77d","title":"LambdaNet: Probabilistic Type Inference using Graph Neural Networks"},{"paperId":"270897709b9e32eccb4b8968402be1700d1e28e6","title":"OptTyper: Probabilistic Type Inference by Optimising Logical and Natural Constraints"},{"paperId":"3944354c42ddfff7414ad06022f96c72858d5fa6","title":"Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"e41452747ac0674a7b6534e78be33134fe8ef650","title":"Learning to Represent Programs with Property Signatures"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"a429c6c8407b10a89c9610ab5e9682ec89772356","title":"TypeWriter: neural type prediction with search-based validation"},{"paperId":"78f4de568564b6b5bb9058779ead6b3be8548e53","title":"Learning to Fix Build Errors with Graph2Diff Neural Networks"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"f669b5406320a83e94b4dd439806211c8b1e95fc","title":"Write, Execute, Assess: Program Synthesis with a REPL"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"6c41bedc4637f3fd504c68baa3b3d8881e056ac1","title":"Execution-Guided Neural Program Synthesis"},{"paperId":"73bade9162bc4df6ca6c9ab8a7eb2d54d35acad2","title":"code2vec: learning distributed representations of code"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"ea1b62a03ab16c4acca6f7041c6f096bedec244b","title":"Deep learning type inference"},{"paperId":"980e4fe84fe5feec42c2a2eea7cc738e1af8acdf","title":"Automatic Program Synthesis of Long Programs with a Learned Garbage Collector"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"d6b7abd5c17f9e7d83c5b709c27da3a82a9788f2","title":"NAPS: Natural Program Synthesis Dataset"},{"paperId":"c70f11da5a8bb2df36def1d99c4c08df315e2233","title":"DeepBugs: a learning approach to name-based bug detection"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"1e077413b25c4d34945cc2707e17e46ed4fe784a","title":"Universal Language Model Fine-tuning for Text Classification"},{"paperId":"d0f2d7236e43f129744e88130fb71f8f872d2b31","title":"Learning to Represent Programs with Graphs"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"d11777ca327c6d91de34d1d2ac50316b905578b2","title":"Neural Sketch Learning for Conditional Program Generation"},{"paperId":"57a30c0a013bc36b4b5181b33c308c00d98b7a9d","title":"Learning Libraries of Subroutines for Neurally-Guided Bayesian Program Induction"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"3ff0af64279929a952ee340e645256b7e0580f65","title":"RobustFill: Neural Program Learning under Noisy I/O"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"784ee73d5363c711118f784428d1ab89f019daa5","title":"Hybrid computing using a neural network with dynamic external memory"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"484ed558a5863060b373e8a2cb7cb302b8c36116","title":"A Convolutional Attention Network for Extreme Summarization of Source Code"},{"paperId":"2f2d8f8072e5cc9b296fad551f65f183bdbff7aa","title":"Exploring the Limits of Language Modeling"},{"paperId":"50358e3f30ab8c99f9d383e5683b31c2311e5651","title":"Automatic patch generation by learning correct code"},{"paperId":"5e4eb58d5b47ac1c73f4cf189497170e75ae6237","title":"Neural GPUs Learn Algorithms"},{"paperId":"a2499dd426c46c645ee805d7594b6687547c72d4","title":"Neural Random Access Machines"},{"paperId":"4aa9f5150b46320f534de4747a2dd0cd7f3fe292","title":"Semi-supervised Sequence Learning"},{"paperId":"be6665462fd6c56e8756dadf23373bb0c90b2b19","title":"Predicting Program Properties from \"Big Code\""},{"paperId":"0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a","title":"Learning to Execute"},{"paperId":"4755b856dc08ac024ae935e7a6f9df325b00ae53","title":"Phrase-Based Statistical Translation of Programming Languages"},{"paperId":"9277dc7dd5b348645e8fcf4a987e5b3fc9132c7c","title":"Towards a Big Data Curated Benchmark of Inter-project Code Clones"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"93ff001eb7ddd019c107879943126c74a973993b","title":"Learning natural coding conventions"},{"paperId":"215999a0e155a21255e4655d4eac312858058a84","title":"Structured Generative Models of Natural Source Code"},{"paperId":null,"title":"Neural turing machines. CoRR, abs/1410"},{"paperId":"69c42a8da4c52b90ee27f9b6c0df37f2731ac890","title":"Growing solver-aided languages with rosette"},{"paperId":"f7ba81057c8ec4f1c039027232034143a5afe6bf","title":"Syntax-guided synthesis"},{"paperId":"5447a3b8701d59f3a2f1a7f7af030f687ba495c3","title":"Lexical statistical machine translation for language migration"},{"paperId":"110b7719f16eae8d82af0f56fca6e802c842c181","title":"Mining source code repositories at massive scale using language modeling"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"3aa35c213730e688fa191a1ce241db31087956f0","title":"GenProg: A Generic Method for Automatic Software Repair"},{"paperId":null,"title":"Alan Turing’s Electronic Brain: The Struggle to Build the ACE, the World’s Fastest Computer"},{"paperId":"e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de","title":"Generating Text with Recurrent Neural Networks"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"9819b600a828a57e1cde047bbe710d3446b30da5","title":"Recurrent neural network based language model"},{"paperId":"1ef301c1b275091b6a50d620b41df4722f2108f0","title":"Combinatorial sketching for finite programs"},{"paperId":"96902d95f53358ea23ff401b5198c7315addeb43","title":"On the synthesis of a reactive module"},{"paperId":"2fb9729c0a3b046c390ad6748773527eae6cfc84","title":"What Is Program Synthesis?"},{"paperId":"dd7abc005846b18c9a78ab80467cbbaedb643456","title":"A Methodology for LISP Program Construction from Examples"},{"paperId":"e6d8af09e433d93cc17548d6cab4a6afcc7090b4","title":"Inferring LISP Programs From Examples"},{"paperId":"d163709460265aa4901ac41a2b903793ad24b3c2","title":"Knowledge and Reasoning in Program Synthesis"},{"paperId":"94d9cbfc474cb6ce961de45a06233b2853ca6724","title":"Toward automatic program synthesis"},{"paperId":"577e96521d62b9ebb5fd67412a21b02e9cd67b90","title":"PROW: A Step Toward Automatic Program Writing"},{"paperId":"8bcc2e1ceaf091239b0b2c0bb354185580587172","title":"The FORTRAN automatic coding system"},{"paperId":null,"title":"Make sure the function signature is not unusual"}],"id":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","summary":"The limits of the current generation of large language models for program synthesis in general purpose programming languages are explored, finding that even the best models are generally unable to predict the output of a program given a speciﬁc input."},{"url":"https://www.semanticscholar.org/paper/a3564f3cf954c05844c757505325a50b4d858e22","title":"Generating Infrastructure-as-Code from Natural Language: Evaluating Fine-Tuned GPT-3 Models for Cloud Infrastructure Provisioning","venue":"","year":2022,"referenceCount":8,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","citations":[],"references":[{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"4891cc27e296e0ead23407a835bcd3bbb802ce67","title":"Automatic Code Generation using Pre-Trained Language Models"},{"paperId":"4cd8cea2f0302acffe1dc509d5f6458d8eb1e234","title":"Pseudocode to Code Translation Using Transformers"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"}],"id":"a3564f3cf954c05844c757505325a50b4d858e22","summary":"A transformer-based model to generate Infrastructure-as-Code from natural language is introduced, which allows both technical and nontechnical users to dynamically generate IaC artifacts, enabling them to request and receive cloud resources using conversational interfaces such as chat bots, SMS, etc."},{"url":"https://www.semanticscholar.org/paper/55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant","venue":"ArXiv","year":2022,"referenceCount":22,"citationCount":8,"influentialCitationCount":2,"publicationDate":"30/01/2022","authors":"Shubham Chandel,Colin B. Clement,Guillermo Serrato,Neel Sundaresan","citations":[{"paperId":"8475c1d1d98932111b194dc139c8f8b30818725d","title":"xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs"}],"references":[{"paperId":"ad18af95ba8125d2c0eb9f9941205678cad38ad2","title":"Generating Accurate Assert Statements for Unit Test Cases using Pretrained Transformers"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"3a863672b072a37ac8d2eb6b780234f6b748a2d9","title":"Generating bug-fixes using pretrained transformers"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"dd1ef7e7dc6ab885d9d64218148f08354c3c6fdb","title":"CPC: Automatically Classifying and Propagating Natural Language Comments via Program Analysis"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"75acc731bdd2b626edc74672a30da3bc51010ae8","title":"CTRL: A Conditional Transformer Language Model for Controllable Generation"},{"paperId":"79b7a61abd4a4b4cf62c26cec815b55b0778f48d","title":"Pythia: AI-assisted Code Completion System"},{"paperId":"7436d86cd6b572a3f52caa7820c07e7bfcf16f86","title":"Gmail Smart Compose: Real-Time Assisted Writing"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"af5a7be7bfc592e400e2f12a24ad3e6a5a44b58f","title":"Learning from examples to improve code completion systems"}],"id":"55ad5e818cfed72317576027fb33a9609210d592","summary":"This work studies the feasibility of a Data Science assistant powered by a sequence-to-sequence transformer by training a new model JuPyT5 on all publicly available Jupyter Notebook GitHub repositories and developing a new metric: Data Science Problems (DSP)."},{"url":"https://www.semanticscholar.org/paper/5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode","venue":"Science","year":2022,"referenceCount":82,"citationCount":201,"influentialCitationCount":28,"publicationDate":"08/02/2022","authors":"Yujia Li,David H. Choi,Junyoung Chung,Nate Kushman,Julian Schrittwieser,Rémi Leblond,Tom,Eccles,James Keeling,Felix Gimeno,Agustin Dal Lago,T. Hubert,Peter Choy,Cyprien de,Masson d’Autume,I. Babuschkin,Xinyun Chen,Po-Sen Huang,Johannes Welbl,Sven Gowal,Alexey,Cherepanov,James Molloy,D. Mankowitz,Esme Sutherland Robson,Pushmeet Kohli,Nando de,Freitas,K. Kavukcuoglu,Oriol Vinyals","citations":[{"paperId":"fbb5cc9e8a46f61d1543bd17b6eca324fd5bf55c","title":"Fully Autonomous Programming with Large Language Models"},{"paperId":"c4e4b72da211dbf2ab2fd5263d453cf22ee0cf44","title":"CodeKGC: Code Language Model for Generative Knowledge Graph Construction"},{"paperId":"f7e41afdd7b54af3ddb2bff8005efbce4de87d38","title":"Constructing Effective In-Context Demonstration for Code Intelligence Tasks: An Empirical Study"},{"paperId":"dd0960a8ffb6a8baff69ec96c3f67deb2912b53a","title":"Self-collaboration Code Generation via ChatGPT"},{"paperId":"8a2a2006e371ce07e0b29538cd11337a5c977caf","title":"“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models"},{"paperId":"eb55442d940accc67d6790e66d3196e1941fc7a4","title":"Teaching Large Language Models to Self-Debug"},{"paperId":"98fa1c0c4837396d4f377294a2ece60374f753d2","title":"Similarity search in the blink of an eye with compressed indices"},{"paperId":"f352a968c8735fac58912870a7bde57fcfc2e6bd","title":"\"It's Weird That it Knows What I Want\": Usability and Interactions with Copilot for Novice Programmers"},{"paperId":"3693683c4e0405819fae7115ad680f769eb83534","title":"Neural Comprehension: Language Models with Compiled Neural Networks"},{"paperId":"70ebcc3716583b7bc0d59e443c83d3ac522b79e9","title":"RunBugRun -- An Executable Dataset for Automated Program Repair"},{"paperId":"0148914677527a3af081588f296075f0e9b751ea","title":"Review: The prevailing mathematical modelling classifications and paradigms to support the advancement of sustainable animal production"},{"paperId":"975da5bb7fdd800ba577535d8c6ee5a5bc835d52","title":"Pair Programming with Large Language Models for Sampling and Estimation of Copulas"},{"paperId":"cbdc75e9c80870164662bb0359e23fd4736c5f0e","title":"A Survey of Large Language Models"},{"paperId":"3b01b5b497e1f359b11da45af029281ee6f64c2c","title":"Towards Enhancing In-Context Learning for Code Generation"},{"paperId":"f24dabf3317abaa3d7393ab7d48115f353749bde","title":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X"},{"paperId":"28dafb94a87daa71ad3edf9f04f3c8c32753398b","title":"Improving Code Generation by Training with Natural Language Feedback"},{"paperId":"98e6e0b3b811193e89b1a033da6c0a454220877a","title":"Foundation Models and Fair Use"},{"paperId":"2da3a84e72a2973504cd9cf1c0a377f5a5a91f09","title":"GPT is becoming a Turing machine: Here are some ways to program it"},{"paperId":"06e5828341aa3926e1d839039363b0673b9461cc","title":"Errors are Useful Prompts: Instruction Guided Task Programming with Verifier-Assisted Iterative Prompting"},{"paperId":"349ec9ca2ff17dbc1aa1caf27f98b7cd85a4747e","title":"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation"},{"paperId":"362cbfd0d05e139cd6cf049754098a6e1520b910","title":"PanGu-{\\Sigma}: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing"},{"paperId":"0b7509abcf9af50ad4e551be35f11b62ce7d2695","title":"TypeT5: Seq2seq Type Inference using Static Analysis"},{"paperId":"9b1367167e45bac5e039ba0407cb3329125e9ff2","title":"Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?"},{"paperId":"d39db76fb80c1f74455a0f6432c2242b727389da","title":"Self-planning Code Generation with Large Language Model"},{"paperId":"8d03ebc375b3d887056e769018467bf0a6ed99e3","title":"Planning with Large Language Models for Code Generation"},{"paperId":"921dace8bf038a34cba5473a72abc8cf65d61e03","title":"Large Language Models (GPT) Struggle to Answer Multiple-Choice Questions about Code"},{"paperId":"dc7d3adcfd3ea596128ec2e5c4f19b3adcdd4e7c","title":"Hierarchical Neural Program Synthesis"},{"paperId":"2f94f03fdac62d05f0f416b7b3855d1f597afee9","title":"Automatically Auditing Large Language Models via Discrete Optimization"},{"paperId":"94ac118c522c065e24d9090b9f7827b83e8ff2f4","title":"Many bioinformatics programming tasks can be automated with ChatGPT"},{"paperId":"35afb57a646592c3a471a4f010d00e1b13dd3c43","title":"From Copilot to Pilot: Towards AI Supported Software Development"},{"paperId":"d32c9701e535a7602ac8c446b8f28d5a832a503b","title":"The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"},{"paperId":"d2edaa84c13fd08735216ec85f2f3b2b5b53ff78","title":"On the Feasibility of Specialized Ability Extracting for Large Language Code Models"},{"paperId":"8475c1d1d98932111b194dc139c8f8b30818725d","title":"xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval"},{"paperId":"196353ab8d340b31b465c22dff904de195fa9a60","title":"LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models"},{"paperId":"f46d544ffee26c9ab866d2b4b85f7b5d5969b075","title":"Select-Them-Repair-Them: Automatically Optimize Pseudocode-to-Code Data"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"a8e0ba16346b72c3a04dd0b1da84bc5f28900174","title":"Using GitHub Copilot to Solve Simple Programming Problems"},{"paperId":"6409fbb6858e7e5499f76b5293d2f2bf6b86c415","title":"Efficient Determination of Social Determinants of Health From Clinical Notes for Timely Identification of Suicidality Among US Veterans."},{"paperId":"57ffd52abfbba0eb76c8406c7331f1f8ca29aaff","title":"R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents"},{"paperId":"49fede098a0d2a48e8100b30189224fc6f5eb25b","title":"Language Model Crossover: Variation through Few-Shot Prompting"},{"paperId":"c2514f2254714fce76656f9ffc2b3241e82b3e61","title":"Reduce, Reuse, Recycle: Compositional Generation with Energy-Based Diffusion Models and MCMC"},{"paperId":"7678d1862140d84f9c15d95e7d4d085857f332eb","title":"Conversational Text-to-SQL: An Odyssey into State-of-the-Art and Challenges Ahead"},{"paperId":"c90d41abc0c43bd5745dc45a34aef0e20ac307b5","title":"Nonlinear response of Silicon Photonics microresonators for reservoir computing neural network"},{"paperId":"383157f9059061faba4364c7d2141d0ff78c87fa","title":"Learning Deep Semantics for Test Completion"},{"paperId":"32cdb20d6f9e1f3038f24f3e8997962b35666c9e","title":"Machine Love"},{"paperId":"681cee58cf7e54199191cf9e0baf6851d8356704","title":"Complex QA and language models hybrid architectures, Survey"},{"paperId":"6d269364de402d4a72ac30b0c8d81324f6849807","title":"LEVER: Learning to Verify Language-to-Code Generation with Execution"},{"paperId":"1786a2f9140ed7211b21302977de64e948b92308","title":"Learning Performance-Improving Code Edits"},{"paperId":"f03f39f735186c4359b719724be6e1c2eb912fef","title":"Studying the effect of AI Code Generators on Supporting Novice Learners in Introductory Programming"},{"paperId":"f5d93cf5d81575aeee61faeddde4437fbcb74cd0","title":"The Programmer’s Assistant: Conversational Interaction with a Large Language Model for Software Development"},{"paperId":"6876fc47f4674fdf4f53208254b45dedc6de3755","title":"Adaptive Test Generation Using a Large Language Model"},{"paperId":"535a3d95a743e1f4b591b5b2af3e778a6347158a","title":"CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code"},{"paperId":"79680e20f0c8038039b243fb5fd385fbe967c799","title":"Controlling Large Language Models to Generate Secure and Vulnerable Code"},{"paperId":"d2170504c4ad9403bea118ae8debdfda95978546","title":"The Wisdom of Hindsight Makes Language Models Better Instruction Followers"},{"paperId":"782f3d43b37790a83c98d5fd3ef142b296f20616","title":"CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code Models"},{"paperId":"7677e9ead35f8f2578acbf5ad15fb330c83762c4","title":"Systematically Finding Security Vulnerabilities in Black-Box Code Generation Models"},{"paperId":"c1014f86c8d801c93b37eed58311da5f9c9da2e4","title":"ChatGPT and Software Testing Education: Promises & Perils"},{"paperId":"cd0988714ea326642d2b1bb18753e187fec71e42","title":"A Categorical Archive of ChatGPT Failures"},{"paperId":"27c2ddc921befc44f7b2cdb8bb39facdd8b0b188","title":"Measuring The Impact Of Programming Language Distribution"},{"paperId":"a764d4f28612a7b5c4767ea6a2540b169fdb052c","title":"CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models"},{"paperId":"e42843bfb05263df00837fa1b287bc816296e1fc","title":"Transformers Meet Directed Graphs"},{"paperId":"68edfd62d2619fb4af7c2469edb95b9e2fe4544a","title":"Execution-based Code Generation using Deep Reinforcement Learning"},{"paperId":"1d34b6cffe67077cdd4df41950b1195f09ae0cb8","title":"Hierarchical Programmatic Reinforcement Learning via Learning to Compose Programs"},{"paperId":"c532f1df90925e5c69789f0cd99248d8a2a2e5bc","title":"My AI Wants to Know if This Will Be on the Exam: Testing OpenAI’s Codex on CS2 Programming Exercises"},{"paperId":"c72ac81c4ac414314b52cf8f5b77370f8d0875d4","title":"Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models"},{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"},{"paperId":"d8405996b4d08c304098636aedd9e1c1a1e262ee","title":"The Premature Obituary of Programming"},{"paperId":"a20875e70a38cb053cd34e170038c4746f85dac9","title":"A Case Study in Engineering a Conversational Programming Assistant's Persona 121-129"},{"paperId":"1f22de83d912176cb8857efa1c6d65b14d6a2f5c","title":"ChatGPT is not all you need. A State of the Art Review of large Generative AI models"},{"paperId":"dae74645479f7c1fa3671066f9e24ec6c20c17ec","title":"TrojanPuzzle: Covertly Poisoning Code-Suggestion Models"},{"paperId":"490f1e8ff352bded30bcde01d5b4769d6c2d2dd5","title":"Adversarial Attacks on Neural Models of Code via Code Difference Reduction"},{"paperId":"b7823997fb185f208b6a6723b60413ff179d2639","title":"Standing on the Shoulders of AI Giants"},{"paperId":"dca3bc28a7d404b28780a813ea7072eda809e6c0","title":"Programming Is Hard - Or at Least It Used to Be: Educational Opportunities and Challenges of AI Code Generation"},{"paperId":"6032212d5790b6a580d68d469a9895aad6238c89","title":"Diverse Title Generation for Stack Overflow Posts with Multiple Sampling Enhanced Transformer"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"0f38267a8ba32789f5d3b1b19820f86940fea052","title":"Generation-Augmented Query Expansion For Code Retrieval"},{"paperId":"bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence"},{"paperId":"433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"9e8f0d9ba4e7af673c8b214b3764e020706dd1f3","title":"Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"0731c710fc09fe7c039aee3e9bff006bf94565aa","title":"XTest: A Parallel Multilingual Corpus with Test Cases for Code Translation and Its Evaluation*"},{"paperId":"e8aa5f51aaf29344174f90d7edca49cc153a6b00","title":"Economic impacts of AI-augmented R&D"},{"paperId":"9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming"},{"paperId":"000b8567d17dd982ae226c29505027ed692911dd","title":"AlphaCode and “data-driven” programming"},{"paperId":"3d3012bfcc8bc7e4dc84c177e94650e66f03bc5b","title":"Are ChatGPT and AlphaCode going to replace programmers?"},{"paperId":"8175ce2cbb99b6a394bdac152ae39d413f4f1380","title":"Codex Hacks HackerRank: Memorization Issues and a Framework for Code Synthesis Evaluation"},{"paperId":"ea55ae4c82b45e3857f37406c94e9f642a2b3d84","title":"Genetic Programming with Local Scoring"},{"paperId":"ebd4bec684808aff360b5f255d15c0d112ba13d3","title":"Explicit Knowledge Transfer for Weakly-Supervised Code Generation"},{"paperId":"6d4a12ea469ff08634eeb24c47b265a7dca2fce2","title":"PADL: Language-Directed Physics-Based Character Control"},{"paperId":"27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation"},{"paperId":"30624a18720bf93a85dc3efe570df271a8c9f4c3","title":"Program Repair"},{"paperId":"c192fb33f308f19ac8a5c4c2d623d56385b839be","title":"Systematic Literature Review on Solving Competitive Programming Problem with Artificial Intelligence (AI)"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","title":"Evaluating How Fine-tuning on Bimodal Data Effects Code Generation"},{"paperId":"a4bdc300db297756f36bedee2859b62df8e268c2","title":"Follow the Wisdom of the Crowd: Effective Text Generation via Minimum Bayes Risk Decoding"},{"paperId":"b1eebb2df3b9ff7ff2b00fb1a786f6ada2caebce","title":"Towards a Mathematics Formalisation Assistant using Large Language Models"},{"paperId":"048ed70192de0232086eb32a95ffb3be8d336c76","title":"Metaphors We Learn By"},{"paperId":"632ab7663e6d64578ceda1d1df9ec525b503bacb","title":"Steps towards prompt-based creation of virtual worlds"},{"paperId":"ac3d7ae8b4acb137492d4a8d8bcff480b89fa000","title":"Programming Pedagogy and Assessment in the Era of AI/ML: A Position Paper"},{"paperId":"eca35805d185374befe4da48c9f96ace6e962fad","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"4af2891ce1aab624c4917e8a69fcee5c8a1f41db","title":"NL2Viz: natural language to visualization via constrained syntax-guided synthesis"},{"paperId":"71280dba5bda65c162f9deaffed7d3d20692ca0a","title":"SecurityEval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques"},{"paperId":"4610ffb1b016acaa82a2065ffd1a3adbae1ce722","title":"Large Language Models Are Human-Level Prompt Engineers"},{"paperId":"621009f1c30951b7c952c65c45ef0064a204e91e","title":"Early Experience with Transformer-Based Similarity Analysis for DataRaceBench"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers"},{"paperId":"66c52c6ef45cdd77c086996bcaaf01470e82dbd2","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?"},{"paperId":"5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","title":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming"},{"paperId":"f031ba42cf82f106200bb03fbb91dd5671a59b9c","title":"Practical Program Repair in the Era of Large Pre-trained Language Models"},{"paperId":"d400a649f0f0a3de22b89a268f48aff2dcb06a09","title":"Entailer: Answering Questions with Faithful and Truthful Chains of Reasoning"},{"paperId":"4634362d75606287955260ef1788171286efbeaa","title":"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs"},{"paperId":"5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"a6ef5bad716091fb1888bf365f6129628ab3a5ee","title":"ObSynth: An Interactive Synthesis System for Generating Object Models from Natural Language Specifications"},{"paperId":"4c2534f9b03ac2f3810c07abc398a11bcf47258e","title":"Transformers Learn Shortcuts to Automata"},{"paperId":"663a41c866d49ce052801fbc88947d39764cad29","title":"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them"},{"paperId":"825333b7efe2cade106eaf36c7e731f757974806","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot"},{"paperId":"045a6f92b58f08d5b305dd5d661316a506ee4d43","title":"Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation"},{"paperId":"0b8772b7790c69f40897b5eb7f8fd57f24138f3d","title":"ContraGen: Effective Contrastive Learning For Causal Language Model"},{"paperId":"8a854331c593c6a766fa3b8037fb2ad1b95a6f06","title":"An Empirical Study of Code Smells in Transformer-based Code Generation Techniques"},{"paperId":"558b7245a54575e16143324df98129254d5a244c","title":"Enhancing Code Similarity with Augmented Data Filtering and Ensemble Strategies"},{"paperId":"95fa2b27ab7eb84738441ee16da97323538938f9","title":"I Speak, You Verify: Toward Trustworthy Neural Program Synthesis"},{"paperId":"2a0456b0408cd4c33f2ff4400374e7be2497a362","title":"Repairing Bugs in Python Assignments Using Large Language Models"},{"paperId":"cd6496bc404e18a24f634e3dded2ed1cdca03e0f","title":"Learning to Learn with Generative Models of Neural Network Checkpoints"},{"paperId":"e16b2de59f7397fec8eb3c6717abba5519cc055c","title":"A Practical Three-phase Approach To Fully Automated Programming Using System Decomposition And Coding Copilots"},{"paperId":"971e875e28f26240987d2c9470d1ee74ad204205","title":"Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction"},{"paperId":"a7435722d8ab595da5a9c70ac9160f57d0dcd75a","title":"Enabling Transformers to Understand Low-Level Programs"},{"paperId":"ef29fb8cc6bdda4b011288f51da521d3c25fc53d","title":"Learning to Prevent Profitless Neural Code Completion"},{"paperId":"e73a16490c29530c37b49f6a30592790e7caaaa4","title":"Don't Complete It! Preventing Unhelpful Code Completion for Productive and Sustainable Neural Code Completion Systems"},{"paperId":"45875d5f55c72c1bdfa6d7c312eead7dcc93123d","title":"Borch: A Deep Universal Probabilistic Programming Language"},{"paperId":"e5993b3afe6384b5e6f90093989773ad1f868f71","title":"Towards Top-Down Deep Code Generation in Limited Scopes"},{"paperId":"0509c25103939d59ed4e27b1393e74ad5734c453","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot"},{"paperId":"befde3e07ce97f02f12fe92ab27e99f23ccd17aa","title":"Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation"},{"paperId":"9b61de7038290751377b64293baaf42f3e7cf441","title":"An Empirical Evaluation of Competitive Programming AI: A Case Study of AlphaCode"},{"paperId":"99f85119f113b5498517928eff74a904b69e37b7","title":"CCTEST: Testing and Repairing Code Completion Systems"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?"},{"paperId":"1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization"},{"paperId":"8c2d9d2aa891e3b53bbd9166c1b804a0741fc44a","title":"Finding Reusable Machine Learning Components to Build Programming Language Processing Pipelines"},{"paperId":"654cd297b307c0c6fa08732511ba852b8dce1977","title":"Out of the BLEU: how should we assess quality of the Code Generation models?"},{"paperId":"c5b47a34fd7e0e595360683b4effa51c0261d1e3","title":"Language Models Can Teach Themselves to Program Better"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"746b6108a72b2b1bd78f70d4b1a211cdebfd8f49","title":"Multi-objective Grammar-guided Genetic Programming with Code Similarity Measurement for Program Synthesis"},{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"d6954c43aa1ca197319c45d3988bc8fcec3de976","title":"GitHub Copilot AI pair programmer: Asset or Liability?"},{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code"},{"paperId":"624b2ea0ba57ce67a4524e029ac11f748a0782fa","title":"Questions Are All You Need to Train a Dense Passage Retriever"},{"paperId":"677545b422ddcfc010d216f9ac8d81cfcbea9f89","title":"Actionable Guidance for High-Consequence AI Risk Management: Towards Standards Addressing AI Catastrophic Risks"},{"paperId":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation"},{"paperId":"29acc890e521f7a6415666ab9eb3432c49b4587a","title":"Self-critiquing models for assisting human evaluators"},{"paperId":"9aed848be4e9e401b0e61a2e5d60dbdafa0c6cc1","title":"A Dataset and Benchmark for Automatically Answering and Generating Machine Learning Final Exams"},{"paperId":"a64871352f8ac7f8aa226fda5cce70251a18a4fc","title":"Assessing Project-Level Fine-Tuning of ML4SE Models"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"40edfa97cd02268fccff75eb9c693b11c1a968b2","title":"Formal Specifications from Natural Language"},{"paperId":"0d08ffccc982781e310bb184397bbe64b9aef157","title":"Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models"},{"paperId":"1100dee3fd78655cddc4b7bfaef1161351d4fab5","title":"Automated Feedback Generation for Competition-Level Code"},{"paperId":"35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs"},{"paperId":"7107d06366b48b3593c8128ed2ca67e0b413628c","title":"Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions"},{"paperId":"e7e1feff05edf89cac6c2e6de46815a3f89144ef","title":"Tensor Program Optimization with Probabilistic Programs"},{"paperId":"0efa0441da820b1905572666ba1974a06a9663fb","title":"NaturalProver: Grounded Mathematical Proof Generation with Language Models"},{"paperId":"6074c7b75f27ca9adb6d74b080c07d5d079c3ea0","title":"Improving automatically generated code from Codex via Automated Program Repair"},{"paperId":"0bcd59da541fdae66884afba8d25475a54a9da1a","title":"Automated Repair of Programs from Large Language Models"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"5922f437512158970c417f4413bface021df5f78","title":"A Generalist Agent"},{"paperId":"dd70bb53640c2fbc18e5bfb6061f060aac4c9829","title":"From {Solution Synthesis} to {Student Attempt Synthesis} for Block-Based Visual Programming Tasks"},{"paperId":"abab9ae27efb40bbe6ffb9f6d27d56001412d856","title":"Scaling Genetic Improvement and Automated Program Repair"},{"paperId":"f8aa0cab09bc0668276e2cb5690bae47fa50d350","title":"An Initial Look at Self-Reprogramming Artificial Intelligence"},{"paperId":"f7820b52e3cdff6625e6bd0430a8d48ca66cca3f","title":"Self-Programming Artificial Intelligence Using Code-Generating Language Models"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"1f87dc41bdf2c4c78e2dce9c5c8adfef5e25a70c","title":"Passport: Improving Automated Formal Verification Using Identifiers"},{"paperId":"bb7e46f316d319f9819c3554c99995ef8361ae9c","title":"CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex"},{"paperId":"905cbe787b20fca3917d2afd6a7a5f073a50386e","title":"MP-CodeCheck: Evolving Logical Expression Code Anomaly Learning with Iterative Self-Supervision"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"3eda53506586216acc96f4f34446f697874f360c","title":"Learning to Induce Causal Structure"},{"paperId":"6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis"},{"paperId":"608df5bd7a0ad0f4d335ae4d071b8cfe60e2f3c5","title":"On the Effectiveness of Pretrained Models for API Learning"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"590f6817b42407f96b079e82c935fae298196359","title":"Less is More: Summary of Long Instructions is Better for Program Synthesis"},{"paperId":"d1bcb86bab0906bd631c4eb5ab81ce342b5927d0","title":"Open-Ended Knowledge Tracing"},{"paperId":"004d6ce718b0edb5b999f26710c5ae80b04bc900","title":"GPT-based Open-Ended Knowledge Tracing"},{"paperId":"29ed68d701f8450853938827b5124c9613c56aff","title":"ET-BERT: A Contextualized Datagram Representation with Pre-training Transformers for Encrypted Traffic Classification"},{"paperId":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective"},{"paperId":"b2c4fdb49bdb23e6ad0ac3272029324157046ea7","title":"Automated Repair of Code from Language Models"},{"paperId":"407b9e9478ba6bff43ce4b20e8b6cb2b303477d2","title":"P LANNING WITH L ARGE L ANGUAGE M ODELS FOR C ODE G ENERATION"},{"paperId":"18bda734a1546eae13f6b13600023ff73f95b6e3","title":"Program Synthesis Through Learning the Input-Output Behavior of Commands"},{"paperId":"239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning"},{"paperId":"78863000eb70945cc8d791d45d4a3fe8a6521cb6","title":"Open-ended Knowledge Tracing for Computer Science Education"},{"paperId":"91260f73dd179487fb16713deb8267634ae14716","title":"CodexDB: Synthesizing Code for Query Processing from Natural Language Instructions using GPT-3 Codex"},{"paperId":"3b0cf543a730e674d4213d344ebc857fada76ead","title":"Understanding High-Level Properties of Low-Level Programs Through Transformers"},{"paperId":null,"title":"Materials ExerciseTeacher Student Attempt Feedback"},{"paperId":"6df98ac2300c6e9c232440147ba976b4f501ca67","title":"C ODEX HACKS H ACKER R ANK : B ENEFITS AND R ISKS OF L ARGE -S CALE S OURCE C ODE M ODELS"},{"paperId":"15ef2d1b88f54fa32a32927463a7116219b89529","title":"SUPEROPTIMIZE REAL-WORLD PROGRAMS"},{"paperId":"36589346063ff26506330451976280011273b935","title":"Towards Teachable Reasoning Systems"},{"paperId":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"DECOMPOSITION IN NEURAL PROGRAM SYNTHESIS"},{"paperId":"27d16a7f2ce8f2b787b34ff1f9b4fece079700c3","title":"Figuring out Figures: Using Textual References to Caption Scientific Figures"},{"paperId":"b35e99bc8f642a54fc03a68d8cc08c1361feddf1","title":"Automated scholarly paper review: Concepts, technologies, and challenges"},{"paperId":"6c2d43e71e240e354b5790a38da78a291ceffe7c","title":"Learning to Superoptimize Real-world Programs"},{"paperId":"4f278ab5ad629267e06196e273252262854c1c57","title":"BF++: a language for general-purpose program synthesis"}],"references":[{"paperId":"002c256d30d6be4b23d365a8de8ae0e67e4c9641","title":"Improving language models by retrieving from trillions of tokens"},{"paperId":"68f141724814839d556a989646194be88641b143","title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"},{"paperId":"fd1b829261ba04bb92e0ab60c4f6e7cea0d99fbf","title":"Ethical and social risks of harm from Language Models"},{"paperId":"f7987fa2aadc0b368c185dc4d2fdb1337a202c32","title":"Solving Probability and Statistics Problems by Program Synthesis"},{"paperId":"4e40595d6ecba027cebb4f2e3b43ae44bcf51daf","title":"Solving Linear Algebra by Program Synthesis"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"a176b0de62840f7118006277d94bbc1547162a4d","title":"Learning to Synthesize Programs as Interpretable and Generalizable Policies"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"309fb5c6ed6d7ec85281ee315760df342e6c4fcc","title":"Ten Lessons From Three Generations Shaped Google’s TPUv4i : Industrial Product"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"62d1a3137b01a69443bebf4d92c1990ec512a6a1","title":"Extracting Training Data from Large Language Models"},{"paperId":"87fe4a67e4cb5b8cdbc5da4d98888c037fb7d043","title":"Learning Autocompletion from Real-World Datasets"},{"paperId":"4dee6b82c7e59973ccd1520ff83f6b66f4d4bed4","title":"Investigating Softmax Tempering for Training Neural Machine Translation Models"},{"paperId":"51c62d63c6204deecb24a1d3f9ea8e0a42d23817","title":"Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces"},{"paperId":"3f97c2067cde9377e50b3160bbd7982c94abd88a","title":"An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions"},{"paperId":"f82f1c722831bc3def1853ac65497d6a4fea01b9","title":"Learning to Generate Code Sketches"},{"paperId":null,"title":"GitHub Copilot research recitation: GitHub Copilot: Parrot or Crow? A first look at rote learning in GitHub Copilot suggestions,"},{"paperId":null,"title":"How to interpret contest ratings"},{"paperId":null,"title":"Research recitation: A first look at rote learning in GitHub"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"c94e49617f569204f989643e5462691b9b3a482b","title":"Estimating Training Data Influence by Tracking Gradient Descent"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":null,"title":"Codeforces: Results of 2020"},{"paperId":"dc52b09089704ebd6f471177474bc29741c50023","title":"Fast Transformer Decoding: One Write-Head is All You Need"},{"paperId":"361c00b22e29d0816ca896513d2c165e26399821","title":"Grandmaster level in StarCraft II using multi-agent reinforcement learning"},{"paperId":"07398e448180ad75c44d30f23a65289d40ff6f52","title":"Achieving Verified Robustness to Symbol Substitutions via Interval Bound Propagation"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"807370352540f171685998aec7a5701f7110f147","title":"Understanding Back-Translation at Scale"},{"paperId":"29de7c0fb3c09eaf55b20619bceaeafe72fd87a6","title":"Hierarchical Neural Story Generation"},{"paperId":"d11777ca327c6d91de34d1d2ac50316b905578b2","title":"Neural Sketch Learning for Conditional Program Generation"},{"paperId":null,"title":"JAX: composable transformations of Python+NumPy programs"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"3ff0af64279929a952ee340e645256b7e0580f65","title":"RobustFill: Neural Program Learning under Noisy I/O"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":null,"title":"Falsehoods programmers believe about time"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":null,"title":"Description2Code Dataset"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"50eba68089cf51323d95631c2f59ff916848863f","title":"The rust language"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"5f00233321d84bf28910752c3bd620cbff06a59d","title":"Code completion with statistical language models"},{"paperId":"ef12383f516840ec1ec998cd5921dfc6e197c9b2","title":"PPDB: The Paraphrase Database"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":null,"title":"Haiku: Sonnet for JAX, 2020"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"4edaded63d9d7ea27f5c4819f35d7168e1ae7974","title":"Scratch: programming for all"},{"paperId":"af5a7be7bfc592e400e2f12a24ad3e6a5a44b58f","title":"Learning from examples to improve code completion systems"},{"paperId":"eb537c6403932af7d1c4d819d2f0179df23baaee","title":"How Program History Can Improve Code Completion"},{"paperId":"ae74531e6e73afaffb264b4e536b8fde95d03202","title":"Program synthesis by sketching"},{"paperId":"2fb9729c0a3b046c390ad6748773527eae6cfc84","title":"What Is Program Synthesis?"},{"paperId":"94d9cbfc474cb6ce961de45a06233b2853ca6724","title":"Toward automatic program synthesis"},{"paperId":"1beae9ef432a57bb5ec0c43944a07182814ab443","title":"Application of Theorem Proving to Problem Solving"},{"paperId":null,"title":"Application of recursive arithmetic to the problem of circuit synthesis."},{"paperId":null,"title":"Dathathri for analyzing our model"},{"paperId":null,"title":"Caballero for granting permission to use Description2Code data"},{"paperId":null,"title":"Stanway for logistically making the project possible"},{"paperId":null,"title":"GitHub's automatic coding tool rests on untested legal ground"},{"paperId":null,"title":"Removed problems that are duplicates of each other, ignoring whitespace. Submissions for duplicate problems were merged"},{"paperId":null,"title":"Irving for developing tools that we use to train large language models and for lending their expertise in model training"},{"paperId":null,"title":"Foley for project management in early stages"},{"paperId":null,"title":"AlphaCode examples, explanations, and visualizations can be"},{"paperId":null,"title":"Ke for helping connect us with E. Caballero"},{"paperId":null,"title":"AlphaCode data materials"},{"paperId":null,"title":"Yogatama for reviewing the paper"},{"paperId":null,"title":"Meeting our match: Buying 100 percent renewable energy"},{"paperId":null,"title":"worked on infrastructure for running and evaluating experiments"},{"paperId":null,"title":"Removed submissions that are duplicates of others"},{"paperId":null,"title":"Electric Sales, Revenue, and Average Price: Summary Table T5.a: 2021 Residential Average Monthly Bill by Census Division, and State"},{"paperId":null,"title":"Facebook hacker cup"},{"paperId":null,"title":"Mitrichev for helping connect us with Codeforces, and lending competitive programming expertise as the paper was being written"}],"id":"5cbe278b65a81602a864184bbca37de91448a5f5","summary":"AlphaCode is introduced, a system for code generation that achieved an average ranking in the top 54.3% in simulated evaluations on recent programming competitions on the Codeforces platform, marking the first time an artificial intelligence system has performed competitively in programming competitions."},{"url":"https://www.semanticscholar.org/paper/7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective","venue":"ArXiv","year":2022,"referenceCount":265,"citationCount":3,"influentialCitationCount":0,"publicationDate":"10/02/2022","authors":"Erfan Al-Hossami,Samira Shaikh","citations":[{"paperId":"13ea8689a4fe12e7443f5a3a0a25c5337a2f4cd8","title":"MarianCG: a code generation transformer model inspired by machine translation"},{"paperId":"e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models"},{"paperId":"862c0b672c9defded3111924310a07760cfa27ff","title":"Can NMT Understand Me? Towards Perturbation-based Evaluation of NMT Models for Code Generation"}],"references":[{"paperId":"2236b6036cd1ecba1792d5e1fedbae7cefc3d43b","title":"Can we generate shellcodes via natural language? An empirical study"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"7ad2b12526e77badd629b10880db147afce4864a","title":"Lyra: A Benchmark for Turducken-Style Code Generation"},{"paperId":"c8559021289f08eaf8cf2294e406bc1c6b506d19","title":"Recent advances in deep learning based dialogue systems: a systematic survey"},{"paperId":"4a160efbe80c38cd5eb2f92c7c095b49b113397d","title":"In-IDE Code Generation from Natural Language: Promise and Challenges"},{"paperId":"40c25232bc3a3f36ac856ff517d5c70704f14965","title":"TF-Coder: Program Synthesis for Tensor Manipulations"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"dace03e57056d736f9e24937bdf486e894f8e866","title":"Is neural machine translation approach accurate enough for coding assistance?"},{"paperId":"46104f79702e959798ebfa7e07bfe22f22c1096b","title":"PyTorrent: A Python Library Corpus for Large-scale Language Models"},{"paperId":"5fbcfccd3736969d95ed660d8e6962c86b7a9113","title":"PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"69acc5f67ea7dba13c58d7281b8f0a25ed64f0e8","title":"EVIL: Exploiting Software via Natural Language"},{"paperId":"c56aced0f0c5cfebefadb530cb08d736c3ac5c05","title":"Retrieval Augmented Code Generation and Summarization"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"ebfcbe0a8b238d5a52286fdfaab7be0170dbc91b","title":"FAPR: Fast and Accurate Program Repair for Introductory Programming Courses"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"fd963ea665e7a6e49753652dc7efe5affba5feb0","title":"DreamCoder: bootstrapping inductive program synthesis with wake-sleep library learning"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"67e8e2d3b276c339588b9551e6b20cd62ebdda7c","title":"Text-to-SQL in the Wild: A Naturally-Occurring Dataset Based on Stack Exchange Data"},{"paperId":"cc9ad384ec0d0176ce05865d3866b44d2519bd68","title":"Reading StackOverflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation"},{"paperId":"0c21334be0228431d619a180c809b43be0065bdd","title":"CoSQA: 20,000+ Web Queries for Code Search and Question Answering"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"0b077c9577f4297dcf3da835e253d21965bbc6e0","title":"CoTexT: Multi-task Learning with Code-Text Transformer"},{"paperId":"b15fa9e57fb791899154a0f6c321eb703f1c0b09","title":"Shellcode_IA32: A Dataset for Automatic Shellcode Generation"},{"paperId":"a48743b889db1faf0f04d4b29382634d19975f3f","title":"Toward Code Generation: A Survey and Lessons from Semantic Parsing"},{"paperId":"3e5e2e6825596e2cfae5fdb7f201d4bd945a267f","title":"Text2App: A Framework for Creating Android Apps from Text Descriptions"},{"paperId":"26e3d58181724f9ef77973ff0f65bac06e499fec","title":"ProphetNet-X: Large-Scale Pre-training Models for English, Chinese, Multi-lingual, Dialog, and Code Generation"},{"paperId":"5bfb0cc16b871c75e32a6a9d54dd7db225260e04","title":"CodeTrans: Towards Cracking the Language of Silicone's Code Through Self-Supervised Deep Learning and High Performance Computing"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"f389c09ad85263bdd1bb2518d61c90a8a558441d","title":"MulCode: A Multi-task Learning Approach for Source Code Understanding"},{"paperId":"171440398a1c0f43063a7689e3b385280336fb68","title":"CURE: Code-Aware Neural Machine Translation for Automatic Program Repair"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"c2c7233293d55f201fe5b496234bed1914eea70e","title":"Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"2ad6b59ff14d6bec3f14d8b6480025bbebc50e46","title":"Optimal Neural Program Synthesis from Multimodal Specifications"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"ed2a639dd4a6b3abdcc0040733a264cb1dbcd7d4","title":"PlotCoder: Hierarchical Decoding for Synthesizing Visualization Code in Programmatic Context"},{"paperId":null,"title":"2021b. Codesc: A large code-description"},{"paperId":"8e5b4aad131263457a38adbbffebe1b252802f1e","title":"Text2PyCode: Machine Translation of Natural Language Intent to Python Source Code"},{"paperId":"e13d317fe0178a8b8b67f4af995e7fac12c35014","title":"Analysis of Tree-Structured Architectures for Code Generation"},{"paperId":"253710d510012727fd3662574cc71d024eccd197","title":"HIJaX: Human Intent JavaScript XSS Generator"},{"paperId":"e3d72c8589301c9254561b85d7afb1eafe2afe0a","title":"A Survey of Automatic Code Generation fromNatural Language"},{"paperId":"b029346bfa85fa2fc7a12498ae5f018922ce55f9","title":"Code Generation from Natural Language with Less Prior Knowledge and More Monolingual Data"},{"paperId":null,"title":"Code Clippy Data: A large dataset of code data from Github for research into code language models"},{"paperId":null,"title":"Natural languageguided programming"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"c5ad46ae286e1c8ad3ffa52d7c5481f7dba83e88","title":"Conversational Semantic Parsing"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"d944bf7942297f5670192c5cd33191c26a87973e","title":"Code to Comment “Translation”: Data, Metrics, Baselining & Evaluation"},{"paperId":"f430773cdc0fdd62192a48a372601d6aa40f3d7b","title":"Task-Oriented Dialogue as Dataflow Synthesis"},{"paperId":"da97bd6d2d0a2f11bb011b9925585e086010cff0","title":"A Promising Path Towards Autoformalization and General Artificial Intelligence"},{"paperId":"e0d4587181a8848e73612e8a32b02bd9cc82b595","title":"CoCoNuT: combining context-aware neural translation models using ensemble for program repair"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"1defaab75eac2d4684fb727ace2960b1ea879e4e","title":"Transition-based Semantic Dependency Parsing with Pointer Networks"},{"paperId":"0071ee55f1749e6be586645bab08f0385ee856b4","title":"Graph-based, Self-Supervised Program Repair from Diagnostic Feedback"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"c9aac0037e8abe2da081490cc9d10610aa8fdb3f","title":"Semantic code search using Code2Vec: A bag-of-paths model"},{"paperId":"5461c2b704a84ed2eb77121bc2e4a4fe81ad8b9c","title":"Generating Question Titles for Stack Overflow from Mined Code Snippets"},{"paperId":"01ec011977fc6cda03e8b447e1b5eb0551f1c961","title":"A Simple Language Model for Task-Oriented Dialogue"},{"paperId":"b01ac6b990770092c6784f6eda8f3e94e2feb5a8","title":"Wrex: A Unified Programming-by-Example Interaction for Synthesizing Readable Code for Data Scientists"},{"paperId":"77910e51a40d17157fc798325d06edfa6cff18d6","title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation"},{"paperId":"5e9b611a476e993f03c90424847311cd84e36a06","title":"Optimising the fit of stack overflow code snippets into existing code"},{"paperId":"9c49cdf0ac4665b320262156eb19bf2e39cb1bb4","title":"End-to-End Slot Alignment and Recognition for Cross-Lingual NLU"},{"paperId":"21d9d58ab9537d10927b749eaa1c8b8c5a970579","title":"Leveraging Code Generation to Improve Code Retrieval and Summarization via Dual Learning"},{"paperId":"4ecf4356c3b451b16780788a3f94e422d4deeda5","title":"Tree-structured Attention with Hierarchical Accumulation"},{"paperId":"f1288516ea2f8c0174d27f4e2cc28efb1826193d","title":"Exploring Neural Models for Parsing Natural Language into First-Order Logic"},{"paperId":"e41452747ac0674a7b6534e78be33134fe8ef650","title":"Learning to Represent Programs with Property Signatures"},{"paperId":"92515b7ed018194e340f9edefeb52d9b19f679ef","title":"Detecting Code Clones with Graph Neural Network and Flow-Augmented Abstract Syntax Tree"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"a470dfc3dd30e4a27c8480d6fb817ece6a11d813","title":"Associating Natural Language Comment and Source Code Entities"},{"paperId":"a3afb3795f7054fc5b334d2d6feb92f0999942c7","title":"Exploration of neural machine translation in autoformalization of mathematics in Mizar"},{"paperId":"51a920c3d201eec57bcc2e97e0268304f53b5161","title":"TreeGen: A Tree-Based Transformer Architecture for Code Generation"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"c20c68c45127439139a08adb0b1f2b8354a94d6c","title":"CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data"},{"paperId":"388e2fcdcefbe0834e153ab2a0be127092f9674d","title":"DIALOGPT : Large-Scale Generative Pre-training for Conversational Response Generation"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":null,"title":"CodeBERT: A PreTrained Model for Programming and Natural"},{"paperId":null,"title":"There are various works in code translation (Lachaux et al., 2020"},{"paperId":"0d1f94b7698b8b3e12bb89c2ec571b1ecdd54d20","title":"Custom \"Caring IDE\" for Online Offering of CS1"},{"paperId":"3efa0276409aa6b4bb55cd5cbefd3500cb67119b","title":"Transformer Semantic Parsing"},{"paperId":null,"title":"2020) propose a custom IDE prototype that can be integrated with a dialogue system to deliver personalized learning interventions"},{"paperId":null,"title":"Gluecode: A benchmark for source code machine learning models"},{"paperId":"788d28e234fc69fb07b4a4da7fb1bcf05e5160b5","title":"Multi-Task Learning for Conversational Question Answering over a Large-Scale Knowledge Base"},{"paperId":"d2d44be771d01e277a9912249f2f7c211c393fee","title":"On the fly synthesis of edit suggestions"},{"paperId":"b7af363ff612f41e0c1071ae24ce68a836aaa678","title":"Code Generation as a Dual Task of Code Summarization"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"d530a007ae0493ef6a8167c25bd007104623c504","title":"DIRE: A Neural Approach to Decompiled Identifier Naming"},{"paperId":"545e60873b0fad25407bb6d647f92c905bd2483d","title":"CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases"},{"paperId":"00549af4bc3270e0f688acbf694f912d7ee39cad","title":"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks"},{"paperId":"27e8b827f29a3e674b44032fcba886e21ac9bead","title":"Survey of conversational agents in health"},{"paperId":"9656eeb6bb9fc1f35418791cf0c7310e378f2435","title":"Neural Code Search Evaluation Dataset"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"79b7a61abd4a4b4cf62c26cec815b55b0778f48d","title":"Pythia: AI-assisted Code Completion System"},{"paperId":"3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c","title":"Reranking for Neural Semantic Parsing"},{"paperId":"68c010953e7e485dd6154f14a80637149f07f918","title":"The Language of Programming: A Cognitive Perspective"},{"paperId":"b7f6f4ced1901c5ce78aa2d09ca16d5088220a1f","title":"SampleFix: Learning to Correct Programs by Sampling Diverse Fixes"},{"paperId":"84f7b32fa871a105b39e457fce1b74f57f72e60b","title":"Deep code comment generation with hybrid lexical and syntactical information"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"6b94dcac41325a03956402ff7862fa80936f9ddb","title":"A Survey of Natural Language Generation Techniques with a Focus on Dialogue Systems - Past, Present and Future Directions"},{"paperId":"f79af5e9e96f9c777e8759d791e33e9da83ffa65","title":"SParC: Cross-Domain Semantic Parsing in Context"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"317124ee4c11b0e1f25482395f23013d8e9bdbb4","title":"A Chatbot for Conflict Detection and Resolution"},{"paperId":"8d257e975b390b0507008bc2efc846be2e4c64f9","title":"A Neural Semantic Parser for Math Problems Incorporating Multi-Sentence Information"},{"paperId":"4fcd069322b2249fbf6db44d1e54f36c0b095212","title":"Context Dependent Semantic Parsing over Temporally Structured Data"},{"paperId":"3092325d55f6aa9ba28b0841bdcfd61991a38d48","title":"A Neural Model for Generating Natural Language Summaries of Program Subroutines"},{"paperId":"3838f0e9a0985e01d68afc731b04e71a274dfffd","title":"Learning-Based Recursive Aggregation of Abstract Syntax Trees for Code Clone Detection"},{"paperId":"516497e7fc39ebb126bed28dcc5c6129e500acb8","title":"An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"98d1307bed619b58b4a44acd8e65ac58495776c2","title":"code2seq: Generating Sequences from Structured Representations of Code"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"09c4ff0031ce0d2ad5c4e27203d3eb4a044cc4a0","title":"Say Hello to 'Coding Tutor'! Design and Evaluation of a Chatbot-based Learning System Supporting Students to Learn to Program"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"Hands-on transfer learning with Python: implement advanced deep learning and neural network models using TensorFlow and Keras"},{"paperId":"e92de0c4ef62a84201fac284eb66c37330b5fe1c","title":"Learning to Generate Corrective Patches using Neural Machine Translation"},{"paperId":"472a5227279b45f25508017816af34e3cb3ac0d7","title":"Semantic Parsing for Task Oriented Dialog using Hierarchical Representations"},{"paperId":"649c1148439a4e875dab414ba67bf8c80350af4a","title":"TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"d6b7abd5c17f9e7d83c5b709c27da3a82a9788f2","title":"NAPS: Natural Program Synthesis Dataset"},{"paperId":"4d1c856275744c0284312a3a50efb6ca9dc4cd4c","title":"Know What You Don’t Know: Unanswerable Questions for SQuAD"},{"paperId":"2f541b24f69798e255e04229e8dc78f4d1873fd7","title":"Improving Text-to-SQL Evaluation Methodology"},{"paperId":"e033a0b29af2939fd44e5765d03380c08897a9e8","title":"Deep Code Search"},{"paperId":"c5ecabddd4cdda1b2e97ce663ec34a46c2eb5588","title":"Context-Aware Conversational Developer Assistants"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"fd04e31c25451f9103a0ac2220ac8d7e7884c343","title":"Coarse-to-Fine Decoding for Neural Semantic Parsing"},{"paperId":"669e6be7cd92ba6bda39d9e3a030e72fde07a418","title":"Improving a Neural Semantic Parser by Counterfactual Learning from Human Bandit Feedback"},{"paperId":"15f16252af84a630f1f9442d4e0367b6fba089cf","title":"Deep Code Comment Generation"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"01d9579b81dc85ea2339ef309423296fef29aaf8","title":"(Almost) Zero-Shot Cross-Lingual Spoken Language Understanding"},{"paperId":"dc030c2e55b266c029356a54bb444b7d9b1f2abc","title":"StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow"},{"paperId":"429b65937d4922578a81e1f0ef5aeab7361ae36b","title":"NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System"},{"paperId":"89365ef9f61a62ea610f210083d14f70b8ee2972","title":"VulDeePecker: A Deep Learning-Based System for Vulnerability Detection"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"9858b40d23d329151f202b76aac7ca515dee5913","title":"pix2code: Generating Code from a Graphical User Interface Screenshot"},{"paperId":"be9350a3e8e3dfbce550d78581e4bdb08fae22a4","title":"Generating Regular Expressions from Natural Language Specifications: Are We There Yet?"},{"paperId":"271b0be2b6dfa6b76421206a4b44a7ea5edf8b69","title":"Measuring Transfer of Data-Driven Code Features Across Tasks in Alice"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":null,"title":"2018. A Survey"},{"paperId":null,"title":"Ashish Nagar, and others"},{"paperId":"116a877c774969b53399b1ccaa34d51bfb492ee4","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"759be48bcb76780957593bc4c7cd3756a61defa3","title":"Retrieving and classifying instances of source code plagiarism"},{"paperId":"3ade4d3be53981a1678b1e3a736d01547f7d3b9e","title":"Dialog for Language to Code"},{"paperId":"b0fd7a0f70b64c06031bb915d9aedd44b6550b16","title":"SemFuzz: Semantics-based Automatic Generation of Proof-of-Concept Exploits"},{"paperId":"7456179fa71bc237e051ecb3c02c043a50549029","title":"SQLizer: query synthesis from natural language"},{"paperId":"a26b00c607b6d6a2697f5444f14a9afc37701444","title":"DéjàVu: a map of code duplicates on GitHub"},{"paperId":"dffd889cc95805a9efd0fe2a8d86c1e1db470e78","title":"IDE-Based Learning Analytics for Computing Education"},{"paperId":"29d2ec9b25e5a0844d156a0e7bc19df24a2c3a48","title":"Supervised Deep Features for Software Functional Clone Detection by Exploiting Lexical and Syntactical Information in Source Code"},{"paperId":"8f0642cad60b338d493f4ea414167b08db6f3e0d","title":"Automated Data-Driven Hints for Computer Programming Students"},{"paperId":"3807a517403cfc7ed67fdfea64baf62b9948bcfe","title":"A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"8ff54aa8045b1e30c348cf2ca42259c946cd7a9e","title":"Search-based Neural Structured Learning for Sequential Question Answering"},{"paperId":"32ce5467ff884d2f90a233f4d9606c6e18b1a9d6","title":"Learning a Neural Semantic Parser from User Feedback"},{"paperId":"2c1e874c3b67510a3215e535f5646b362de5bc89","title":"Abstract Syntax Networks for Code Generation and Semantic Parsing"},{"paperId":"668db48c6a79826456341680ee1175dfc4cced71","title":"Get To The Point: Summarization with Pointer-Generator Networks"},{"paperId":"3ff0af64279929a952ee340e645256b7e0580f65","title":"RobustFill: Neural Program Learning under Noisy I/O"},{"paperId":"554c9b3aa8c3b7504810270b18b942a0d22156e7","title":"DeepFix: Fixing Common C Language Errors by Deep Learning"},{"paperId":"e4bcb45990a928ac06df4dc24dc07e16fcac9141","title":"Synthesizing benchmarks for predictive modeling"},{"paperId":"a486e2839291111bb44fa1f07731ada123539f75","title":"Google’s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":"0d24a0695c9fc669e643bad51d4e14f056329dec","title":"An Actor-Critic Algorithm for Sequence Prediction"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":null,"title":"Artificial intelligenceassisted online social therapy for youth"},{"paperId":null,"title":"2017) proposed a fourphase process model for IDE-based data analytics consisting of: (1) data collection, (2) data analysis, (3) intervention design, and (4) intervention"},{"paperId":"7bd07765ed544a784bb1b93aaed36df6dd3cf6ee","title":"NLmaps: A Natural Language Interface to Query OpenStreetMap"},{"paperId":"3abce2e6f85817a1f66398d098115bca0edcfea2","title":"Learning to Fuzz: Application-Independent Fuzz Testing with Probabilistic, Generative Models of Input Data"},{"paperId":"74157ae408173bf713f1e94f15aca1475c43bd74","title":"Neural Generation of Regular Expressions from Natural Language with Minimal Domain Knowledge"},{"paperId":"c7fcaa13db8c89ff1f39b5687ba47f8beee107c4","title":"The Value of Semantic Parse Labeling for Knowledge Base Question Answering"},{"paperId":"32aa2517b03c871c11e521c2a3406f457833e2c3","title":"Summarizing Source Code using a Neural Attention Model"},{"paperId":"97fb4e3d45bb098e27e0071448b6152217bd35a5","title":"Layer Normalization"},{"paperId":"aad646a4d164c586b996fae852679a80c36ad43d","title":"Effective compiler error message enhancement for novice programming students"},{"paperId":"15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7","title":"Gaussian Error Linear Units (GELUs)"},{"paperId":"7306437b2145677fe7bf3b7711ac8aa25989f1e3","title":"Simpler Context-Dependent Logical Forms via Model Projections"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"a418437e3f75865bcad2da65ebf422c49a36eacd","title":"A Corpus and Semantic Parser for Multilingual Natural Language Querying of OpenStreetMap"},{"paperId":"b7eac64a8410976759445cce235469163d23ee65","title":"Data Recombination for Neural Semantic Parsing"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"ba30df190664193514d1d309cb673728ed48f449","title":"Incorporating Copying Mechanism in Sequence-to-Sequence Learning"},{"paperId":"b1b59ab6063a856b1e608bdfe640263e4c0ced22","title":"Automated Correction for Syntax Errors in Programming Assignments using Recurrent Neural Networks"},{"paperId":"eda2880f9ddd31aa17cfccfac75fffeb02069562","title":"Students' Syntactic Mistakes in Writing Seven Different Types of SQL Queries and its Application to Predicting Students' Success"},{"paperId":"484ed558a5863060b373e8a2cb7cb302b8c36116","title":"A Convolutional Attention Network for Extreme Summarization of Source Code"},{"paperId":"558ac446dc26bee9789d660a251b75728cb6eeb2","title":"Language to Logical Form with Neural Attention"},{"paperId":"9f2a8e923965b23c11066a2ead79658208f1fae1","title":"Minimum Risk Training for Neural Machine Translation"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"1d5972b32a9b5a455a6eef389de5b7fca25771ad","title":"Domain-Adversarial Training of Neural Networks"},{"paperId":"49512270b39636375880d611d7b2192d324f4ba6","title":"Convolutional Neural Networks over Tree Structures for Programming Language Processing"},{"paperId":"89f240184b1260fbdd7307f5ca5284b9dea192e1","title":"Data-Driven Hint Generation in Vast Solution Spaces: a Self-Improving Python Programming Tutor"},{"paperId":"27e1dbe9f7c71cd6cc1b0357f49aef497e572d09","title":"Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"},{"paperId":"9f832bdcbc9d9566f7ab07b7455364bee62086fb","title":"Pseudogen: A Tool to Automatically Generate Pseudo-Code from Source Code"},{"paperId":"bf1262a77e79538ecff1338a71245530fb43e106","title":"Learning a strategy for adapting a program analysis via bayesian optimisation"},{"paperId":"bd06a0d166957b169be53434f1192cc985a587cc","title":"A user-guided approach to program analysis"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"b41e95c8c97846d5ca4c11ef79d7814499cc9663","title":"Compositional Semantic Parsing on Semi-Structured Tables"},{"paperId":"5b85caebb78ccbbd068d3cc8c8d2bc82d32ed4eb","title":"The Normalized Programming State Model: Predicting Student Performance in Computing Courses Based on Programming Behavior"},{"paperId":"25369f56a933e3bfb1d8e1588cdc6c50df93ecae","title":"Building a Semantic Parser Overnight"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"9653d5c2c7844347343d073bbedd96e05d52f69b","title":"Pointer Networks"},{"paperId":"d6f2f611da110b5b5061731be3fc4c7f45d8ee23","title":"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"},{"paperId":"be6665462fd6c56e8756dadf23373bb0c90b2b19","title":"Predicting Program Properties from \"Big Code\""},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"d4ab3e01c4d1308371c76fbc9665701100461e88","title":"Language to Code: Learning Semantic Parsers for If-This-Then-That Recipes"},{"paperId":"d8c9cab98a5fe33cb75aae9cc79a9bafdc4b0fa3","title":"SemEval 2015 Task 18: Broad-Coverage Semantic Dependency Parsing"},{"paperId":null,"title":"enhanced error messages (Becker et al., 2016), and generating hints to programmers (Chow et al., 2017; Rivers and Koedinger, 2017)"},{"paperId":"9277dc7dd5b348645e8fcf4a987e5b3fc9132c7c","title":"Towards a Big Data Curated Benchmark of Inter-project Code Clones"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"cb9588146a4335b96e3bd8c51e4b4e02fc10b7cd","title":"Constructing an Interactive Natural Language Interface for Relational Databases"},{"paperId":"baa919534218b7dfad833f3bd47314be7044c84b","title":"NLyze: interactive programming by natural language for spreadsheet data analysis and manipulation"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"3d1d42c9435b419ac928ebf7bcf4c86a460d6ef4","title":"Semantic Parsing via Paraphrasing"},{"paperId":"75963cf489c5935f27c280906772c2d48999f1c3","title":"Broad-Coverage Semantic Dependency Parsing"},{"paperId":"b29447ba499507a259ae9d8f685d60cc1597d7d3","title":"Semantic Parsing on Freebase from Question-Answer Pairs"},{"paperId":"393f96be13a96a161f0e99f3d90bae945031a57a","title":"An Intelligence-Based Optimization Model of Passenger Flow in a Transportation Station"},{"paperId":"e72e5ee5de14fd463ab58ce830474157258e3578","title":"Abstract Meaning Representation for Sembanking"},{"paperId":"6d7645d6a64b197083cf09eefb87e6e40a91eca8","title":"Universal Conceptual Cognitive Annotation (UCCA)"},{"paperId":"2335e7ff062b7be074e4c4eb1bee025fdb16d1ff","title":"Semantic Parsing with Combinatory Categorial Grammars"},{"paperId":"f7c6b122ee4e928fb88e287c26b9b50d2783415a","title":"Predicting Performance in an Introductory Programming Course by Logging and Analyzing Student Programming Behavior"},{"paperId":"86dd5d1493bbc0c72a739fba5a79f2582d8a497c","title":"Semantic Parsing Freebase: Towards Open-domain Semantic Parsing"},{"paperId":"e510bae5437936c24c18dfb81e5659f9f08a9531","title":"Using Semantic Unification to Generate Regular Expressions from Natural Language"},{"paperId":"cde902f11b0870c695428d865a35eb819b1d24b7","title":"Weakly Supervised Learning of Semantic Parsers for Mapping Instructions to Actions"},{"paperId":"0cdfa97e66ceb76144d4783d25e52c61fd0c4786","title":"Automatic Generation and Reranking of SQL-Derived Answers to NL Questions"},{"paperId":"304f7fd4af9cc82355eaf912a8f120f7429362de","title":"Spreadsheet data manipulation using examples"},{"paperId":"3954a3f80cf1b1f76430d80e924d85f2f1ba6799","title":"Learning to Parse Natural Language Commands to a Robot Control System"},{"paperId":"36d69fec4884389c1709d3ca74394cac814ce4a4","title":"Lexical Generalization in CCG Grammar Induction for Semantic Parsing"},{"paperId":"508e5b724c8b841aecfae864e5d6dcd02eb28772","title":"Bootstrapping Semantic Parsers from Conversations"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"03c85aa6d213f56cbd3602d9eaf2cc72de9f9a7e","title":"AEG: Automatic Exploit Generation"},{"paperId":"a25fbcbbae1e8f79c4360d26aa11a3abf1a11972","title":"A Survey on Transfer Learning"},{"paperId":"fbc6562814e08e416e28a268ce7beeaa3d0708c8","title":"Large-Scale Machine Learning with Stochastic Gradient Descent"},{"paperId":null,"title":"UCI source code data sets"},{"paperId":"81e982e91ab1c307e001301948caeda47cad43e0","title":"Type-logical semantics"},{"paperId":"0ac7b7eb0c9f75154d6faaa7585578c993072956","title":"Introduction to Information Retrieval Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze, Cambridge University Press, 2008"},{"paperId":null,"title":", and Eran Yahav . 2019 . code 2 vec : Learning distributed representations of code"},{"paperId":"774113732db34ce0b797fc3dcceded811fb6edbc","title":"Online Learning of Relaxed CCG Grammars for Parsing to Logical Form"},{"paperId":"812355cec91fa30bb50e9e992a3549af39e4f6eb","title":"One-shot learning of object categories"},{"paperId":"74fe7ec751cd50295b15cfd46389a8fefb37c414","title":"Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars"},{"paperId":"dd2d5ddc0399e0b87c339ebea4042ef2ad6f0317","title":"Guiding a Reinforcement Learner with Natural Language Advice: Initial Results in RoboCup Soccer"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"b50f78c9534182a09c060580811274928702b38d","title":"Towards a theory of natural language interfaces to databases"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"f60d8dd8ca3a7dfa7d0a14988af73084ad93619d","title":"Using Multiple Clause Constructors in Inductive Logic Programming for Semantic Parsing"},{"paperId":"151411a7b32e40dca8a51503135c6e9e3cdbc70c","title":"Automated Construction of Database Interfaces: Intergrating Statistical and Relational Learning for Semantic Parsing"},{"paperId":null,"title":"Chapter 14 - the role of goal orientation in self-regulated learning"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"b7c0e47f8b768258b7d536c21b218e6c46ab8791","title":"Learning to Parse Database Queries Using Inductive Logic Programming"},{"paperId":"c0a48ed7577a7b48288dfb2711cbd86e30636b5f","title":"Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval"},{"paperId":"6dae782f8be8fe5c25dbf2d0d681b3f708bf1a32","title":"Expanding the Scope of the ATIS Task: The ATIS-3 Corpus"},{"paperId":"f3d06b7a298e17e7fe91a902423b66e91f1a20cc","title":"Learning Semantic Grammars with Constructive Inductive Logic Programming"},{"paperId":"880cbe45abf7a3f92c9c52b391050814f8b0031b","title":"Pragmatics of Word Order Flexibility"},{"paperId":"8df509919b31397e225280962c59384fbe83144e","title":"Evaluation of Spoken Language Systems: the ATIS Domain"},{"paperId":"052b1d8ce63b07fec3de9dbb583772d860b7c769","title":"Learning representations by back-propagating errors"},{"paperId":"cd99e520e299e71c67ec2064f48e2394405e85c4","title":"The Commercial Application of Expert Systems Technology"},{"paperId":"ceb3163c56465fda5fef591d0ff0a6c7f434a04d","title":"A Deductive Approach to Program Synthesis"},{"paperId":"ce8cca19455e8d3055c57a9bafe882984c95a201","title":"Syntactic Process"},{"paperId":"6f0aa57820d5f1700461b317faabad9b98d0f70d","title":"Developing a natural language interface to complex data"},{"paperId":"d163709460265aa4901ac41a2b903793ad24b3c2","title":"Knowledge and Reasoning in Program Synthesis"},{"paperId":"e9533991eb9569e4cf515f8520bd9a3e43746fc7","title":"Progress in natural language understanding: an application to lunar geology"},{"paperId":"94d9cbfc474cb6ce961de45a06233b2853ca6724","title":"Toward automatic program synthesis"},{"paperId":"577e96521d62b9ebb5fd67412a21b02e9cd67b90","title":"PROW: A Step Toward Automatic Program Writing"},{"paperId":"a798bca71c8833e49ad9bac22da4b5c3503f1e6a","title":"ELIZA—a computer program for the study of natural language communication between man and machine"},{"paperId":"9b486c647916df9f8be0f8d4fc5c94c493bfaa80","title":"PRINCIPLES OF NEURODYNAMICS. PERCEPTRONS AND THE THEORY OF BRAIN MECHANISMS"},{"paperId":"cccc0a4817fd5f6d8758c66b4065a23897d49f1d","title":"Principles of neurodynamics"}],"id":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","summary":"This survey paper overviews major deep learning methods used in Natural Language Processing (NLP) and source code over the last 35 years and presents a software-engineering centered taxonomy for CI placing each of the works into one category describing how it best assists the software development cycle."},{"url":"https://www.semanticscholar.org/paper/76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","title":"Investigating Explainability of Generative AI for Code through Scenario-based Design","venue":"International Conference on Intelligent User Interfaces","year":2022,"referenceCount":102,"citationCount":23,"influentialCitationCount":0,"publicationDate":"10/02/2022","authors":"Jiao Sun,Q. Liao,Michael J. Muller,Mayank Agarwal,Stephanie Houde,Kartik Talamadupula,Justin D. Weisz","citations":[{"paperId":"14990625adb451ec89733af784f8f43663f1f680","title":"Charting the Evolution and Future of Conversational Agents: A Research Agenda Along Five Waves and New Frontiers"},{"paperId":"d0c3fad605a29ab67e8befddcafac4f024e2def2","title":"What is Human-Centered about Human-Centered AI? A Map of the Research Landscape"},{"paperId":"5694c7a8759847a4f13a5c0ba7ee37297372f3ca","title":"On Selective, Mutable and Dialogic XAI: a Review of What Users Say about Different Types of Interactive Explanations"},{"paperId":"c5a87d691e880674c8bd982585a56759fad504e5","title":"Why is AI not a Panacea for Data Workers? An Interview Study on Human-AI Collaboration in Data Storytelling"},{"paperId":"edd0de7d3377d2653387b9c42fd74ffac7b5a9dc","title":"Human-AI Co-Creation Approach to Find Forever Chemicals Replacements"},{"paperId":"12225e55685add962f78a67a04cbc8d216a9f5ee","title":"Follow the Successful Herd: Towards Explanations for Improved Use and Mental Models of Natural Language Systems"},{"paperId":"1b492746ee3a304a13950cad1a59861b9ee44645","title":"A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?"},{"paperId":"57ffd52abfbba0eb76c8406c7331f1f8ca29aaff","title":"R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents"},{"paperId":"91810509404a4d4e233d13cf10e4cc07838b3096","title":"Exploring Challenges and Opportunities to Support Designers in Learning to Co-create with AI-based Manufacturing Design Tools"},{"paperId":"f03f39f735186c4359b719724be6e1c2eb912fef","title":"Studying the effect of AI Code Generators on Supporting Novice Learners in Introductory Programming"},{"paperId":"7c817fa57edd087820d3b4d16e4d1f40d4cb5200","title":"Generation Probabilities Are Not Enough: Exploring the Effectiveness of Uncertainty Highlighting in AI-Powered Code Completions"},{"paperId":"c2ad7f32927392f22addc227d97a777e3ad57f6f","title":"A Systematic Literature Review of Explainable AI for Software Engineering"},{"paperId":"80a6501f518eaaf495e72373fc4128d374d25395","title":"Charting the Sociotechnical Gap in Explainable AI: A Framework to Address the Gap in XAI"},{"paperId":"074baf835aec44a100990178859b35451975f339","title":"Navigating Complexity in Software Engineering: A Prototype for Comparing GPT-n Solutions"},{"paperId":"24c5450d8fa785e5f85d9427d2d65cf66476ac3a","title":"Toward General Design Principles for Generative AI Applications 130-144"},{"paperId":"3078a916b57e465614efea50628c889799e32289","title":"\"It would work for me too\": How Online Communities Shape Software Developers' Trust in AI-Powered Code Generation Tools"},{"paperId":"edb6c93255bbaa879f6d4af173a947a2026ed4c6","title":"A nascent design theory for explainable intelligent systems"},{"paperId":"be747953df4b3269534c54addddc889986550343","title":"Psychological Impact and Influence of Animation on Viewer's Visual Attention and Cognition: A Systematic Literature Review, Open Challenges, and Future Research Directions"},{"paperId":"ba77991d19cf8c50ae2d2efcc9b5fb141acaa7b4","title":"Requirements Engineering for Machine Learning: A Review and Reflection"},{"paperId":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code"},{"paperId":"3fbea6c84b8c78c59392d7bf864dbe681924015f","title":"How Can We Develop Explainable Systems? Insights from a Literature Review and an Interview Study"},{"paperId":"a1ef81e17a9ca41e09aba802040a2eca2744716f","title":"Generation Probabilities are Not Enough: Improving Error Highlighting for AI Code Suggestions"},{"paperId":"a81d05e8812cd4adbd76bf408efdcab05d6bb8d7","title":"Creative Use of XAI In Socio-Technical Systems: A Case Study"}],"references":[{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"59641c10ed7431a3cf841f308367dc2dc0281b74","title":"What Makes Good In-Context Examples for GPT-3?"},{"paperId":"5e1746995debd1f17c24af01514c727598cc5613","title":"Human-Centered Explainable AI (XAI): From Algorithms to User Experiences"},{"paperId":"4abb90edf2ec4045ae62cf6e25725043209bf57b","title":"Explainability Pitfalls: Beyond Dark Patterns in Explainable AI"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"bc89a6fbf43cf911f71e5428d0b4a70fa5a40be9","title":"A Human-Centered Agenda for Intelligible Machine Learning"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"c32b72c3ed3de87aa0b28b2361ee344e18721b9e","title":"Applied AI matters: AI4Code"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"6648728ea8257eec4740f9602d18e75329779201","title":"Who needs to know what, when?: Broadening the Explainable AI (XAI) Design Space by Looking at Explanations Across the AI Lifecycle"},{"paperId":"93db03584d742ee2989016d83f571773ee79afcd","title":"Uncertainty Quantification 360: A Holistic Toolkit for Quantifying and Communicating the Uncertainty of AI"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"310bec02ca79f537f6854f76b991f94ebde70f3a","title":"Operationalizing Human-Centered Perspectives in Explainable AI"},{"paperId":"02d4f8a782da9cb195aeb1efbc79a66b5ed0e32e","title":"Introduction to Explainable AI"},{"paperId":"1c9b419b8d28aaca4f2923f673a0286ab0140ec7","title":"Method for Exploring Generative Adversarial Networks (GANs) via Automatically Generated Image Galleries"},{"paperId":"a56601de82b4f19ae1afab6e6edf18b9b9103f22","title":"Trade-offs for Substituting a Human with an Agent in a Pair Programming Context: The Good, the Bad, and the Ugly"},{"paperId":"ba741d13c7fe68bafdc7268fccac112558a37db9","title":"Designing Ground Truth and the Social Life of Labels"},{"paperId":"6e2f3594eb097c03ec0b046ae3443078b4245f24","title":"Towards A Process Model for Co-Creating AI Experiences"},{"paperId":"82b4a1dde07fa277333d80546501268e8471e7ff","title":"HAI-GEN 2021: 2nd Workshop on Human-AI Co-Creation with Generative Models"},{"paperId":"72ee8a42b05fa5d748f6e9160147b43c4a417acf","title":"Question-Driven Design Process for Explainable AI User Experiences"},{"paperId":"2a36402451c4fcab33c9e7f59fc027167c070aa6","title":"Perfection Not Required? Human-AI Partnerships in Code Translation"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"b0fd54759bc37461bf91b13f7a15c63cc24051b5","title":"Evaluating the Interpretability of Generative Models by Interactive Reconstruction"},{"paperId":"349e9e9c332465f02feba784bd6cb247aa741cb2","title":"The Sanction of Authority: Promoting Public Trust in AI"},{"paperId":"aa6b9d6081646a5461d953dc2412b6e0344cc2d9","title":"How AI Developers Overcome Communication Challenges in a Multidisciplinary Team"},{"paperId":"c8965761083d80ff762ce76c08df92d66e01f37d","title":"Expanding Explainability: Towards Social Transparency in AI systems"},{"paperId":"f156ecbbb9243522275490d698c6825f4d2e01af","title":"Explainable AI: A Review of Machine Learning Interpretability Methods"},{"paperId":"3973e0fab69a00f5ed6a81ca408f60c420fa6e61","title":"Uncertainty as a Form of Transparency: Measuring, Communicating, and Using Uncertainty"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"1311adebe439ccd9e373d594a8df1d720a685190","title":"Code Prediction by Feeding Trees to Transformers"},{"paperId":"c1f6777acbb7d7b22d683ce9eb19deda740c56b0","title":"How Data Scientists Improve Generated Code Documentation in Jupyter Notebooks"},{"paperId":"2a127ba560b74a86bf281fa1960e9e0100acad21","title":"Applied AI matters: AI4Code: applying artificial intelligence to source code"},{"paperId":null,"title":"HCAI@NeurIPS2021: Human Centered AI workshop at NeurIPS 2021"},{"paperId":"61d93a88b4db300f963e6e1dfd86d0cb8b5d231f","title":"Machine Learning Model Cards Transparency Review : Using model card toolkit"},{"paperId":"3a76c06f55f50d4c4d432f6036228e5ef2b841de","title":"Quality Estimation & Interpretability for Code Translation"},{"paperId":"0e1fce327f17d42c42997a67a9f4ad7944e5789a","title":"Participatory Machine Learning Using Community-Based System Dynamics"},{"paperId":"58eaf1cb6dab101551af6256c891f2f27b0d8e60","title":"Towards evaluating and eliciting high-quality documentation for intelligent systems"},{"paperId":"5a68df7665071a69355adffde6db71d263de8ff4","title":"Generative adversarial networks"},{"paperId":"05ceac747bec286129818e9e2fbd37ef034f5ada","title":"Interrogating Data Science"},{"paperId":"684f4cba51f717953ccef410094eb159fea58f33","title":"Bridging the Gap Between Ethics and Practice"},{"paperId":"b82287ae96f67ff12daebb59d953b4b08a9c1e02","title":"Unit Test Case Generation with Transformers"},{"paperId":"eb522cb3651416d8499a2c8ba8d9f95aa33be7a8","title":"Towards Designing Conversational Agents for Pair Programming: Accounting for Creativity Strategies and Conversational Styles"},{"paperId":"8befc59280bca3333d5b3075007b44d37024cfb2","title":"A Methodology for Creating AI FactSheets"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"7e02e25869b60f23c15bff7ffd8d406ff3321fca","title":"Human-Centered Approaches to Fair and Responsible AI"},{"paperId":"77a5e08f361b6f91cac8a24b380a14c12bb93383","title":"Novice-AI Music Co-Creation via AI-Steering Tools for Deep Generative Models"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"2c9d71966e1e8a527a392bbe28aa53f8e0918755","title":"Human-centered Explainable AI: Towards a Reflective Sociotechnical Approach"},{"paperId":"7d089d4cc4aff5c10c1704f02119e2487fc898c9","title":"Questioning the AI: Informing Design Practices for Explainable AI User Experiences"},{"paperId":"a3c86ee77f9622cbdfddf2f335de2ae858fe400a","title":"Mapping Out Human-Centered Data Science: Methods, Approaches, and Best Practices"},{"paperId":"4678aff07f985b72950645a119aa874865ece789","title":"Experiences with Improving the Transparency of AI Models and Services"},{"paperId":"1b0f4bd3872bb590d457990ac2b26b29f770fc44","title":"Explainable machine learning in deployment"},{"paperId":"aac29b1b8627f56a258c52e95092a3fdd863b137","title":"ORES: Lowering Barriers with Participatory Machine Learning in Wikipedia"},{"paperId":"5998dc629e57ad944c3f910a6b43da7780ec6997","title":"Cococo: AI-Steering Tools for Music Novices Co-Creating with Generative Models"},{"paperId":"2695145c36541c70f7f3c01711d3dc885cfe4e0b","title":"Enabling Value Sensitive AI Systems through Participatory Design Fictions"},{"paperId":"9a81b5aed97d222b2506357fce41b3a9651e0ab5","title":"ABOUT ML: Annotation and Benchmarking on Understanding and Transparency of Machine Learning Lifecycles"},{"paperId":"6d1db22a4a5995323c9911058d694c231d4a15f0","title":"WeBuildAI: Participatory Framework for Algorithmic Governance"},{"paperId":"ce177672b00ddf46e4906157a7e997ca9338b8b9","title":"Attention is not not Explanation"},{"paperId":"0de0a44b859a3719d11834479112314b4caba669","title":"A Multiscale Visualization of Attention in the Transformer Model"},{"paperId":"a039ea239e37f53a2cb60c68e0a1967994353166","title":"Analyzing the Structure of Attention in a Transformer Language Model"},{"paperId":"96bbcc5ba6cc1d196c533bc94e98bbdb4df93edc","title":"The Pragmatic Turn in Explainable Artificial Intelligence (XAI)"},{"paperId":"2081ed6854290a479f796f2432c7951ff24232fe","title":"Human-Centered Study of Data Science Work Practices"},{"paperId":"d2f18e0675d86e64fdc6e8beaa83c2b74f6a81b6","title":"Visualizing Uncertainty and Alternatives in Event Sequence Predictions"},{"paperId":"55640ff387069d205fa59da549680892db407a0a","title":"Explainability scenarios: towards scenario-based XAI design"},{"paperId":"1fccba11583dc9e1030713d61bd65e9e9990e39f","title":"Human-Centered Artificial Intelligence and Machine Learning"},{"paperId":"7365f887c938ca21a6adbef08b5a520ebbd4638f","title":"Model Cards for Model Reporting"},{"paperId":"4bb18e9211ee79190bd0c455837a5d89ddf239c4","title":"Increasing Trust in AI Services through Supplier's Declarations of Conformity"},{"paperId":"f7325d232c7ac7d2daaf6605377058db5b5b83cc","title":"A Survey of Methods for Explaining Black Box Models"},{"paperId":"9fb11647537cd9030e966481b77063cb19dc8cd4","title":"Value-Sensitive Algorithm Design"},{"paperId":"21dff47a4142445f83016da0819ffe6dd2947f66","title":"Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)"},{"paperId":"6fe99c4969c2f2d3dde8ada84e7388d74eaf0528","title":"Isolating Sources of Disentanglement in Variational Autoencoders"},{"paperId":"8deee44e1473a441b97b6407e52ca5304f9b15ba","title":"Learning Deep Disentangled Embeddings with the F-Statistic Loss"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"d516daff247f7157fccde6649ace91d969cd1973","title":"The Mythos of Model Interpretability"},{"paperId":"9a22c177e8e6bb0530141c3b7db3f9f40e04ae1a","title":"A detailed study of Software Development Life Cycle (SDLC) Models"},{"paperId":"442e10a3c6640ded9408622005e3c2a8906ce4c2","title":"A Unified Approach to Interpreting Model Predictions"},{"paperId":"9777caede85b213aa567d5257a456942580ede3b","title":"A Survey of Inductive Biases for Factorial Representation-Learning"},{"paperId":"b710f0c9ca89f997371ce06c142ad25be1b35cca","title":"Interpretable Decision Sets: A Joint Framework for Description and Prediction"},{"paperId":"467d5d8fc766e73bfd3e9415f75479823f92c2f7","title":"Rationalizing Neural Predictions"},{"paperId":"022788ecf8685ed30e3f69f9121b5a3d242a22d6","title":"On the naturalness of software"},{"paperId":"c04aaf36c8587e40747212e316d9bf44186ef64a","title":"Developing a Research Agenda for Human-Centered Data Science"},{"paperId":"5091316bb1c6db6c6a813f4391911a5c311fdfe0","title":"“Why Should I Trust You?”: Explaining the Predictions of Any Classifier"},{"paperId":"31f9eb39d840821979e5df9f34a6e92dd9c879f2","title":"Learning Deep Features for Discriminative Localization"},{"paperId":"27e1dbe9f7c71cd6cc1b0357f49aef497e572d09","title":"Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"},{"paperId":"cb030975a3dbcdf52a01cbd1c140711332313e13","title":"Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission"},{"paperId":"1f713ea97c87166875daf650fbdc3950eed8973a","title":"New Initiative: The Naturalness of Software"},{"paperId":"825ca26af5a2a510dbc1a7b97587212bc98ae968","title":"Power to the People: The Role of Humans in Interactive Machine Learning"},{"paperId":"3b6f2f5a97ff34a94c66e19fbed9858e714db113","title":"Migrating code with statistical machine translation"},{"paperId":"2b6382bef79d846cb1492bd0ce24a97432d8370a","title":"Social transparency in networked information exchange: a theoretical framework"},{"paperId":"2c154458fdbed11e77715ba26d60fb527fab90ba","title":"Toolkit to support intelligibility in context-aware applications"},{"paperId":"b062423d2c021274300e5a9f58fc66eea39d1c55","title":"Why and why not explanations improve the intelligibility of context-aware intelligent systems"},{"paperId":"83bc768a6e3d9049e68c64eb50e8f6b2c55b4cce","title":"How HCI interprets the probes"},{"paperId":null,"title":"Phoebe Sengers, and Paul Dourish"},{"paperId":"57c790c90f20a9015ce9aca20e7c0a218bb5cc8a","title":"Scenario-based design"},{"paperId":"14ae6f2231e09e226b99002aa04b5c70f3c59f2b","title":"A model for types and levels of human interaction with automation"},{"paperId":"5df85ae89af55c6d82a1a14836ea6bcfbfc2c0ec","title":"Principles of mixed-initiative user interfaces"},{"paperId":"6ae8b855704dd60df8b186037dd38b43d92c40cd","title":"Design: Cultural probes"},{"paperId":"50934979694fb48e55d0cf38888f67b84ad6601b","title":"Conversational Processes and Causal Explanation"},{"paperId":null,"title":"Can Now Write Its Own Computer Code. That's Good News for Humans"}],"id":"76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","summary":"This work explores explainability needs for GenAI for code and demonstrates how human-centered approaches can drive the technical development of XAI in novel domains."},{"url":"https://www.semanticscholar.org/paper/590f6817b42407f96b079e82c935fae298196359","title":"Less is More: Summary of Long Instructions is Better for Program Synthesis","venue":"ArXiv","year":2022,"referenceCount":28,"citationCount":9,"influentialCitationCount":0,"publicationDate":"16/03/2022","authors":"Kirby Kuznia,Swaroop Mishra,Mihir Parmar,Chitta Baral","citations":[{"paperId":"fbb5cc9e8a46f61d1543bd17b6eca324fd5bf55c","title":"Fully Autonomous Programming with Large Language Models"},{"paperId":"8a2a2006e371ce07e0b29538cd11337a5c977caf","title":"“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models"},{"paperId":"7857d80f516a943f96757b21053f7635ac1cf2af","title":"Revisiting the Plastic Surgery Hypothesis via Large Language Models"},{"paperId":"39064f05b8c39fa63c0d7a9fc2a1a623a3711c07","title":"InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis"},{"paperId":"47dc00cf21a0e7452b6f61207f119a9cc01c52b8","title":"HELP ME THINK: A Simple Prompting Strategy for Non-experts to Create Customized Content with Models"},{"paperId":"6cf8a4d05e66266233380f989edaf647eba7e1a5","title":"BioTABQA: Instruction Learning for Biomedical Table Question Answering"},{"paperId":"99752e255a866484291866a5ff5cf94e96d6bdc4","title":"Is a Question Decomposition Unit All We Need?"},{"paperId":"fb30166c218bef3597b0d9789ad340defc3989ca","title":"In-BoXBART: Get Instructions into Biomedical Multi-Task Learning"},{"paperId":"31e396eab8edb44f79e3158eeefc3280afb404f4","title":"How Many Data Samples is an Additional Instruction Worth?"}],"references":[{"paperId":"6cf8a4d05e66266233380f989edaf647eba7e1a5","title":"BioTABQA: Instruction Learning for Biomedical Table Question Answering"},{"paperId":"99752e255a866484291866a5ff5cf94e96d6bdc4","title":"Is a Question Decomposition Unit All We Need?"},{"paperId":"3a6a97a50695d43d95a015bbb554b2bc0d40394e","title":"Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions"},{"paperId":"31e396eab8edb44f79e3158eeefc3280afb404f4","title":"How Many Data Samples is an Additional Instruction Worth?"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"17dd3555fd1ccf1141cf984347fa1b3fd6b009ca","title":"Multitask Prompted Training Enables Zero-Shot Task Generalization"},{"paperId":"3ac5aa6ac59253611ef3cb72a95cbe21ef5dda1b","title":"Reframing Instructional Prompts to GPTk’s Language"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"cbdb45fc16b0885905b91d84281c310e6cb49e9c","title":"Cross-Task Generalization via Natural Language Crowdsourcing Instructions"},{"paperId":null,"title":"InBoXBART: Get instructions into biomedical multi-task learning"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":null,"title":"Jurassic-1: Technical details and evaluation"},{"paperId":null,"title":"The Codex model (Chen et al., 2021) is an advanced code generation model that powers GitHub’s Copilot. The state of the art model for program synthesis was introduced by Deepmind called AlphaCode"},{"paperId":null,"title":"2021) introduced the APPS dataset for testing the accuracy of large LMs on program synthesis"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"f4061bd225b3be5b3f5b18eb1a229ce991efefeb","title":"PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization"},{"paperId":"0abcbdf40f872e6baf1c082811d4ae93df787698","title":"Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets"},{"paperId":"3ff0af64279929a952ee340e645256b7e0580f65","title":"RobustFill: Neural Program Learning under Noisy I/O"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":"7ae207cfaf01dc2b6799da67f454190b34994870","title":"A Statistical Semantic Parser that Integrates Syntax and Semantics"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"590f6817b42407f96b079e82c935fae298196359","summary":"A meta-dataset consisting of human and synthesized summaries of the long and complicated programming questions shows that summaries improve performance for introductory and interview programming questions and shows improvement by a small margin for competitive programming questions, implying scope for future research in this direction."},{"url":"https://www.semanticscholar.org/paper/1a903282f7c19dbdb2714b852fb42dbb4675422b","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis","venue":"","year":2022,"referenceCount":48,"citationCount":35,"influentialCitationCount":2,"publicationDate":"25/03/2022","authors":"Erik Nijkamp,Bo Pang,Hiroaki Hayashi,Lifu Tu,Haiquan Wang,Yingbo Zhou,S. Savarese,Caiming Xiong","citations":[{"paperId":"dd0960a8ffb6a8baff69ec96c3f67deb2912b53a","title":"Self-collaboration Code Generation via ChatGPT"},{"paperId":"f7e41afdd7b54af3ddb2bff8005efbce4de87d38","title":"Constructing Effective In-Context Demonstration for Code Intelligence Tasks: An Empirical Study"},{"paperId":"1c00f88b69f1d4efe3e1695ca49aa38a1340bd98","title":"Improving Few-Shot Prompts with Relevant Static Analysis Products"},{"paperId":"eb55442d940accc67d6790e66d3196e1941fc7a4","title":"Teaching Large Language Models to Self-Debug"},{"paperId":"e74dee333546d9ad1a611a18fb0d5fa82980c006","title":"Explainable Automated Debugging via Large Language Model-driven Scientific Debugging"},{"paperId":"470754e17de89081f63dde4719922fe9b63251d5","title":"Large Language Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via FuzzGPT"},{"paperId":"3693683c4e0405819fae7115ad680f769eb83534","title":"Neural Comprehension: Language Models with Compiled Neural Networks"},{"paperId":"a845bce29105f55dab1e47d2f92d7338eb183a4e","title":"Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT"},{"paperId":"cbdc75e9c80870164662bb0359e23fd4736c5f0e","title":"A Survey of Large Language Models"},{"paperId":"28dafb94a87daa71ad3edf9f04f3c8c32753398b","title":"Improving Code Generation by Training with Natural Language Feedback"},{"paperId":"349ec9ca2ff17dbc1aa1caf27f98b7cd85a4747e","title":"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation"},{"paperId":"7857d80f516a943f96757b21053f7635ac1cf2af","title":"Revisiting the Plastic Surgery Hypothesis via Large Language Models"},{"paperId":"92f19090599910af1b1c9ed2b318abc0adea0527","title":"Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences"},{"paperId":"d39db76fb80c1f74455a0f6432c2242b727389da","title":"Self-planning Code Generation with Large Language Model"},{"paperId":"a7b3a868a80dbe97689135c99b1a6b6e10dcdfe5","title":"A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT"},{"paperId":"8475c1d1d98932111b194dc139c8f8b30818725d","title":"xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"57ffd52abfbba0eb76c8406c7331f1f8ca29aaff","title":"R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"49fede098a0d2a48e8100b30189224fc6f5eb25b","title":"Language Model Crossover: Variation through Few-Shot Prompting"},{"paperId":"1786a2f9140ed7211b21302977de64e948b92308","title":"Learning Performance-Improving Code Edits"},{"paperId":"79680e20f0c8038039b243fb5fd385fbe967c799","title":"Controlling Large Language Models to Generate Secure and Vulnerable Code"},{"paperId":"7677e9ead35f8f2578acbf5ad15fb330c83762c4","title":"Systematically Finding Security Vulnerabilities in Black-Box Code Generation Models"},{"paperId":"2af6a21a1b682ceb585165359d3605e89f4cf6b0","title":"Fixing Hardware Security Bugs with Large Language Models"},{"paperId":"a23e0cf74f10b6a3061ab71497fe2c8c476fecd1","title":"Conversational Automated Program Repair"},{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"},{"paperId":"433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context"},{"paperId":"0f38267a8ba32789f5d3b1b19820f86940fea052","title":"Generation-Augmented Query Expansion For Code Retrieval"},{"paperId":"bf5fbd690f24f873df86d1b0a06579cf42f7dc36","title":"Dialog2API: Task-Oriented Dialogue with API Description and Example Programs"},{"paperId":"8210cef990b8e5cddbc95000e46309bdd25337f7","title":"Asking Clarification Questions for Code Generation in General-Purpose Programming Language"},{"paperId":"27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation"},{"paperId":"b15cddd33b36d1f38a8e59412026f6dfde0ca38d","title":"Calibrated Interpretation: Confidence Estimation in Semantic Parsing"},{"paperId":"e73a16490c29530c37b49f6a30592790e7caaaa4","title":"Don't Complete It! Preventing Unhelpful Code Completion for Productive and Sustainable Neural Code Completion Systems"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"ec229b004ebaee27b9efa84b834825dbc2937538","title":"Is GitHub's Copilot as Bad As Humans at Introducing Vulnerabilities in Code?"}],"references":[{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"dc32a984b651256a8ec282be52310e6bd33d9815","title":"Highly accurate protein structure prediction with AlphaFold"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"66c10bf1f11bc1b2d92204d8f8391d087f6de1c4","title":"RoFormer: Enhanced Transformer with Rotary Position Embedding"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f430773cdc0fdd62192a48a372601d6aa40f3d7b","title":"Task-Oriented Dialogue as Dataflow Synthesis"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"00c957711b12468cb38424caccdf5291bb354033","title":"ZeRO: Memory optimizations Toward Training Trillion Parameter Models"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"8323c591e119eb09b28b29fd6c7bc76bd889df7a","title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"},{"paperId":"545e60873b0fad25407bb6d647f92c905bd2483d","title":"CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"f79af5e9e96f9c777e8759d791e33e9da83ffa65","title":"SParC: Cross-Domain Semantic Parsing in Context"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"b227f3e4c0dc96e5ac5426b85485a70f2175a205","title":"Representation Learning with Contrastive Predictive Coding"},{"paperId":null,"title":"JAX: composable transformations of Python+NumPy programs, 2018"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"644ca74f80463415613847ab01cff067fb58f0ad","title":"Neuro-Symbolic Program Synthesis"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"7f6245d527e84092111a6adb0f1939d0209d3360","title":"FlashMeta: a framework for inductive program synthesis"},{"paperId":"307fb6c6cfc456ab3e510c08fde39e6e3574fe5d","title":"Automatically improving accuracy for floating point expressions"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"a72055d6b6581f880837c8981300fc93641ef25e","title":"Optimizing database-backed applications with query synthesis"},{"paperId":"84069287da0a6b488b8c933f3cb5be759cb6237e","title":"On the difficulty of training recurrent neural networks"},{"paperId":"308388616c12158423fbf8bd8c441d11d1f432a2","title":"Stochastic superoptimization"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"5090ed315d3ab9f0135c83f287c5021d61929760","title":"Denali: a goal-directed superoptimizer"},{"paperId":"2fb9729c0a3b046c390ad6748773527eae6cfc84","title":"What Is Program Synthesis?"},{"paperId":"94d9cbfc474cb6ce961de45a06233b2853ca6724","title":"Toward automatic program synthesis"},{"paperId":null,"title":"If you use this software, please cite it using these metadata"}],"id":"1a903282f7c19dbdb2714b852fb42dbb4675422b","summary":"The utility of the trained model is shown by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval, and experiments show that the multi-step program synthesis capacity scales as a function of the model size and data size."},{"url":"https://www.semanticscholar.org/paper/a8fc183c089bd596ccc48b3d666f8814e1b41e55","title":"InCoder: A Generative Model for Code Infilling and Synthesis","venue":"ArXiv","year":2022,"referenceCount":81,"citationCount":81,"influentialCitationCount":17,"publicationDate":"12/04/2022","authors":"Daniel Fried,Armen Aghajanyan,Jessy Lin,Sida I. Wang,Eric Wallace,Freda Shi,Ruiqi Zhong,Wen-tau Yih,Luke Zettlemoyer,M. Lewis","citations":[{"paperId":"01f9b773408115a16fe872147348db175789e82f","title":"Tool Learning with Foundation Models"},{"paperId":"f7e41afdd7b54af3ddb2bff8005efbce4de87d38","title":"Constructing Effective In-Context Demonstration for Code Intelligence Tasks: An Empirical Study"},{"paperId":"dd0960a8ffb6a8baff69ec96c3f67deb2912b53a","title":"Self-collaboration Code Generation via ChatGPT"},{"paperId":"470754e17de89081f63dde4719922fe9b63251d5","title":"Large Language Models are Edge-Case Fuzzers: Testing Deep Learning Libraries via FuzzGPT"},{"paperId":"a845bce29105f55dab1e47d2f92d7338eb183a4e","title":"Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT"},{"paperId":"3b01b5b497e1f359b11da45af029281ee6f64c2c","title":"Towards Enhancing In-Context Learning for Code Generation"},{"paperId":"cbdc75e9c80870164662bb0359e23fd4736c5f0e","title":"A Survey of Large Language Models"},{"paperId":"f24dabf3317abaa3d7393ab7d48115f353749bde","title":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X"},{"paperId":"af622206f1e5a6632600075897385cdabcae463d","title":"Large Language Models and Simple, Stupid Bugs"},{"paperId":"362cbfd0d05e139cd6cf049754098a6e1520b910","title":"PanGu-{\\Sigma}: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing"},{"paperId":"7857d80f516a943f96757b21053f7635ac1cf2af","title":"Revisiting the Plastic Surgery Hypothesis via Large Language Models"},{"paperId":"fbdd496c421e050a47c4fb2e0019635d2f4b97e7","title":"Meet in the Middle: A New Pre-training Paradigm"},{"paperId":"34d24b2d9f116f8f652c112d4ac924afcf11bd0d","title":"InferFix: End-to-End Program Repair with LLMs"},{"paperId":"d39db76fb80c1f74455a0f6432c2242b727389da","title":"Self-planning Code Generation with Large Language Model"},{"paperId":"aae61ba5b629eba965b7f49a685b3d9f1bfb358c","title":"Model-Agnostic Syntactical Information for Pre-Trained Programming Language Models"},{"paperId":"eb0b9965732ce09b67e088efdbe0978aeafcdcc6","title":"Greener yet Powerful: Taming Large Code Generation Models with Quantization"},{"paperId":"196353ab8d340b31b465c22dff904de195fa9a60","title":"LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"05ae2f22e150e47ff8030aa3024158a28c98d51d","title":"Do Machine Learning Models Produce TypeScript Types that Type Check?"},{"paperId":"1786a2f9140ed7211b21302977de64e948b92308","title":"Learning Performance-Improving Code Edits"},{"paperId":"535a3d95a743e1f4b591b5b2af3e778a6347158a","title":"CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code"},{"paperId":"79680e20f0c8038039b243fb5fd385fbe967c799","title":"Controlling Large Language Models to Generate Secure and Vulnerable Code"},{"paperId":"d3de0bac5703825796c240bfab8dc3c8e0a90222","title":"Impact of Code Language Models on Automated Program Repair"},{"paperId":"7677e9ead35f8f2578acbf5ad15fb330c83762c4","title":"Systematically Finding Security Vulnerabilities in Black-Box Code Generation Models"},{"paperId":"27c2ddc921befc44f7b2cdb8bb39facdd8b0b188","title":"Measuring The Impact Of Programming Language Distribution"},{"paperId":"18a8a2a74b878aed15b36eb0848b5f543b8fc585","title":"KNOD: Domain Knowledge Distilled Tree Decoder for Automated Program Repair"},{"paperId":"a764d4f28612a7b5c4767ea6a2540b169fdb052c","title":"CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models"},{"paperId":"63396fceb84286b02796dc58e55c07ec1095c4dc","title":"FLAME: A small language model for spreadsheet formulas"},{"paperId":"0192638593c430e15e2804a74a3e8a7ecb52d435","title":"On the Design of AI-powered Code Assistants for Notebooks"},{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"},{"paperId":"7ed237af793f43c442b3e8e1bc9ace906a276b2a","title":"Transfer Knowledge from Natural Language to Electrocardiography: Can We Detect Cardiovascular Disease Through Language Models?"},{"paperId":"468992bf970c37bd1fef58b78a6c2fcd8c018868","title":"Scaling Laws for Generative Mixed-Modal Language Models"},{"paperId":"1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a","title":"SantaCoder: don't reach for the stars!"},{"paperId":"dae74645479f7c1fa3671066f9e24ec6c20c17ec","title":"TrojanPuzzle: Covertly Poisoning Code-Suggestion Models"},{"paperId":"6032212d5790b6a580d68d469a9895aad6238c89","title":"Diverse Title Generation for Stack Overflow Posts with Multiple Sampling Enhanced Transformer"},{"paperId":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"66476832701361c9f9b2a7eb2354ee8cd9f72e67","title":"Large Language Models are Zero-Shot Fuzzers: Fuzzing Deep-Learning Libraries via Large Language Models"},{"paperId":"269df328eec08b56b7b1f38a7555797fe2b999b6","title":"ReCode: Robustness Evaluation of Code Generation Models"},{"paperId":"433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming"},{"paperId":"ebd4bec684808aff360b5f255d15c0d112ba13d3","title":"Explicit Knowledge Transfer for Weakly-Supervised Code Generation"},{"paperId":"27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation"},{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"78f1ca609cd6f789749365c2870e2c2efd8f1fdf","title":"UniMASK: Unified Inference in Sequential Decision Problems"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","title":"Evaluating How Fine-tuning on Bimodal Data Effects Code Generation"},{"paperId":"b1eebb2df3b9ff7ff2b00fb1a786f6ada2caebce","title":"Towards a Mathematics Formalisation Assistant using Large Language Models"},{"paperId":"eca35805d185374befe4da48c9f96ace6e962fad","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"71280dba5bda65c162f9deaffed7d3d20692ca0a","title":"SecurityEval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques"},{"paperId":"3cba16fc46ac5b35c1cc72a822208aa0097384cc","title":"CodePAD: Sequence-based Code Generation with Pushdown Automaton"},{"paperId":"4ddc26b3a5fe9044b97b408d163f7464d769ebbf","title":"CODEP: Grammatical Seq2Seq Model for General-Purpose Code Generation"},{"paperId":"20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers"},{"paperId":"2577d053f8aab912d29b424e1f09133d83740fd2","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"f031ba42cf82f106200bb03fbb91dd5671a59b9c","title":"Practical Program Repair in the Era of Large Pre-trained Language Models"},{"paperId":"39e40821b7207125e54e6ed7112e55cd38c6f0c3","title":"Language Models of Code are Few-Shot Commonsense Learners"},{"paperId":"259b7a01700c39d5669e88d1434873ea38a13528","title":"In-Context Policy Iteration"},{"paperId":"8fbd7ddf1ea30c991f3b1152a245df77caa18e16","title":"Learning by Distilling Context"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"1b4c19168410fb2690d285b205ab2281793db81a","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"},{"paperId":"1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization"},{"paperId":"ec4c8d99eb1c028c43af6d8bbf727392d351cb59","title":"Efficient Training of Language Models to Fill in the Middle"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"96f0f08e2dbeacc89a30d419a9cfb24312bd8da7","title":"BigIssue: A Realistic Bug Localization Benchmark"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs"},{"paperId":"0a39442979d6e678dd36bb443ad529c14e86a86e","title":"DocPrompting: Generating Code by Retrieving the Docs"},{"paperId":"53661ff6fdbfb8557c5b19895fad151792c62da7","title":"Few-shot training LLMs for project-specific code-summarization"},{"paperId":"9bfda45ef365466cba27e47a9fdb98d15d78aa15","title":"Repository-Level Prompt Generation for Large Language Models of Code"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"3d5463a16d85d9d1d0d8ebb4117a31aca3c240ff","title":"Fooling MOSS Detection with Pretrained Language Models"},{"paperId":"ab8a53be2ff8a756776aa0245ad8da41189d60d6","title":"Fuzzing Deep-Learning Libraries via Large Language Models"},{"paperId":"ef58e99dbb90bebbdc6187b5cba94dd5973dbb16","title":"Retrieval-Augmented Multimodal Language Modeling"},{"paperId":"3994eb8e237a94dae1efc6e767a09044b8550ace","title":"FCM: Forgetful Causal Masking Makes Causal Language Models Better Zero-Shot Learners"},{"paperId":"09e14c4c80e20e80c052e0adb0d49df51aff718d","title":"Yes, You Can Make an App Too: A Systematic Study of Prompt Engineering in the Automatic Generation of Mobile Applications from User Queries"}],"references":[{"paperId":"ec4c8d99eb1c028c43af6d8bbf727392d351cb59","title":"Efficient Training of Language Models to Fill in the Middle"},{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"55c36748f2a7c060c3313349c730b053ed03fbf7","title":"Deduplicating Training Data Mitigates Privacy Risks in Language Models"},{"paperId":"d407aa1cc3f9ce8478d3052344947fb49baa04d4","title":"CodeFill: Multi-token Code Completion by Jointly learning from Structure and Naming Sequences"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"c783e1fb3ce8514f981925ee590c00884660ee4e","title":"CM3: A Causal Masked Multimodal Model of the Internet"},{"paperId":"fb01415a0decfa3f3d6339930e95028ae1ff4170","title":"Efficient Large Scale Language Modeling with Mixtures of Experts"},{"paperId":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","title":"Finetuned Language Models Are Zero-Shot Learners"},{"paperId":"e596b8adbffa546dbc163e817fb3de72744ec4f6","title":"HTLM: Hyper-Text Pre-Training and Prompting of Language Models"},{"paperId":"4566c0d22ebf3c31180066ab23b6c445aeec78d5","title":"Deduplicating Training Data Makes Language Models Better"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":null,"title":"2022): we remove files that contain any line longer than 3000 tokens or an average line length greater than 100 tokens, have less than 40% of their characters being alphanumeric or underscores"},{"paperId":null,"title":"Natural Language Processing with Transformers"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"1aaddc7a5bfa9fbfad540f682f45ecd1ededcfd8","title":"Learning type annotation: is big data enough?"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"83a86fdf5d42fc70a07a2badd4fc9d42863f9b64","title":"SpreadsheetCoder: Formula Prediction from Semi-structured Context"},{"paperId":"a989f5daa7c438d6b8433efbb1461f1b0af1aa8e","title":"Break-It-Fix-It: Unsupervised Learning for Program Repair"},{"paperId":"4b0ec90dc10e51c1fc983edcd57bb86636d7b3ca","title":"Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"3a1efac4a8aa2f40a8128e034c01f18660652dd5","title":"DOBF: A Deobfuscation Pre-Training Objective for Programming Languages"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"1dc513a688ca92809e504144c3d1e361d1df9927","title":"Reflective Decoding: Beyond Unidirectional Generation with Off-the-Shelf Language Models"},{"paperId":"59cc0871534a20ac98db0663539c6675ab63566f","title":"SequenceR: Sequence-to-Sequence Learning for End-to-End Program Repair"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax"},{"paperId":null,"title":"FairScale: A general purpose modular PyTorch library for high performance and large scale training"},{"paperId":"3efbcfeeb0ea1051a71101d3318da4411081f0b8","title":"Scaling Laws for Autoregressive Generative Modeling"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0071ee55f1749e6be586645bab08f0385ee856b4","title":"Graph-based, Self-Supervised Program Repair from Diagnostic Feedback"},{"paperId":"94551d326be51a57434659093904524c39b877cd","title":"Enabling Language Models to Fill in the Blanks"},{"paperId":"29fc7ff02e68537a86d32978bfd2e8e45258e77d","title":"LambdaNet: Probabilistic Type Inference using Graph Neural Networks"},{"paperId":"1e655fa69c62b430b051224153f701f1b607fd9c","title":"Typilus: neural type hints"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"e6c561d02500b2596a230b341a8eb8b921ca5bf2","title":"Scaling Laws for Neural Language Models"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"a429c6c8407b10a89c9610ab5e9682ec89772356","title":"TypeWriter: neural type prediction with search-based validation"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"db392858262b17aa9c8ff8659738f68fbf832ebe","title":"Structural Language Models of Code"},{"paperId":"eaddaa76aa4c5b1804802d512c7cc1f854b0bda9","title":"CLN2INV: Learning Loop Invariants with Continuous Logic Networks"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"1e411c5e2fe44529a4033a2121adeecc60a7b409","title":"XL-Editor: Post-editing Sentences with XLNet"},{"paperId":"eb4c28a41db510d33bdfe56c4689289f534c9ba0","title":"Restoring ancient text using deep learning: a case study on Greek epigraphy"},{"paperId":"26d9141ed3f021af7533e1d84fc83111d20df925","title":"AutoPandas: neural-backed generators for program synthesis"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"75acc731bdd2b626edc74672a30da3bc51010ae8","title":"CTRL: A Conditional Transformer Language Model for Controllable Generation"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"130277ff64c7171c90d98d7e73f4bda8a0b0c1f9","title":"KERMIT: Generative Insertion-Based Modeling for Sequences"},{"paperId":"ad7129af0644dbcafa9aa2f111cb76526ea444a1","title":"Defending Against Neural Fake News"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"5efadc9019ce3378a0eb6c8f939cdde6c8918b1e","title":"Mask-Predict: Parallel Decoding of Conditional Masked Language Models"},{"paperId":"58d34a4fb936ffe95917d8fb4016ff5e3520429a","title":"Insertion Transformer: Flexible Sequence Generation via Insertion Operations"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"13We also search for occurrences of code"},{"paperId":"ea1b62a03ab16c4acca6f7041c6f096bedec244b","title":"Deep learning type inference"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"554c9b3aa8c3b7504810270b18b942a0d22156e7","title":"DeepFix: Fixing Common C Language Errors by Deep Learning"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":"f801f962c895d5c606cb52db85f099a4ed8c34e2","title":"Python probabilistic type inference with natural language support"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"216b98bb3d9221d5f5d261864975612e4d0faaa6","title":"EvoSuite: automatic test suite generation for object-oriented software"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"1ef301c1b275091b6a50d620b41df4722f2108f0","title":"Combinatorial sketching for finite programs"},{"paperId":"b7c0e47f8b768258b7d536c21b218e6c46ab8791","title":"Learning to Parse Database Queries Using Inductive Logic Programming"},{"paperId":null,"title":"RFC1321: The MD5"},{"paperId":"2fb9729c0a3b046c390ad6748773527eae6cfc84","title":"What Is Program Synthesis?"},{"paperId":"f39a2c11983b21fd5054d5393614959bfbc4e50f","title":"Space/time trade-offs in hash coding with allowable errors"}],"id":"a8fc183c089bd596ccc48b3d666f8814e1b41e55","summary":"INCODER is introduced, a unified generative model that can perform program synthesis (via left-to-right generation) as well as editing (via infilling) and the ability to condition on bidirectional context substantially improves performance on challenging tasks such as type inference, comment generation, and variable re-naming."},{"url":"https://www.semanticscholar.org/paper/47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution","venue":"ArXiv","year":2022,"referenceCount":47,"citationCount":19,"influentialCitationCount":4,"publicationDate":"25/04/2022","authors":"Freda Shi,Daniel Fried,Marjan Ghazvininejad,Luke Zettlemoyer,Sida I. Wang","citations":[{"paperId":"eb55442d940accc67d6790e66d3196e1941fc7a4","title":"Teaching Large Language Models to Self-Debug"},{"paperId":"57ffd52abfbba0eb76c8406c7331f1f8ca29aaff","title":"R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents"},{"paperId":"383157f9059061faba4364c7d2141d0ff78c87fa","title":"Learning Deep Semantics for Test Completion"},{"paperId":"6d269364de402d4a72ac30b0c8d81324f6849807","title":"LEVER: Learning to Verify Language-to-Code Generation with Execution"},{"paperId":"8bfc22de7fe66286ad9ae705d677246757fbf8a8","title":"Large Language Models Can Be Easily Distracted by Irrelevant Context"},{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"},{"paperId":"6032212d5790b6a580d68d469a9895aad6238c89","title":"Diverse Title Generation for Stack Overflow Posts with Multiple Sampling Enhanced Transformer"},{"paperId":"27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"a4bdc300db297756f36bedee2859b62df8e268c2","title":"Follow the Wisdom of the Crowd: Effective Text Generation via Minimum Bayes Risk Decoding"},{"paperId":"2577d053f8aab912d29b424e1f09133d83740fd2","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab","title":"Binding Language Models in Symbolic Languages"},{"paperId":"62f0db3a5ad5c795ec18fc7a6e7b01836809df57","title":"Language Models are Multilingual Chain-of-Thought Reasoners"},{"paperId":"95fa2b27ab7eb84738441ee16da97323538938f9","title":"I Speak, You Verify: Toward Trustworthy Neural Program Synthesis"},{"paperId":"40047a74b707743157051d38f76061ba5ff9aab4","title":"Compositional Semantic Parsing with Large Language Models"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs"},{"paperId":"0a39442979d6e678dd36bb443ad529c14e86a86e","title":"DocPrompting: Generating Code by Retrieving the Docs"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"}],"references":[{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"53c0abe83fe9b4fdaf2208295d8504fcf5241694","title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models"},{"paperId":"eea7bca03bda3ee2448cd012bbcb2b33822861d8","title":"Noisy Channel Language Model Prompting for Few-Shot Text Classification"},{"paperId":"5fbcfccd3736969d95ed660d8e6962c86b7a9113","title":"PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"a6a7724763d8adba466519489b0b9d209e7f2d15","title":"BARTScore: Evaluating Generated Text as Text Generation"},{"paperId":"ac879df2cc36f3f824fa24149517622b6bc7bd09","title":"Implicit Representations of Meaning in Neural Language Models"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"a847237e36b954c60e1959152468ebed0118f286","title":"Factual Probing Is [MASK]: Learning vs. Learning to Recall"},{"paperId":"3a1efac4a8aa2f40a8128e034c01f18660652dd5","title":"DOBF: A Deobfuscation Pre-Training Objective for Programming Languages"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"85e7d63f75c0916bd350a229e040c5fbb1472e7a","title":"Making Pre-trained Language Models Better Few-shot Learners"},{"paperId":"994dbb06b970eb842b7782e4df75ca1633f83e4e","title":"Sampling-Based Minimum Bayes Risk Decoding for Neural Machine Translation"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":null,"title":"Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing"},{"paperId":"e6e8a2c56243847b77b604259cde9e10a2daccb8","title":"Semantic Evaluation for Text-to-SQL with Distilled Test Suite"},{"paperId":"02eaaf87f9cae34cca398fed146079e6eeb1f868","title":"Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"8b6c9adf85a9d6391e3ccd503c2e5af929a36735","title":"Is MAP Decoding All You Need? The Inadequacy of the Mode in Neural Machine Translation"},{"paperId":"ce18780963b067a1295fc847e7ab33f2fcbfaca1","title":"Efficient Second-Order TreeCRF for Neural Dependency Parsing"},{"paperId":"ad5970584754cc7a1d91c95ab84a1e210258183a","title":"UnifiedQA: Crossing Format Boundaries With a Single QA System"},{"paperId":"77910e51a40d17157fc798325d06edfa6cff18d6","title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation"},{"paperId":"53a77e8f73f2ca422d6e38fa9ecc490231ac044c","title":"Neural Text Generation with Unlikelihood Training"},{"paperId":null,"title":"AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts"},{"paperId":null,"title":"How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423–438"},{"paperId":"708f8c0eb5032edd6f31663a27febbb0529cbcf3","title":"Visually Grounded Neural Syntax Acquisition"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"fd04e31c25451f9103a0ac2220ac8d7e7884c343","title":"Coarse-to-Fine Decoding for Neural Semantic Parsing"},{"paperId":"2012c176d7b003eb57a282bfd8681190704fb965","title":"Learning to Map Context-Dependent Sentences to Executable Formal Queries"},{"paperId":"429b65937d4922578a81e1f0ef5aeab7361ae36b","title":"NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System"},{"paperId":"3807a517403cfc7ed67fdfea64baf62b9948bcfe","title":"A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2c1e874c3b67510a3215e535f5646b362de5bc89","title":"Abstract Syntax Networks for Code Generation and Semantic Parsing"},{"paperId":"bcf16c08a41009d9f9174c6f72b2ff534232c147","title":"Sequence-based Structured Prediction for Semantic Parsing"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"67d5f03c4921e75a9c0dd5bd699ad74512e52eb8","title":"Mathematical Statistics : Basic Ideas and Selected Topics, Volumes I-II Package"},{"paperId":"eaba49c37be13c5bfadb270e3308cda307f27d69","title":"Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation"},{"paperId":null,"title":"Efficient multipass decoding for synchronous context free grammars"},{"paperId":"779b9d2437e68ea12d2f271b01f913460e16b91d","title":"Bayes Risk Minimization in Natural Language Parsing"},{"paperId":"e2a68774f92d1e894cbbbef2c819e4592990eb4b","title":"Minimum Bayes-Risk Decoding for Statistical Machine Translation"},{"paperId":"cd5a169879504ea91660a443b9151753cc29c42f","title":"Minimum Bayes-risk automatic speech recognition"}],"id":"47e15941c8b157873c8264e4bf50318d1ba5cd18","summary":"This work introduces execution result– based minimum Bayes risk decoding (MBR-EXEC) for program selection and shows that it improves the few-shot performance of pretrained code models on natural-language-to-code tasks, suggesting it as an effective approach for natural language to code translation."},{"url":"https://www.semanticscholar.org/paper/6074c7b75f27ca9adb6d74b080c07d5d079c3ea0","title":"Improving automatically generated code from Codex via Automated Program Repair","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":6,"influentialCitationCount":0,"publicationDate":2022,"authors":"Zhiyu Fan,Xiang Gao,Abhik Roychoudhury,Shin Hwei Tan","citations":[{"paperId":"c6808575096a6e4f3cbdc5f893384bc5a01cc6f8","title":"A study on Prompt Design, Advantages and Limitations of ChatGPT for Deep Learning Program Repair"},{"paperId":"fba0b0817dbc8200b41a1de22654b54b778a11e9","title":"Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models"},{"paperId":"e9a64e58855dbbc725203d0202ceb9e7f8b7bb36","title":"A Survey of Learning-based Automated Program Repair"},{"paperId":"30624a18720bf93a85dc3efe570df271a8c9f4c3","title":"Program Repair"},{"paperId":"2577d053f8aab912d29b424e1f09133d83740fd2","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"459176532c85ae72f8b5cb35589b72468401d844","title":"SelfAPR: Self-supervised Program Repair with Test Execution Diagnostics"}],"references":[{"paperId":"76f023c3a819fc58989a064a1b50825b11fce95d","title":"Capturing Failures of Large Language Models via Human Cognitive Biases"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis"},{"paperId":"5ff9032d0f7f246d01ae7b2c231ab03469a7344a","title":"Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?"},{"paperId":"1444536496d8064f33e10b38b5820fecfab5b367","title":"Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs"},{"paperId":"a8863de15a5ee8eed98107f423138a1a8f5a2ba8","title":"Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"0e587fba8c75f5861ce68559f5a007d508011534","title":"A syntax-guided edit decoder for neural program repair"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"171440398a1c0f43063a7689e3b385280336fb68","title":"CURE: Code-Aware Neural Machine Translation for Automatic Program Repair"},{"paperId":"59cc0871534a20ac98db0663539c6675ab63566f","title":"SequenceR: Sequence-to-Sequence Learning for End-to-End Program Repair"},{"paperId":"e0d4587181a8848e73612e8a32b02bd9cc82b595","title":"CoCoNuT: combining context-aware neural translation models using ensemble for program repair"},{"paperId":"e24c70ebdcf7aba490fdbdc62bf6436ddc9721af","title":"DLFix: Context-based Code Transformation Learning for Automated Program Repair"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0071ee55f1749e6be586645bab08f0385ee856b4","title":"Graph-based, Self-Supervised Program Repair from Diagnostic Feedback"},{"paperId":"b9a076ae227b04dde9ed57fa791f59e7b4e8b8ad","title":"ARJA: Automated Repair of Java Programs via Multi-Objective Genetic Programming"},{"paperId":"b5abecf78c1bd975449e34350289a904e6ef35cc","title":"Automated program repair"},{"paperId":"3c9b9664ca8cbbd17f5153997c276b0ce5c4f3bd","title":"TBar: revisiting template-based automated program repair"},{"paperId":"c27b906fe26bb7b9cfec3e8f495313214f3d5e00","title":"AVATAR: Fixing Semantic Bugs with Fix Patterns of Static Analysis Violations"},{"paperId":"ca91838da0b013ff84af8a59c1c5525c5bdf3997","title":"Test-Equivalence Analysis for Automatic Patch Generation"},{"paperId":"33d9da222a9de0e52021f42d5ad739d0ba061469","title":"Shaping program repair space with existing patches and similar code"},{"paperId":"84a56df6ab124e14c20d3973167d4d78272f8c1c","title":"Repairing Crashes in Android Apps"},{"paperId":"df6061b9d3da5865e3385bbcbc7d039b3ef6bb0f","title":"Context-Aware Patch Generation for Better Automated Program Repair"},{"paperId":"b599ce0f548a73c156e94c7bcca8aed5b0a9678b","title":"Compilation Error Repair: For the Student Programs, From the Student Programs"},{"paperId":"b0db907d372e2776a0c9e963a291e100033534a7","title":"A correlation study between automated program repair and test-suite metrics"},{"paperId":"c0f0b2eac4390e49cfe8b601a81535e2e08f5faa","title":"Nopol: Automatic Repair of Conditional Statement Bugs in Java Programs"},{"paperId":"6e5f3c4507aeb175da8712cff43cb8b2e60b5a12","title":"A feasibility study of using automated program repair for introductory programming assignments"},{"paperId":"aef542decbd262689a16b06809b37ef319807cba","title":"Codeflaws: A Programming Competition Benchmark for Evaluating Automated Program Repair Tools"},{"paperId":"554c9b3aa8c3b7504810270b18b942a0d22156e7","title":"DeepFix: Fixing Common C Language Errors by Deep Learning"},{"paperId":"bfc52c5679434fd7243ab8215696aa9a543247f2","title":"Anti-patterns in search-based program repair"},{"paperId":"775dd65fd70c13bc4144c28b25aa3376bbab3254","title":"Deep learning code fragments for code clone detection"},{"paperId":"feabe0fbb48f740e2251f6173a0949caac81a0c0","title":"ASTOR: a program repair library for Java (demo)"},{"paperId":"3f215e83b39a0887257a03274002f353c5a57537","title":"Angelix: Scalable Multiline Program Patch Synthesis via Symbolic Analysis"},{"paperId":"a331e24a763fb411a51a4962731976d9030e2556","title":"History Driven Program Repair"},{"paperId":null,"title":"2016. Description2Code Dataset"},{"paperId":"c14982de9adbfaa1848052fad254f8d9c5cbb61f","title":"Is the cure worse than the disease? overfitting in automated program repair"},{"paperId":"c503999384a688cd608c8f7c8afc89f87c8c9bd0","title":"relifix: Automated Repair of Software Regressions"},{"paperId":"37e2106bebd02f4ac9c410941fde7f358279e4a4","title":"Defects4J: a database of existing faults to enable controlled testing studies for Java programs"},{"paperId":"05607111cf79330d56164a10d351dbf94e2cfa44","title":"SemFix: Program repair via semantic analysis"},{"paperId":"3136ad216d30bdff223e5c3f02e07f980a6a45a5","title":"Automatic patch generation learned from human-written patches"},{"paperId":"38ac5d19ca64be51a1782668b5d9e3f0e536e2c7","title":"GZoltar: an eclipse plug-in for testing and debugging"},{"paperId":"820c566bbdb6ddaa7d801b92480cfd2b7e472c7b","title":"Automatically finding patches using genetic programming"},{"paperId":"eb956ab442810f783f44600ce6d714583a96a4aa","title":"On the Accuracy of Spectrum-based Fault Localization"}],"id":"6074c7b75f27ca9adb6d74b080c07d5d079c3ea0","summary":"This study systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests, revealing that automatically generated codes share some common programming mistakes with human-crafted solutions, indicating existing APR tools have the potential to fix auto-generated code."},{"url":"https://www.semanticscholar.org/paper/6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning","venue":"ArXiv","year":2022,"referenceCount":77,"citationCount":23,"influentialCitationCount":1,"publicationDate":"05/07/2022","authors":"Hung Le,Yue Wang,Akhilesh Deepak Gotmare,S. Savarese,S. Hoi","citations":[{"paperId":"941812e16d209b106e631cea24fc7941ebe19275","title":"Tingle Just for You: A Preliminary Study of AI-based Customized ASMR Content Generation"},{"paperId":"70ebcc3716583b7bc0d59e443c83d3ac522b79e9","title":"RunBugRun -- An Executable Dataset for Automated Program Repair"},{"paperId":"762b4b11220d2d8a9c57f0a3af327840a67e7284","title":"Self-Refine: Iterative Refinement with Self-Feedback"},{"paperId":"06e5828341aa3926e1d839039363b0673b9461cc","title":"Errors are Useful Prompts: Instruction Guided Task Programming with Verifier-Assisted Iterative Prompting"},{"paperId":"66718e87b70de80cbc2a4120050ca36fda49f8d6","title":"Exploring Distributional Shifts in Large Language Models for Code Analysis"},{"paperId":"8d03ebc375b3d887056e769018467bf0a6ed99e3","title":"Planning with Large Language Models for Code Generation"},{"paperId":"eb0b9965732ce09b67e088efdbe0978aeafcdcc6","title":"Greener yet Powerful: Taming Large Code Generation Models with Quantization"},{"paperId":"372dd962ff8c92ac5cf9a195fd52b4b79372c149","title":"A Syntax-Guided Multi-Task Learning Approach for Turducken-Style Code Generation"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"57ffd52abfbba0eb76c8406c7331f1f8ca29aaff","title":"R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents"},{"paperId":"4fbf624b9a848425c81f21f1171c32426541330f","title":"On the Reliability and Explainability of Automated Code Generation Approaches"},{"paperId":"79680e20f0c8038039b243fb5fd385fbe967c799","title":"Controlling Large Language Models to Generate Secure and Vulnerable Code"},{"paperId":"68edfd62d2619fb4af7c2469edb95b9e2fe4544a","title":"Execution-based Code Generation using Deep Reinforcement Learning"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming"},{"paperId":"045a6f92b58f08d5b305dd5d661316a506ee4d43","title":"Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation"},{"paperId":"6bf9476c53c0934b232a3cc2e6396a1d9826309a","title":"Mind Your Data! Hiding Backdoors in Offline Reinforcement Learning Datasets"},{"paperId":"c5b47a34fd7e0e595360683b4effa51c0261d1e3","title":"Language Models Can Teach Themselves to Program Better"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"8cb76c58ce65f47bbc7c1ca173f1362720e30729","title":"Joint Generator-Ranker Learning for Natural Language Generation"},{"paperId":"f7820b52e3cdff6625e6bd0430a8d48ca66cca3f","title":"Self-Programming Artificial Intelligence Using Code-Generating Language Models"},{"paperId":"407b9e9478ba6bff43ce4b20e8b6cb2b303477d2","title":"P LANNING WITH L ARGE L ANGUAGE M ODELS FOR C ODE G ENERATION"},{"paperId":"8d282bf295d655e38b63ddbc7cdc94b188df8bb2","title":"G ENERATING S EQUENCES BY L EARNING TO [S ELF -]C ORRECT"}],"references":[{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"47c740e858d3dfec0bf95104600851a8a2bec9ff","title":"VarCLR: Variable Semantic Representation Pre-training via Contrastive Learning"},{"paperId":"3f791ea6584b78fdf0ed80560d09d9496cf5a353","title":"Learning to Complete Code with Sketches"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a176b0de62840f7118006277d94bbc1547162a4d","title":"Learning to Synthesize Programs as Interpretable and Generalizable Policies"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"87fe4a67e4cb5b8cdbc5da4d98888c037fb7d043","title":"Learning Autocompletion from Real-World Datasets"},{"paperId":"07bcda1dff9bb696ea9cbc69303eee8bd3d85bd6","title":"GeDi: Generative Discriminator Guided Sequence Generation"},{"paperId":"04413d13c76c4eadf1adcbbe88c3a72e6462f166","title":"Fast and Memory-Efficient Neural Code Completion"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax"},{"paperId":"7093ad29f2de2781cf5ed6e6d212ed31ebaa2c8b","title":"Latent Execution for Neural Program Synthesis Beyond Domain-Specific Languages"},{"paperId":"3f97c2067cde9377e50b3160bbd7982c94abd88a","title":"An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions"},{"paperId":"4fa24cc5b17e8ff1eb5a01fd37a9d267a57ac563","title":"Recipes for Safety in Open-domain Chatbots"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"f669b5406320a83e94b4dd439806211c8b1e95fc","title":"Write, Execute, Assess: Program Synthesis with a REPL"},{"paperId":"7e2f5eca9465cf114043ed6c95ea59d9dbea45a1","title":"Learning to Infer and Execute 3D Shape Programs"},{"paperId":"58dcc24c63ed179d9a9b458d5f1284a7a297c5d6","title":"Learning to Describe Scenes with Programs"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"c35f8f48f748c040c33bbe1f21c794e209341987","title":"Neural Program Synthesis from Diverse Demonstration Videos"},{"paperId":"2fc1cfc75d6ba80846d64fbec424f6c35682f5d8","title":"Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing"},{"paperId":"5218e7da3b13d097e8a9fdc9fdb5d68ff619edc0","title":"Synthesizing Programs for Images using Reinforced Adversarial Learning"},{"paperId":"dea6aeb514b1969ab879c793d46a0d2eceaa2cbf","title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis"},{"paperId":"74b284a66e75b65f5970d05bac000fe91243ee49","title":"Video Captioning via Hierarchical Reinforcement Learning"},{"paperId":"9f0b217cb21ec8c19610e28bd0805d7871456e4b","title":"Learning to Infer Graphics Programs from Hand-Drawn Images"},{"paperId":"116a877c774969b53399b1ccaa34d51bfb492ee4","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"47f65c165f7ccedd4c18189d4690eec5369dd9c5","title":"SQLNet: Generating Structured Queries From Natural Language Without Reinforcement Learning"},{"paperId":"e60f6be86bc88fc2309b17d3df2885886448d16e","title":"Neural Scene De-rendering"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2e17cf6a339fd071ad222062f868e882ef4120a4","title":"Inferring and Executing Programs for Visual Reasoning"},{"paperId":"1ae84940e212e2e3ca132c7aa0878baf0fda06cd","title":"From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood"},{"paperId":"c689f73f8ea65c6e81c628f2b37feae09b29e46b","title":"Deep Reinforcement Learning-Based Image Captioning with Embedding Reward"},{"paperId":"3ff0af64279929a952ee340e645256b7e0580f65","title":"RobustFill: Neural Program Learning under Noisy I/O"},{"paperId":"6c8353697cdbb98dfba4f493875778c4286d3e3a","title":"Self-Critical Sequence Training for Image Captioning"},{"paperId":"644ca74f80463415613847ab01cff067fb58f0ad","title":"Neuro-Symbolic Program Synthesis"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":"0d24a0695c9fc669e643bad51d4e14f056329dec","title":"An Actor-Critic Algorithm for Sequence Prediction"},{"paperId":"35c1668dc64d24a28c6041978e5fcca754eb2f4b","title":"Sequence Level Training with Recurrent Neural Networks"},{"paperId":"a2499dd426c46c645ee805d7594b6687547c72d4","title":"Neural Random Access Machines"},{"paperId":"024006d4c2a89f7acacc6e4438d156525b60a98f","title":"Continuous control with deep reinforcement learning"},{"paperId":"3411535f7888a943853895e8eef2bb0b6d328c2a","title":"Weakly-supervised Disentangling with Recurrent Transformations for 3D View Synthesis"},{"paperId":"df137487e20ba7c6e1e2b9a1e749f2a578b5ad99","title":"Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks"},{"paperId":"370a5f4b305540f35821d2179c384f95f5a63eee","title":"Toward Deep Learning Software Repositories"},{"paperId":"687e80eb70c7bbad6001006d9269b202650a3354","title":"Deep Convolutional Inverse Graphics Network"},{"paperId":"d38e8631bba0720becdaf7b89f79d9f9dca45d82","title":"Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets"},{"paperId":"258986132bf17755fe8263e42429fe73218c1534","title":"CIDEr: Consensus-based image description evaluation"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"304f7fd4af9cc82355eaf912a8f120f7429362de","title":"Spreadsheet data manipulation using examples"},{"paperId":"c26770f29afafe22f2a507506e3f43c413f6a619","title":"Learning Programs: A Hierarchical Bayesian Approach"},{"paperId":"af5a7be7bfc592e400e2f12a24ad3e6a5a44b58f","title":"Learning from examples to improve code completion systems"},{"paperId":"eb537c6403932af7d1c4d819d2f0179df23baaee","title":"How Program History Can Improve Code Completion"},{"paperId":"97efafdb4a3942ab3efba53ded7413199f79c054","title":"Reinforcement Learning: An Introduction"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"4c915c1eecb217c123a36dc6d3ce52d12c742614","title":"Simple statistical gradient-following algorithms for connectionist reinforcement learning"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"a20f0ce0616def7cc9a87446c228906cd5da093b","title":"Policy Gradient Methods for Reinforcement Learning with Function Approximation"},{"paperId":"ac4af1df88e178386d782705acc159eaa0c3904a","title":"Actor-Critic Algorithms"},{"paperId":"69d7086300e7f5322c06f2f242a565b3a182efb5","title":"In Advances in Neural Information Processing Systems"},{"paperId":"22069cd4504656d3bb85748a4d43be7a4d7d5545","title":"Temporal credit assignment in reinforcement learning"},{"paperId":"dd7abc005846b18c9a78ab80467cbbaedb643456","title":"A Methodology for LISP Program Construction from Examples"},{"paperId":"94d9cbfc474cb6ce961de45a06233b2853ca6724","title":"Toward automatic program synthesis"},{"paperId":"577e96521d62b9ebb5fd67412a21b02e9cd67b90","title":"PROW: A Step Toward Automatic Program Writing"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"}],"id":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","summary":"This work proposes “CodeRL”, a new framework for program synthesis tasks through pretrained LMs and deep reinforcement learning (RL), and treats the code-generating LM as an actor network, and introduces a critic network that is trained to predict the functional correctness of generated programs and provide dense feedback signals to the actor."},{"url":"https://www.semanticscholar.org/paper/63c670ba8018da0a7e33c34ffd84c2a3ca54b894","title":"Language Models Can Teach Themselves to Program Better","venue":"ArXiv","year":2022,"referenceCount":47,"citationCount":5,"influentialCitationCount":1,"publicationDate":"29/07/2022","authors":"Patrick M. Haluptzok,Matthew Bowers,A. Kalai","citations":[{"paperId":"46299fee72ca833337b3882ae1d8316f44b32b3c","title":"Reflexion: an autonomous agent with dynamic memory and self-reflection"},{"paperId":"49fede098a0d2a48e8100b30189224fc6f5eb25b","title":"Language Model Crossover: Variation through Few-Shot Prompting"},{"paperId":"681cee58cf7e54199191cf9e0baf6851d8356704","title":"Complex QA and language models hybrid architectures, Survey"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"66eae7128c34dd7967d79224eb9dbc978773c3d0","title":"I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation"}],"references":[{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"0d08ffccc982781e310bb184397bbe64b9aef157","title":"Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"8342b592fe238f3d230e4959b06fd10153c45db1","title":"Training Compute-Optimal Large Language Models"},{"paperId":"23c265ba884b92ecbd9d18641078d964697e4590","title":"Generating Training Data with Language Models: Towards Zero-Shot Language Understanding"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"521ccc898395a2818fced22b4cf371b0e5121f94","title":"Symbolic Knowledge Distillation: from General Language Models to Commonsense Models"},{"paperId":"1aed58bd07026492194672adec494dc37c894a28","title":"Leveraging Automated Unit Tests for Unsupervised Code Translation"},{"paperId":"be7cb8f79bc018e57467168fc0c7f8ad59bba04f","title":"Adaptive Testing and Debugging of NLP Models"},{"paperId":null,"title":"2022] use a human-in-the-loop approach to NLP"},{"paperId":null,"title":"2022] has enabled synthesis in general-purpose programming languages"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"fd963ea665e7a6e49753652dc7efe5affba5feb0","title":"DreamCoder: bootstrapping inductive program synthesis with wake-sleep library learning"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"b769b629c8de35b16735214251d6b4e99cb55762","title":"Generating Datasets with Pretrained Language Models"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"58c74cec28f3416b9a1d308bb2d6519d21d53ab0","title":"LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"1728cb805a9573b59330890ba9723e73d6c3c974","title":"Knowledge Distillation: A Survey"},{"paperId":"8990154f515dadf6d1bfda745e62a67dc3b0e709","title":"A large-scale benchmark for few-shot program induction and synthesis"},{"paperId":null,"title":"2021]. However, these works do not consider the AI system"},{"paperId":null,"title":"Recent work in problem solving Cobbe et al"},{"paperId":null,"title":"Temperature controls the amount of diversity in the code solutions generated by Neo. All experiments in our paper were done with a fixed temperature of 0.8, based on the recommendation"},{"paperId":"5fe0a4af3bd1479d5e39fbda2215c86bce54722b","title":"Generative Language Modeling for Automated Theorem Proving"},{"paperId":"f3313f5c49b756243191c3f39c5f93692c6e4a69","title":"Learning to Prove from Synthetic Theorems"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":null,"title":"GitHub - lmcinnes/umap: Uniform Manifold Approximation and Projection (UMAP)"},{"paperId":null,"title":"2020] show potential value in learning to prove theorems or solve problems"},{"paperId":"a15763582df784b43548c6d53edfd55568c35168","title":"Synthetic Datasets for Neural Program Synthesis"},{"paperId":"f669b5406320a83e94b4dd439806211c8b1e95fc","title":"Write, Execute, Assess: Program Synthesis with a REPL"},{"paperId":"f9717d29840f4d8f1cc19d1b1e80c5d12ec40608","title":"A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play"},{"paperId":"d6b7abd5c17f9e7d83c5b709c27da3a82a9788f2","title":"NAPS: Natural Program Synthesis Dataset"},{"paperId":"3a288c63576fc385910cb5bc44eaea75b442e62e","title":"UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction"},{"paperId":"45dfef0cc1ed96558c1c650432ce39d6a1050b6a","title":"Fixing Weight Decay Regularization in Adam"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":null,"title":"Like English descriptions, PBE is inherently ambiguous"},{"paperId":"0c908739fbff75f03469d13d4a1a07de3414ee19","title":"Distilling the Knowledge in a Neural Network"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"43dcda631f8a0d39949ba3f9e3e22101db4daba0","title":"A Machine Learning Framework for Programming by Example"},{"paperId":"168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74","title":"Scikit-learn: Machine Learning in Python"},{"paperId":"1c46943103bd7b7a2c7be86859995a4144d1938b","title":"Visualizing Data using t-SNE"},{"paperId":"2722b9e5ab8da95f03e578bb65879c452c105385","title":"Catastrophic forgetting in connectionist networks"},{"paperId":"2fb9729c0a3b046c390ad6748773527eae6cfc84","title":"What Is Program Synthesis?"},{"paperId":null,"title":"(set(days)) <= k and (n -len(set(days))) * n >= n * (1 + (n -1) // k) and numx <= n // 2 and numx != nums"}],"id":"63c670ba8018da0a7e33c34ffd84c2a3ca54b894","summary":"This work shows how generating synthetic programming puzzles and solutions, veriﬁed for correctness by a Python interpreter, can be used to improve performance in solving test puzzles from P3, a public benchmark set of Python Programming Puzzles."},{"url":"https://www.semanticscholar.org/paper/b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs","venue":"ArXiv","year":2022,"referenceCount":33,"citationCount":5,"influentialCitationCount":1,"publicationDate":2022,"authors":"Shuyan Zhou,Uri Alon,Frank F. Xu,Zhengbao Jiang,Graham Neubig","citations":[{"paperId":"349ec9ca2ff17dbc1aa1caf27f98b7cd85a4747e","title":"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation"},{"paperId":"aae61ba5b629eba965b7f49a685b3d9f1bfb358c","title":"Model-Agnostic Syntactical Information for Pre-Trained Programming Language Models"},{"paperId":"433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"}],"references":[{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"1f11601f9eb8c0bee1d1d2cc7f4fa187e8c5e525","title":"The impact of lexical and grammatical processing on generating code from natural language"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"a0e92f6e9564b8c38b6649ae71b892ddfb988faa","title":"Controllable Semantic Parsing via Retrieval Augmentation"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"c56aced0f0c5cfebefadb530cb08d736c3ac5c05","title":"Retrieval Augmented Code Generation and Summarization"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"c26759e6c701201af2f62f7ee4eb68742b5bf085","title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"bde0c85ed3d61de2a8874ddad70497b3d68bc8ad","title":"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"},{"paperId":"45793225c6593db60e9efd95bce1d70bf4844198","title":"Evaluation Paradigms in Question Answering"},{"paperId":"58ed1fbaabe027345f7bb3a6312d41c5aac63e22","title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"paperId":"77910e51a40d17157fc798325d06edfa6cff18d6","title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation"},{"paperId":"79cd9f77e5258f62c0e15d11534aea6393ef73fe","title":"Dense Passage Retrieval for Open-Domain Question Answering"},{"paperId":"34733eaf66007516347a40ad5d9bbe1cc9dacb6b","title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"832fff14d2ed50eb7969c4c4b976c35776548f56","title":"REALM: Retrieval-Augmented Language Model Pre-Training"},{"paperId":"3c9fa66674e1053eb9de12076c48f13e46422c29","title":"CLAI: A Platform for AI Skills on the Command Line"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2b88aefb70cd52a4d6899020f4be97c669a5edcb","title":"RTFM: Generalising to Novel Environment Dynamics via Reading"},{"paperId":"d3e13d2514edaf74b863bfbe45a739c32a7689e1","title":"Retrieval-Based Neural Code Generation"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"429b65937d4922578a81e1f0ef5aeab7361ae36b","title":"NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"2c1e874c3b67510a3215e535f5646b362de5bc89","title":"Abstract Syntax Networks for Code Generation and Semantic Parsing"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"b5defb8729294614d40e0247eabfdc483bf1a992","title":"Bimodal Modelling of Source Code and Natural Language"},{"paperId":"99f1921a8ba5ceecb18e5ca7ff19b7f95c4e250d","title":"Learning to Win by Reading Manuals in a Monte-Carlo Framework"},{"paperId":"f6e3e57567e9803718623ec088cd7fea65cfbc9d","title":"Relevance weighting of search terms"}],"id":"b31b21d0750e849badfe76000e8170482f32b9be","summary":"DocCoder is introduced : an approach that explicitly leverages code manuals and documentation by retrieving the relevant documentation given the natural language intent, and generating the code based on the NL intent and the retrieved documentation."},{"url":"https://www.semanticscholar.org/paper/453a8fac3be9282be53908f0735160d0d21e0f48","title":"Repair Is Nearly Generation: Multilingual Program Repair with LLMs","venue":"ArXiv","year":2022,"referenceCount":48,"citationCount":12,"influentialCitationCount":0,"publicationDate":"24/08/2022","authors":"Harshit Joshi,J. Cambronero,Sumit Gulwani,Vu Le,Ivan Radicek,Gust Verbruggen","citations":[{"paperId":"fbb5cc9e8a46f61d1543bd17b6eca324fd5bf55c","title":"Fully Autonomous Programming with Large Language Models"},{"paperId":"1c00f88b69f1d4efe3e1695ca49aa38a1340bd98","title":"Improving Few-Shot Prompts with Relevant Static Analysis Products"},{"paperId":"3e7055980da1853d196fcee0d3b7f390fb518d62","title":"A Survey on Automated Program Repair Techniques"},{"paperId":"052a5e2bcc999810ee6f1eedcf758c528e4f125f","title":"Retrieving Multimodal Information for Augmented Generation: A Survey"},{"paperId":"34d24b2d9f116f8f652c112d4ac924afcf11bd0d","title":"InferFix: End-to-End Program Repair with LLMs"},{"paperId":"63396fceb84286b02796dc58e55c07ec1095c4dc","title":"FLAME: A small language model for spreadsheet formulas"},{"paperId":"0885556b71b24a641b4ffe139afd4d2712228cff","title":"Beware of the Unexpected: Bimodal Taint Analysis"},{"paperId":"c72ac81c4ac414314b52cf8f5b77370f8d0875d4","title":"Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models"},{"paperId":"fba0b0817dbc8200b41a1de22654b54b778a11e9","title":"Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models"},{"paperId":"e9a64e58855dbbc725203d0202ceb9e7f8b7bb36","title":"A Survey of Learning-based Automated Program Repair"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"2a0456b0408cd4c33f2ff4400374e7be2497a362","title":"Repairing Bugs in Python Assignments Using Large Language Models"}],"references":[{"paperId":"1e3ec5709b2ca43233c566cd77dbabbd6892bfa6","title":"Neurosymbolic repair for low-code formula languages"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code"},{"paperId":"23e8ed7568454e11d9a6fecb8242e1d16b1828d5","title":"Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"166b64f2ae8e52f5779682fab756cbd617a6e74b","title":"A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level"},{"paperId":"3f791ea6584b78fdf0ed80560d09d9496cf5a353","title":"Learning to Complete Code with Sketches"},{"paperId":null,"title":"ANNOY library"},{"paperId":null,"title":"StackOverflow Website"},{"paperId":"1444536496d8064f33e10b38b5820fecfab5b367","title":"Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs"},{"paperId":"4f68e07c6c3173480053fd52391851d6f80d651b","title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"a989f5daa7c438d6b8433efbb1461f1b0af1aa8e","title":"Break-It-Fix-It: Unsupervised Learning for Program Repair"},{"paperId":"5d47236f0e81e03484752908d476cd988af6c5b1","title":"SYNFIX: Automatically Fixing Syntax Errors using Compiler Diagnostics"},{"paperId":"1a26fac55bbcc6d40070742e7dc8b9752281f435","title":"A critical review on the evaluation of automated program repair systems"},{"paperId":"2cbb8de53759e75411bc528518947a3094fbce3a","title":"Billion-Scale Similarity Search with GPUs"},{"paperId":"0505f17c4052366cbc4fad99150d3542edf85faa","title":"TFix: Learning to Fix Coding Errors with a Text-to-Text Transformer"},{"paperId":null,"title":"Software developers, Quality Assurance Analysts, and testers : Occupational outlook handbook"},{"paperId":"5eba2afc862d642c9ebec3946875130fd43823ab","title":"The Adoption of JavaScript Linters in Practice: A Case Study on ESLint"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0071ee55f1749e6be586645bab08f0385ee856b4","title":"Graph-based, Self-Supervised Program Repair from Diagnostic Feedback"},{"paperId":"e662ce9f731ceb0ba4d19b3baf004bc4878fe210","title":"Calibration of probability predictions from machine‐learning and statistical models"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"246063d33bf9fd2d95a2cd26096c7ce6ecde0ab7","title":"Don't Panic! Better, Fewer, Syntax Errors for LR Parsers"},{"paperId":"9b1d319e06e3d224ebdb59c23a32a7883027cbd6","title":"SampleFix: Learning to Correct Programs by Efficient Sampling of Diverse Fixes"},{"paperId":"b5abecf78c1bd975449e34350289a904e6ef35cc","title":"Automated program repair"},{"paperId":"3b656bba99dbcf6c6d8fd764d53ea64cd38c7050","title":"Measuring Calibration in Deep Learning"},{"paperId":"c99179ca3784e3465fd9ed049d7f34b50d39393e","title":"Ensemble learning: A survey"},{"paperId":"e1531c72052c009822f13590964715e1ef027e14","title":"Automatic Software Repair: A Survey"},{"paperId":"b599ce0f548a73c156e94c7bcca8aed5b0a9678b","title":"Compilation Error Repair: For the Student Programs, From the Student Programs"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"a948e30cb47360d23d22f73008880c9c27d034c6","title":"HappyFace: Identifying and predicting frustrating obstacles for learning programming at scale"},{"paperId":"d6d0080e4ef0897e5d17b33f55f44f2b7af40454","title":"Automatic Grading and Feedback using Program Repair for Introductory Programming Courses"},{"paperId":"554c9b3aa8c3b7504810270b18b942a0d22156e7","title":"DeepFix: Fixing Common C Language Errors by Deep Learning"},{"paperId":"5b0602f9d0f1384dc95335c6ed220fef40a7e186","title":"sk_p: a neural program corrector for MOOCs"},{"paperId":"7f6245d527e84092111a6adb0f1939d0209d3360","title":"FlashMeta: a framework for inductive program synthesis"},{"paperId":"2e4e8bdd65e66ac24a85589ee4a46ff0e26c155f","title":"An Empirical Study on Real Bug Fixes"},{"paperId":"010d163129deb9a5fdb611a565ed1fbb4e62a114","title":"37 Million Compilations: Investigating Novice Programming Mistakes in Large-Scale Student Data"},{"paperId":"90441b975380c806320420724fc9d0ec77dbefdb","title":"The strength of random search on automated program repair"},{"paperId":"05607111cf79330d56164a10d351dbf94e2cfa44","title":"SemFix: Program repair via semantic analysis"},{"paperId":"3284cce500a3138fac2a0b9802325589a6a75984","title":"Calibration of Machine Learning Models"},{"paperId":"f69412d8c00780b66ec14b09c5045a5c2ec8250e","title":"Using Mutation to Automatically Suggest Fixes for Faulty Programs"},{"paperId":"ab2648283e6bf167d6b6ff843fe80ed4b428ceed","title":"On the automation of fixing software bugs"},{"paperId":"46ff473758bc14f31bfecda49f7339b91cfb5228","title":"Debugging: the good, the bad, and the quirky -- a qualitative analysis of novices' strategies"},{"paperId":"6d12a1d23b21a9b170118a56386552bc5d4727de","title":"A Mathematical Theory of Communication"},{"paperId":"a49ae26b7cc9017e9578e32d73586164662f23b8","title":"Maxims for malfeasant designers, or how to design languages to make programming as difficult as possible"},{"paperId":"b2f8876482c97e804bb50a5e2433881ae31d0cdd","title":"Binary codes capable of correcting deletions, insertions, and reversals"}],"id":"453a8fac3be9282be53908f0735160d0d21e0f48","summary":"This work introduces RING, a multilingual repair engine powered by a large language model trained on code (LLMC) such as Codex that enables a ﬂipped model for programming assistance, one where the programmer writes code and the AI assistance suggests code, compared to traditional code suggestion technology."},{"url":"https://www.semanticscholar.org/paper/41f5e1ad7793593befc0b9c38f756836e8b07c98","title":"Code4ML: a Large-scale Dataset of annotated Machine Learning Code","venue":"ArXiv","year":2022,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/10/2022","authors":"Anastasia Drozdova,P. Guseva,E. Trofimova,Anna Scherbakova,Andrey Ustyuzhanin","citations":[],"references":[{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"bea6af010fc02f5ac29edfc17096be6078edab46","title":"A Survey on Deep Learning for Software Engineering"},{"paperId":null,"title":"Code4ML: a Large-scale Dataset of annotated Machine Learning Code (1.0.1) [Data set]"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"24dbf2b111ad3e5f9984f127c694e9298a92a7f8","title":"KGTorrent: A Dataset of Python Jupyter Notebooks from Kaggle"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"3a7fa673ff8ec4ec2f322473de005f3cd09ea820","title":"AutoML: A Survey of the State-of-the-Art"},{"paperId":null,"title":"Artificial Intelligence (AI) -Assessment of the robustness of neural networks -Part 1: Overview. Standard, International Organization for Standardization"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"d08483307335b3571cb29c3b1903741df19148a9","title":"A systematic literature review of machine learning techniques for software maintainability prediction"},{"paperId":"20ba55ee3229db5cb190a00e788c59f08d2a767d","title":"Self-Training With Noisy Student Improves ImageNet Classification"},{"paperId":"40df572b0fbeae0f3db9b364be838c6467d189f2","title":"A Self-Attentional Neural Architecture for Code Completion with Multi-Task Learning"},{"paperId":"f2e5702330e52b730378ca1e7f1a041fc4f81c07","title":"Vulnerability Prediction From Source Code Using Machine Learning"},{"paperId":null,"title":"Six levels of auto ml"},{"paperId":"b5abecf78c1bd975449e34350289a904e6ef35cc","title":"Automated program repair"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"4cdf2fad22afc865999747336c7399fe422e6e8e","title":"Optuna: A Next-generation Hyperparameter Optimization Framework"},{"paperId":"df3f507f3d46dece98a527999676b978af4ae987","title":"Boa Meets Python: A Boa Dataset of Data Science Software in Python Language"},{"paperId":"d02285385fd2b53a6100b8d62421bba6693b788c","title":"When Code Completion Fails: A Case Study on Real-World Completions"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"273dfbcb68080251f5e9ff38b4413d7bd84b10a1","title":"LIBSVM: A library for support vector machines"},{"paperId":"9bca4d7b932e0854c3325f1578cfd17341dd8ea8","title":"A Kernel Method for the Two-Sample-Problem"},{"paperId":"6dd38e533bf912364f8d5af73f66c67263933dd5","title":"How are Java software developers using the Elipse IDE?"},{"paperId":"0fe16f0424e32a849b48ca87e37e5bb2817aec6d","title":"Why Inverse Document Frequency?"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"},{"paperId":null,"title":"The crisp-dm model: the new blueprint for data mining"},{"paperId":"2599131a4bc2fa957338732a37c744cfe3e17b24","title":"A training algorithm for optimal margin classifiers"}],"id":"41f5e1ad7793593befc0b9c38f756836e8b07c98","summary":"The Code4ML corpus, which contains code snippets, task summaries, competitions and dataset descriptions publicly available from Kaggle, can potentially help address a number of software engineering or data science challenges through a data-driven approach."},{"url":"https://www.semanticscholar.org/paper/4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":3,"influentialCitationCount":0,"publicationDate":"31/10/2022","authors":"Daoguang Zan,Bei Chen,Zeqi Lin,Bei Guan,Yongji Wang,Jian-Guang Lou","citations":[{"paperId":"349ec9ca2ff17dbc1aa1caf27f98b7cd85a4747e","title":"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"}],"references":[{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation"},{"paperId":"34503c0b6a615124eaf82cb0e4a1dab2866e8980","title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"},{"paperId":"2e12b5f42d3df4b3c42fff7f4ade444bb2c67811","title":"From Distillation to Hard Negative Sampling: Making Sparse Neural IR Models More Effective"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"2b556fcf2ac634f03c1fb0ace5e602e829418e65","title":"ReACC: A Retrieval-Augmented Code Completion Framework"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis"},{"paperId":"590432f953b6ce1b4b36bf66a2ac65eeee567515","title":"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction"},{"paperId":"737daa49c4234a8897b1f5b466c004db56241d83","title":"S2QL: Retrieval Augmented Zero-Shot Question Answering over Knowledge Graph"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"c56aced0f0c5cfebefadb530cb08d736c3ac5c05","title":"Retrieval Augmented Code Generation and Summarization"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"3416e5e5694855f7175125b5fe2e0b659c3cdbfa","title":"RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"bde0c85ed3d61de2a8874ddad70497b3d68bc8ad","title":"Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering"},{"paperId":"c9b8593db099869fe7254aa1fa53f3c9073b0176","title":"Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval"},{"paperId":"2cbb8de53759e75411bc528518947a3094fbce3a","title":"Billion-Scale Similarity Search with GPUs"},{"paperId":"375b9b36ef68678185f2b6e4dbbbe7bbfad6535a","title":"Large-Scale Relation Learning for Question Answering over Knowledge Bases with Pre-trained Language Models"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"79cd9f77e5258f62c0e15d11534aea6393ef73fe","title":"Dense Passage Retrieval for Open-Domain Question Answering"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"1fa9ed2bea208511ae698a967875e943049f16b6","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"072c23038332c34473e0a511099f064930c368d1","title":"API practices and paradigms: Exploring the protocological parameters of APIs as key facilitators of sociotechnical forms of exchange"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"e4a323a3a1ca5ff4cc2dd74ede65f48fd62b897b","title":"Deep API learning"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"}],"id":"4fbe0cb0777b228e39243692bf29e2829060b8de","summary":"This paper investigates how to equip pre-trained language models with the ability of code generation for private libraries, and proposes a novel framework with two modules: the APIRetriever and the APICoder, which generates code using these APIs."},{"url":"https://www.semanticscholar.org/paper/ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","title":"Evaluating How Fine-tuning on Bimodal Data Effects Code Generation","venue":"ArXiv","year":2022,"referenceCount":31,"citationCount":2,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Gabriel Orlanski,Seonhye Yang,Michael Healy","citations":[{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"}],"references":[{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"3f791ea6584b78fdf0ed80560d09d9496cf5a353","title":"Learning to Complete Code with Sketches"},{"paperId":null,"title":"Natural Language Processing with Transformers: Building Language Applications with Hugging Face"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"cc9ad384ec0d0176ce05865d3866b44d2519bd68","title":"Reading StackOverflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"3a1efac4a8aa2f40a8128e034c01f18660652dd5","title":"DOBF: A Deobfuscation Pre-Training Objective for Programming Languages"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"6faf62ac9e69e66cb92c923b749fa071554d23ca","title":"Learning Programmatic Idioms for Scalable Semantic Parsing"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"649c1148439a4e875dab414ba67bf8c80350af4a","title":"TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"558ac446dc26bee9789d660a251b75728cb6eeb2","title":"Language to Logical Form with Neural Attention"},{"paperId":"b7c0e47f8b768258b7d536c21b218e6c46ab8791","title":"Learning to Parse Database Queries Using Inductive Logic Programming"}],"id":"ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","summary":"It is found that at higher temperatures, there are decreases to the model’s ability to generate runnable programs despite higher pass @ k scores, underscoring the need for better methods of incorporating such data that mitigate these side effects."},{"url":"https://www.semanticscholar.org/paper/20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":2,"influentialCitationCount":0,"publicationDate":"31/10/2022","authors":"Spyridon Mouselinos,Mateusz Malinowski,H. Michalewski","citations":[{"paperId":"7677e9ead35f8f2578acbf5ad15fb330c83762c4","title":"Systematically Finding Security Vulnerabilities in Black-Box Code Generation Models"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"}],"references":[{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"5e3a7235c730b512f0bb3004a6deb0c88ec9fc8e","title":"On the Paradox of Learning to Reason from Data"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"6847e2ba6796b8a786b3b6d8d8a2d922a6c7c31d","title":"Measuring CLEVRness: Blackbox testing of Visual Reasoning Models"},{"paperId":"af46b5ee6d0c1aada1c482d53018a50909aa4c90","title":"Impact of Pretraining Term Frequencies on Few-Shot Reasoning"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"59641c10ed7431a3cf841f308367dc2dc0281b74","title":"What Makes Good In-Context Examples for GPT-3?"},{"paperId":null,"title":"Natural language processing with transformers"},{"paperId":"9cac09098aa611bd9a94d080d2401840632ab16f","title":"LM-Critic: Language Models for Unsupervised Grammatical Error Correction"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"114aa720872462b0ca1b97bfdec0ebd56c36fd0a","title":"Towards Understanding and Mitigating Social Biases in Language Models"},{"paperId":"d65a064eb837f838faf6ff67781b62450b92b159","title":"CommonsenseQA 2.0: Exposing the Limits of AI through Gamification"},{"paperId":"57a1258571a21817d89197dc84c986861fb6e580","title":"Measuring and Improving BERT’s Mathematical Abilities by Predicting the Order of Reasoning."},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset"},{"paperId":"56fa0b9cba4d9aee5ccc327365b3b3a721031c69","title":"Calibrate Before Use: Improving Few-Shot Performance of Language Models"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":null,"title":"Gpt-j-6b: A 6 billion parameter autoregressive language model"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"016760dc4a05489ddf5dbb48aecbb49e214e1b71","title":"Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models"},{"paperId":"bf9a8fb50aca26774ebf4815db2d8712e2c5830c","title":"TextAttack: A Framework for Adversarial Attacks in Natural Language Processing"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"463fefdbd81a4a0a32cf59bc58a9545757c8cf2e","title":"Pre-trained Contextual Embedding of Source Code"},{"paperId":null,"title":"Keybert: Minimal keyword extraction with bert"},{"paperId":null,"title":"Codeforces: Results of 2020"},{"paperId":"18a1c21f35153c45d0ef30c564bffb7d70a13ccc","title":"Universal Adversarial Triggers for Attacking and Analyzing NLP"},{"paperId":"735ce0447e459e13f89ae751b19323d76a2af786","title":"Program Synthesis and Semantic Parsing with Learned Code Idioms"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"eef7cfe8267954adbb4675576072a1d80ca7a3a8","title":"MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms"},{"paperId":"afed6dc6900d3b37e528b9086661bba583d60bf6","title":"Analysing Mathematical Reasoning Abilities of Neural Models"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"413a03a146e6f7b16c11e73243d83e6f1a6627a3","title":"Breaking NLI Systems with Sentences that Require Simple Lexical Inferences"},{"paperId":"b123a0d46ad917b79c43c5ae981e03ed2458ed11","title":"Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems"},{"paperId":null,"title":"Description2code dataset"},{"paperId":"f48eed915cbb9c6592cdb9df80c1edaeb46959af","title":"A measure of intelligence"},{"paperId":"7e0006dc1972e276805d1bd6c5dc2813c7a6d824","title":"Automatic Keyword Extraction from Individual Documents"},{"paperId":"0d175746355f293187ed491665563018ee690cfa","title":"Machine super intelligence"},{"paperId":null,"title":"The world's largest open multilingual language model"}],"id":"20fae749e3d469c331731ffa2f811079db792fdc","summary":"This work shows that current code generation systems exhibit biases inherited from large language model backbones, which might leak into generated code under specific circumstances, and proposes a framework that automatically removes hints and exposes various biases that these code generation models use."},{"url":"https://www.semanticscholar.org/paper/e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":4,"influentialCitationCount":2,"publicationDate":"17/11/2022","authors":"Junjie Huang,Chenglong Wang,Jipeng Zhang,Cong Yan,Haotian Cui,J. Inala,Colin B. Clement,Nan Duan,Jianfeng Gao","citations":[{"paperId":"372dd962ff8c92ac5cf9a195fd52b4b79372c149","title":"A Syntax-Guided Multi-Task Learning Approach for Turducken-Style Code Generation"},{"paperId":"8475c1d1d98932111b194dc139c8f8b30818725d","title":"xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"}],"references":[{"paperId":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":null,"title":"Natural Language Processing with Transformers: Building Language Applications with Hugging Face"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"21d6bb1a79a69c207a5d1187ebfce5150b58e441","title":"Graph-Augmented Code Summarization in Computational Notebooks"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"6e2b1038682cd116b2e38bec19b5721196c41eea","title":"HAConvGNN: Hierarchical Attention Based Convolutional Graph Neural Network for Code Documentation Generation in Jupyter Notebooks"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"010c9f63c51ccc504de36d2d1693e0ea9d525da1","title":"Deep Learning for Source Code Modeling and Generation"},{"paperId":"ed2a639dd4a6b3abdcc0040733a264cb1dbcd7d4","title":"PlotCoder: Hierarchical Decoding for Synthesizing Visualization Code in Programmatic Context"},{"paperId":null,"title":"2021b. PlotCoder: Hierarchical decoding"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"ed3273b49e9b2fa17ea35f4721689927caa4263b","title":"Auto-Suggest: Learning-to-Recommend Data Preparation Steps Using Data Science Notebooks"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"c7462e0ee928f095a7fc40b91f1e7557d283ae8e","title":"Release Strategies and the Social Impacts of Language Models"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"d6b7abd5c17f9e7d83c5b709c27da3a82a9788f2","title":"NAPS: Natural Program Synthesis Dataset"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"d4c85ef35c5224792186100c29df141761d6abd8","title":"Neural Program Search: Solving Data Processing Tasks from Description and Examples"},{"paperId":"429b65937d4922578a81e1f0ef5aeab7361ae36b","title":"NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System"},{"paperId":"116a877c774969b53399b1ccaa34d51bfb492ee4","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"f56425ec56586dcfd2694ab83643e9e76f314e91","title":"50 Years of Data Science"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"e47868841d87efe261451a43b00d6c81cf7fb7a3","title":"Jupyter Notebooks - a publishing format for reproducible computational workflows"},{"paperId":"27e1dbe9f7c71cd6cc1b0357f49aef497e572d09","title":"Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"},{"paperId":"25369f56a933e3bfb1d8e1588cdc6c50df93ecae","title":"Building a Semantic Parser Overnight"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"e402dd77eba504ea93bc38e2a052398bb95db351","summary":"ExeDS is introduced, an evaluation dataset for execution evaluation for data science code generation tasks that contains a set of 534 problems from Jupyter Notebooks, each consisting of code context, task description, reference program, and the desired execution output."},{"url":"https://www.semanticscholar.org/paper/20b60fb3993d2e9a5af04611f7bdf248e5a3a736","title":"Programming by Example and Text-to-Code Translation for Conversational Code Generation","venue":"ArXiv","year":2022,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Eli Whitehouse,William Gerard,Yauhen Klimovich,Marc Franco-Salvador","citations":[],"references":[{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"808a1560c5c50e559d8ad1e03a00b91697dfbdad","title":"Leveraging Language to Learn Program Abstractions and Search Heuristics"},{"paperId":"fd963ea665e7a6e49753652dc7efe5affba5feb0","title":"DreamCoder: bootstrapping inductive program synthesis with wake-sleep library learning"},{"paperId":"98485ce6532d69f34a8ec67de6b09a39532bd221","title":"Communicating Natural Programs to Humans and Machines"},{"paperId":"0b077c9577f4297dcf3da835e253d21965bbc6e0","title":"CoTexT: Multi-task Learning with Code-Text Transformer"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"a9453111cb31e69a4ddc43a45cb20985699ac063","title":"Conversational Semantic Parsing for Dialog State Tracking"},{"paperId":"f430773cdc0fdd62192a48a372601d6aa40f3d7b","title":"Task-Oriented Dialogue as Dataflow Synthesis"},{"paperId":"2512228ada8b379f9571eba648cec950cd08ec48","title":"Turning 30: New Ideas in Inductive Logic Programming"},{"paperId":"775113b55052994ddadea3cf9e316309a32c99e5","title":"Span-based Hierarchical Semantic Parsing for Task-Oriented Dialog"},{"paperId":"735ce0447e459e13f89ae751b19323d76a2af786","title":"Program Synthesis and Semantic Parsing with Learned Code Idioms"},{"paperId":"6faf62ac9e69e66cb92c923b749fa071554d23ca","title":"Learning Programmatic Idioms for Scalable Semantic Parsing"},{"paperId":"57a30c0a013bc36b4b5181b33c308c00d98b7a9d","title":"Learning Libraries of Subroutines for Neurally-Guided Bayesian Program Induction"},{"paperId":"7f6245d527e84092111a6adb0f1939d0209d3360","title":"FlashMeta: a framework for inductive program synthesis"},{"paperId":"c51274cdd2a0305cab0bbaf0298cab63db1b9152","title":"Bootstrap Learning via Modular Concept Discovery"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"18b8ef71bc01b8658b4ef2c8b9a9e4e6e5c2a07b","title":"Dimensions in program synthesis"},{"paperId":"543c15d744dd7390858f4d3ad2574ff45feebd8e","title":"Knowledge in Action: Logical Foundations for Specifying and Implementing Dynamical Systems"}],"id":"20b60fb3993d2e9a5af04611f7bdf248e5a3a736","summary":"Modular Programs for Text-guided Hierarchical Synthesis (MPaTHS), a method for integrating Programming by Example and text-to-code systems which uses an accessible natural language interface for synthesizing general programs, is proposed."},{"url":"https://www.semanticscholar.org/paper/f3a6115e5fb2237df938976e005468f0b18da797","title":"The Stack: 3 TB of permissively licensed source code","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":5,"influentialCitationCount":0,"publicationDate":"20/11/2022","authors":"Denis Kocetkov,Raymond Li,Loubna Ben Allal,Jia Li,Chenghao Mou,Carlos Muñoz Ferrandis,Yacine Jernite,Margaret Mitchell,Sean Hughes,Thomas Wolf,Dzmitry Bahdanau,Leandro von Werra,Harm de Vries","citations":[{"paperId":"98e6e0b3b811193e89b1a033da6c0a454220877a","title":"Foundation Models and Fair Use"},{"paperId":"27c2ddc921befc44f7b2cdb8bb39facdd8b0b188","title":"Measuring The Impact Of Programming Language Distribution"},{"paperId":"1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a","title":"SantaCoder: don't reach for the stars!"},{"paperId":"bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence"},{"paperId":"568eb10d17f1643228303670fe0f1d6608bd6f4d","title":"Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models"}],"references":[{"paperId":"d32c9701e535a7602ac8c446b8f28d5a832a503b","title":"The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset"},{"paperId":"ec4c8d99eb1c028c43af6d8bbf727392d351cb59","title":"Efficient Training of Language Models to Fill in the Middle"},{"paperId":"dd112d4dbd4656223770989778f39700de3052bc","title":"A Hazard Analysis Framework for Code Synthesis Large Language Models"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"aa4d9972af3264d032dbee58501ed4ac49477103","title":"Scaling Laws and Interpretability of Learning from Repeated Data"},{"paperId":"3ae3716f125d71e9daccac3dafa4fab7482fb16a","title":"Data Governance in the Age of Large-Scale Data-Driven Language Technology"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"55c36748f2a7c060c3313349c730b053ed03fbf7","title":"Deduplicating Training Data Mitigates Privacy Risks in Language Models"},{"paperId":"d407aa1cc3f9ce8478d3052344947fb49baa04d4","title":"CodeFill: Multi-token Code Completion by Jointly learning from Structure and Naming Sequences"},{"paperId":"4566c0d22ebf3c31180066ab23b6c445aeec78d5","title":"Deduplicating Training Data Makes Language Models Better"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":null,"title":"This CoPilot is stupid and wants to kill me"},{"paperId":null,"title":"Copyright Implications of the Use of Code Repositories to Train a Machine Learning Model"},{"paperId":null,"title":"If Software is My Copilot, Who Programmed My Software"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"4f68e07c6c3173480053fd52391851d6f80d651b","title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"3a1efac4a8aa2f40a8128e034c01f18660652dd5","title":"DOBF: A Deobfuscation Pre-Training Objective for Programming Languages"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"62d1a3137b01a69443bebf4d92c1990ec512a6a1","title":"Extracting Training Data from Large Language Models"},{"paperId":"0df347f5e3118fac7c351917e3a497899b071d1e","title":"Datasheets for datasets"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"8323c591e119eb09b28b29fd6c7bc76bd889df7a","title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"97bfa89addc6e5d76361e4c1e296949cad887b86","title":"Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science"},{"paperId":"a26b00c607b6d6a2697f5444f14a9afc37701444","title":"DéjàVu: a map of code duplicates on GitHub"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"cf6023368a683572a090203406a7e4285566e9db","title":"Identifying and Filtering Near-Duplicate Documents"},{"paperId":"1955266a8a58d94e41ad0efe20d707c92a069e95","title":"Approximate nearest neighbors: towards removing the curse of dimensionality"},{"paperId":null,"title":"• BSD-2-Clause-NetBSD • zlib-acknowledgement • OLDAP-2"}],"id":"f3a6115e5fb2237df938976e005468f0b18da797","summary":"This work introduces The Stack, a 3.1 TB dataset consisting of permissively licensed source code in 30 programming languages, and describes how to collect the full dataset, construct a permissically licensed subset, and present a data governance plan."},{"url":"https://www.semanticscholar.org/paper/27961ae80ad008bd4006704b1b8fa82664137d69","title":"Coder Reviewer Reranking for Code Generation","venue":"ArXiv","year":2022,"referenceCount":27,"citationCount":3,"influentialCitationCount":0,"publicationDate":"29/11/2022","authors":"Tianyi Zhang,Tao Yu,Tatsunori Hashimoto,M. Lewis,Wen-tau Yih,Daniel Fried,Sida I. Wang","citations":[{"paperId":"dd0960a8ffb6a8baff69ec96c3f67deb2912b53a","title":"Self-collaboration Code Generation via ChatGPT"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"6d269364de402d4a72ac30b0c8d81324f6849807","title":"LEVER: Learning to Verify Language-to-Code Generation with Execution"}],"references":[{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"07ec0d4cc6a2be39def51139d228292c6a0dc627","title":"Guess the Instruction! Flipped Learning Makes Language Models Stronger Zero-Shot Learners"},{"paperId":"4a4581003f56e8cb581ad6f383c037964765d3d5","title":"Active Programming by Example with a Natural Language Prior"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"eea7bca03bda3ee2448cd012bbcb2b33822861d8","title":"Noisy Channel Language Model Prompting for Few-Shot Text Classification"},{"paperId":null,"title":"Scaling language modeling with pathways"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0071ee55f1749e6be586645bab08f0385ee856b4","title":"Graph-based, Self-Supervised Program Repair from Diagnostic Feedback"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"d3231772937a2182b2377d028417245c49868dd1","title":"On NMT Search Errors and Model Errors: Cat Got Your Tongue?"},{"paperId":"3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c","title":"Reranking for Neural Semantic Parsing"},{"paperId":"6182aed90596acd1573bd5ccbc2284b1e8a7291b","title":"Generative Question Answering: Learning to Answer the Whole Question"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"ede8ba65c4db10d357d9c3bf8e75b092f536fc84","title":"Speaker-Follower Models for Vision-and-Language Navigation"},{"paperId":"429b65937d4922578a81e1f0ef5aeab7361ae36b","title":"NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System"},{"paperId":null,"title":"A syntactic neural model for generalpurpose code generation"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"651e5bcc14f14605a879303e97572a27ea8c7956","title":"A Diversity-Promoting Objective Function for Neural Conversation Models"},{"paperId":"cd7ee037c029b25e691650a26cec7538d255405c","title":"Suggesting accurate method and class names"},{"paperId":"b5f09ce0dd760857e0d0e4879f6e2543f04c5d33","title":"Maximum mutual information estimation of hidden Markov model parameters for speech recognition"}],"id":"27961ae80ad008bd4006704b1b8fa82664137d69","summary":"Experimental results show that Coder-Reviewer reranking leads to consistent and signiﬁcant improvement (up to 17 % absolute accuracy gain) over reranking with the Coder model only, and when combined with executability ﬁltering,Coder- reviewer reranking can often outperform the minimum Bayes risk method."},{"url":"https://www.semanticscholar.org/paper/9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming","venue":"ArXiv","year":2022,"referenceCount":79,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/12/2022","authors":"Qingfu Zhu,Xianzhen Luo,Fang Liu,Cuiyun Gao,Wanxiang Che","citations":[],"references":[{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"62851a515ea0ee3a547d94e8a493d978c22d0be9","title":"Multilingual Code Snippets Training for Program Translation"},{"paperId":"2417ab25a53e97410f44a20af69b82fff077fd53","title":"Deep Learning Meets Software Engineering: A Survey on Pre-Trained Models of Source Code"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"4b27f18bff43d605805c92696a979714ced0b805","title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"a1e1297fb132d7769dda3f7917e57757e6e22605","title":"Impact of Evaluation Methodologies on Code Summarization"},{"paperId":null,"title":"A conversational"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"51654d8317478fe1497678b15e36ac99d73a65b2","title":"CAST: Enhancing Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"05e396e79a2f88f0b8f8d99f5ab36ab3efa95c14","title":"Language-Agnostic Representation Learning of Source Code from Structure and Context"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"ceaff192479db6faee58ae88e053b0b319cf1893","title":"Retrieval-Augmented Generation for Code Summarization via Hybrid GNN"},{"paperId":"1cedb352a10828b3ec263a4da907794bea052282","title":"PLUR: A Unifying, Graph-Based View of Program Learning, Understanding, and Repair"},{"paperId":"5a129c5aa13334c7ff10c40d2d33165656a628f0","title":"Proceedings of the 1st Workshop on Natural Language Processing for Programming (NLP4Prog 2021)"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"7157d9e7a8dcd7e9900dcfd3d10d8a07d33468bc","title":"Synthesize, Execute and Debug: Learning to Repair for Neural Program Synthesis"},{"paperId":"89fdb0555f6643b18c0094bcce66eaea701bef85","title":"Programming in Natural Language with fuSE: Synthesizing Methods from Spoken Utterances Using Deep Natural Language Understanding"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"805a6d1df9f460abfcea3d51d181cf1e80680be4","title":"A Transformer-based Approach for Source Code Summarization"},{"paperId":"a266b37f98928f27fddd863d11b38a5563043315","title":"Learning to Update Natural Language Comments Based on Code Changes"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"495da6f19baa09c6db3697d839e10432cdc25934","title":"Multilingual Denoising Pre-training for Neural Machine Translation"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"b7af363ff612f41e0c1071ae24ce68a836aaa678","title":"Code Generation as a Dual Task of Code Summarization"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"00549af4bc3270e0f688acbf694f912d7ee39cad","title":"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks"},{"paperId":"08218ed6095e407eba3f02ce141994241d03a5b9","title":"Automatic Source Code Summarization with Extended Tree-LSTM"},{"paperId":"516497e7fc39ebb126bed28dcc5c6129e500acb8","title":"An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation"},{"paperId":"98d1307bed619b58b4a44acd8e65ac58495776c2","title":"code2seq: Generating Sequences from Structured Representations of Code"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"1a2fb75103ea9dc9ff31711ba5aa357088c026bc","title":"Automated Vulnerability Detection in Source Code Using Deep Representation Learning"},{"paperId":"15f16252af84a630f1f9442d4e0367b6fba089cf","title":"Deep Code Comment Generation"},{"paperId":"be82528df1512ea65bc5edcaeefc1ec33cd27c82","title":"Automatic Generation of Text Descriptive Comments for Code Blocks"},{"paperId":"dea6aeb514b1969ab879c793d46a0d2eceaa2cbf","title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis"},{"paperId":"dd908c09eaf8f3a3c20d64fff798ec902433737d","title":"Code Completion with Neural Attention and Pointer Networks"},{"paperId":"d0f2d7236e43f129744e88130fb71f8f872d2b31","title":"Learning to Represent Programs with Graphs"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"6cf6dc8bb7995a1f529d51b11f1677e045337337","title":"SmartPaste: Learning to Adapt Source Code"},{"paperId":"45416ffd8fa572c23c8dbc43cc7b8b5095fcbcc2","title":"A Neural Architecture for Generating Natural Language Descriptions from Source Code Changes"},{"paperId":"e235bc8ccbe85de40f406d1a1201d50aec893b2d","title":"A Simple, Fast Diverse Decoding Algorithm for Neural Generation"},{"paperId":"165f45f42a1418526c2fd5eeb0ebf6c147f4f507","title":"Latent Attention For If-Then Program Synthesis"},{"paperId":"32aa2517b03c871c11e521c2a3406f457833e2c3","title":"Summarizing Source Code using a Neural Attention Model"},{"paperId":"221ef0a2f185036c06f9fb089109ded5c888c4c6","title":"Sequence-to-Sequence RNNs for Text Summarization"},{"paperId":"49512270b39636375880d611d7b2192d324f4ba6","title":"Convolutional Neural Networks over Tree Structures for Programming Language Processing"},{"paperId":"7f6245d527e84092111a6adb0f1939d0209d3360","title":"FlashMeta: a framework for inductive program synthesis"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"9ef911fea38b43aedad131d5d08efa6a1833d747","title":"On the localness of software"},{"paperId":"9277dc7dd5b348645e8fcf4a987e5b3fc9132c7c","title":"Towards a Big Data Curated Benchmark of Inter-project Code Clones"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"07c4549be429a52274bc0ec083bf5598a3e5c365","title":"Modeling and Discovering Vulnerabilities with Code Property Graphs"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"37cb53b8cdb46ba1d7b933e3fc0e22dd7ee41bcb","title":"Software vulnerability prediction using text analysis techniques"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"9819b600a828a57e1cde047bbe710d3446b30da5","title":"Recurrent neural network based language model"},{"paperId":"0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7","title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"9bdac1f28f7d4c5f3b28c25ea5bbc6a3be0449b1","title":"Identifying similar code with program dependence graphs"},{"paperId":"1cc3f5cdd4204f8e55e46d9cbaef730d17ca647c","title":"Your Wish is My Command: Programming By Example"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"a8e8f3c8d4418c8d62e306538c9c1292635e9d27","title":"Backpropagation Applied to Handwritten Zip Code Recognition"},{"paperId":"ece80f049a527954af1c153d61cafee5789c2afe","title":"The program dependence graph and its use in optimization"},{"paperId":"ceb3163c56465fda5fef591d0ff0a6c7f434a04d","title":"A Deductive Approach to Program Synthesis"}],"id":"9b4055674cd9849f8595240695bed69cd02492bc","summary":"This paper comprehensively investigates existing work in natural language processing for programming, rang-ing from early deductive models to the latest competition-level models."},{"url":"https://www.semanticscholar.org/paper/ab8a53be2ff8a756776aa0245ad8da41189d60d6","title":"Fuzzing Deep-Learning Libraries via Large Language Models","venue":"ArXiv","year":2022,"referenceCount":79,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/12/2022","authors":"Yinlin Deng,Chun Xia,Haoran Peng,Chenyuan Yang,Lingming Zhang","citations":[{"paperId":"a0cb2edfacd9d43c73afef10e287f9255b4d1dc5","title":"FASER: Balancing Effectiveness and Flakiness of Non-Deterministic Machine Learning Tests"}],"references":[{"paperId":"383157f9059061faba4364c7d2141d0ff78c87fa","title":"Learning Deep Semantics for Test Completion"},{"paperId":"6876fc47f4674fdf4f53208254b45dedc6de3755","title":"Adaptive Test Generation Using a Large Language Model"},{"paperId":"61c4abaef7564161ca39f40d3c511d0036a85f6f","title":"Fuzzing Automatic Differentiation in Deep-Learning Libraries"},{"paperId":"4dc58ffbe6bbcc76cc4c436e9eb8f55dd661bcce","title":"NNSmith: Generating Diverse and Valid Test Cases for Deep Learning Compilers"},{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":null,"title":"CODAMOSA: Escaping Coverage Plateaus in Test Generation with Pretrained Large Language Models"},{"paperId":"f031ba42cf82f106200bb03fbb91dd5671a59b9c","title":"Practical Program Repair in the Era of Large Pre-trained Language Models"},{"paperId":"e37155d21818513bd40d64ee212099aac82bd6f8","title":"Less training, more repairing please: revisiting automated program repair via zero-shot learning"},{"paperId":"57de3be56d84cb0fc5795df93bc87c1c52721e56","title":"Fuzzing deep-learning libraries via automated relational API inference"},{"paperId":"d092e753c112f1e71cc9f51b94eddcee6568785f","title":"EAGLE: Creating Equivalent Graphs to Test Deep Learning Libraries"},{"paperId":"b2a2059105ded9dcd0be69f82d389c438a76e789","title":"Muffin: Testing Deep Learning Libraries via Neural Architecture Fuzzing"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"8a02e3747123a605174beac0d622dff386d2a8db","title":"Coverage-guided tensor compiler fuzzing with joint IR-pass mutation"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"8c8d4698873fc56e0358cfd5c346ca8c669aaf3a","title":"Free Lunch for Testing: Fuzzing Deep-Learning Libraries from Open Source"},{"paperId":"3c211b5500daba9769d82ffa70b50719ad45637d","title":"DocTer: documentation-guided fuzzing for testing deep learning API functions"},{"paperId":null,"title":"ChatGPT: Optimizing Language Models for Dialogue"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"3f650c0d0a046c1fb6c068d636b138e14ded77b9","title":"Generative type-aware mutation for testing SMT solvers"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"ffdbd7f0b03b85747b001b4734d5ee31b5229aa4","title":"The Power of Scale for Parameter-Efficient Prompt Tuning"},{"paperId":"dd63c76b40d937dc3a7af3de5ba42232b858bd6c","title":"Automated conformance testing for JavaScript engines via deep compiler fuzzing"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"ac3cdb50606f7770eef8e4cd951840a4f71287a0","title":"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"99ddf774164df29e733a99a56534b187f224e15c","title":"Fuzzing: Challenges and Reflections"},{"paperId":"30a156f17ca8f54aa14d01d32c2315c11fcbe723","title":"BUSTLE: Bottom-up program-Synthesis Through Learning-guided Exploration"},{"paperId":null,"title":"Writing Better Tests with AI and GitHub Copilot"},{"paperId":null,"title":"A Smarter App Is Watching Your Wallet"},{"paperId":null,"title":"Hospitals turn to artificial intelligence to help with an age-old problem: Doctors’ poor bedside manners"},{"paperId":"368f36c67c70d484901c3f303aa25312e864d875","title":"Deep learning library testing via effective model generation"},{"paperId":"e9e9a51b46b81feaa8475f116c17e93b03b24f02","title":"Audee: Automated Testing for Deep Learning Frameworks"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"f976a25fbbb86fc7c10008b1276940885cee41d0","title":"Montage: A Neural Network Language Model-Guided JavaScript Engine Fuzzer"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"ff4804de2b9db68a052d7113cae41ef2122a1c51","title":"AFL++ : Combining Incremental Steps of Fuzzing Research"},{"paperId":"b96c1b6446b35cf18ff4364b3104bcde1a7204a1","title":"Coverage guided, property based testing"},{"paperId":"87ba716837af374824ea825cdfc8ab0002dcdd03","title":"DeepFuzz: Automatic Generation of Syntax Valid C Programs for Fuzz Testing"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"c1f55baff1e8bce91de083d9741045342ddb0b29","title":"CRADLE: Cross-Backend Validation to Detect and Localize Bugs in Deep Learning Libraries"},{"paperId":"9898df29efc9016ebeb7098e56fd9f325aa89fc6","title":"SeqFuzzer: An Industrial Protocol Fuzzing Framework from a Deep Learning Perspective"},{"paperId":"a10a6495723ea8c928680ecdd61714f5750586c3","title":"Fine-tune BERT for Extractive Summarization"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"The fuzzing book"},{"paperId":"ab758c7d163dba039f1b1badaa9ea72064c887ba","title":"DeepRoad: GAN-Based Metamorphic Testing and Input Validation Framework for Autonomous Driving Systems"},{"paperId":"c897ad6ee09811644dd516038502501b102792d6","title":"FairFuzz: A Targeted Mutation Strategy for Increasing Greybox Fuzz Testing Coverage"},{"paperId":"76655792a9c3ebcbc64d2665ecf5bba81966e478","title":"Evaluating Fuzz Testing"},{"paperId":"0ab21244b20b42dd152f8ae367fba636d32d2049","title":"Compiler fuzzing through deep learning"},{"paperId":"358d45ed3157cbc9e741a292b94ce1173728c66d","title":"GANFuzz: a GAN-based industrial network protocol fuzzing framework"},{"paperId":"01f6488741f1527201a428c7b05df9649cb9a631","title":"A Tutorial on Thompson Sampling"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"5bd13e6313008e1555e530dda6d84c5004aa09ed","title":"Learn&Fuzz: Machine learning for input fuzzing"},{"paperId":null,"title":"American Fuzzy Lop -Whitepaper"},{"paperId":"12806c298e01083a79db77927530367d85939907","title":"An Empirical Evaluation of Deep Learning on Highway Driving"},{"paperId":null,"title":"libFuzzer a library for coverage-guided fuzz testing. https: //llvm.org/docs/LibFuzzer.html"},{"paperId":"54e325aee6b2d476bbbb88615ac15e251c6e8214","title":"Generative Adversarial Nets"},{"paperId":"79bbd54d5bdfd20980e5f9a65480f5e127fc1221","title":"Compiler validation via equivalence modulo inputs"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"39e6029da19a357449b48e2f6d9c8b35a802e7c4","title":"Whole Test Suite Generation"},{"paperId":"4275190822a329f06adb7f576d115f3618888edf","title":"Fuzzing with Code Fragments"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"ab867c140d2947511979c87e7ae580d9d3f0aeab","title":"An Empirical Evaluation of Thompson Sampling"},{"paperId":"216b98bb3d9221d5f5d261864975612e4d0faaa6","title":"EvoSuite: automatic test suite generation for object-oriented software"},{"paperId":"3b1bae14269d1e3bbb45f79bb471af3bd0bf4e1e","title":"Finding and understanding bugs in C compilers"},{"paperId":"ae74531e6e73afaffb264b4e536b8fde95d03202","title":"Program synthesis by sketching"},{"paperId":"4db2b87478c87c677135e802dbeccc8e5e384aed","title":"Fuzzing: Brute Force Vulnerability Discovery"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"15bd56e6081f4f8b6751744e3e2190fd278c1a3a","title":"The Gamma Function"},{"paperId":"94d9cbfc474cb6ce961de45a06233b2853ca6724","title":"Toward automatic program synthesis"},{"paperId":"ee2cd1d17f833d3c157a1016a778c7c22af555a2","title":"ON THE LIKELIHOOD THAT ONE UNKNOWN PROBABILITY EXCEEDS ANOTHER IN VIEW OF THE EVIDENCE OF TWO SAMPLES"},{"paperId":"c9bc5651c1560ea5111c6246c3916d3c1905d9eb","title":"Elementary Principles in Statistical Mechanics: Developed with Especial Reference to the Rational Foundation of Thermodynamics"},{"paperId":null,"title":"Keras 2020"},{"paperId":null,"title":"TensorFlow 2020"}],"id":"ab8a53be2ff8a756776aa0245ad8da41189d60d6","summary":"LLMFuzz is the first automated approach to directly leveraging Large Pre-trained Language Models (LLMs) to generate input programs for fuzzing DL libraries, and is able to detect 65 bugs, with 41 already confirmed as previously unknown bugs."},{"url":"https://www.semanticscholar.org/paper/690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation","venue":"ArXiv","year":2022,"referenceCount":30,"citationCount":4,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Zhiruo Wang,Shuyan Zhou,Daniel Fried,Graham Neubig","citations":[{"paperId":"cbdc75e9c80870164662bb0359e23fd4736c5f0e","title":"A Survey of Large Language Models"},{"paperId":"8475c1d1d98932111b194dc139c8f8b30818725d","title":"xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval"},{"paperId":"535a3d95a743e1f4b591b5b2af3e778a6347158a","title":"CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code"},{"paperId":"0a39442979d6e678dd36bb443ad529c14e86a86e","title":"DocPrompting: Generating Code by Retrieving the Docs"}],"references":[{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models"},{"paperId":"654cd297b307c0c6fa08732511ba852b8dce1977","title":"Out of the BLEU: how should we assess quality of the Code Generation models?"},{"paperId":"c5b47a34fd7e0e595360683b4effa51c0261d1e3","title":"Language Models Can Teach Themselves to Program Better"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"d6b7bc4e7968398101d8d9b4da7d4a0186763ff3","title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages"},{"paperId":"eadcf2f484b2d326f4d32ba4a897b009e4de1784","title":"Pynguin: Automated Unit Test Generation for Python"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"2222fe87201177339c89fbfe9ef3c9c8e67674a5","title":"Prefix-to-SQL: Text-to-SQL Generation from Incomplete User Questions"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"15a60130ea9cb4a00e0dac6d11a368894974cedf","title":"Unit Test Case Generation with Transformers and Focal Context"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"649c1148439a4e875dab414ba67bf8c80350af4a","title":"TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"116a877c774969b53399b1ccaa34d51bfb492ee4","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"558ac446dc26bee9789d660a251b75728cb6eeb2","title":"Language to Logical Form with Neural Attention"},{"paperId":"285c165c81fc9275955147a892b9a039ec8b1052","title":"chrF: character n-gram F-score for automatic MT evaluation"},{"paperId":"0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7","title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"9ca86842aad16797d0fe0323358f3beb1ac6a5c6","title":"Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"690c210564226c9307b3bab977cdc07a6a45863a","summary":"ODEX corroborates the mer-its of execution-based evaluation over metrics without execution but also unveils their complementary effects, and is released to facilitate research into open-domain problems for the code generation community."},{"url":"https://www.semanticscholar.org/paper/642e280df732665249315d6c144871f0e2ceeae6","title":"NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash Commands","venue":"NeurIPS","year":2021,"referenceCount":40,"citationCount":7,"influentialCitationCount":3,"publicationDate":"03/03/2021","authors":"Mayank Agarwal,T. Chakraborti,Quchen Fu,David Gros,Xi Victoria Lin,Jaron Maene,Kartik Talamadupula,Zhongwei Teng,Jules White","citations":[{"paperId":"988cb68d6510f3c4477b8c8ffe9cbdbea7971474","title":"Towards NLP-based Processing of Honeypot Logs"},{"paperId":"9a7d4c2f4309a2a38ea8e5b23c6d616fa0952d44","title":"Neural language models for network configuration: Opportunities and reality check"},{"paperId":"94ac02f13ac3252a55b8740b7b310383cdf53445","title":"ShellFusion: Answer Generation for Shell Programming Tasks via Knowledge Fusion"},{"paperId":"012d5d4346e84b6e158b252de9c87589dd62b16e","title":"Efficient Constituency Tree based Encoding for Natural Language to Bash Translation"},{"paperId":"7497360b0f411a44aa6afbd8b830050c40ec8aed","title":"Dataset of Student Solutions to Algorithm and Data Structure Programming Assignments"},{"paperId":"bd5d3022dc395ca85f72e346022ed6175e13a278","title":"A Transformer-based Approach for Translating Natural Language to Bash Commands"},{"paperId":"6fe61d77b8a4a090899867b79e32efd658f848e7","title":"Explainable Natural Language to Bash Translation using Abstract Syntax Tree"}],"references":[{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"62d1a3137b01a69443bebf4d92c1990ec512a6a1","title":"Extracting Training Data from Large Language Models"},{"paperId":"e6e8a2c56243847b77b604259cde9e10a2daccb8","title":"Semantic Evaluation for Text-to-SQL with Distilled Test Suite"},{"paperId":"0544af8d6b6b6e9ce14be9c6425e520a638be380","title":"Photon: A Robust Cross-Domain Text-to-SQL System"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"74b4f16c5ac91e3e7c88ae81cc8c91416b71d151","title":"Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning"},{"paperId":"3c9fa66674e1053eb9de12076c48f13e46422c29","title":"CLAI: A Platform for AI Skills on the Command Line"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":null,"title":"Bash shell scripting"},{"paperId":null,"title":"NLC2CMD Report from JB Team"},{"paperId":null,"title":"Hierarchical Decoding of Bash Commands"},{"paperId":null,"title":"Shellshock -High Voltage"},{"paperId":"1d2d3ba8511767d0311413d9081284f6d550b559","title":"Energy Usage Reports: Environmental awareness as part of algorithmic accountability"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"fc6e36c9df6521aa805bc622d066e2964d2e471d","title":"Semantic program alignment for equivalence checking"},{"paperId":"d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","title":"Energy and Policy Considerations for Deep Learning in NLP"},{"paperId":"9e955da40c62b2543b963454b90928ad6cbd5f8a","title":"Does BLEU Score Work for Code Migration?"},{"paperId":"0d96ac48e92b6b42737276a319f48d9d27080fce","title":"EvalAI: Towards Better Evaluation Systems for AI Agents"},{"paperId":"444dcd6b84b1ce1d4aa02aaab71812974735cabb","title":"AInix: An open platform for natural language interfaces to shell commands"},{"paperId":"a46bd6fadfb95a562252c2a2c0d184882a30c579","title":"Coda: An End-to-End Neural Program Decompiler"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"e3ee61f49cd2639c15c8662a45f1d0c2b83a60c1","title":"Why Self-Attention? A Targeted Evaluation of Neural Machine Translation Architectures"},{"paperId":"807370352540f171685998aec7a5701f7110f147","title":"Understanding Back-Translation at Scale"},{"paperId":"6c6170ffb39cdc8cfffbeda9c7a2259eda5875f2","title":"Tree-to-tree Neural Networks for Program Translation"},{"paperId":"429b65937d4922578a81e1f0ef5aeab7361ae36b","title":"NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"d13bb317e87f3f6da10da11059ebf4350b754814","title":"Survey of the State of the Art in Natural Language Generation: Core tasks, applications and evaluation"},{"paperId":"116a877c774969b53399b1ccaa34d51bfb492ee4","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"1ae84940e212e2e3ca132c7aa0878baf0fda06cd","title":"From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood"},{"paperId":"ea67d2d5f2a7d5760ec6b67ea93d11dd5affa921","title":"StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks"},{"paperId":"c3ab52cc41f402650f250a90fc3ced1a22afe082","title":"Word Embeddings for Practical Information Retrieval"},{"paperId":"8ec6abfdc5009b4e490e975991c871dfeec05434","title":"Program Synthesis from Natural Language Using Recurrent Neural Networks"},{"paperId":"cbd569036fc72ae7ff747350b91816440282596b","title":"Seq 2 SQL : Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"259bd09bc382763f864986498e46ab0178714f58","title":"Lifelong Machine Learning"},{"paperId":"6cd61b00eb6248ead7ea029d2daa9d79e68572cb","title":"Evaluating the Energy Efficiency of Deep Convolutional Neural Networks on CPUs and GPUs"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":null,"title":"Command Injection"}],"id":"642e280df732665249315d6c144871f0e2ceeae6","summary":"The NLC2CMD Competition hosted at NeurIPS 2020 aimed to bring the power of natural language processing to the command line by building models that can transform descriptions of command line tasks in English to their Bash syntax."},{"url":"https://www.semanticscholar.org/paper/205ac1373eb7981aca2d08f2ab651871a001271e","title":"CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code","venue":"2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)","year":2022,"referenceCount":55,"citationCount":4,"influentialCitationCount":0,"publicationDate":"01/05/2022","authors":"A. Eghbali,Michael Pradel","citations":[{"paperId":"372dd962ff8c92ac5cf9a195fd52b4b79372c149","title":"A Syntax-Guided Multi-Task Learning Approach for Turducken-Style Code Generation"},{"paperId":"535a3d95a743e1f4b591b5b2af3e778a6347158a","title":"CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code"},{"paperId":"ef29fb8cc6bdda4b011288f51da521d3c25fc53d","title":"Learning to Prevent Profitless Neural Code Completion"},{"paperId":"e73a16490c29530c37b49f6a30592790e7caaaa4","title":"Don't Complete It! Preventing Unhelpful Code Completion for Productive and Sustainable Neural Code Completion Systems"}],"references":[{"paperId":"78fff371e2b512360fa6f6494eca9354a52b0932","title":"Generating realistic vulnerabilities via neural code editing: an empirical study"},{"paperId":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code"},{"paperId":"26f1c97c9e38ed0da329e3197ed554553b305d18","title":"Neural software analysis"},{"paperId":"c317f4d5ddeb11da9a5c8fea1e3cdec2f7de485d","title":"Deep Learning Based Program Generation From Requirements Text: Are We There Yet?"},{"paperId":"6162b14349877749b280baa9060daaf7f7faeede","title":"Semantic bug seeding: a learning-based approach for creating realistic bugs"},{"paperId":"1311adebe439ccd9e373d594a8df1d720a685190","title":"Code Prediction by Feeding Trees to Transformers"},{"paperId":"59cc0871534a20ac98db0663539c6675ab63566f","title":"SequenceR: Sequence-to-Sequence Learning for End-to-End Program Repair"},{"paperId":"708932f350dd6a28a8aaabac9600e2181ac8b70f","title":"On the naturalness of hardware descriptions"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"fcb1063cf60e48ee694be9dccdcb37ae066b9259","title":"Learning to Handle Exceptions"},{"paperId":"ee25620eea19f793d598dbbf6241cf7520a60ccd","title":"Patching as Translation: the Data and the Metaphor"},{"paperId":"bd47bb8cdd749a3356149da6155d2dcd7458779f","title":"Retrieval-based Neural Source Code Summarization"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"6acf0d7b24a93953269a4d52c6b2f0c7d11f9ae4","title":"Hoppity: Learning Graph Transformations to Detect and Fix Bugs in Programs"},{"paperId":"4ddb9135fc6fdf3f9e01fdfcd115fdbf6f7486b4","title":"Sequence Model Design for Code Completion in the Modern IDE"},{"paperId":"3944354c42ddfff7414ad06022f96c72858d5fa6","title":"Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code"},{"paperId":"0dfe706526e5234338411489e1826b4060acc4e8","title":"CC2Vec: Distributed Representations of Code Changes"},{"paperId":"fd12fc2b3e825885f3b921c8f796abf45f5a5b91","title":"On Learning Meaningful Assert Statements for Unit Test Cases"},{"paperId":"92515b7ed018194e340f9edefeb52d9b19f679ef","title":"Detecting Code Clones with Graph Neural Network and Flow-Augmented Abstract Syntax Tree"},{"paperId":"88e6da7240d1b524b6f8dada89e0ab2bca34039e","title":"Bridging Semantic Gaps between Natural Languages and APIs with Word Embedding"},{"paperId":"b5abecf78c1bd975449e34350289a904e6ef35cc","title":"Automated program repair"},{"paperId":"558b72b9e7a1e4d6633c2836aba5896354d37d24","title":"DeepDelta: learning to repair compilation errors"},{"paperId":"9e955da40c62b2543b963454b90928ad6cbd5f8a","title":"Does BLEU Score Work for Code Migration?"},{"paperId":"b3a6e5a38bb5984a27823ecc040526cde10eb730","title":"On Learning Meaningful Code Changes Via Neural Machine Translation"},{"paperId":"9c2ab4707e902f6174910847e3f94f3b85017ec3","title":"A Grammar-Based Structural CNN Decoder for Code Generation"},{"paperId":"98d1307bed619b58b4a44acd8e65ac58495776c2","title":"code2seq: Generating Sequences from Structured Representations of Code"},{"paperId":"28eff8fbf69faa452ac8a5317cc501a04845687b","title":"Automatically Generating API Usage Patterns from Natural Language Queries"},{"paperId":"643de4fe3fcd960bea7b2491831308bc0febcef9","title":"Tree2Tree Neural Translation Model for Learning Source Code Changes"},{"paperId":"077ea3a496bd76bd22d76c432b79b9fa40e136e8","title":"Improving Automatic Source Code Summarization via Deep Reinforcement Learning"},{"paperId":"814db99ccf6b88d6af5b406b0c344b64c0a710b7","title":"A Structured Review of the Validity of BLEU"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"3646885c6ffec41d5ac0c3688b99bd2cd913fe91","title":"TreeGAN: Syntax-Aware Sequence Generation with Generative Adversarial Networks"},{"paperId":"9bab309fa2cc8e6da01378bd693b0333724f7e63","title":"From UI Design Image to GUI Skeleton: A Neural Machine Translator to Bootstrap Mobile GUI Implementation"},{"paperId":"2b80df13ad4ce21d85b36355403d5aded39e3f6e","title":"Learning to Repair Software Vulnerabilities with Generative Adversarial Networks"},{"paperId":"15f16252af84a630f1f9442d4e0367b6fba089cf","title":"Deep Code Comment Generation"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"02b92dcc30865950517894d8adafd3bd4474b018","title":"A general path-based representation for predicting program properties"},{"paperId":"1c159db22f8a6e7c177d9c9fdb5c3972514a12f9","title":"A deep neural network language model with contexts for source code"},{"paperId":"dd908c09eaf8f3a3c20d64fff798ec902433737d","title":"Code Completion with Neural Attention and Pointer Networks"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"554c9b3aa8c3b7504810270b18b942a0d22156e7","title":"DeepFix: Fixing Common C Language Errors by Deep Learning"},{"paperId":"e4a323a3a1ca5ff4cc2dd74ede65f48fd62b897b","title":"Deep API learning"},{"paperId":"927aaa20318d18564bd331ed37428e05820d647e","title":"Divide-and-Conquer Approach for Multi-phase Statistical Migration for Source Code (T)"},{"paperId":"27e1dbe9f7c71cd6cc1b0357f49aef497e572d09","title":"Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"},{"paperId":"333f98412ff246cd646551b4ca6f4b059dc1ea81","title":"Re-evaluating Automatic Summarization with BLEU and 192 Shades of ROUGE"},{"paperId":"9277dc7dd5b348645e8fcf4a987e5b3fc9132c7c","title":"Towards a Big Data Curated Benchmark of Inter-project Code Clones"},{"paperId":"8bc150dc49fc81c7c4dacd35a2b8b1afe1a1692a","title":"A Systematic Comparison of Smoothing Techniques for Sentence-Level BLEU"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":null,"title":"Re-evaluation the Role of Bleu inMachine Translation Research"},{"paperId":"ea5cf5569eef0a99df9b6d92b628a33fc82ca2e7","title":"On Some Pitfalls in Automatic Evaluation and Significance Testing for MT"},{"paperId":"4774432f02ef4c5285952dd8c7daff0852c3a601","title":"Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization"},{"paperId":"4fb5c3f5dcb865134e99402a39a3fb2eff0ab628","title":"Extending the BLEU MT Evaluation Method with Frequency Weightings"},{"paperId":"c63bb976dc0d3a897f3b0920170a4c573ef904c6","title":"Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"205ac1373eb7981aca2d08f2ab651871a001271e","summary":"The results show that CrystalBLEU differentiates similar and unrelated programs better than the original BLEU score and also a variant designed specifically for source code, CodeBLEU."},{"url":"https://www.semanticscholar.org/paper/69e330037afd18e5546fdcacbc9a9f7deb69bba9","title":"Memorization and Generalization in Neural Code Intelligence Models","venue":"Information and Software Technology","year":2021,"referenceCount":93,"citationCount":10,"influentialCitationCount":0,"publicationDate":"16/06/2021","authors":"Md Rafiqul Islam Rabin,Aftab Hussain,V. Hellendoorn,Mohammad Amin Alipour","citations":[{"paperId":"8049fe469d455f9cca4fd252002bf737986cdce5","title":"Study of Distractors in Neural Models of Code"},{"paperId":"ccbc16c0fd9957674da44a812855bb790d9fa8fc","title":"Code2Snapshot: Using Code Snapshots for Learning Representations of Source Code"},{"paperId":"81ce2664e892fc5f71fa4f8d61e7b42314dccb5e","title":"FeatureExtractor: A tool for extracting key input features of code intelligence models"},{"paperId":"22df866f9605d27d1e5cca9b3ab721f33673e158","title":"ProgramTransformer: A tool for generating semantically equivalent transformed programs"},{"paperId":"6042c51ccce53b94b84d1bdbcb33c3ab493323b4","title":"Syntax-guided program reduction for understanding neural code intelligence models"},{"paperId":"b070d2c844d5b18d0c94fb6b20bef3946d60abfd","title":"Readle: A Formal Framework for Designing AI-based Edge Systems"},{"paperId":"f0aacc7a0379883c4ab67d9a2d852c7bd99d9797","title":"Extracting Label-specific Key Input Features for Neural Code Intelligence Models"},{"paperId":"87bb5593d04450bbbd29afc5b2ef395127d1ba6a","title":"Testing the Robustness of a BiLSTM-based Structural Story Classifier"},{"paperId":"4b85c2ee560ccb3c62c75ee52fb5dda94353591a","title":"Code2Snapshot: Using Code Snapshots for Learning Representations of Source Code"},{"paperId":"6a3b6512de2caa712311feb876f1be599a7c0b68","title":"Encoding Program as Image: Evaluating Visual Representation of Source Code"}],"references":[{"paperId":"6042c51ccce53b94b84d1bdbcb33c3ab493323b4","title":"Syntax-guided program reduction for understanding neural code intelligence models"},{"paperId":"8b293973061026d9d0eed90e71e30928e029171e","title":"Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"4b85c2ee560ccb3c62c75ee52fb5dda94353591a","title":"Code2Snapshot: Using Code Snapshots for Learning Representations of Source Code"},{"paperId":"17c8d5d173d915d9662ad7b45c593d1ab3b742e1","title":"Semantic Robustness of Models of Source Code"},{"paperId":"218062f45c15f39bc8f4fb2c930ddf20b5809b11","title":"Machine Learning Testing: Survey, Landscapes and Horizons"},{"paperId":"416bb2238e92cc9c664bc932a5e968bf03bd2845","title":"A Survey on Machine Learning Techniques for Source Code Analysis"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"0a082e6dedf0f65e0e7c2010653d4d4768ecb4f4","title":"Multimodal Representation for Neural Code Search"},{"paperId":"98097b7499bffd76e89d6c28449346f383143553","title":"Understanding neural code intelligence through program simplification"},{"paperId":"a4f9e7e695bba1ffb90b30752a40d5ee907dcb36","title":"Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks"},{"paperId":"6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"9eea59c34f139f3d2153226c8cf026e975622074","title":"Memorization vs. Generalization : Quantifying Data Leakage in NLP Performance Evaluation"},{"paperId":"4b0e5209664ff0f5429d6d12a59921ad448db08b","title":"Probing model signal-awareness via prediction-preserving input minimization"},{"paperId":"be910753f93c24712942536c8dc69e320247c680","title":"On the generalizability of Neural Program Models with respect to semantic-preserving program transformations"},{"paperId":"010c9f63c51ccc504de36d2d1693e0ea9d525da1","title":"Deep Learning for Source Code Modeling and Generation"},{"paperId":"e190c23abd3cbc3dd23280076b97bd9c05da4d09","title":"Demystifying Code Summarization Models"},{"paperId":"2bc1d0c1465ddc402e97e74cea5aaf409ff800e2","title":"Towards demystifying dimensions of source code embeddings"},{"paperId":"aa92c4a7fdc93eadee1752b72812f43f8f037fb3","title":"Measuring Memorization Effect in Word-Level Neural Networks Probing"},{"paperId":"08066c80919620397e8e4e5372ff84caf401e675","title":"Global Relational Models of Source Code"},{"paperId":"6acf0d7b24a93953269a4d52c6b2f0c7d11f9ae4","title":"Hoppity: Learning Graph Transformations to Detect and Fix Bugs in Programs"},{"paperId":"9e35c6dd644fa8cbba878f3661b0c85dc538768c","title":"Evaluation of Generalizability of Neural Program Analyzers under Semantic-Preserving Transformations"},{"paperId":"145319588bb154948edbe736afd3561bb280da84","title":"Embedding Java Classes with code2vec: Improvements from Variable Obfuscation"},{"paperId":"c345b74be9f98cca9592cc376465118df5c9f2da","title":"Improved Code Summarization via a Graph Neural Network"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"60739c6a62f86d274d7e6a0565acc78e6f319ab1","title":"Adversarial Robustness for Code"},{"paperId":"ced24140297c67622a929bf577105650ffa5aea3","title":"Rethinking Generalization of Neural Models: A Named Entity Recognition Case Study"},{"paperId":"7be8c119dbe065c52125ee7716601751f3116844","title":"Generalization through Memorization: Nearest Neighbor Language Models"},{"paperId":"2ab44880e1763baf3d8753ccb43ad3bd5f122b70","title":"Adversarial examples for models of code"},{"paperId":"a14ef6eee732a0d66a6c534d4e696e36767332d7","title":"How Often Do Single-Statement Bugs Occur?: The ManySStuBs4J Dataset"},{"paperId":"45a7ce70b9a1c46f76a9eac22bcf7bc08e2befc9","title":"Let's Agree to Agree: Neural Networks Share Classification Order on Real Datasets"},{"paperId":"8f977a3831132ffaad2f13eea05c1fa46205b8ec","title":"Identity Crisis: Memorization and Generalization under Extreme Overparameterization"},{"paperId":"d47f1239d13c153af15a6bb62e67109bcb5beaad","title":"Assessing the Generalizability of Code2vec Token Embeddings"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"6966a83bbfb67f973e85cb761c83e41758305006","title":"Testing Neural Program Analyzers"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"fb3d9eca57e117c7a9f2d652c7a8c38de45e2d04","title":"COSET: A Benchmark for Evaluating Neural Program Embeddings"},{"paperId":"0073547de99221662cc1dd3ea4babab91a2512f6","title":"Learning to Spot and Refactor Inconsistent Method Names"},{"paperId":"b235c83564f8cd4b27343ff30faf744929d7b961","title":"Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels"},{"paperId":"564bce85c8ad9a50f4652a4d05e1ed0aaa22df49","title":"Neural Program Repair by Jointly Learning to Localize and Repair"},{"paperId":"4e0bb8c1c683b43357c5d5216f6b74ff2cb32434","title":"Do ImageNet Classifiers Generalize to ImageNet?"},{"paperId":"7c894129cac23ba881d5b4c2b4d032e3949e6d7e","title":"Bilateral Dependency Neural Networks for Cross-Language Algorithm Classification"},{"paperId":"ef5561ae5da38ef899333e0444276f5d97923372","title":"Studying the difference between natural and programming language corpora"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"2e4316e7c38373d068f8ff55f26ff83dfc4238b8","title":"Learning to Learn From Noisy Labeled Data"},{"paperId":"98d1307bed619b58b4a44acd8e65ac58495776c2","title":"code2seq: Generating Sequences from Structured Representations of Code"},{"paperId":"73bade9162bc4df6ca6c9ab8a7eb2d54d35acad2","title":"code2vec: learning distributed representations of code"},{"paperId":"520ec00dc35475e0554dbb72f27bd2eeb6f4191d","title":"The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"ea1b62a03ab16c4acca6f7041c6f096bedec244b","title":"Deep learning type inference"},{"paperId":"2d7cd527303f8f515cd282be0243c38e24d2e11d","title":"Prediction of relatedness in stack overflow: deep learning vs. SVM: a reproducibility study"},{"paperId":"077ea3a496bd76bd22d76c432b79b9fa40e136e8","title":"Improving Automatic Source Code Summarization via Deep Reinforcement Learning"},{"paperId":"4d1c856275744c0284312a3a50efb6ca9dc4cd4c","title":"Know What You Don’t Know: Unanswerable Questions for SQuAD"},{"paperId":"e5a95a679774e069e1e36d96f92bac6b93027118","title":"Insights on representational similarity in neural networks with canonical correlation"},{"paperId":"e033a0b29af2939fd44e5765d03380c08897a9e8","title":"Deep Code Search"},{"paperId":"a8b2c73f7c19f4e6e3783a5c19304025d9b7025f","title":"Learning to Attack: Adversarial Transformation Networks"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"804fb9542f4f56e264dd2df57c255a9a2011c00f","title":"Adversarially Robust Generalization Requires More Data"},{"paperId":"d0f2d7236e43f129744e88130fb71f8f872d2b31","title":"Learning to Represent Programs with Graphs"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"1f10df986e3235f1deda73e1200155f4f4b88713","title":"Curating GitHub for engineered software projects"},{"paperId":"a26b00c607b6d6a2697f5444f14a9afc37701444","title":"DéjàVu: a map of code duplicates on GitHub"},{"paperId":"5ddd38a5df945e4afee68d96ed51fd6ca1f7d4cf","title":"A Closer Look at Memorization in Deep Networks"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"82364428995c29b3dcb60c1835548eeff4adcd20","title":"What do Neural Machine Translation Models Learn about Morphology?"},{"paperId":"54ddb00fa691728944fd8becea90a373d21597cf","title":"Understanding deep learning requires rethinking generalization"},{"paperId":"d1cbd204ab108ac508cd773ffc9fad3f7e06f8d7","title":"Memorization in Recurrent Neural Networks"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"32aa2517b03c871c11e521c2a3406f457833e2c3","title":"Summarizing Source Code using a Neural Attention Model"},{"paperId":"0a93c60ed946c31edeed419fd564a405487161f8","title":"Learning Unified Features from Natural and Programming Languages for Locating Buggy Source Code"},{"paperId":"e4a323a3a1ca5ff4cc2dd74ede65f48fd62b897b","title":"Deep API learning"},{"paperId":"77f0a39b8e02686fd85b01971f8feb7f60971f80","title":"Identity Mappings in Deep Residual Networks"},{"paperId":"484ed558a5863060b373e8a2cb7cb302b8c36116","title":"A Convolutional Attention Network for Extreme Summarization of Source Code"},{"paperId":"8ab441ed65209e807c6ffe47a7ca8ab7d02f3142","title":"Google AI algorithm masters ancient game of Go"},{"paperId":"e2db78c001e4644f4841da5a8d13548157175161","title":"Learning programs from noisy data"},{"paperId":"492f57ee9ceb61fb5a47ad7aebfec1121887a175","title":"Gated Graph Sequence Neural Networks"},{"paperId":"cd7ee037c029b25e691650a26cec7538d255405c","title":"Suggesting accurate method and class names"},{"paperId":"370a5f4b305540f35821d2179c384f95f5a63eee","title":"Toward Deep Learning Software Repositories"},{"paperId":null,"title":"Mario Linares-Vásquez, and Denys Poshyvanyk"},{"paperId":"6081ceb60d07fa0a2f0037ece6e540228e4edf73","title":"Automatic documentation generation via source code summarization of method context"},{"paperId":"34f25a8704614163c4095b3ee2fc969b60de4698","title":"Dropout: a simple way to prevent neural networks from overfitting"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"9eaae704216b322f5a17400377fce443853d7b86","title":"Dealing with noise in defect prediction"},{"paperId":"d664cc7336b6874f4e9a955659920bcd41264734","title":"A study of the uniqueness of source code"},{"paperId":"443516aeb2819d4d362ffe7d5418a54e5427a016","title":"ORANGE: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"f42b865e20e61a954239f421b42007236e671f19","title":"GradientBased Learning Applied to Document Recognition"},{"paperId":"162d958ff885f1462aeda91cd72582323fd6a1f4","title":"Gradient-based learning applied to document recognition"},{"paperId":"95c4ebb6df40abc74c9cf36994c0f914be3b04bd","title":"The Association for Computing Machinery"},{"paperId":null,"title":"On the measure of concentration with special reference to income and statistics"}],"id":"69e330037afd18e5546fdcacbc9a9f7deb69bba9","summary":"This work evaluates the memorization and generalization tendencies in neural code intelligence models through a case study across several benchmarks and model families by leveraging established approaches from other fields that use DNNs, such as introducing targeted noise into the training dataset."},{"url":"https://www.semanticscholar.org/paper/09e14c4c80e20e80c052e0adb0d49df51aff718d","title":"Yes, You Can Make an App Too: A Systematic Study of Prompt Engineering in the Automatic Generation of Mobile Applications from User Queries","venue":"","year":2022,"referenceCount":16,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Jasmine Shone","citations":[],"references":[{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"6d7d4fca9840504f630e9bea6acaa07322a6e889","title":"Text and Code Embeddings by Contrastive Pre-Training"},{"paperId":"166b64f2ae8e52f5779682fab756cbd617a6e74b","title":"A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level"},{"paperId":"a2aa222de671fafa94b75ddc001388f693c21e14","title":"The Impacts of Low/No-Code Development on Digital Transformation and Software Development"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"56fa0b9cba4d9aee5ccc327365b3b3a721031c69","title":"Calibrate Before Use: Improving Few-Shot Performance of Language Models"},{"paperId":"ac3cdb50606f7770eef8e4cd951840a4f71287a0","title":"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"},{"paperId":"1fa9ed2bea208511ae698a967875e943049f16b6","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"c5e921a67842dcc2b508d49d08c90a3a1ed7c459","title":"Maximum Relevance and Minimum Redundancy Feature Selection Methods for a Marketing Machine Learning Platform"},{"paperId":"3b5505eaec8583a2dd98d72a84d95b9eff475a81","title":"Few-shot Learning: A Survey"},{"paperId":"7a65f23d990231d461418067c808b09d84c19b2c","title":"Natural Language Processing with Python"},{"paperId":"86d2df70fa9caddad48f479a902b8e052852ac51","title":"Minimum redundancy feature selection from microarray gene expression data"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":null,"title":"Structured testing"},{"paperId":null,"title":"Few-shot learning: A survey. CoRR, abs"},{"paperId":null,"title":"The future of it lies in democratising application development"}],"id":"09e14c4c80e20e80c052e0adb0d49df51aff718d","summary":"One of the first systematic studies of prompt engineering for the Codex model, a LLM that produces code from a natural-language input, is embarked on, improving the pipeline’s performance from baseline for complex apps using example selection mechanisms and 43% for simple apps."},{"url":"https://www.semanticscholar.org/paper/91260f73dd179487fb16713deb8267634ae14716","title":"CodexDB: Synthesizing Code for Query Processing from Natural Language Instructions using GPT-3 Codex","venue":"Proceedings of the VLDB Endowment","year":2022,"referenceCount":39,"citationCount":6,"influentialCitationCount":0,"publicationDate":2022,"authors":"Immanuel Trummer","citations":[{"paperId":"2ef1c2438c3a4552db9e7080e15d8c51bc071f58","title":"Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes"},{"paperId":"645960e8bcb2a55bc7e8816c49e60e8f98303acb","title":"Combining Contexts from Multiple Sources for Documentation-Specific Code Example Generation"},{"paperId":"9b5de3f83649c347b2b2b9fdecb142fbca293f57","title":"Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference"},{"paperId":"270b015093073d3ba254928b6d736a59870d3fb1","title":"Chatbots As Fluent Polyglots: Revisiting Breakthrough Code Snippets"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"9bf75110ea0923bbed49256b5491f1ec284019ec","title":"From BERT to GPT-3 Codex: Harnessing the Potential of Very Large Language Models for Data Management"}],"references":[{"paperId":"51000d9f79be0eefd7972fe94e3c71dddc90d2c6","title":"Evaluating the Text-to-SQL Capabilities of Large Language Models"},{"paperId":"2236b6036cd1ecba1792d5e1fedbae7cefc3d43b","title":"Can we generate shellcodes via natural language? An empirical study"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis"},{"paperId":"6fa43591e6da321e43722dac609f6b8ef8204768","title":"SeaD: End-to-end Text-to-SQL Generation with Schema-aware Denoising"},{"paperId":"59641c10ed7431a3cf841f308367dc2dc0281b74","title":"What Makes Good In-Context Examples for GPT-3?"},{"paperId":null,"title":"A first look at rote learning in GitHub Copilot suggestions"},{"paperId":"5fbcfccd3736969d95ed660d8e6962c86b7a9113","title":"PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models"},{"paperId":"4f68e07c6c3173480053fd52391851d6f80d651b","title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"fe935caed47ef090a306d6d09240f76adc43a420","title":"Ember: No-Code Context Enrichment via Similarity-Based Keyless Joins"},{"paperId":"6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"bfe6cce534d76df1aa7ebd629c09e30d6b8abdd4","title":"RPT: Relational Pre-trained Transformer Is Almost All You Need towards Democratizing Data Preparation"},{"paperId":null,"title":"Introducing GitHub Copilot: your AI pair programmer"},{"paperId":null,"title":"GitHub Copilot: Copyright, Fair Use, Creativity, Transformativity, and Algorithms "},{"paperId":"232b40980acb55afa89ec50dd9806a5e551f699b","title":"Bridging Textual and Tabular Data for Cross-Domain Text-to-SQL Semantic Parsing"},{"paperId":"43a712e8b9d2db596abd6dc2ca0ffedb8878cde3","title":"ATHENA++: Natural Language Querying for Complex Nested SQL Queries"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":null,"title":"AUTOPROMPT: Eliciting knowledge from language models with automatically generated prompts"},{"paperId":"5452cd8a1ebb941e473f0e5c8ba6cc7359e40b24","title":"DBPal: Weak Supervision for Learning a Natural Language Interface to Databases"},{"paperId":"157a7ae44613a1fcf34e2be8c1e19a4f6e3c50e3","title":"Transfer Learning in Natural Language Processing"},{"paperId":"5ba52bbe1101939c490a06cc0cf316a09000834e","title":"Neo: A Learned Query Optimizer"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"287050dc91b146768c9d4435e5582fc9975ba84c","title":"Learned Cardinalities: Estimating Correlated Joins with Deep Learning"},{"paperId":null,"title":"Request for Comments on Intellectual Property Protection for Artificial Intelligence Innovation"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"f6eee02a2f4c74c2b543bf419f76cef60d5752f8","title":"The Case for Learned Index Structures"},{"paperId":"116a877c774969b53399b1ccaa34d51bfb492ee4","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"ac67d5f9c89d8d72fbd074f94079608220348f3f","title":"ATHENA: An Ontology-Driven System for Natural Language Querying over Relational Data Stores"},{"paperId":"1c39f884bc53d7d5465d0f56c4b432a9352afbe7","title":"Understanding Natural Language Queries over Relational Databases"},{"paperId":"888764f05a60d770cfc0b49944308fd92ed45ee5","title":"How Good Are Query Optimizers, Really?"},{"paperId":"42414b70fc61def8adcf5c159604c72e4508e9c1","title":"NaLIR: an interactive natural language interface for querying relational databases"},{"paperId":"bac4169d6b6f713c76271b5ccf3d45293351f785","title":"Runtime Code Generation in Cloudera Impala"},{"paperId":"e89fb3dd3412fc45506bfd06ea338d31093215ab","title":"Polynomial heuristics for query optimization"},{"paperId":"56eeedcd45e384f1d73cd27b4aa426c154554bbb","title":"Generating code for holistic query evaluation"},{"paperId":"cf6657ee417fb38bed405793e06a3656e2bd1d23","title":"Access path selection in a relational database management system"},{"paperId":null,"title":"Training a causal language model from scratch"}],"id":"91260f73dd179487fb16713deb8267634ae14716","summary":"The CodexDB framework is a framework on top of GPT-3 Codex that decomposes complex SQL queries into a series of simple processing steps, described in natural language, that enables users to customize SQL query processing via natural language instructions."},{"url":"https://www.semanticscholar.org/paper/d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis","venue":"International Conference on Software Engineering","year":2021,"referenceCount":42,"citationCount":31,"influentialCitationCount":1,"publicationDate":"06/12/2021","authors":"Naman Jain,Skanda Vaidyanath,Arun Shankar Iyer,Nagarajan Natarajan,Suresh Parthasarathy,S. Rajamani,Rahul Sharma","citations":[{"paperId":"1c00f88b69f1d4efe3e1695ca49aa38a1340bd98","title":"Improving Few-Shot Prompts with Relevant Static Analysis Products"},{"paperId":"8a2a2006e371ce07e0b29538cd11337a5c977caf","title":"“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models"},{"paperId":"d9bd490aba3a7995f728594403b3358cb79aacd6","title":"Synthesis of Mathematical programs from Natural Language Specifications"},{"paperId":"af622206f1e5a6632600075897385cdabcae463d","title":"Large Language Models and Simple, Stupid Bugs"},{"paperId":"35afb57a646592c3a471a4f010d00e1b13dd3c43","title":"From Copilot to Pilot: Towards AI Supported Software Development"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"1786a2f9140ed7211b21302977de64e948b92308","title":"Learning Performance-Improving Code Edits"},{"paperId":"6248474933664013e5b0615dc474a7f6de5e97f4","title":"LExecutor: Learning-Guided Execution"},{"paperId":"0192638593c430e15e2804a74a3e8a7ecb52d435","title":"On the Design of AI-powered Code Assistants for Notebooks"},{"paperId":"0885556b71b24a641b4ffe139afd4d2712228cff","title":"Beware of the Unexpected: Bimodal Taint Analysis"},{"paperId":"d8405996b4d08c304098636aedd9e1c1a1e262ee","title":"The Premature Obituary of Programming"},{"paperId":"fba0b0817dbc8200b41a1de22654b54b778a11e9","title":"Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models"},{"paperId":"9e8f0d9ba4e7af673c8b214b3764e020706dd1f3","title":"Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"30624a18720bf93a85dc3efe570df271a8c9f4c3","title":"Program Repair"},{"paperId":"0908532a6d86bfaba8cacc42e1585f22128a0c38","title":"CLAWSAT: Towards Both Robust and Accurate Code Models"},{"paperId":"b1eebb2df3b9ff7ff2b00fb1a786f6ada2caebce","title":"Towards a Mathematics Formalisation Assistant using Large Language Models"},{"paperId":"4610ffb1b016acaa82a2065ffd1a3adbae1ce722","title":"Large Language Models Are Human-Level Prompt Engineers"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"43112f25190a9e19dc84cc7a0851318fdd1d9f71","title":"INTENT: Interactive Tensor Transformation Synthesis"},{"paperId":"a6ef5bad716091fb1888bf365f6129628ab3a5ee","title":"ObSynth: An Interactive Synthesis System for Generating Object Models from Natural Language Specifications"},{"paperId":"045a6f92b58f08d5b305dd5d661316a506ee4d43","title":"Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation"},{"paperId":"55e3fe05598be7c3dd357d51166869f6571b824f","title":"Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors"},{"paperId":"1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization"},{"paperId":"2f2750b48a6f958ff12cba90e99695123d1e2f47","title":"Using pre-trained language models to resolve textual and semantic merge conflicts (experience paper)"},{"paperId":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code"},{"paperId":"6074c7b75f27ca9adb6d74b080c07d5d079c3ea0","title":"Improving automatically generated code from Codex via Automated Program Repair"},{"paperId":"0bcd59da541fdae66884afba8d25475a54a9da1a","title":"Automated Repair of Programs from Large Language Models"},{"paperId":"bb7e46f316d319f9819c3554c99995ef8361ae9c","title":"CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex"},{"paperId":"b2c4fdb49bdb23e6ad0ac3272029324157046ea7","title":"Automated Repair of Code from Language Models"},{"paperId":"91260f73dd179487fb16713deb8267634ae14716","title":"CodexDB: Synthesizing Code for Query Processing from Natural Language Instructions using GPT-3 Codex"}],"references":[{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"40c25232bc3a3f36ac856ff517d5c70704f14965","title":"TF-Coder: Program Synthesis for Tensor Manipulations"},{"paperId":"4b2137280915ccc0e06e97b604778b05876a34ad","title":"Evaluating Large Language Models"},{"paperId":"a8863de15a5ee8eed98107f423138a1a8f5a2ba8","title":"Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"b58d8579ece27a60432e667bfbdb750590fa65d9","title":"True Few-Shot Learning with Language Models"},{"paperId":"93b9c18e5e4924f793241bb0fc359741032c9ff6","title":"Web question answering with neurosymbolic program synthesis"},{"paperId":"56fa0b9cba4d9aee5ccc327365b3b3a721031c69","title":"Calibrate Before Use: Improving Few-Shot Performance of Language Models"},{"paperId":"5868a7bfe6a4590d332ca66b8097dbe5490c8a73","title":"SmBoP: Semi-autoregressive Bottom-up Semantic Parsing"},{"paperId":"3f97c2067cde9377e50b3160bbd7982c94abd88a","title":"An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions"},{"paperId":"6099feb5eb536f3666614565b1bda5b701d40be2","title":"B2: Bridging Code and Interactive Visualization in Computational Notebooks"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0c5bc409e62e65f86838968a2a7cdae5fa0b288b","title":"RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers"},{"paperId":"1bb27a0180f0c5fdd66cf34864752dfb1d6d94d2","title":"Sketch-Driven Regular Expression Generation from Natural Language and Examples"},{"paperId":"9d2f40625123d7000ab044a5c38eb47168a4481c","title":"Multi-modal synthesis of regular expressions"},{"paperId":"26d9141ed3f021af7533e1d84fc83111d20df925","title":"AutoPandas: neural-backed generators for program synthesis"},{"paperId":"d2d44be771d01e277a9912249f2f7c211c393fee","title":"On the fly synthesis of edit suggestions"},{"paperId":"93d63ec754f29fa22572615320afe0521f7ec66d","title":"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"},{"paperId":"4e7fab8c0f695f50128045f0394a4d7a92be32b5","title":"Maximal multi-layer specification synthesis"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"62fb54f5736be4a55b2da17010a115cee701d72c","title":"Accelerating search-based program synthesis using learned probabilistic models"},{"paperId":"93096257f8418b0414cddad54d3f53bc78273355","title":"Transform-Data-by-Example (TDE): An Extensible Search Engine for Data Transformations"},{"paperId":"85bdb70962147cd3586d99f50328025c82c9ac8e","title":"Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples"},{"paperId":"bbe832982b47a6b39904d5abc608a8c2fc10c5ee","title":"Program synthesis using conflict-driven learning"},{"paperId":"3ff0af64279929a952ee340e645256b7e0580f65","title":"RobustFill: Neural Program Learning under Noisy I/O"},{"paperId":"644ca74f80463415613847ab01cff067fb58f0ad","title":"Neuro-Symbolic Program Synthesis"},{"paperId":"cf404026f1e66dc04417d699ef2b4731cbec842c","title":"Learning Syntactic Program Transformations from Examples"},{"paperId":"bb1f78612d80cf5e72345b21be1d129c6ec02629","title":"Programming by Examples"},{"paperId":null,"title":"DeepCoder: Learning toWrite Programs"},{"paperId":"7f6245d527e84092111a6adb0f1939d0209d3360","title":"FlashMeta: a framework for inductive program synthesis"},{"paperId":"e2d2099379af74d7be80a1964830f99edddaeb11","title":"Compositional Program Synthesis from Natural Language and Examples"},{"paperId":"e5412b187655613ce29ec0651a450fed3bac288b","title":"Predicting a Correct Program in Programming by Example"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"8ad378c449db1ac08e8b98238e5387b62c549020","title":"Integrating Programming by Example and Natural Language Programming"},{"paperId":"43dcda631f8a0d39949ba3f9e3e22101db4daba0","title":"A Machine Learning Framework for Programming by Example"},{"paperId":"49af3e80343eb80c61e727ae0c27541628c7c5e2","title":"Introduction to Modern Information Retrieval"},{"paperId":null,"title":"GitHub Copilot · Your AI pair programmer"},{"paperId":null,"title":"The pandas development team. 2020. pandas-dev/pandas: Pandas"},{"paperId":null,"title":"Spider 1.0: Yale Semantic Parsing and Text-to-SQL Challenge"}],"id":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","summary":"This paper presents an approach to augment these large language models with post-processing steps based on program analysis and synthesis techniques, that understand the syntax and semantics of programs, and presents the experiences from building and evaluating such a tool Jigsaw."},{"url":"https://www.semanticscholar.org/paper/075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":8,"influentialCitationCount":2,"publicationDate":"04/06/2022","authors":"J. Inala,Chenglong Wang,Mei Yang,Andrés Codas,Mark Encarnaci'on,Shuvendu K. Lahiri,M. Musuvathi,Jianfeng Gao","citations":[{"paperId":"3b01b5b497e1f359b11da45af029281ee6f64c2c","title":"Towards Enhancing In-Context Learning for Code Generation"},{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"},{"paperId":"6032212d5790b6a580d68d469a9895aad6238c89","title":"Diverse Title Generation for Stack Overflow Posts with Multiple Sampling Enhanced Transformer"},{"paperId":"045a6f92b58f08d5b305dd5d661316a506ee4d43","title":"Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation"},{"paperId":"0b8772b7790c69f40897b5eb7f8fd57f24138f3d","title":"ContraGen: Effective Contrastive Learning For Causal Language Model"},{"paperId":"95fa2b27ab7eb84738441ee16da97323538938f9","title":"I Speak, You Verify: Toward Trustworthy Neural Program Synthesis"},{"paperId":"453a8fac3be9282be53908f0735160d0d21e0f48","title":"Repair Is Nearly Generation: Multilingual Program Repair with LLMs"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"}],"references":[{"paperId":"e37018d3cfab9cfc29a7b78404e6c86ea18a907e","title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"bd02bcf2a437217eaac245abe443b2f672b3b36a","title":"Deep Learning Based Vulnerability Detection: Are We There Yet?"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"570a6a5b8ec2827c3f33bb1b1bd027190a0d3e07","title":"Neural Program Generation Modulo Static Analysis"},{"paperId":"4698fc4712f0212c8a3810fd67b41ee8b8896aba","title":"Generate & Rank: A Multi-task Framework for Math Word Problems"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"7093ad29f2de2781cf5ed6e6d212ed31ebaa2c8b","title":"Latent Execution for Neural Program Synthesis Beyond Domain-Specific Languages"},{"paperId":"482f1befc7574c282aa35ec242ee8dc14030725e","title":"DeepWukong: Statically Detecting Software Vulnerabilities Using Deep Graph Neural Network"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"f2e5702330e52b730378ca1e7f1a041fc4f81c07","title":"Vulnerability Prediction From Source Code Using Machine Learning"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"00549af4bc3270e0f688acbf694f912d7ee39cad","title":"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"f669b5406320a83e94b4dd439806211c8b1e95fc","title":"Write, Execute, Assess: Program Synthesis with a REPL"},{"paperId":"0e412349f4c5161b413b43e0fe27e00f4ef83043","title":"When deep learning met code search"},{"paperId":"6c41bedc4637f3fd504c68baa3b3d8881e056ac1","title":"Execution-Guided Neural Program Synthesis"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"e033a0b29af2939fd44e5765d03380c08897a9e8","title":"Deep Code Search"},{"paperId":"6c6170ffb39cdc8cfffbeda9c7a2259eda5875f2","title":"Tree-to-tree Neural Networks for Program Translation"},{"paperId":"3807a517403cfc7ed67fdfea64baf62b9948bcfe","title":"A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation"},{"paperId":"49512270b39636375880d611d7b2192d324f4ba6","title":"Convolutional Neural Networks over Tree Structures for Programming Language Processing"},{"paperId":"927aaa20318d18564bd331ed37428e05820d647e","title":"Divide-and-Conquer Approach for Multi-phase Statistical Migration for Source Code (T)"},{"paperId":"9277dc7dd5b348645e8fcf4a987e5b3fc9132c7c","title":"Towards a Big Data Curated Benchmark of Inter-project Code Clones"},{"paperId":null,"title":"Github copilot: Your ai pair programmer"},{"paperId":null,"title":"Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation"},{"paperId":null,"title":"Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?"}],"id":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","summary":"C ODE R ANKER is a neural ranker that can predict the correctness of a sampled program without executing it and can signiﬁcantly increase the pass@1 accuracy of various code generation models on APPS, HumanEval, and MBPP datasets."},{"url":"https://www.semanticscholar.org/paper/239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning","venue":"ArXiv","year":2022,"referenceCount":65,"citationCount":2,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"E. Zelikman,Qian Huang,Gabriel Poesia,Noah D. Goodman,N. Haber","citations":[{"paperId":"dd0960a8ffb6a8baff69ec96c3f67deb2912b53a","title":"Self-collaboration Code Generation via ChatGPT"},{"paperId":"207b767bcc2d916afdd27e7c1b25ed261363548c","title":"Text2Motion: From Natural Language Instructions to Feasible Plans"}],"references":[{"paperId":"fb03acf0a7d90f4bc2cc91ed8980c7ec055edbf9","title":"Top-Down Synthesis for Library Learning"},{"paperId":"0566c1c3eeeef5c968fced6d80b77fe22d02bbd9","title":"Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language"},{"paperId":"815c6ca281536d18ec0eb408b6e46e72a0826163","title":"Natural Language to Code Generation in Interactive Data Science Notebooks"},{"paperId":"ae9a0d7c98f8a74a538198291b0925609e5c26ac","title":"Neural Story Planning"},{"paperId":"a8afd12bcd51488ba69bd838ef6dbf2728d5121a","title":"Prompting Is Programming: A Query Language For Large Language Models"},{"paperId":"c90151f00b1ac4abf1cc353849b453aa21cc2df3","title":"Successive Prompting for Decomposing Complex Questions"},{"paperId":"6d7b8a478801bd9d21df82d5f33ae6eced90da5e","title":"Solving math word problems with process- and outcome-based feedback"},{"paperId":"5032c0946ee96ff11a292762f23e6377a6cf2731","title":"Holistic Evaluation of Language Models"},{"paperId":"4d17732d90440682b0500f4e209c6cc4fac20e0e","title":"Teaching Algorithmic Reasoning via In-context Learning"},{"paperId":"ce3f027b68dad014a58aa35f52380932c8d0b209","title":"Do Users Write More Insecure Code with AI Assistants?"},{"paperId":"2577d053f8aab912d29b424e1f09133d83740fd2","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"4634362d75606287955260ef1788171286efbeaa","title":"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs"},{"paperId":"2d2ca2e54c54748557b8aac7d328ce32ebfe8944","title":"ReAct: Synergizing Reasoning and Acting in Language Models"},{"paperId":"f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab","title":"Binding Language Models in Symbolic Languages"},{"paperId":"c03fa01fbb9c77fe3d10609ba5f1dee33a723867","title":"ProgPrompt: Generating Situated Robot Task Plans using Large Language Models"},{"paperId":"29bc052f3c46bf1110cc02fd71d454ebe8b13b80","title":"A Generalist Neural Algorithmic Learner"},{"paperId":"e3a9af420cd2c0c8241856da92374027fefb87be","title":"Language Model Cascades"},{"paperId":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"6d86f08a5d936780a4785acfad92f5f3e82004ad","title":"Sub-Task Decomposition Enables Learning in Sequence to Sequence Tasks"},{"paperId":"341bdbcfc3febef7691a97c216ad394653211095","title":"Can language models learn from explanations in context?"},{"paperId":"cb5e3f085caefd1f3d5e08637ab55d39e61234fc","title":"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"},{"paperId":"23dd78e424d32f6a48660dcd67ce994b8a7db8be","title":"STaR: Bootstrapping Reasoning With Reasoning"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"},{"paperId":"9a258f42e333ed5ff79037724eb01747ede0bb49","title":"Few-Shot Self-Rationalization with Natural Language Prompts"},{"paperId":"1c8e15f15d67c5974445634bb971e2275e957aff","title":"Security Implications of Large Language Model Code Assistants: A User Study"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":null,"title":"Ml-enhanced code completion improves developer productivity"},{"paperId":null,"title":"Formal Environment Multi-step Planning Also encouragingly, several existing works can be expressed in Parsel"},{"paperId":null,"title":"That is, it would be valuable to allow a model to determine which target language to use, possibly combining them. For example, for large parts of the Tensorflow and PyTorch"},{"paperId":null,"title":"Abstract Reasoning Corpus (Chollet, 2019), tended to provide step-by-step hierarchical descriptions with many verification"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"fd963ea665e7a6e49753652dc7efe5affba5feb0","title":"DreamCoder: bootstrapping inductive program synthesis with wake-sleep library learning"},{"paperId":"98485ce6532d69f34a8ec67de6b09a39532bd221","title":"Communicating Natural Programs to Humans and Machines"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"2bb1e1a5b9a16f6828fe94736cea5dab264533a6","title":"Provable Limitations of Acquiring Meaning from Ungrounded Form: What Will Future Language Models Understand?"},{"paperId":"30a156f17ca8f54aa14d01d32c2315c11fcbe723","title":"BUSTLE: Bottom-up program-Synthesis Through Learning-guided Exploration"},{"paperId":null,"title":"2021) showed that giving language models access to a calculator allowed them to solve more complex math word problems"},{"paperId":"5fe0a4af3bd1479d5e39fbda2215c86bce54722b","title":"Generative Language Modeling for Automated Theorem Proving"},{"paperId":"9808d59113029d96f48a0376b1578dbab5427bb4","title":"Unsupervised Commonsense Question Answering with Self-Talk"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"874e9318c09c711ecd48a903b3824a3a03e2cd62","title":"Explain Yourself! Leveraging Language Models for Commonsense Reasoning"},{"paperId":"7139a5f730652abbeabf9e140009907d2c7da3e5","title":"VirtualHome: Simulating Household Activities Via Programs"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"745c614dbd23bc1e3def79f600680b88cee28700","title":"Thinking Fast and Slow with Deep Learning and Tree Search"},{"paperId":"bb1f78612d80cf5e72345b21be1d129c6ec02629","title":"Programming by Examples"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"2579e826f22da988fbc1b5d545aa59ab68bde4f3","title":"Program Synthesis Using Natural Language"},{"paperId":"e2d2099379af74d7be80a1964830f99edddaeb11","title":"Compositional Program Synthesis from Natural Language and Examples"},{"paperId":null,"title":"TensorFlow: Largescale machine learning on heterogeneous systems"},{"paperId":"f193e68ce6f3200b1f801e64bf49e56f668fd3ef","title":"A Survey on Unit Testing Practices and Problems"},{"paperId":"f48eed915cbb9c6592cdb9df80c1edaeb46959af","title":"A measure of intelligence"},{"paperId":"1bc831136e380489a0a76ff07f501003866cb954","title":"The status of the P versus NP problem"},{"paperId":"2fb9729c0a3b046c390ad6748773527eae6cfc84","title":"What Is Program Synthesis?"},{"paperId":"84bdb440504732f38cf08a996c808156cd504e32","title":"An algorithm for reduction of operator strength"},{"paperId":"18ce82b07ac84aaf30b502c93076cec2accbfcaa","title":"Human problem solving: The state of the theory in 1970."},{"paperId":null,"title":"Unified Natural Language Framework for Algorithmic Reasoning Simon"},{"paperId":null,"title":"The fantastic combinations of john conway’s new solitaire game “life” by martin gardner"}],"id":"239b5649b12f28fd610de036afba41b9246db6c9","summary":"This work introduces Parsel 2, a framework enabling automatic implementation and validation of complex algorithms with code LLMs, based on hierarchical function descriptions in natural language, which can be used across domains requiring hierarchical reasoning, e.g. code synthesis, theorem proving, and robotic planning."},{"url":"https://www.semanticscholar.org/paper/35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs","venue":"ArXiv","year":2022,"referenceCount":28,"citationCount":5,"influentialCitationCount":3,"publicationDate":"28/05/2022","authors":"Ansong Ni,J. Inala,Chenglong Wang,Oleksandr Polozov,Christopher Meek,Dragomir R. Radev,Jianfeng Gao","citations":[{"paperId":"6d269364de402d4a72ac30b0c8d81324f6849807","title":"LEVER: Learning to Verify Language-to-Code Generation with Execution"},{"paperId":"d3a7a4543d83f568f79d1febe8379465ff0140c9","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"ebd4bec684808aff360b5f255d15c0d112ba13d3","title":"Explicit Knowledge Transfer for Weakly-Supervised Code Generation"},{"paperId":"538288d24bdad73d831dfed44b706958287ed318","title":"Generating Sequences by Learning to Self-Correct"},{"paperId":"8d282bf295d655e38b63ddbc7cdc94b188df8bb2","title":"G ENERATING S EQUENCES BY L EARNING TO [S ELF -]C ORRECT"}],"references":[{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"55ad5e818cfed72317576027fb33a9609210d592","title":"Training and Evaluating a Jupyter Notebook Data Science Assistant"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"4116e65ef8a05c82c0fd739d98ca72e50802cf83","title":"Representing Partial Programs with Blended Abstract Semantics"},{"paperId":"7093ad29f2de2781cf5ed6e6d212ed31ebaa2c8b","title":"Latent Execution for Neural Program Synthesis Beyond Domain-Specific Languages"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"f669b5406320a83e94b4dd439806211c8b1e95fc","title":"Write, Execute, Assess: Program Synthesis with a REPL"},{"paperId":"84b888b66d598f4fd5909c524a1818e1004a3254","title":"Learning to Generalize from Sparse and Underspecified Rewards"},{"paperId":"6c41bedc4637f3fd504c68baa3b3d8881e056ac1","title":"Execution-Guided Neural Program Synthesis"},{"paperId":"2fc1cfc75d6ba80846d64fbec424f6c35682f5d8","title":"Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing"},{"paperId":"e3deff41f9d3eb1e141207b69d6c7a4c007bd63e","title":"Program Synthesis Through Reinforcement Learning Guided Tree Search"},{"paperId":"2f541b24f69798e255e04229e8dc78f4d1873fd7","title":"Improving Text-to-SQL Evaluation Methodology"},{"paperId":"dea6aeb514b1969ab879c793d46a0d2eceaa2cbf","title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis"},{"paperId":"1ae84940e212e2e3ca132c7aa0878baf0fda06cd","title":"From Language to Programs: Bridging Reinforcement Learning and Maximum Marginal Likelihood"},{"paperId":"4ca430d4640afa4a3838371a08f8f418284bdb7c","title":"Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"94d9cbfc474cb6ce961de45a06233b2853ca6724","title":"Toward automatic program synthesis"}],"id":"35afb74de9660962ebac2843d26de22a6fac2ef6","summary":"This work proposes to let the model perform sampling during training and learn from both self-sampled fully-Correct programs, which yield the gold execution results, as well as partially-correct programs, whose intermediate execution state matches another correct program."},{"url":"https://www.semanticscholar.org/paper/06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling","venue":"ArXiv","year":2022,"referenceCount":72,"citationCount":9,"influentialCitationCount":0,"publicationDate":"22/07/2022","authors":"Fenia Christopoulou,Gerasimos Lampouras,Milan Gritta,Guchun Zhang,Yinpeng Guo,Zhong-Yi Li,Qi Zhang,M. Xiao,Bo Shen,Lin Li,Hao Yu,Li-yu Yan,Pingyi Zhou,Xin Wang,Yu Ma,Ignacio Iacobacci,Yasheng Wang,Guangtai Liang,Jia Wei,Xin Jiang,Qianxiang Wang,Qun Liu","citations":[{"paperId":"362cbfd0d05e139cd6cf049754098a6e1520b910","title":"PanGu-{\\Sigma}: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"fb243dfd1234b8f76dfda740a62402663da74085","title":"Exploring Data Augmentation for Code Generation Tasks"},{"paperId":"1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a","title":"SantaCoder: don't reach for the stars!"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"9b4055674cd9849f8595240695bed69cd02492bc","title":"A Survey on Natural Language Processing for Programming"},{"paperId":"46d0a832fada6147bceb0bd4e39928e482733246","title":"How Important are Good Method Names in Neural Code Generation? A Model Robustness Perspective"},{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"045a6f92b58f08d5b305dd5d661316a506ee4d43","title":"Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation"}],"references":[{"paperId":"0121c151c96b32cd7851e4bcda2a468b279c1e6f","title":"CODE-MVP: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"4b27f18bff43d605805c92696a979714ced0b805","title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"6d7d4fca9840504f630e9bea6acaa07322a6e889","title":"Text and Code Embeddings by Contrastive Pre-Training"},{"paperId":"42a7015e48a1e00b70ebb442a82afb4b10017c0b","title":"When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"1b7ffb7d059f32083712127c9bef9e669d57f2b2","title":"Language Model for Text Analytic in Cybersecurity"},{"paperId":null,"title":"TS-BERT: A fusion model for pre-trainning time series-text representations, 2022"},{"paperId":null,"title":"The 1st Intl. Workshop on Natural Language-based Software Engineering Co-located with ICSE 2022"},{"paperId":null,"title":"Deep Learning For Code (DL4C) Workshop at ICLR 2022"},{"paperId":null,"title":"2nd International Workshop on Software Engineering Automation: A Natural Language Perspective (NLP-SEA 2021) at ASE 2021"},{"paperId":"a09395dc9312625d7e2dfcda0160b51344352289","title":"NLPaSE 2021 Organization"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"feba0c47bf12a02c3a725174bb53df78658a72a8","title":"Pre-Trained Models: Past, Present and Future"},{"paperId":"b5f9c1cc4c74d973986bc4b352b85a6ee2f475d6","title":"TreeBERT: A Tree-Based Pre-Trained Model for Programming Language"},{"paperId":"c07651110d3b98b63607557b57808d15d99013dd","title":"ProteinBERT: a universal deep-learning model of protein sequence and function"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"0b077c9577f4297dcf3da835e253d21965bbc6e0","title":"CoTexT: Multi-task Learning with Code-Text Transformer"},{"paperId":"78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f","title":"PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"ad7ddcc14984caae308c397f1a589aae75d4ab71","title":"Training data-efficient image transformers & distillation through attention"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"021bbcefc993c389bad6c1daefd8ff92d0fc2441","title":"Contrastive Code Representation Learning"},{"paperId":"036fda02d93139ad0ef37d892d41796a2a39fdcd","title":"CLSEBERT: Contrastive Learning for Syntax Enhanced Code Pre-Trained Model"},{"paperId":null,"title":"Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f6245b3e6270e4dc2e279c4b728030523dffcff4","title":"LEGAL-BERT: The Muppets straight out of Law School"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8","title":"Generative Pretraining From Pixels"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"962dc29fdc3fbdc5930a10aba114050b82fe5a3e","title":"End-to-End Object Detection with Transformers"},{"paperId":"75e924bd79d27a23f3f93d9b1ab62a779505c8d2","title":"Connecting the Dots: Multivariate Time Series Forecasting with Graph Neural Networks"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"f64e1d6bc13aae99aab5449fc9ae742a9ba7761e","title":"UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"34733eaf66007516347a40ad5d9bbe1cc9dacb6b","title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"f5a28db512357b700b62fb655ef4a90864e2fe7e","title":"Deep Transformer Models for Time Series Forecasting: The Influenza Prevalence Case"},{"paperId":"add2f205338d70e10ce5e686df4a690e2851bdfc","title":"Momentum Contrast for Unsupervised Visual Representation Learning"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"463fefdbd81a4a0a32cf59bc58a9545757c8cf2e","title":"Pre-trained Contextual Embedding of Source Code"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining"},{"paperId":null,"title":"CodeBERT: A pre-trained 19 model for programming and natural languages. In Findings of the Association for Computational Linguistics: EMNLP"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"7102bb3fe73bd057ff161d9db5214a267c1ef312","title":"FinBERT: Financial Sentiment Analysis with Pre-trained Language Models"},{"paperId":"5466ee5f16fc3c776fd1da667917592e5fd06720","title":"Selfie: Self-supervised Pretraining for Image Embedding"},{"paperId":"1c71771c701aadfd72c5866170a9f5d71464bb88","title":"Unified Language Model Pre-training for Natural Language Understanding and Generation"},{"paperId":"b3c2c9f53ab130f3eb76eaaab3afa481c5a405eb","title":"ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission"},{"paperId":"87078d95bee341a1767034d9432fb34937ecf65a","title":"SciBERT: Pretrained Contextualized Embeddings for Scientific Text"},{"paperId":"73bade9162bc4df6ca6c9ab8a7eb2d54d35acad2","title":"code2vec: learning distributed representations of code"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"2867eb64cc4bb1945363fdae12a168962ea823aa","title":"The GHTorent dataset and tool suite"},{"paperId":"330da625c15427c6e42ccfa3b747fb29e5835bf0","title":"Efficient Estimation of Word Representations in Vector Space"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"06ea568379211ffa07d9605f66f26f6f736ea5e0","summary":"A pretrained decoder-only language model adopting the P AN G U - α architecture for text-to-code generation, i.e. the synthesis of programming language solutions given a natural language problem description is presented."},{"url":"https://www.semanticscholar.org/paper/3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion","venue":"MAPS@PLDI","year":2022,"referenceCount":21,"citationCount":33,"influentialCitationCount":2,"publicationDate":"13/05/2022","authors":"Albert Ziegler,Eirini Kalliamvakou,Shawn Simister,Ganesh Sittampalam,X. A. Li,A. Rice,Devon Rifkin,E. Aftandilian","citations":[{"paperId":"e1a12117f15a6ee07133851a51439394bb9e7406","title":"ChemCrow: Augmenting large-language models with chemistry tools"},{"paperId":"dca09e13fe9b4f46b1e10b966f3ede63f63f9489","title":"Understanding the Usability of AI Programming Assistants"},{"paperId":"f24dabf3317abaa3d7393ab7d48115f353749bde","title":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X"},{"paperId":"28dafb94a87daa71ad3edf9f04f3c8c32753398b","title":"Improving Code Generation by Training with Natural Language Feedback"},{"paperId":"8ca62fdf4c276ea3052dc96dcfd8ee96ca425a48","title":"GPT-4 Technical Report"},{"paperId":"8475c1d1d98932111b194dc139c8f8b30818725d","title":"xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval"},{"paperId":"d2edaa84c13fd08735216ec85f2f3b2b5b53ff78","title":"On the Feasibility of Specialized Ability Extracting for Large Language Code Models"},{"paperId":"7c817fa57edd087820d3b4d16e4d1f40d4cb5200","title":"Generation Probabilities Are Not Enough: Exploring the Effectiveness of Uncertainty Highlighting in AI-Powered Code Completions"},{"paperId":"f5d93cf5d81575aeee61faeddde4437fbcb74cd0","title":"The Programmer’s Assistant: Conversational Interaction with a Large Language Model for Software Development"},{"paperId":"038f249ab708cebae2a58265b768b9b1cbadad3a","title":"The Impact of AI on Developer Productivity: Evidence from GitHub Copilot"},{"paperId":"ef10b30de48ce654d435e62594441b5b317f3cda","title":"CatAlyst: Domain-Extensible Intervention for Preventing Task Procrastination Using Large Generative Models"},{"paperId":"522069bf612482f913fd83b2982127a19b2ab9b3","title":"Source Code Recommender Systems: The Practitioners' Perspective"},{"paperId":"861916af6428277a3ce2e18034e4b40dc6616eb9","title":"On the Robustness of Code Generation Techniques: An Empirical Study on GitHub Copilot"},{"paperId":"0192638593c430e15e2804a74a3e8a7ecb52d435","title":"On the Design of AI-powered Code Assistants for Notebooks"},{"paperId":"5a05d7f6a2ee8fdb20f2e27baa95bd1e1a71c634","title":"Practitioners' Expectations on Code Completion"},{"paperId":"5e8bc5f84f3a550319b0d2b54cc0062b410d2328","title":"Taking Flight with Copilot"},{"paperId":"3078a916b57e465614efea50628c889799e32289","title":"\"It would work for me too\": How Online Communities Shape Software Developers' Trust in AI-Powered Code Generation Tools"},{"paperId":"ce3f027b68dad014a58aa35f52380932c8d0b209","title":"Do Users Write More Insecure Code with AI Assistants?"},{"paperId":"cba98048f3e85a974c287b271692bf6c197db940","title":"Aligning Offline Metrics and Human Judgments of Value of AI-Pair Programmers"},{"paperId":"66c52c6ef45cdd77c086996bcaaf01470e82dbd2","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?"},{"paperId":"5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","title":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming"},{"paperId":"825333b7efe2cade106eaf36c7e731f757974806","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot"},{"paperId":"ef29fb8cc6bdda4b011288f51da521d3c25fc53d","title":"Learning to Prevent Profitless Neural Code Completion"},{"paperId":"e73a16490c29530c37b49f6a30592790e7caaaa4","title":"Don't Complete It! Preventing Unhelpful Code Completion for Productive and Sustainable Neural Code Completion Systems"},{"paperId":"0509c25103939d59ed4e27b1393e74ad5734c453","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot"},{"paperId":"a889a606734cd47f802765866487c677497a70d6","title":"Lost at C: A User Study on the Security Implications of Large Language Model Code Assistants"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?"},{"paperId":"1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization"},{"paperId":"d6954c43aa1ca197319c45d3988bc8fcec3de976","title":"GitHub Copilot AI pair programmer: Asset or Liability?"},{"paperId":"ec229b004ebaee27b9efa84b834825dbc2937538","title":"Is GitHub's Copilot as Bad As Humans at Introducing Vulnerabilities in Code?"},{"paperId":"a1ef81e17a9ca41e09aba802040a2eca2744716f","title":"Generation Probabilities are Not Enough: Improving Error Highlighting for AI Code Suggestions"},{"paperId":"1c8e15f15d67c5974445634bb971e2275e957aff","title":"Security Implications of Large Language Model Code Assistants: A User Study"}],"references":[{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"57d8532ea99294a40696ae0f0a3fcd71440dc52a","title":"Improving Code Autocompletion with Transfer Learning"},{"paperId":"4a160efbe80c38cd5eb2f92c7c095b49b113397d","title":"In-IDE Code Generation from Natural Language: Promise and Challenges"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"2a36402451c4fcab33c9e7f59fc027167c070aa6","title":"Perfection Not Required? Human-AI Partnerships in Code Translation"},{"paperId":"6c107f38e218d967f5ad0618dfce2e9edf796063","title":"The SPACE of Developer Productivity"},{"paperId":"696aff001f8aa43a5ea0565841e9fc8842999027","title":"Mind the Gap: On the Relationship Between Automatically Measured and Self-Reported Productivity"},{"paperId":"87fe4a67e4cb5b8cdbc5da4d98888c037fb7d043","title":"Learning Autocompletion from Real-World Datasets"},{"paperId":"04413d13c76c4eadf1adcbbe88c3a72e6462f166","title":"Fast and Memory-Efficient Neural Code Completion"},{"paperId":"28797c288403828f29d3faca6ab9d64d134a4ea3","title":"The SPACE of Developer Productivity: There's more to it than you think"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"660a2244efa8af0b77fd314a1c75dcc01aa677fe","title":"From Human-Human Collaboration to Human-AI Collaboration: Designing AI Systems That Can Work Together with People"},{"paperId":"d1aa325a5adefeba786d4c50a8ca9a8df9598f32","title":"Best practices for the human evaluation of automatically generated text"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"d02285385fd2b53a6100b8d62421bba6693b788c","title":"When Code Completion Fails: A Case Study on Real-World Completions"},{"paperId":"8059b85332572e60c8a1daa0ccb8ddc008513f00","title":"What makes a good conversation? How controllable attributes affect human judgments"},{"paperId":"71f85c2fab38e26e286609144a445d610f2a9ded","title":"A Study of Visual Studio Usage in Practice"},{"paperId":"adbe76a937270c76d50c1cd956be447271c3a8f0","title":"PLS-regression: a basic tool of chemometrics"}],"id":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","summary":"It is found that the rate with which shown suggestions are accepted, rather than more specific metrics regarding the persistence of completions in the code over time, drives developers’ perception of productivity."},{"url":"https://www.semanticscholar.org/paper/1100dee3fd78655cddc4b7bfaef1161351d4fab5","title":"Automated Feedback Generation for Competition-Level Code","venue":"International Conference on Automated Software Engineering","year":2022,"referenceCount":48,"citationCount":2,"influentialCitationCount":0,"publicationDate":"03/06/2022","authors":"Jialu Zhang,De Li,John C. Kolesar,Hanyuan Shi,R. Piskac","citations":[{"paperId":"c1b1df160522a933c41efeb671c3ecf9bdae238b","title":"SOBO: A Feedback Bot to Nudge Code Quality in Programming Courses"},{"paperId":"2a0456b0408cd4c33f2ff4400374e7be2497a362","title":"Repairing Bugs in Python Assignments Using Large Language Models"}],"references":[{"paperId":"b78d0f729d44b54b8044afb0fbd426c2201cba02","title":"Generating Concise Patches for Newly Released Programming Assignments"},{"paperId":"1e3ec5709b2ca43233c566cd77dbabbd6892bfa6","title":"Neurosymbolic repair for low-code formula languages"},{"paperId":"2f2750b48a6f958ff12cba90e99695123d1e2f47","title":"Using pre-trained language models to resolve textual and semantic merge conflicts (experience paper)"},{"paperId":"cdc609428066914e1b5e17984d31894ad360aad4","title":"Learning CI Configuration Correctness for Early Build Feedback"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"c7684a8a4f24dc0fc9416e3cd0782cdbd467f27c","title":"Verifix: Verified Repair of Programming Assignments"},{"paperId":null,"title":"An example of repairing a faulty submission that need an algorithm-level redesign"},{"paperId":null,"title":"The Yandex Algorithm Cup"},{"paperId":null,"title":"The 10 Most Prestigious Programming Contests and Coding Challenges"},{"paperId":"46ca52b858c0ac1088cdcfb5bed7006b55cadf33","title":"Context-aware and data-driven feedback generation for programming assignments"},{"paperId":"ebfcbe0a8b238d5a52286fdfaab7be0170dbc91b","title":"FAPR: Fast and Accurate Program Repair for Introductory Programming Courses"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"a989f5daa7c438d6b8433efbb1461f1b0af1aa8e","title":"Break-It-Fix-It: Unsupervised Learning for Program Repair"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"305da9d093cde16cd568f16bf0cbbc0950d8d1f1","title":"Guide to Competitive Programming: Learning and Improving Algorithms Through Contests"},{"paperId":"b5abecf78c1bd975449e34350289a904e6ef35cc","title":"Automated program repair"},{"paperId":"325aecaa34bcf6e5f0e6683e526dc9946a6c1f4f","title":"Re-Factoring Based Program Repair Applied to Programming Assignments"},{"paperId":"0da0dcfa3642bf91ce53d88dee686279c65ab89b","title":"SemCluster: clustering of imperative programming assignments based on quantitative semantic features"},{"paperId":"f1bb2cd6b4455fa91784a90f3e114a7e445b80a3","title":"Automatic diagnosis and correction of logical errors for functional programming assignments"},{"paperId":"d6b7abd5c17f9e7d83c5b709c27da3a82a9788f2","title":"NAPS: Natural Program Synthesis Dataset"},{"paperId":"2a1392a112919229b2b5512e441dfe3a388ad2b5","title":"Automated clustering and program repair for introductory programming assignments"},{"paperId":"a1b9e544b74bb9e1b7ef20ee18d90644367bab17","title":"Semantic Program Repair Using a Reference Implementation"},{"paperId":"d591ed7f34cd0c169baf0ee75f82a11c338249ca","title":"Search, align, and repair: data-driven feedback generation for introductory programming exercises"},{"paperId":"c492e49c9eb3cfd51e77ab814201e81fe33cdcc5","title":"Dynamic Neural Program Embedding for Program Repair"},{"paperId":"2f17bcaa861dd6e6dff107e8ff39d92a24af5c74","title":"Evaluating and improving semistructured merge"},{"paperId":"380fdb98d66241e3769143d9c6564f23c053ec89","title":"The Continuous Hint Factory - Providing Hints in Vast and Sparsely Populated Edit Distance Spaces"},{"paperId":"6e5f3c4507aeb175da8712cff43cb8b2e60b5a12","title":"A feasibility study of using automated program repair for introductory programming assignments"},{"paperId":"bef6f21fccdcac24ebcd6b4287801ff1c9734966","title":"Automatic inference of code transforms for patch generation"},{"paperId":"cf404026f1e66dc04417d699ef2b4731cbec842c","title":"Learning Syntactic Program Transformations from Examples"},{"paperId":"33c65aace24e26979cc9ba6242310d06710ad8d2","title":"Qlose: Program Repair with Quantitative Objectives"},{"paperId":"5b0602f9d0f1384dc95335c6ed220fef40a7e186","title":"sk_p: a neural program corrector for MOOCs"},{"paperId":"3f215e83b39a0887257a03274002f353c5a57537","title":"Angelix: Scalable Multiline Program Patch Synthesis via Symbolic Analysis"},{"paperId":"d3e5f4ab52923337ff04cca46291204c583a3968","title":"Semi-supervised verified feedback generation"},{"paperId":"e8ec01a64c16f4e2fa40030b3e19fa6abe3d3b3a","title":"A Programming Contest Strategy Guide"},{"paperId":"50358e3f30ab8c99f9d383e5683b31c2311e5651","title":"Automatic patch generation by learning correct code"},{"paperId":"5e2ae818c7e387c0cd3ed7f09a85dbef3df3e296","title":"An Automated Framework for Recommending Program Elements to Novices (N)"},{"paperId":"0c1bf6da80dad0d272f3c87bc0e8ba365219b017","title":"CARAMEL: Detecting and Fixing Performance Problems That Have Non-Intrusive Fixes"},{"paperId":"90441b975380c806320420724fc9d0ec77dbefdb","title":"The strength of random search on automated program repair"},{"paperId":"b8afcb72c82c0f3ce8b85b211c99ee5b1591598b","title":"Current challenges in automatic software repair"},{"paperId":"d7aa22f4db5ca61d029b93db3770a2a05cb169a8","title":"Cachetor: detecting cacheable data to remove bloat"},{"paperId":"3136ad216d30bdff223e5c3f02e07f980a6a45a5","title":"Automatic patch generation learned from human-written patches"},{"paperId":"169a72730c870f30a4452bfc0648e35ac4561f42","title":"Automated feedback generation for introductory programming assignments"},{"paperId":"3aa35c213730e688fa191a1ce241db31087956f0","title":"GenProg: A Generic Method for Automatic Software Repair"},{"paperId":"d96d84083a1785bff755f3900e847604401e0590","title":"Semistructured merge: rethinking merge in revision control systems"},{"paperId":"aa6afb8ba5addd1b2cf47adc3d344d87a39a0b53","title":"ACM International Collegiate Programming Contest"},{"paperId":"428250f9387b2de004ac85c8000fd14c4e7580da","title":"MJRTY: A Fast Majority Vote Algorithm"},{"paperId":"277ff0c74cc72663d0aabbeae25a3e97b245457c","title":"Simple Fast Algorithms for the Editing Distance Between Trees and Related Problems"}],"id":"1100dee3fd78655cddc4b7bfaef1161351d4fab5","summary":"This work presents Clef, the first data-driven tool that can generate feedback on competition-level code automatically by repairing programmers’ incorrect submissions, and introduces a new data structure, merge trees, to capture the changes between submissions."},{"url":"https://www.semanticscholar.org/paper/618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?","venue":"ArXiv","year":2022,"referenceCount":97,"citationCount":11,"influentialCitationCount":2,"publicationDate":"12/08/2022","authors":"Advait Sarkar,A. Gordon,C. Negreanu,Christian Poelitz,Sruti Srinivasa Ragavan,B. Zorn","citations":[{"paperId":"78602a7112b6ad1e6150ad04023a3b96636425e1","title":"GenAICHI 2023: Generative AI and HCI at CHI 2023"},{"paperId":"fa0b17014e5c1b4c38ae5feb20f2047853728813","title":"Towards Objective-Tailored Genetic Improvement Through Large Language Models"},{"paperId":"8a2a2006e371ce07e0b29538cd11337a5c977caf","title":"“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models"},{"paperId":"af622206f1e5a6632600075897385cdabcae463d","title":"Large Language Models and Simple, Stupid Bugs"},{"paperId":"e1a17cb16742cf484aedc9ae106363d82454172e","title":"Practices and Challenges of Using GitHub Copilot: An Empirical Study"},{"paperId":"a764d4f28612a7b5c4767ea6a2540b169fdb052c","title":"CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models"},{"paperId":"0192638593c430e15e2804a74a3e8a7ecb52d435","title":"On the Design of AI-powered Code Assistants for Notebooks"},{"paperId":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"3078a916b57e465614efea50628c889799e32289","title":"\"It would work for me too\": How Online Communities Shape Software Developers' Trust in AI-Powered Code Generation Tools"},{"paperId":"5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","title":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming"},{"paperId":"971e875e28f26240987d2c9470d1ee74ad204205","title":"Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction"}],"references":[{"paperId":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"84e8912ba21d9dd6eb2e3a92282afef3fc361691","title":"End-user encounters with lambda abstraction in spreadsheets: Apollo’s bow or Achilles’ heel?"},{"paperId":"0509c25103939d59ed4e27b1393e74ad5734c453","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot"},{"paperId":"340535d68dedefd64b530154eeda5c4569da4a36","title":"Is Explainable AI a Race Against Model Complexity? 192-199"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"1f3a97d255a6d0b2d5535af75737ed43b1e96fc4","title":"Is GitHub Copilot a Substitute for Human Pair-programming? An Empirical Study"},{"paperId":"4bc040835fbff57ce6612306d794b8c6c8226086","title":"Discovering the Syntax and Strategies of Natural Language Programming with Generative Language Models"},{"paperId":"9203a57752ba96bde4e54dd7fb3f84e8161895b8","title":"“It’s Freedom to Put Things Where My Mind Wants”: Understanding and Improving the User Experience of Structuring Data in Spreadsheets"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"e67c6c075916da9ba9c2067418ddf961307b1c59","title":"GridBook: Natural Language Formulas for the Spreadsheet Grid"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"},{"paperId":"4a160efbe80c38cd5eb2f92c7c095b49b113397d","title":"In-IDE Code Generation from Natural Language: Promise and Challenges"},{"paperId":null,"title":"Blog / tabnine announcements / announcing our next-generation ai models. Tabnine. Retrieved from https://www.tabnine.com/blog/announcing-tabnine-next -generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"d7a7ebd1565c3795bc2bcdec4334d42a65ad17c5","title":"Pretrained Language Models for Text Generation: A Survey"},{"paperId":"aa2eac44e9725b6201c272a1a1bb75adee837058","title":"TweakIt: Supporting End-User Programmers Who Transmogrify Code"},{"paperId":"6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"7c0b558bf433c5aaaf774cd5d3c767bfd3dbe123","title":"Retrieval-Augmented Generation for Code Summarization via Hybrid GNN"},{"paperId":"24bf9857fbb90270b97a7d462ab150de75580364","title":"FAccT '21: 2021 ACM Conference on Fairness, Accountability, and Transparency, Virtual Event / Toronto, Canada, March 3-10, 2021"},{"paperId":null,"title":"Github copilot research recitation"},{"paperId":"77b50a2c0dd166d8f36aec05d1539312bb0422c4","title":"The dilemma of the direct answer"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"3e7898bee9e13b92545d4815af6af9b20f674c48","title":"Understanding and Inferring Units in Spreadsheets"},{"paperId":"29fc7ff02e68537a86d32978bfd2e8e45258e77d","title":"LambdaNet: Probabilistic Type Inference using Graph Neural Networks"},{"paperId":"27e83a8ea690dd26267f9e6dcb9a6d95948ec963","title":"Do We Need Natural Language?: Exploring Restricted Language Interfaces for Complex Domains"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"6370d741f01b6cd59671d995c97dfd9a08827c36","title":"How do people learn to use spreadsheets? (Work in progress)"},{"paperId":"0dfef27d777569d92965c2dfa2b3c6f1b355ca2e","title":"How should compilers explain problems to developers?"},{"paperId":"7af1150059f3c6e7062e9bb8ac8af2a23098d826","title":"No half-measures: A study of manual and tool-assisted end-user programming tasks in Excel"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"ebd874cc9a886ea6283227c5eef034f5c7a4293c","title":"Interactive analytical modelling"},{"paperId":null,"title":"Introducing visual studio intellicode"},{"paperId":"d93c6df00eb08a93a9e3567de506456db79009cd","title":"Exploring exploratory programming"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"6cf6dc8bb7995a1f529d51b11f1677e045337337","title":"SmartPaste: Learning to Adapt Source Code"},{"paperId":"f4f874c431b75432ed6c1badd1359c2af1cdd70e","title":"Bing developer assistant: improving developer productivity by recommending sample code"},{"paperId":"5008cd9c1f65c34088bebdd1e86e033265d61c6a","title":"The practices of programming"},{"paperId":"6d1f937d56ef4251d52bbe3c85a6ff4090925884","title":"Improving API usability"},{"paperId":"814e62bb4f0ec6facd640fbd3e3fcbd189716d20","title":"\"Like Having a Really Bad PA\": The Gulf between User Expectation and Experience of Conversational Agents"},{"paperId":"27fdb872558ea94409418ef1831e1bde92d3ab4a","title":"Foraging Among an Overabundance of Similar Variants"},{"paperId":"a5d0d3d23b3fbe1ac6c2ac0310e94fb1de7b0415","title":"Pair Programming vs. Solo Programming: What Do We Know After 15 Years of Research?"},{"paperId":"49512270b39636375880d611d7b2192d324f4ba6","title":"Convolutional Neural Networks over Tree Structures for Programming Language Processing"},{"paperId":"f4c6dbcc0a5f3d7f29aac705ccd9bb991d63ae07","title":"Software and How it Lives On - Embedding Live Programs in the World Around Them"},{"paperId":"2095acfccc9750d681c78d919e3f884c64818aad","title":"API Usability at Scale"},{"paperId":"2ef43455fe58699374d8fc5c6ed57eb753978a86","title":"Interactive visual machine learning in spreadsheets"},{"paperId":"927aaa20318d18564bd331ed37428e05820d647e","title":"Divide-and-Conquer Approach for Multi-phase Statistical Migration for Source Code (T)"},{"paperId":"a747e4db56a78c94e63608649fbdb3889be0c811","title":"Idea Garden: Situated Support for Problem Solving by End-User Programmers"},{"paperId":"e0641bbac3a091c932b81c449a0c6facf71de4d1","title":"I heart hacker news: expanding qualitative research findings by analyzing social news websites"},{"paperId":"4ac8cf91eeac90bb91f2419316747ae0934ba765","title":"Third-wave HCI, 10 years later---participation and sharing"},{"paperId":"cfab0a9412f2aff106080b2adb66e51f6dca497c","title":"Supporting Selective Undo in a Code Editor"},{"paperId":"75ae9a4a66a50a02cd1396e3e22036080f56d496","title":"Building Bing Developer Assistant"},{"paperId":"be6665462fd6c56e8756dadf23373bb0c90b2b19","title":"Predicting Program Properties from \"Big Code\""},{"paperId":"62cc6be687cddeadf5c77b221d49615e11b5e86d","title":"An empirical investigation of code completion usage by professional software developers"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"c8ffe876bfbe85231b236a9af35ad7a2d2ccfcef","title":"Structured labeling for facilitating concept evolution in machine learning"},{"paperId":"cf616c398eda90178db3520994dbf42bd22e627c","title":"The patchworks code editor: toward faster navigation with less code arranging and fewer navigation mistakes"},{"paperId":"1edaa5cd10740839433c83b64b996258fd8b4767","title":"Detecting and refactoring code smells in spreadsheet formulas"},{"paperId":"693932c48cb315c373e48fed8ba4e2725fb492a4","title":"An Empirical Study of API Usability"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"cd95f3f397e967005d6f108d0dbf3c5c7ffbcce9","title":"A perspective on the evolution of live programming"},{"paperId":"8f880975990a8e0ef817d997e64f65cf00742b7c","title":"Axiomatic Basis for Computer Programming"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":null,"title":"Research notebook: Computational thinking-what and why. The link magazine"},{"paperId":"3685e0436523d8c06bbf954b73b506bdc9a66ca1","title":"The effectiveness of pair programming: A meta-analysis"},{"paperId":"3896a88ad7b6a01f49ac37eb317eceddd51a5ff5","title":"Reducing Overconfidence in Spreadsheet Development"},{"paperId":"7b03a16039decbc8f949fc267f9661902bf0977a","title":"Aspects of PROLOG History: Logic Programming and Professional Dynamics"},{"paperId":"4d52e4f8037d797b7f9829bfa1854141790c1d7e","title":"Feasibility Studies for Programming in Natural Language"},{"paperId":"01dbd4ceafcd428bbb7b15db10d53952d0ad8ae7","title":"Designing the whyline: a debugging interface for asking questions about program behavior"},{"paperId":"448c454d10dccdda8b3a2457be2b2b4665819376","title":"First steps in programming: a rationale for attention investment models"},{"paperId":null,"title":"What is programming? In Ppig"},{"paperId":"1cc3f5cdd4204f8e55e46d9cbaef730d17ca647c","title":"Your Wish is My Command: Programming By Example"},{"paperId":"06b5370585b0d853de37e9671d831221399146c0","title":"All I really need to know about pair programming I learned in kindergarten"},{"paperId":"5df85ae89af55c6d82a1a14836ea6bcfbfc2c0ec","title":"Principles of mixed-initiative user interfaces"},{"paperId":"2058347c34d15dba2051c8474048902c1e70e585","title":"Cognitive Dimensions of Information Artefacts: a tutorial"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"6d32d1f65434d4cc94bd2cee0f5a889fa29d8edd","title":"Watch what I do: programming by demonstration"},{"paperId":"169f585547b17ed2b3c64bd37a5b58688646ef05","title":"A Small Matter of Programming: Perspectives on End User Computing"},{"paperId":"e7316c33d627b94d2fb9674e4af6408d70dbb52f","title":"The birth of Prolog"},{"paperId":null,"title":"1.1 direct manipulation: a step beyond programming"},{"paperId":"46835cd4618dab26589d5a64a712f87668515db4","title":"Demonstrational interfaces: A step beyond direct manipulation"},{"paperId":"5d294399ae7141784563525f8fdefc9eabc6285f","title":"When Visual Programs are Harder to Read than Textual Programs"},{"paperId":"4ca24a6a487c3fa92d60a17b760cc3515708896a","title":"Cognitive dimensions of notations"},{"paperId":"67a614470d28b44a6e3b50f91cb4b99403e41389","title":"Direct Manipulation Interfaces"},{"paperId":"2a49ee0e7fa2048c483856cbbf5b24b05340ae8f","title":"Natural Language Programming: Styles, Strategies, and Contrasts"},{"paperId":null,"title":"Don't fully trust AI in dev work! /yet"},{"paperId":null,"title":"Using Github copilot to get the tweets for a keyword and find the sentiment of each tweet in 2 mins"},{"paperId":null,"title":"Building games and apps entirely through natural language using OpenAI's code-davinci model"},{"paperId":null,"title":"Hacker News discussion"},{"paperId":null,"title":"Building a No-Code Machine Learning Model by Chatting with GitHub Copilot"},{"paperId":null,"title":"A.1. Blog posts and corresponding Hacker News discussions"},{"paperId":null,"title":"Coding with GitHub Copilot"}],"id":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","summary":"This paper explores how programming with large language models (LLM-assisted programming) is similar to, and differs from, prior conceptualisations of programmer assistance, and draws upon publicly available experience reports of LLM- assisted programming, as well as prior usability and design studies."},{"url":"https://www.semanticscholar.org/paper/f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":83,"citationCount":15,"influentialCitationCount":0,"publicationDate":"10/06/2021","authors":"Tal Schuster,A. Kalyan,Oleksandr Polozov,A. Kalai","citations":[{"paperId":"6d92d23965f46b317d73f8deb57d3c86e017009b","title":"Review of Programming for the Puzzled"},{"paperId":"27c2ddc921befc44f7b2cdb8bb39facdd8b0b188","title":"Measuring The Impact Of Programming Language Distribution"},{"paperId":"0d1efbacde9cb8ec6540df30abc3db0d89e5a635","title":"Understanding and Supporting Debugging Workflows in Multiverse Analysis"},{"paperId":"d3a7a4543d83f568f79d1febe8379465ff0140c9","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"4fbe0cb0777b228e39243692bf29e2829060b8de","title":"When Language Model Meets Private Library"},{"paperId":"4c2534f9b03ac2f3810c07abc398a11bcf47258e","title":"Transformers Learn Shortcuts to Automata"},{"paperId":"ace0745f4449f20e4f4297476941fcd7dc7ab05c","title":"Higher Cognition: A Mechanical Perspective"},{"paperId":"c5b47a34fd7e0e595360683b4effa51c0261d1e3","title":"Language Models Can Teach Themselves to Program Better"},{"paperId":"2ef60a4ea4ea53056be811ff55679eb59fb4b586","title":"Confident Adaptive Language Modeling"},{"paperId":"35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs"},{"paperId":"7107d06366b48b3593c8128ed2ca67e0b413628c","title":"Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions"},{"paperId":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective"},{"paperId":"f40aeae3e522ada1f6a9f326841b01ef5c8657b6","title":"Unifying Language Learning Paradigms"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"}],"references":[{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"77a096d80eb4dd4ccd103d1660c5a5498f7d026b","title":"Dynabench: Rethinking Benchmarking in NLP"},{"paperId":"21ec9c0f869bdb33b06c7dbc8880169db0397d08","title":"UNICORN on RAINBOW: A Universal Commonsense Reasoning Model on a New Multitask Benchmark"},{"paperId":"eebc1811c55c2e5e8b3b78d0b0382ad50f22e32a","title":"Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"1ec69f1a1a9d4ff5c5bc70db0e5087157b620570","title":"You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion"},{"paperId":null,"title":"Research recitation: A first look at rote learning in github copilot suggestions"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"5fe0a4af3bd1479d5e39fbda2215c86bce54722b","title":"Generative Language Modeling for Automated Theorem Proving"},{"paperId":"f430773cdc0fdd62192a48a372601d6aa40f3d7b","title":"Task-Oriented Dialogue as Dataflow Synthesis"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"db392858262b17aa9c8ff8659738f68fbf832ebe","title":"Structural Language Models of Code"},{"paperId":"305da9d093cde16cd568f16bf0cbbc0950d8d1f1","title":"Guide to Competitive Programming: Learning and Improving Algorithms Through Contests"},{"paperId":null,"title":"Transformers: State-of-theart natural language processing"},{"paperId":null,"title":"CodeBERT: A pre-trained 455 model for programming and natural languages. In Findings of the Association for Com- 456 putational Linguistics: EMNLP 2020, pages 1536–1547"},{"paperId":null,"title":"Python One-Liners: Write Concise, Eloquent Python Like a Professional"},{"paperId":null,"title":"Karel the robot learns python"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"d0086b86103a620a86bc918746df0aa642e2a8a3","title":"Language Models as Knowledge Bases?"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"6dfc2ff03534a4325d06c6f88c3144831996629b","title":"From Recognition to Cognition: Visual Commonsense Reasoning"},{"paperId":"421cb75cc91e8e5683d41ee6a918121aedf6d24d","title":"Social IQA: Commonsense Reasoning about Social Interactions"},{"paperId":"c21a4d70d83e0f6eb2a9e1c41d034842dd561e47","title":"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"f9717d29840f4d8f1cc19d1b1e80c5d12ec40608","title":"A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play"},{"paperId":"af5c4b80fbf847f69a202ba5a780a3dd18c1a027","title":"SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference"},{"paperId":"d6b7abd5c17f9e7d83c5b709c27da3a82a9788f2","title":"NAPS: Natural Program Synthesis Dataset"},{"paperId":"93b8da28d006415866bf48f9a6e06b5242129195","title":"GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding"},{"paperId":"d4c85ef35c5224792186100c29df141761d6abd8","title":"Neural Program Search: Solving Data Processing Tasks from Description and Examples"},{"paperId":"d102c0ed7c2559cd22fd167c1c2acff2b1023042","title":"Glass-Box Program Synthesis: A Machine Learning Approach"},{"paperId":"c27db32efa8137cbf654902f8f728f338e55cd1c","title":"Mastering the game of Go without human knowledge"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":null,"title":"Five $1,000 problems (update 2017)"},{"paperId":null,"title":"George van den Driessche, Thore Graepel, and Demis Hassabis"},{"paperId":"15f4c35889ccc1ae258b680c2ca2fcbfe1e260f7","title":"Gaussian Error Linear Units (GELUs)"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"7f6245d527e84092111a6adb0f1939d0209d3360","title":"FlashMeta: a framework for inductive program synthesis"},{"paperId":"4d376d6978dad0374edfa6709c9556b42d3594d3","title":"Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"},{"paperId":"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd","title":"ImageNet Large Scale Visual Recognition Challenge"},{"paperId":"c51274cdd2a0305cab0bbaf0298cab63db1b9152","title":"Bootstrap Learning via Modular Concept Discovery"},{"paperId":"43dcda631f8a0d39949ba3f9e3e22101db4daba0","title":"A Machine Learning Framework for Programming by Example"},{"paperId":"f48eed915cbb9c6592cdb9df80c1edaeb46959af","title":"A measure of intelligence"},{"paperId":"168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74","title":"Scikit-learn: Machine Learning in Python"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"f04af7291d49a0912def77a432b9ad79baa2096d","title":"Interactive Theorem Proving and Program Development: Coq'Art The Calculus of Inductive Constructions"},{"paperId":"a298defcbe68f5a09b0b8f6e5379164f7fd79bc5","title":"Handbook of Satisfiability"},{"paperId":"7130de3696c9e1942fbdbc8fba3cc78cd9ce7eed","title":"Computational Complexity: A Modern Approach"},{"paperId":"f491c61a874b049981053d86894bdcef9a0e5530","title":"Bebras International Contest on Informatics and Computer Literacy: Criteria for Good Tasks"},{"paperId":"ef78e1bd3bea4122217a93a72533a59f1b18a18c","title":"RSA factoring challenge"},{"paperId":"f87f327f64573fd79a5c3feabb608cf04837881c","title":"Noise-tolerant learning, the parity problem, and the statistical query model"},{"paperId":"fd77430f6f5c5e35e8a45ff3478032b680fa0b0c","title":"To all authors"},{"paperId":"5ed59f49c1bb7de06cfa2a9467d5efb535103277","title":"Temporal difference learning and TD-Gammon"},{"paperId":"18c1fdffdce47d319bd6874302dadb81f5d4f000","title":"The 3x + 1 Problem and its Generalizations"},{"paperId":"2fb9729c0a3b046c390ad6748773527eae6cfc84","title":"What Is Program Synthesis?"},{"paperId":"5e29000d24d5ded11e7a32216a91bdadaa9877f1","title":"On the inherent intractability of certain coding problems (Corresp.)"},{"paperId":"c903d23753bae10adf9b2821658249230e909033","title":"Finite Groups of Automorphisms: Course Given at the University of Southampton, October-December 1969"},{"paperId":null,"title":"Mathematical Society, London Mathematical Society lecture note series, and S.P.G.N.J"},{"paperId":"a973eb892f233acb3093589393181ae633d3a244","title":"ACTIVITY ANALYSIS OF PRODUCTION AND ALLOCATION"},{"paperId":"60b1cdf630031816f71cd6dd321195e9aaa5ec31","title":"An Introduction to the Theory of Numbers"},{"paperId":null,"title":"Bootstrap solved after 151 tries: def f26(s: str, target='foobarbazwow', length=6): return target[(len(target) -length) // 2:(len(target) + length) // 2] == s SOL: \"str"},{"paperId":null,"title":"Association 605 for Computational Linguistics"},{"paperId":null,"title":"]): return len(bi) == len(set(bi)) and {(i, j) for i, j in g1} == {(bi[i], bi[j]) for i"},{"paperId":null,"title":"From conversation to code: Microsoft introduces its first product features powered by GPT-3"},{"paperId":null,"title":"668 (a) Did you include the full text of instructions given to participants and screenshots, if 669 applicable? [Yes] See the figures in G 670 (b) Did you describe any potential participant risks"},{"paperId":null,"title":"code, data, models) or curating/releasing new assets"},{"paperId":null,"title":"The Monkey and the coconuts\" ancient puzzle) def f86(n: int): for i in range"},{"paperId":null,"title":"If you are including theoretical results"},{"paperId":null,"title":", Yacine Jernite , Julien Plu , Canwen Xu , Teven Le Scao , Sylvain Gugger , Mariama Drame , Quentin Lhoest , and Alexander M . Rush . Transformers : State - ofthe - art natural language processing"},{"paperId":null,"title":"663 (d) Did you discuss whether and how consent was obtained from people whose data you're 664 using/curating?"},{"paperId":null,"title":"Board (IRB) approvals, if applicable? [Yes] See 5.1 and G for details about the 672 recruiting and agreement for the user study"},{"paperId":null,"title":"Hint\" return len(nums) == 3 and sum([i ** 3 for i in nums"},{"paperId":null,"title":"627 (a) Do the main claims made in the abstract and introduction accurately reflect the paper's 628 contributions and scope? [Yes] as mentioned at the end of the introduction"},{"paperId":null,"title":"list(map(lambda i: 2**i, range"},{"paperId":null,"title":"Bootstrap solved after 1039 tries: def f61(li: List[int]): return all(sum(li[:i]) == 2 ** i -1 for i in range"},{"paperId":null,"title":"and abs(sum(probs) -1) < 1e-6 return max(probs[(i + 2) % 3] -probs[(i + 1) % 3] for i in range(3)) < 1e-6 SOL"},{"paperId":null,"title":"639 (a) Did you state the full set of assumptions of all theoretical results? [Yes] 640 (b) Did you include complete proofs of all theoretical results?"}],"id":"f7664102a451332ed7e1286561b2f621eaff128d","summary":"A positive correlation between puzzlesolving performance and coding experience, and between the puzzle difficulty for humans and AI solvers are found, and further improvements on P3 could have a significant impact on many program synthesis areas."},{"url":"https://www.semanticscholar.org/paper/5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","title":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":5,"influentialCitationCount":0,"publicationDate":"25/10/2022","authors":"Hussein Mozannar,Gagan Bansal,Adam Fourney,E. Horvitz","citations":[{"paperId":"9109389eab4c63fa18b7cf13feee96dfb2f11387","title":"Learning Personalized Decision Support Policies"},{"paperId":"01713a5f38c0164151caa9b6cc740a8c864420cc","title":"Scientists' Perspectives on the Potential for Generative AI in their Fields"},{"paperId":"dca09e13fe9b4f46b1e10b966f3ede63f63f9489","title":"Understanding the Usability of AI Programming Assistants"},{"paperId":"348a1efa54376fa39053e5e25d52bd0eb6a0ba68","title":"Capabilities of GPT-4 on Medical Challenge Problems"},{"paperId":"7c817fa57edd087820d3b4d16e4d1f40d4cb5200","title":"Generation Probabilities Are Not Enough: Exploring the Effectiveness of Uncertainty Highlighting in AI-Powered Code Completions"}],"references":[{"paperId":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?"},{"paperId":"654cd297b307c0c6fa08732511ba852b8dce1977","title":"Out of the BLEU: how should we assess quality of the Code Generation models?"},{"paperId":"d6954c43aa1ca197319c45d3988bc8fcec3de976","title":"GitHub Copilot AI pair programmer: Asset or Liability?"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"4bc040835fbff57ce6612306d794b8c6c8226086","title":"Discovering the Syntax and Strategies of Natural Language Programming with Generative Language Models"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"ec229b004ebaee27b9efa84b834825dbc2937538","title":"Is GitHub's Copilot as Bad As Humans at Introducing Vulnerabilities in Code?"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"},{"paperId":null,"title":"Ml-enhanced code completion improves developer productivity, Jul 2022"},{"paperId":null,"title":"Ml-powered coding companion -amazon codewhisperer"},{"paperId":null,"title":"Task Instructions The tasks are shown to participants as image files to deter copying of the instructions as a prompt"},{"paperId":null,"title":"A Details User Study A.1 Interfaces Figure 13: Screenshot of Labeling Tool represented in Figure 4 Figure 14: Screenshot of Virtual Machine interface with VS Code"},{"paperId":null,"title":"Probability of Accept by State Table 4: Probability of accepting suggestion in next two events given the user was in the particular CUPS"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"2a36402451c4fcab33c9e7f59fc027167c070aa6","title":"Perfection Not Required? Human-AI Partnerships in Code Translation"},{"paperId":"6c107f38e218d967f5ad0618dfce2e9edf796063","title":"The SPACE of Developer Productivity"},{"paperId":"5ff9032d0f7f246d01ae7b2c231ab03469a7344a","title":"Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?"},{"paperId":"28797c288403828f29d3faca6ab9d64d134a4ea3","title":"The SPACE of Developer Productivity: There's more to it than you think"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"d02285385fd2b53a6100b8d62421bba6693b788c","title":"When Code Completion Fails: A Case Study on Real-World Completions"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"8d115c3b2ee80e0754360a154a9369bc1658b607","title":"Obtaining Well Calibrated Probabilities Using Bayesian Binning"},{"paperId":null,"title":"Xgboost: extreme gradient boosting"},{"paperId":"2244595cc05a687ff04390f63a4443e1cd0cb3b0","title":"Flow and the Foundations of Positive Psychology"},{"paperId":"b722582997d8ed7a49d0582a2140acbce4464683","title":"The GOMS family of user interface analysis techniques: comparison and contrast"},{"paperId":"9fa713bc9971717aeed966676d8018ee73687bec","title":"The Entropy Of Markov Trajectories"},{"paperId":"82f8fbc66004ac438ac742c1ad6016d07d1ae037","title":"The keystroke-level model for user performance time with interactive systems"},{"paperId":null,"title":"Research: Quantifying github copilot's impact on developer productivity and happiness"}],"id":"5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","summary":"This work studied GitHub Copilot, developed CUPS– a taxonomy of 12 programmer activities common to AI code completion systems, and conducted a study with 21 programmers who completed coding tasks and used the labeling tool to retrospectively label their sessions with CUPS."},{"url":"https://www.semanticscholar.org/paper/1c336c18e53ad878bf4688c864acd99f137ae29f","title":"Interactive Code Generation via Test-Driven User-Intent Formalization","venue":"ArXiv","year":2022,"referenceCount":25,"citationCount":8,"influentialCitationCount":0,"publicationDate":"11/08/2022","authors":"Shuvendu K. Lahiri,Aaditya Naik,Georgios Sakkas,Piali Choudhury,Curtis von Veh,M. Musuvathi,J. Inala,Chenglong Wang,Jianfeng Gao","citations":[{"paperId":"8a2a2006e371ce07e0b29538cd11337a5c977caf","title":"“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models"},{"paperId":"34d12432af63915caf14eab9a362f7e7d24e4c13","title":"Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions"},{"paperId":"a7b3a868a80dbe97689135c99b1a6b6e10dcdfe5","title":"A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT"},{"paperId":"6876fc47f4674fdf4f53208254b45dedc6de3755","title":"Adaptive Test Generation Using a Large Language Model"},{"paperId":"a93dab80894abc5116b95782db5871e6288812b6","title":"NASA is launching a spacecraft to Mars in 2020 ... NLG model NASA launching spacecraft"},{"paperId":null,"title":"NLG model user input NASA is launching a spacecraft to the moon on Monday ... NLG model NLG model"},{"paperId":"8210cef990b8e5cddbc95000e46309bdd25337f7","title":"Asking Clarification Questions for Code Generation in General-Purpose Programming Language"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"}],"references":[{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis"},{"paperId":"8c40f0259aac79db7203ffdb452fffe006cd7e18","title":"TOGA: A Neural Method for Test Oracle Generation"},{"paperId":"ad18af95ba8125d2c0eb9f9941205678cad38ad2","title":"Generating Accurate Assert Statements for Unit Test Cases using Pretrained Transformers"},{"paperId":null,"title":"Fiedel. Palm: Scaling language modeling with pathways, 2022"},{"paperId":"a8863de15a5ee8eed98107f423138a1a8f5a2ba8","title":"Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"ac3cdb50606f7770eef8e4cd951840a4f71287a0","title":"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"},{"paperId":"15a60130ea9cb4a00e0dac6d11a368894974cedf","title":"Unit Test Case Generation with Transformers and Focal Context"},{"paperId":"eff168312f519f6cc4cb867147a105c75abef735","title":"Question selection for interactive program synthesis"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":null,"title":"Abhishek Udupa, and Sumit Gulwani"},{"paperId":null,"title":"Program synthesis. Found"},{"paperId":"363c9c645dc8c303c3d7ad995f60beae32ce10fa","title":"An empirical analysis of flaky tests"},{"paperId":"698fe1da46e077cb3a0050ab986bcfdd82af114a","title":"Evolutionary Generation of Whole Test Suites"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"2d38fad1c1d1b9cdd0ca8e0f3061b32fce12e987","title":"The Sketching Approach to Program Synthesis"},{"paperId":"6dc7017e357767dbc210bcb27080260fb0816701","title":"Feedback-Directed Random Test Generation"}],"id":"1c336c18e53ad878bf4688c864acd99f137ae29f","summary":"This paper proposes the workflow of test-driven user-intent formalization (TDUIF), which leverages lightweight user feedback to jointly formalize the user intent as tests (a partial specification), and generates code that meets the formal user intent."},{"url":"https://www.semanticscholar.org/paper/ef29fb8cc6bdda4b011288f51da521d3c25fc53d","title":"Learning to Prevent Profitless Neural Code Completion","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/09/2022","authors":"Zhensu Sun,Xiaoning Du,Fu Song,Shangwen Wang,Mingze Ni,Li Li","citations":[],"references":[{"paperId":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"654cd297b307c0c6fa08732511ba852b8dce1977","title":"Out of the BLEU: how should we assess quality of the Code Generation models?"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"205ac1373eb7981aca2d08f2ab651871a001271e","title":"CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"97010ef4c4bd80b1f959df236501cf7741053d04","title":"On the Importance of Building High-quality Training Datasets for Neural Code Search"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"02183e69f1dfd6e9b2d0fb876153299bab4bb82b","title":"CoProtector: Protect Open-Source Code against Unauthorized Training Usage with Data Poisoning"},{"paperId":null,"title":"Code faster with ai completions — tabnine"},{"paperId":null,"title":"Turing-nlg: A 17-billion-parameter language model by microsoft - microsoft research"},{"paperId":null,"title":"aixcoder"},{"paperId":null,"title":"Amazon ec2 update – inf1 instances with aws inferentia chips for high performance cost-effective inferencing"},{"paperId":null,"title":"Ml-powered coding companion – amazon codewhisperer – amazon web services"},{"paperId":null,"title":"Code conventions for the java programming language: 9. naming conventions"},{"paperId":null,"title":"Github copilot · your ai pair programmer."},{"paperId":"ff69d758764157e612f92f97a987838312c568a9","title":"Compute and Energy Consumption Trends in Deep Learning Inference"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"8e136c0944c9181b4b853df4f642d8bb7afddecb","title":"Reassessing automatic evaluation metrics for code summarization tasks"},{"paperId":"a3223d46f3091307525e32b5a059a4bc4a5980a8","title":"Efficient Neural Architecture Search with Performance Prediction"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"79b8ef3905a42b771248719495a2117271906445","title":"Carbon Emissions and Large Neural Network Training"},{"paperId":"774591fdd988eaaff3917e7c5171d044b0843e63","title":"Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM"},{"paperId":"eb90f7a7f281ccbf64084e11adf822fae2d9bc3f","title":"Sustainable AI: AI for sustainability and the sustainability of AI"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"1ec69f1a1a9d4ff5c5bc70db0e5087157b620570","title":"You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion"},{"paperId":"1728cb805a9573b59330890ba9723e73d6c3c974","title":"Knowledge Distillation: A Survey"},{"paperId":"c9c944146322321d2565a735b6fd10676e6bd69b","title":"Meta-learning Hyperparameter Performance Prediction with Neural Processes"},{"paperId":"da5d78b3e3a1544fde98fba86088e1215e97cbe8","title":"All NLP Tasks Are Generation Tasks: A General Pretraining Framework"},{"paperId":null,"title":"Ai and memory wall"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"deedb9b61a01d686b28e6034770fccc142e77fab","title":"Predicting Performance for Natural Language Processing Tasks"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"eb606d9ce65139754232cee62f6ab77f3e0c665f","title":"Leveraging Pre-trained Checkpoints for Sequence Generation Tasks"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"8323c591e119eb09b28b29fd6c7bc76bd889df7a","title":"Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism"},{"paperId":"79b7a61abd4a4b4cf62c26cec815b55b0778f48d","title":"Pythia: AI-assisted Code Completion System"},{"paperId":"d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","title":"Energy and Policy Considerations for Deep Learning in NLP"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"Aws to offer nvidiaś t4 gpus for ai inferencing"},{"paperId":"dd908c09eaf8f3a3c20d64fff798ec902433737d","title":"Code Completion with Neural Attention and Pointer Networks"},{"paperId":"9d34951c328c02c062d829f7e3c2ebf657b9d031","title":"Determining Sample Size for Research Activities"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"442e10a3c6640ded9408622005e3c2a8906ce4c2","title":"A Unified Approach to Interpreting Model Predictions"},{"paperId":"230579a14d54ae00073d6c3522ffcef313320be9","title":"Compression of Neural Machine Translation Models via Pruning"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"443516aeb2819d4d362ffe7d5418a54e5427a016","title":"ORANGE: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"2153bd13bf4b094bd4be8e5e764a344930d5209f","title":"Source Code"},{"paperId":"1e41ed1ac234cba0138329047e16a8a424389e77","title":"A Complexity Measure"}],"id":"ef29fb8cc6bdda4b011288f51da521d3c25fc53d","summary":"An early-rejection mechanism to turn down low-return prompts by foretelling the completion qualities without sending them to the LCM is proposed and a lightweight Transformer-based estimator is proposed to demonstrate the feasibility of the mechanism."},{"url":"https://www.semanticscholar.org/paper/1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow","venue":"IEEE Working Conference on Mining Software Repositories","year":2018,"referenceCount":48,"citationCount":155,"influentialCitationCount":41,"publicationDate":"23/05/2018","authors":"Pengcheng Yin,Bowen Deng,Edgar Chen,Bogdan Vasilescu,Graham Neubig","citations":[{"paperId":"f7e41afdd7b54af3ddb2bff8005efbce4de87d38","title":"Constructing Effective In-Context Demonstration for Code Intelligence Tasks: An Empirical Study"},{"paperId":"445e5b31fc98f7acd2274aaa77d4de98816d89e3","title":"Towards Usable Neural Comment Generation via Code-Comment Linkage Interpretation: Method and Empirical Study"},{"paperId":"443d898928eb8e32d2e6f8f287beaa63f5b00eb9","title":"JaCoText: A Pretrained Model for Java Code-Text Generation"},{"paperId":"d2edaa84c13fd08735216ec85f2f3b2b5b53ff78","title":"On the Feasibility of Specialized Ability Extracting for Large Language Code Models"},{"paperId":"8475c1d1d98932111b194dc139c8f8b30818725d","title":"xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"0bc2753f59e653de718b5c7a2a0a7e00d13778c7","title":"NL2CMD: An Updated Workflow for Natural Language to Bash Commands Translation"},{"paperId":"535a3d95a743e1f4b591b5b2af3e778a6347158a","title":"CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code"},{"paperId":"27c2ddc921befc44f7b2cdb8bb39facdd8b0b188","title":"Measuring The Impact Of Programming Language Distribution"},{"paperId":"42630c03d3817b1153d245f20742ad4b30a80b75","title":"JEMMA: An extensible Java dataset for ML4Code applications"},{"paperId":"0bc9cedda48551847cc741b74c1fc299c5a9eed2","title":"Who Evaluates the Evaluators? On Automatic Metrics for Assessing AI-based Offensive Code Generators"},{"paperId":"41f5e1ad7793593befc0b9c38f756836e8b07c98","title":"Code4ML: a Large-scale Dataset of annotated Machine Learning Code"},{"paperId":"a630c70aed27b52f6d04d1e772b153c5a7b6f6fe","title":"Later datasets increase in complexity and scale , incorporating reading comprehension"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"8210cef990b8e5cddbc95000e46309bdd25337f7","title":"Asking Clarification Questions for Code Generation in General-Purpose Programming Language"},{"paperId":"4290a70025f29d7054c550c75ae6b24c38a79d12","title":"SE Factual Knowledge in Frozen Giant Code Model: A Study on FQN and its Retrieval"},{"paperId":"6bc87e51018d6de55011e95a0d43c588dd44a1e8","title":"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages"},{"paperId":"52a902d1a3b74e3e186d71d83d300a49dd9191b3","title":"An Effective Low-Dimensional Software Code Representation using BERT and ELMo"},{"paperId":"06ae73a2b826544943e961a9c33ec0f246264696","title":"An additional approach to pre-trained code model with multilingual natural languages"},{"paperId":"5fd0533ba1068af1cb075d4eb71ee551691a71ac","title":"ExploitGen: Template-augmented exploit code generation based on CodeBERT"},{"paperId":"13ea8689a4fe12e7443f5a3a0a25c5337a2f4cd8","title":"MarianCG: a code generation transformer model inspired by machine translation"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"e402dd77eba504ea93bc38e2a052398bb95db351","title":"Execution-based Evaluation for Data Science Code Generation Models"},{"paperId":"ae7e1cdd6dbfd375a7fecc558e4565ccc792fba4","title":"Evaluating How Fine-tuning on Bimodal Data Effects Code Generation"},{"paperId":"4ddc26b3a5fe9044b97b408d163f7464d769ebbf","title":"CODEP: Grammatical Seq2Seq Model for General-Purpose Code Generation"},{"paperId":"3cba16fc46ac5b35c1cc72a822208aa0097384cc","title":"CodePAD: Sequence-based Code Generation with Pushdown Automaton"},{"paperId":"a630c70aed27b52f6d04d1e772b153c5a7b6f6fe","title":"LILA: A Unified Benchmark for Mathematical Reasoning"},{"paperId":"2577d053f8aab912d29b424e1f09133d83740fd2","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"b109588511459bf46e94cb4eb68b5cf79b092795","title":"Antecedent Predictions Are More Important Than You Think: An Effective Method for Tree-Based Code Generation"},{"paperId":"02da1f11b2620ee4a6c8b010fd1c8fe6f5ab5119","title":"Antecedent Predictions Are Dominant for Tree-Based Code Generation"},{"paperId":"1b4c19168410fb2690d285b205ab2281793db81a","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"},{"paperId":"654cd297b307c0c6fa08732511ba852b8dce1977","title":"Out of the BLEU: how should we assess quality of the Code Generation models?"},{"paperId":"0a39442979d6e678dd36bb443ad529c14e86a86e","title":"DocPrompting: Generating Code by Retrieving the Docs"},{"paperId":"b31b21d0750e849badfe76000e8170482f32b9be","title":"DocCoder: Generating Code by Retrieving and Reading Docs"},{"paperId":"66b7836001407120ad0000369af8b30c44788a33","title":"Transformer with Tree-order Encoding for Neural Program Generation"},{"paperId":"ebc08a933e053e57bc2102848efa1119c0ce2bd9","title":"NS3: Neuro-Symbolic Semantic Code Search"},{"paperId":"0121c151c96b32cd7851e4bcda2a468b279c1e6f","title":"CODE-MVP: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training"},{"paperId":"703f79763d534dbf9674132cec890f432dcc19ec","title":"A Survey of Deep Learning Models for Structural Code Understanding"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"8bf6109ed9dc5379d2340913e4184904d342918c","title":"DETECTION OF SOURCE CODE IN INTERNET TEXTS USING AUTOMATICALLY GENERATED MACHINE LEARNING MODELS"},{"paperId":"862c0b672c9defded3111924310a07760cfa27ff","title":"Can NMT Understand Me? Towards Perturbation-based Evaluation of NMT Models for Code Generation"},{"paperId":"56a75a76b86ea27c106f3e3e4a4d546c24e8678c","title":"Programming Language Agnostic Mining of Code and Language Pairs with Sequence Labeling Based Question Answering"},{"paperId":"d6b7bc4e7968398101d8d9b4da7d4a0186763ff3","title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages"},{"paperId":"f557534feabd3fa25052fd590a64d687b9754986","title":"Can we generate shellcodes via natural language? An empirical study"},{"paperId":"7ae55d0c0f501846deb1b6f13a03a249d9a2db4a","title":"Code Generation for Unknown Libraries via Reading API Documentations"},{"paperId":"97010ef4c4bd80b1f959df236501cf7741053d04","title":"On the Importance of Building High-quality Training Datasets for Neural Code Search"},{"paperId":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective"},{"paperId":"2236b6036cd1ecba1792d5e1fedbae7cefc3d43b","title":"Can we generate shellcodes via natural language? An empirical study"},{"paperId":"f5b3be8b0f06eba6d26ed02656fac82928b1ae05","title":"Natural Language to Code Using Transformers"},{"paperId":"532f32be1e918d6b75650947318e57fc8f4fb415","title":"CodeRetriever: Unimodal and Bimodal Contrastive Learning"},{"paperId":"758d85dd98f5caf59d51f9f3c09c2423471d56fd","title":"AstBERT: Enabling Language Model for Code Understanding with Abstract Syntax Tree"},{"paperId":"2e375d8f4c4dbbf2fdf209f54eda70c9546e4096","title":"AstBERT: Enabling Language Model for Financial Code Understanding with Abstract Syntax Trees"},{"paperId":"5bc74a4add942c340e6038235507e5f00b02d3b2","title":"Learning to Describe Solutions for Bug Reports Based on Developer Discussions"},{"paperId":"7ad2b12526e77badd629b10880db147afce4864a","title":"Lyra: A Benchmark for Turducken-Style Code Generation"},{"paperId":"4a160efbe80c38cd5eb2f92c7c095b49b113397d","title":"In-IDE Code Generation from Natural Language: Promise and Challenges"},{"paperId":"bea6af010fc02f5ac29edfc17096be6078edab46","title":"A Survey on Deep Learning for Software Engineering"},{"paperId":"cc80244d69c2c5bb7fe79fc6ca2561f2bed830ca","title":"Learning to Represent Programs with Heterogeneous Graphs"},{"paperId":"1334674796f2122c4ede1eb6c1ad07cec9d77a28","title":"A Systematic Literature Review on the Use of Deep Learning in Software Engineering Research"},{"paperId":"c317f4d5ddeb11da9a5c8fea1e3cdec2f7de485d","title":"Deep Learning Based Program Generation From Requirements Text: Are We There Yet?"},{"paperId":"56e6d62c638a24411f12d15cdc8821a31fc495c8","title":"Source Code Generation from Descriptions in a Natural Language"},{"paperId":"74f57a9ffa73e1bee99300b177904c06199840aa","title":"CodeRetriever: A Large Scale Contrastive Pre-Training Method for Code Search"},{"paperId":"d388b3cbc820975da21c57c9fb9cef2d9c6d08f0","title":"CodeRetriever: Large-scale Contrastive Pre-training for Code Search"},{"paperId":"239b5649b12f28fd610de036afba41b9246db6c9","title":"Parsel: A Unified Natural Language Framework for Algorithmic Reasoning"},{"paperId":"e1d534c754d821bc405ecb0a954c8a3761d48a20","title":"Repo4QA: Answering Coding Questions via Dense Retrieval on GitHub Repositories"},{"paperId":"4797e960f8e7a2b47ae0d95c7071ef84fa5d4b5b","title":"Code Generation Using Machine Learning: A Systematic Review"},{"paperId":"7497360b0f411a44aa6afbd8b830050c40ec8aed","title":"Dataset of Student Solutions to Algorithm and Data Structure Programming Assignments"},{"paperId":"bd5d3022dc395ca85f72e346022ed6175e13a278","title":"A Transformer-based Approach for Translating Natural Language to Bash Commands"},{"paperId":"fa25e37fbd55dd05c0563de8f1e277f90c4ea589","title":"Text Classification for Task-based Source Code Related Questions"},{"paperId":"416bb2238e92cc9c664bc932a5e968bf03bd2845","title":"A Survey on Machine Learning Techniques for Source Code Analysis"},{"paperId":"dace03e57056d736f9e24937bdf486e894f8e866","title":"Is neural machine translation approach accurate enough for coding assistance?"},{"paperId":"0916d3112978bbe5f123553b5460ac1d05c6a8fd","title":"Mapping Language to Programs using Multiple Reward Components with Inverse Reinforcement Learning"},{"paperId":"1601a4b1af2c22e047790ab189861368a5008f5f","title":"APIzation: Generating Reusable APIs from StackOverflow Code Snippets"},{"paperId":"5436193122dff271796bca07df7cecb7a8d6dea6","title":"Natural language-guided programming"},{"paperId":"04a9caa68ea24d183dce56903c5c05e49ae1d289","title":"Exploiting API Description Information to Improve Code Comment Generation"},{"paperId":"83a86fdf5d42fc70a07a2badd4fc9d42863f9b64","title":"SpreadsheetCoder: Formula Prediction from Semi-structured Context"},{"paperId":"cc9ad384ec0d0176ce05865d3866b44d2519bd68","title":"Reading StackOverflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation"},{"paperId":"48db3e5425e1582f5659d98154fc8406cea0dc54","title":"A Globally Normalized Neural Model for Semantic Parsing"},{"paperId":"3ca1430fb5bbf6ffa8f377f6b603648268f7546e","title":"Exploring Dynamic Selection of Branch Expansion Orders for Code Generation"},{"paperId":"47fa6b2645a95165399fc5c2e966f67ac2110df0","title":"Enriching API Documentation with Code Samples and Usage Scenarios from Crowd Knowledge"},{"paperId":"8c4f89a9ac30cf94186916be1bfaa02dbfb3600d","title":"CoDesc: A Large Code–Description Parallel Dataset"},{"paperId":"0c21334be0228431d619a180c809b43be0065bdd","title":"CoSQA: 20,000+ Web Queries for Code Search and Question Answering"},{"paperId":"43e8ad04354bb40c1d10cb58b3187473a8275f82","title":"One Size Does not Fit All: Constructing Complementary Digital Re-Skilling Strategies Using Online Labour Market Data"},{"paperId":"b15fa9e57fb791899154a0f6c321eb703f1c0b09","title":"Shellcode_IA32: A Dataset for Automatic Shellcode Generation"},{"paperId":"969e8c2c7cdf26c35e6c3fc19a9a56b3e7fcd6f9","title":"Generating Code with the Help of Retrieved Template Functions and Stack Overflow Answers"},{"paperId":"b6262aa661b5195ad36b6a588c7160b1681abc2a","title":"Mining software architecture knowledge: Classifying stack overflow posts using machine learning"},{"paperId":"f389c09ad85263bdd1bb2518d61c90a8a558441d","title":"MulCode: A Multi-task Learning Approach for Source Code Understanding"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"4f278ab5ad629267e06196e273252262854c1c57","title":"BF++: a language for general-purpose program synthesis"},{"paperId":"216425e1407875c94a754389b20bf983afe242c5","title":"Teach me how to Label: Labeling Functions from Natural Language with Text-to-text Transformers"},{"paperId":"8c779ca55bef46f2380c1aea163ce9415f47b335","title":"One size does not fit all: Constructing complementary digital reskilling strategies using online labour market data"},{"paperId":"2ad6b59ff14d6bec3f14d8b6480025bbebc50e46","title":"Optimal Neural Program Synthesis from Multimodal Specifications"},{"paperId":"010c9f63c51ccc504de36d2d1693e0ea9d525da1","title":"Deep Learning for Source Code Modeling and Generation"},{"paperId":"ceeac1741f8c8dee5a1365828f79e8d3e84b0623","title":"Semantic Parsing with Less Prior and More Monolingual Data"},{"paperId":"b029346bfa85fa2fc7a12498ae5f018922ce55f9","title":"Code Generation from Natural Language with Less Prior Knowledge and More Monolingual Data"},{"paperId":"4d8bfc6b35d46a5a1dbad7aa273e81f49f5f6e4b","title":"Deep Learning Based Code Generation from Requirements Text: Are We There Yet?"},{"paperId":"62aceee1eca236841fddbad25833802750a92656","title":"Institutional Knowledge at Singapore Management University Institutional Knowledge at Singapore Management University CC2Vec: Distributed representations of code changes CC2Vec: Distributed representations of code changes"},{"paperId":"8e5b4aad131263457a38adbbffebe1b252802f1e","title":"Text2PyCode: Machine Translation of Natural Language Intent to Python Source Code"},{"paperId":"e13d317fe0178a8b8b67f4af995e7fac12c35014","title":"Analysis of Tree-Structured Architectures for Code Generation"},{"paperId":"ed2a639dd4a6b3abdcc0040733a264cb1dbcd7d4","title":"PlotCoder: Hierarchical Decoding for Synthesizing Visualization Code in Programmatic Context"},{"paperId":"b28ca9fb2056c8fd3f37e066f7809e7af27ef9b0","title":"BF++: a language for general-purpose neural program synthesis"},{"paperId":"35f0482b4eadd92d5cd00932a729f00c64ce9e76","title":"Neural joint attention code search over structure embeddings for software Q&A sites"},{"paperId":"2df83c8604ac9260a77d85241b9da0539b29effa","title":"Statistical machine translation outperforms neural machine translation in software engineering: why and how"},{"paperId":"f0a9ff1391bdefaf6d712e5c27e747d7d710bb40","title":"AlgoLabel: A Large Dataset for Multi-Label Classification of Algorithmic Challenges"},{"paperId":"3421e608d478161146d795752b6bcd222ed4c106","title":"When Does it Pay Off to Learn a New Skill? Revealing the Complementary Benefit of Cross-Skilling"},{"paperId":"e6e8a2c56243847b77b604259cde9e10a2daccb8","title":"Semantic Evaluation for Text-to-SQL with Distilled Test Suite"},{"paperId":"c6f608a3731a1fde355835a0e10a65ac71f80643","title":"Towards Full-line Code Completion with Neural Language Models"},{"paperId":"179076fe7aad3d5f085535b37bae85cbbae3c240","title":"CROKAGE: effective solution recommendation for programming tasks by leveraging crowd knowledge"},{"paperId":"d944bf7942297f5670192c5cd33191c26a87973e","title":"Code to Comment “Translation”: Data, Metrics, Baselining & Evaluation"},{"paperId":"2dc59238ad0f010f505238fcd0dd0695681200ef","title":"Neural Code Search Revisited: Enhancing Code Snippet Retrieval through Natural Language Intent"},{"paperId":"b5b9ed1e6d3cd2e51d155831cdfae39f2e0f4578","title":"Synergy between Machine/Deep Learning and Software Engineering: How Far Are We?"},{"paperId":"f30a7e6a1f7bbaf34493586fa61972a4789f7594","title":"Hierarchical Embedding for Code Search in Software Q&A Sites"},{"paperId":"7c41e58832f3af5fd9e09674924d6b5f822e8eac","title":"Exploring Unexplored Generalization Challenges for Cross-Database Semantic Parsing"},{"paperId":"ca0810e7658b72cdce6ea807d2ed0ed39ca70d4e","title":"Bug severity prediction using question-and-answer pairs from Stack Overflow"},{"paperId":"8fbb41e21a8ba6072de60980d3c96e5e50c8e8fa","title":"CC2Vec"},{"paperId":"4a7e7b24389d190d20f214054156e43dc269e595","title":"POSIT"},{"paperId":"eeb37b1b3019a03669b9b8b8173efbbd7c236184","title":"POSIT: Simultaneously Tagging Natural and Programming Languages"},{"paperId":"4e0daa62e8467c46123bc957f0068f7901e1e94f","title":"Mining API usage scenarios from stack overflow"},{"paperId":"0a01de6020e43db362e92664d1832a488d4a4c5b","title":"Improving Quality of a Post’s Set of Answers in Stack Overflow"},{"paperId":"c9aac0037e8abe2da081490cc9d10610aa8fdb3f","title":"Semantic code search using Code2Vec: A bag-of-paths model"},{"paperId":"eccd7060c4f81e92d65601f5c7ac7cade2f68807","title":"TAG : Type Auxiliary Guiding for Code Comment Generation"},{"paperId":"037aa837d95b5f0edef494a2392b1788dc840a47","title":"A Multi-Perspective Architecture for Semantic Code Search"},{"paperId":"a266b37f98928f27fddd863d11b38a5563043315","title":"Learning to Update Natural Language Comments Based on Code Changes"},{"paperId":"57348a5e75b89c1d3e8d5b85027872c35ebc6d36","title":"Relevance Transformer: Generating Concise Code Snippets with Relevance Feedback"},{"paperId":"77910e51a40d17157fc798325d06edfa6cff18d6","title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation"},{"paperId":"5e9b611a476e993f03c90424847311cd84e36a06","title":"Optimising the fit of stack overflow code snippets into existing code"},{"paperId":"3944354c42ddfff7414ad06022f96c72858d5fa6","title":"Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code"},{"paperId":"0dfe706526e5234338411489e1826b4060acc4e8","title":"CC2Vec: Distributed Representations of Code Changes"},{"paperId":"b68f2d939ea3acaf4bee6a487522c27ef7d04cee","title":"Are the Code Snippets What We Are Searching for? A Benchmark and an Empirical Study on Code Search with Natural-Language Queries"},{"paperId":"a470dfc3dd30e4a27c8480d6fb817ece6a11d813","title":"Associating Natural Language Comment and Source Code Entities"},{"paperId":"1f8885c1dea94b97be0d0a89afe642c37ebfbc55","title":"Learning Based Methods for Code Runtime Complexity Prediction"},{"paperId":"d6fef46fd53c3bb13dc2cd32d8b8050b3f67058c","title":"Neural Semantic Parsing for Syntax-Aware Code Generation"},{"paperId":"fd551f9b634b43b4b6be6fbda8369f95868ae94a","title":"POSIT: Simultaneously TaggingNatural and Programming Languages"},{"paperId":"539268757e7d29201a13e5ca90454eb151d3d022","title":"Abstraction, Generalization, and Embodiment in Neural Program Synthesis"},{"paperId":"5ad7ce7810964bde2c86e07d63156270e0b17e0b","title":"Extracting Code-relevant Description Sentences Based on Structural Similarity"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"98ec93df77d6f672b4f682cbe315fedf0e2d4ee7","title":"PUMICE: A Multi-Modal Agent that Learns Concepts and Conditionals from Natural Language and Demonstrations"},{"paperId":"561b279dd17a232f7466a5dda89d879f75a2bf3b","title":"Identifying Algorithm Names in Code Comments"},{"paperId":"3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c","title":"Reranking for Neural Semantic Parsing"},{"paperId":"735ce0447e459e13f89ae751b19323d76a2af786","title":"Program Synthesis and Semantic Parsing with Learned Code Idioms"},{"paperId":"1fbaed00dbda975a6209761857dd1a78618c6585","title":"Jointly Learning Semantic Parser and Natural Language Generator via Dual Information Maximization"},{"paperId":"a57131646a16445df5ddbc86da917fb497cc27da","title":"Cleaning StackOverflow for Machine Translation"},{"paperId":"dfd252415b37208617da78d09cfd87480b9800eb","title":"Modeling Vocabulary for Big Code Machine Learning"},{"paperId":"f44ceb54fa773920b767c1e93ea0bc8725f248df","title":"Automatic Acquisition of Annotated Training Corpora for Test-Code Generation"},{"paperId":"3092325d55f6aa9ba28b0841bdcfd61991a38d48","title":"A Neural Model for Generating Natural Language Summaries of Program Subroutines"},{"paperId":"4fdbf9af4058ff17c31db8bc8ca751d69b90ae43","title":"Semantic Parser and Natural Language Generator via Dual Information Maximization"},{"paperId":"4fbddbe0f23f76bd779f8d1e524374d6bf1bea81","title":"Learning to Map Natural Language to General Purpose Source Code"},{"paperId":"8cd26c197ad9635bd7508f01bdad67b6562099dd","title":"Deep Code-Comment Understanding and Assessment"},{"paperId":"970d34f38106023cfdacfb0bf59b7f3f64dcc4c3","title":"A Multi-Modal Intelligent Agent that Learns from Demonstrations and Natural Language Instructions"},{"paperId":"e6b035ff8d839810593769b96edb18dd188c118c","title":"Reinforcing Diversity Company Policies: Insights from StackOverflow Developers Survey"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"a9d240d911e338e6083cee83570c10af6f9dafb8","title":"A Comprehensive Study of StaQC for Deep Code Summarization"},{"paperId":"1b13cf2971199d48722192b8c290d4c4eb63ca80","title":"Semi-Supervised Code Generation by Editing Intents"}],"references":[{"paperId":"44673a5ec512a81e0c366226135d1ce363d782e6","title":"Meaningful Variable Names for Decompiled Code: A Machine Translation Approach"},{"paperId":"dc030c2e55b266c029356a54bb444b7d9b1f2abc","title":"StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow"},{"paperId":"429b65937d4922578a81e1f0ef5aeab7361ae36b","title":"NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System"},{"paperId":"53da3749987b3a4d166449e32af7b474ce776096","title":"Recovering clear, natural identifiers from obfuscated JS names"},{"paperId":"3807a517403cfc7ed67fdfea64baf62b9948bcfe","title":"A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation"},{"paperId":"2c1e874c3b67510a3215e535f5646b362de5bc89","title":"Abstract Syntax Networks for Code Generation and Semantic Parsing"},{"paperId":"2b63812db40152b12925ce4a848b929fa591b858","title":"Neural Machine Translation and Sequence-to-sequence Models: A Tutorial"},{"paperId":"480d545ac4a4ffff5b1bc291c2de613192e35d91","title":"DyNet: The Dynamic Neural Network Toolkit"},{"paperId":null,"title":"A Syntactic Neural Model for General-Purpose Code Generation"},{"paperId":"74157ae408173bf713f1e94f15aca1475c43bd74","title":"Neural Generation of Regular Expressions from Natural Language with Minimal Domain Knowledge"},{"paperId":"32aa2517b03c871c11e521c2a3406f457833e2c3","title":"Summarizing Source Code using a Neural Attention Model"},{"paperId":"274be5e898828f84766bf06f579024cc363bd975","title":"From Query to Usable Code: An Analysis of Stack Overflow Code Snippets"},{"paperId":"022788ecf8685ed30e3f69f9121b5a3d242a22d6","title":"On the naturalness of software"},{"paperId":"ba30df190664193514d1d309cb673728ed48f449","title":"Incorporating Copying Mechanism in Sequence-to-Sequence Learning"},{"paperId":"484ed558a5863060b373e8a2cb7cb302b8c36116","title":"A Convolutional Attention Network for Extreme Summarization of Source Code"},{"paperId":"0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652","title":"A Theoretically Grounded Application of Dropout in Recurrent Neural Networks"},{"paperId":"2579e826f22da988fbc1b5d545aa59ab68bde4f3","title":"Program Synthesis Using Natural Language"},{"paperId":"124f7b5169e71416282e2c91ace8b3bc7d73cd66","title":"Learning to rank code examples for code search engines"},{"paperId":"27e1dbe9f7c71cd6cc1b0357f49aef497e572d09","title":"Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"},{"paperId":"cd7ee037c029b25e691650a26cec7538d255405c","title":"Suggesting accurate method and class names"},{"paperId":"93499a7c7f699b6630a86fad964536f9423bb6d0","title":"Effective Approaches to Attention-based Neural Machine Translation"},{"paperId":"b5defb8729294614d40e0247eabfdc483bf1a992","title":"Bimodal Modelling of Source Code and Natural Language"},{"paperId":"25369f56a933e3bfb1d8e1588cdc6c50df93ecae","title":"Building a Semantic Parser Overnight"},{"paperId":"b5fc5e7ad9a21fe7b92090ffb88bd63370b89bb1","title":"CACHECA: A Cache Language Model Based Code Suggestion Tool"},{"paperId":"1f713ea97c87166875daf650fbdc3950eed8973a","title":"New Initiative: The Naturalness of Software"},{"paperId":"75ae9a4a66a50a02cd1396e3e22036080f56d496","title":"Building Bing Developer Assistant"},{"paperId":"abb80b4a82f3455f6b271f8384d3affe464f5dba","title":"CloCom: Mining existing source code for automatic comment generation"},{"paperId":"be6665462fd6c56e8756dadf23373bb0c90b2b19","title":"Predicting Program Properties from \"Big Code\""},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"d4ab3e01c4d1308371c76fbc9665701100461e88","title":"Language to Code: Learning Semantic Parsers for If-This-Then-That Recipes"},{"paperId":"dd427a1f782991aef6fdc5747af525f088b49cc1","title":"Statistical learning approach for mining API usage mappings for code migration"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"34f25a8704614163c4095b3ee2fc969b60de4698","title":"Dropout: a simple way to prevent neural networks from overfitting"},{"paperId":"600586ed0ca601a03f754463639b737861c5c994","title":"AutoComment: Mining question and answer sites for automatic comment generation"},{"paperId":"fdb813d8b927bdd21ae1858cafa6c34b66a36268","title":"Learning deep structured semantic models for web search using clickthrough data"},{"paperId":"944a1cfd79dbfb6fef460360a0765ba790f4027a","title":"Recurrent Continuous Translation Models"},{"paperId":"5447a3b8701d59f3a2f1a7f7af030f687ba495c3","title":"Lexical statistical machine translation for language migration"},{"paperId":"9d555ed29496850c4ef8a3facd7dce734c86aae7","title":"Natural Language Models for Predicting Programming Comments"},{"paperId":"7cf82b201c28d4bdef0f88ffef4f287f454e53f8","title":"Mining source code descriptions from developer communications"},{"paperId":"77fe510b8f30533e92b9b03651168a00e5638e79","title":"Example Overflow: Using social media for code recommendation"},{"paperId":"ae5e6c6f5513613a161b2c85563f9708bf2e9178","title":"Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection"},{"paperId":"a7fb08b6b94c3b8b9f80cbfd2055aebc68353cb6","title":"Generating Parameter Comments and Integrating with Method Summaries"},{"paperId":"d664cc7336b6874f4e9a955659920bcd41264734","title":"A study of the uniqueness of source code"},{"paperId":"3b44f86ff382586d05699f3ce47063d2f1a5c53e","title":"SNIFF: A Search Engine for Java Using Free-Form Queries"},{"paperId":"a3f853572e12b51c4c227590168c95b7cd0ca666","title":"Get another label? improving data quality and data mining using multiple, noisy labelers"},{"paperId":"87dadbe571092cb67a4183730a885efaf42e634c","title":"Statistical machine translation"},{"paperId":"2a49ee0e7fa2048c483856cbbf5b24b05340ae8f","title":"Natural Language Programming: Styles, Strategies, and Contrasts"}],"id":"1a53e7446274016f737236bdd48e3ff05d966384","summary":"A novel method to mine high-quality aligned data from SO using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a probabilistic model to capture the correlation between NL and code using neural networks."},{"url":"https://www.semanticscholar.org/paper/407b9e9478ba6bff43ce4b20e8b6cb2b303477d2","title":"P LANNING WITH L ARGE L ANGUAGE M ODELS FOR C ODE G ENERATION","venue":"","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","citations":[],"references":[{"paperId":"6d994b4f5a46cd14e8f09f1e9e49120546b15e31","title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"69a500b9cc0f0755efb748cff10bb90e6facd632","title":"NeuroLogic A*esque Decoding: Constrained Text Generation with Lookahead Heuristics"},{"paperId":"1aed58bd07026492194672adec494dc37c894a28","title":"Leveraging Automated Unit Tests for Unsupervised Code Translation"},{"paperId":"6dc1db69749fcb6484a11cd9465e9945068027bf","title":"PPL-MCTS: Constrained Textual Generation Through Discriminator-Guided MCTS Decoding"},{"paperId":"ad18af95ba8125d2c0eb9f9941205678cad38ad2","title":"Generating Accurate Assert Statements for Unit Test Cases using Pretrained Transformers"},{"paperId":null,"title":"Inductive program synthesis through using monte carlo tree search guided by a heuristic-based loss function"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"fd963ea665e7a6e49753652dc7efe5affba5feb0","title":"DreamCoder: bootstrapping inductive program synthesis with wake-sleep library learning"},{"paperId":"5d7d34abbc14739e40b53ec3c33a3c698a37e70e","title":"To Beam Or Not To Beam: That is a Question of Cooperation for Language GANs"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"e8cc5b6204970a88cd1b2df491aa10c4333e083e","title":"Machine Translation Decoding beyond Beam Search"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"4dee6b82c7e59973ccd1520ff83f6b66f4d4bed4","title":"Investigating Softmax Tempering for Training Neural Machine Translation Models"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"7093ad29f2de2781cf5ed6e6d212ed31ebaa2c8b","title":"Latent Execution for Neural Program Synthesis Beyond Domain-Specific Languages"},{"paperId":"15a60130ea9cb4a00e0dac6d11a368894974cedf","title":"Unit Test Case Generation with Transformers and Focal Context"},{"paperId":"7157d9e7a8dcd7e9900dcfd3d10d8a07d33468bc","title":"Synthesize, Execute and Debug: Learning to Repair for Neural Program Synthesis"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"937fef6a786c4463a3bb19770c704945d1600b66","title":"Learning Compositional Rules via Neural Program Synthesis"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"f669b5406320a83e94b4dd439806211c8b1e95fc","title":"Write, Execute, Assess: Program Synthesis with a REPL"},{"paperId":"6c41bedc4637f3fd504c68baa3b3d8881e056ac1","title":"Execution-Guided Neural Program Synthesis"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"e3deff41f9d3eb1e141207b69d6c7a4c007bd63e","title":"Program Synthesis Through Reinforcement Learning Guided Tree Search"},{"paperId":"dea6aeb514b1969ab879c793d46a0d2eceaa2cbf","title":"Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis"},{"paperId":null,"title":"Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"},{"paperId":"c27db32efa8137cbf654902f8f728f338e55cd1c","title":"Mastering the game of Go without human knowledge"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"e7bd5c479af21110e458dcb57ad208e1ba31db0e","title":"Field Report: Applying Monte Carlo Tree Search for Program Synthesis"},{"paperId":"846aedd869a00c09b40f1f1f35673cb22bc87490","title":"Mastering the game of Go with deep neural networks and tree search"},{"paperId":"54e325aee6b2d476bbbb88615ac15e251c6e8214","title":"Generative Adversarial Nets"},{"paperId":"7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f","title":"Sequence Transduction with Recurrent Neural Networks"},{"paperId":"c542aaafcf80a87b37ffa350344e65fe19b9c0ce","title":"Monte-Carlo tree search and rapid action value estimation in computer Go"},{"paperId":"3b41773cf65c4d7cde97e3b8c82295433b9a8f39","title":"Parallel Monte-Carlo Tree Search"},{"paperId":"e635d81a617d1239232a9c9a11a196c53dab8240","title":"Bandit Based Monte-Carlo Planning"},{"paperId":"97efafdb4a3942ab3efba53ded7413199f79c054","title":"Reinforcement Learning: An Introduction"},{"paperId":"4415305be938adf533701141735a15558538dca4","title":"Art of Software Testing"},{"paperId":"221aa3be55a4ead8fc2aa83b12aac370bfba72f5","title":"A Formal Basis for the Heuristic Determination of Minimum Cost Paths"}],"id":"407b9e9478ba6bff43ce4b20e8b6cb2b303477d2","summary":"A novel Transformer decoding algorithm that uses a planning algorithm to do lookahead search and guide the Transformer to generate better programs, and enables controllable code generation, such as concise codes and highly-commented codes by optimizing modified objectives."},{"url":"https://www.semanticscholar.org/paper/0192638593c430e15e2804a74a3e8a7ecb52d435","title":"On the Design of AI-powered Code Assistants for Notebooks","venue":"ArXiv","year":2023,"referenceCount":91,"citationCount":2,"influentialCitationCount":0,"publicationDate":"26/01/2023","authors":"Andrew M. McNutt,Chenglong Wang,R. DeLine,S. Drucker","citations":[{"paperId":"dca09e13fe9b4f46b1e10b966f3ede63f63f9489","title":"Understanding the Usability of AI Programming Assistants"},{"paperId":"b0335eb2b9a1684fc16ff79234f0b206b30da169","title":"A Study of Editor Features in a Creative Coding Classroom"}],"references":[{"paperId":"0392d58335ce674a70f5e58ac8c438de296a0e6a","title":"Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models"},{"paperId":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"e2cca5638aaa98c289a2acfb923f095c5e189947","title":"EDAssistant: Supporting Exploratory Data Analysis in Computational Notebooks with In Situ Code Search and Recommendation"},{"paperId":"99323bd786ee5be1e1aa589858e14e89630f207b","title":"Exploring the Learnability of Program Synthesizers by Novice Programmers"},{"paperId":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?"},{"paperId":"f2da3e57946099f979644478acfb2a404e74440a","title":"Maniposynth: Bimodal Tangible Functional Programming"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"a8c072f6cbd4653a1f0888b3529898f66c0e6bb0","title":"Strategies for Reuse and Sharing among Data Scientists in Software Teams"},{"paperId":"1f3a97d255a6d0b2d5535af75737ed43b1e96fc4","title":"Is GitHub Copilot a Substitute for Human Pair-programming? An Empirical Study"},{"paperId":"8dec8614a5f280589e4cd6fbf12672a18eedfddc","title":"Designing for Responsible Trust in AI Systems: A Communication Perspective"},{"paperId":"4bc040835fbff57ce6612306d794b8c6c8226086","title":"Discovering the Syntax and Strategies of Natural Language Programming with Generative Language Models"},{"paperId":"076b4126d9b511655f6a1090f6c5f82a87862e3b","title":"How Experienced Designers of Enterprise Applications Engage AI as a Design Material"},{"paperId":"59cad9487a3632a9fb697a9d58c2b30040e9366d","title":"Diff in the Loop: Supporting Data Comparison in Exploratory Data Analysis"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"7975445581a8dcdcbc26cdc38ac98020d98975ca","title":"Lodestar: Supporting Independent Learning and Rapid Experimentation Through Data-Driven Analysis Recommendations"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"e2baf81813d5a515f554ee60bcddcc57548f8c22","title":"Code Search: A Survey of Techniques for Finding Code"},{"paperId":"e67c6c075916da9ba9c2067418ddf961307b1c59","title":"GridBook: Natural Language Formulas for the Spreadsheet Grid"},{"paperId":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis"},{"paperId":"d3640eb3b542eaf36fee2261f037a6bf0d8eac9c","title":"AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts"},{"paperId":"d34bcf4c7084d9ec5252e6c498a52d596642711e","title":"VizLinter: A Linter and Fixer Framework for Data Visualization"},{"paperId":"4a160efbe80c38cd5eb2f92c7c095b49b113397d","title":"In-IDE Code Generation from Natural Language: Promise and Challenges"},{"paperId":"6f8805497f831b002ffecb7e416a005babe18c44","title":"Opportunities and Challenges in Code Search Tools"},{"paperId":null,"title":"Hex Blog: Introducing: “No-Code"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":null,"title":"Use AutoComplete when entering formulas"},{"paperId":null,"title":"Seeing multiple suggestions in a new tab"},{"paperId":null,"title":"Developing Design Spaces for Visualization"},{"paperId":null,"title":"FauxPilot - an open-source GitHub"},{"paperId":"02b4f06bcaead27c38154aedbb9ffb610455dcd2","title":"VizSmith: Automated Visualization Synthesis by Mining Data-Science Notebooks"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"ffd17941bbdf26ef6117569face7118068a0e0ef","title":"Gauss: program synthesis by reasoning over graphs"},{"paperId":"1e633e4d85ec36f799599b4e8d1226ca5e846caf","title":"Where-Provenance for Bidirectional Editing in Spreadsheets"},{"paperId":"1093adc2a47212ef599e4708fe3e41ff7c9ec6d0","title":"GenLine and GenForm: Two Tools for Interacting with Generative Language Models in a Code Editor"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"2014f0975363fe79f4eaff258006e338afa6a137","title":"Glinda: Supporting Data Science with Live Programming, GUIs and a Domain-specific Language"},{"paperId":"e48c9be4b0b64e670ff370d3884fc56105972e38","title":"Fork It: Supporting Stateful Alternatives in Computational Notebooks"},{"paperId":"8b7362332e30b6f1987cd06660d73780ab765073","title":"Lux: Always-on Visualization Recommendations for Exploratory Dataframe Workflows"},{"paperId":"52b4c5a3e95f3f8922828ee815e9e509c01f4bff","title":"Data-centric disambiguation for data transformation with programming-by-example"},{"paperId":"2a36402451c4fcab33c9e7f59fc027167c070aa6","title":"Perfection Not Required? Human-AI Partnerships in Code Translation"},{"paperId":"6d9727f1f058614cada3fe296eeebd8ec4fc512a","title":"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜"},{"paperId":"ac3cdb50606f7770eef8e4cd951840a4f71287a0","title":"Prompt Programming for Large Language Models: Beyond the Few-Shot Paradigm"},{"paperId":"b0010eb52398619fcaafc8343f7611399c00c41d","title":"NBSearch: Semantic Search and Visual Exploration of Computational Notebooks"},{"paperId":"5c1233f5457c7780f67d694adfcac1b37b261daa","title":"Integrated Visualization Editing via Parameterized Declarative Templates"},{"paperId":"b16ef64a90e9e300931d6a92fd2b9277f6cfd584","title":"Passing the Data Baton : A Retrospective Analysis on Data Science Work and Workers"},{"paperId":null,"title":"The Clippy-ization of Human-Computer Design"},{"paperId":null,"title":"McMillan-Major, and Shmargaret Shmitchell"},{"paperId":"cb7ecee937f3aed98a9ce50e0881e43a60270466","title":"Small-Step Live Programming by Example"},{"paperId":"72a6a3f8dc2e157809637bb9bfabc9ae4337f3fc","title":"Multi-Modal Repairs of Conversational Breakdowns in Task-Oriented Dialogs"},{"paperId":"6099feb5eb536f3666614565b1bda5b701d40be2","title":"B2: Bridging Code and Interactive Visualization in Computational Notebooks"},{"paperId":"21ea5e0048b34ef870c91594581fabd5d98315af","title":"mage: Fluid Moves Between Code and Graphical Work in Computational Notebooks"},{"paperId":"9693d5f1dcb27dc06261fa854728fcd61deed2e7","title":"What Makes Things Funny? An Integrative Review of the Antecedents of Laughter and Amusement"},{"paperId":"42f0bb955fc1ce1ce8fa5b3c7cb364092be5610a","title":"The Design Space of Computational Notebooks: An Analysis of 60 Systems in Academia and Industry"},{"paperId":"f3ac5157fd2c1e65770372d64274036cd57db5e8","title":"Divining Insights: Visual Analytics Through Cartomancy"},{"paperId":"b01ac6b990770092c6784f6eda8f3e94e2feb5a8","title":"Wrex: A Unified Programming-by-Example Interaction for Synthesizing Readable Code for Data Scientists"},{"paperId":"e2b6ec35bd2f195387def5906797643ebbd91516","title":"Bacatá: Notebooks for DSLs, Almost for Free"},{"paperId":"065f3d3a8b8ffe1af435e066983fc09029c286ae","title":"Surfacing Visualization Mirages"},{"paperId":null,"title":"Mito: Edit a Spreadsheet. Generate Production Ready Python"},{"paperId":"3514060ffc5f811058f79c1f6e02e3afaad08d78","title":"Sketch-n-Sketch: Output-Directed Programming for SVG"},{"paperId":"55f80d39d1903438b2a72767ff755fe8cee98b97","title":"Ten simple rules for writing and sharing computational analyses in Jupyter Notebooks"},{"paperId":"41a270cdb0405a454a6d55e86caaff67e99abe4f","title":"Managing Messes in Computational Notebooks"},{"paperId":"ad3cf68bae32d21f25ac142287d4a556155619d2","title":"Guidelines for Human-AI Interaction"},{"paperId":"dd7b86773ecd274c8dbc6d7883869135d3fd9aa6","title":"Design Exposition with Literate Visualization"},{"paperId":"2c763b13fa9c51d150940f9b7e5bd8e376cccf7f","title":"Data Scientists in Software Teams: State of the Art and Challenges"},{"paperId":"4fe917d13e672121d969183b457351f304acd13e","title":"ExceLint: automatically finding spreadsheet formula errors"},{"paperId":"2e19108209e351e7739939c748f27f886c6a8562","title":"The Story in the Notebook: Exploratory Data Science using a Literate Programming Tool"},{"paperId":"fcc86317d6a0e5f67968d2e84d20dd5ba0524cf2","title":"Exploration and Explanation in Computational Notebooks"},{"paperId":"24c2b2a4974aec04f5aad5fb2bda7c635f916668","title":"Variolite: Supporting Exploratory Programming by Data Scientists"},{"paperId":"6c97abdadc157180e727835b7e5faf57960e745d","title":"Seeking the user interface"},{"paperId":"80ba37dee9fdf340ce3b37eb1a09b6308e94169d","title":"The Data Linter: Lightweight Automated Sanity Checking for ML Data Sets"},{"paperId":"f4f874c431b75432ed6c1badd1359c2af1cdd70e","title":"Bing developer assistant: improving developer productivity by recommending sample code"},{"paperId":"de87e638c6c615d5e85eea97bb4a9a47495f9995","title":"CodeMend: Assisting Interactive Programming with Bimodal Embedding"},{"paperId":"0fa8a4cbb7cacfe161280e5b6a1f780929ddc743","title":"User Interaction Models for Disambiguation in Programming by Example"},{"paperId":"5ae28a33bc0e853d75b1aaeebe89168036ab936c","title":"Wicked Problems"},{"paperId":"fc3483d30eddefe7a10e24a62f2b9e662cef426d","title":"Codelets: linking interactive documentation and example code in the editor"},{"paperId":"8f3890d16756f0a7f90c002c8b8de4eed759c925","title":"Active code completion"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"cecc5d4bcbaeceaca68fadd2cb9a0ee13fdba24d","title":"Calcite: Completing Code Completion for Constructors Using Crowds"},{"paperId":"8df351f66f514b75ff5db29ad21face73f571951","title":"Example-centric programming: integrating web search into the development environment"},{"paperId":"4edaded63d9d7ea27f5c4819f35d7168e1ae7974","title":"Scratch: programming for all"},{"paperId":"85ed0cf27f07dbad705835eca35dba84bb56554e","title":"Why PBD systems fail: Lessons learned for usable AI"},{"paperId":"ef10af10c87ab39e82865da94b77be64973a6de7","title":"IPython: A System for Interactive Scientific Computing"},{"paperId":"0817e8343afb4e2f4cc41c3ddc548b43067cca4a","title":"Polite computing"},{"paperId":"e2a3078e79435e420da88cbd6a7f871fa2b20eed","title":"Reuse-Conducive Development Environments"},{"paperId":"ffe15cd5dc239bb02248f43b371090f66333c66a","title":"Creative Coding : Programming for Personal Expression"},{"paperId":"409388ab6a828604784d1c14454dc9ad7a87ff9d","title":"Designing interaction, not interfaces"},{"paperId":null,"title":"Notational Systems–the Cognitive Dimensions of Notations Framework. HCI Models, Theories, And Frameworks: Toward An Interdisciplinary Science"},{"paperId":"5df85ae89af55c6d82a1a14836ea6bcfbfc2c0ec","title":"Principles of mixed-initiative user interfaces"},{"paperId":"b547028a5aef9dc78bba195d7669dd087cb7cc8b","title":"Qualitative evaluation and research methods"},{"paperId":"ea349162d97873d4493502e205968ffccb23fcf2","title":"Perceived Usefulness, Perceived Ease of Use, and User Acceptance of Information Technology"},{"paperId":"2fb9729c0a3b046c390ad6748773527eae6cfc84","title":"What Is Program Synthesis?"}],"id":"0192638593c430e15e2804a74a3e8a7ecb52d435","summary":"Challenges and opportunities for future systems in this space are identified, such as the value of disambiguation for tasks like data visualization, the potential of tightly scoped domain-specific tools (like linters), and the importance of polite assistants."},{"url":"https://www.semanticscholar.org/paper/6f84d0cc33c7c58f74b28ddcc1cbda91ea608c9f","title":"C ODE S UMMARIZATION : D O T RANSFORMERS R E ALLY U C","venue":"","year":2022,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Manasi S. Patwardhan,L. Vig,Raveendra Kumar Medicherla,Ravindra Naik,Gautam M. Shroff","citations":[],"references":[{"paperId":"343594d16840c3841e70ca603f500a79c433848b","title":"On the Evaluation of Neural Code Summarization"},{"paperId":"cc80244d69c2c5bb7fe79fc6ca2561f2bed830ca","title":"Learning to Represent Programs with Heterogeneous Graphs"},{"paperId":"4ff62cc81aec762e47647f9daf14f4bcb0ab31c9","title":"EditSum: A Retrieve-and-Edit Framework for Source Code Summarization"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"69acc5f67ea7dba13c58d7281b8f0a25ed64f0e8","title":"EVIL: Exploiting Software via Natural Language"},{"paperId":"51654d8317478fe1497678b15e36ac99d73a65b2","title":"CAST: Enhancing Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees"},{"paperId":"70087677fd1a6309829b42968934575d05a95f92","title":"What do pre-trained code models know about code?"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"a6a7724763d8adba466519489b0b9d209e7f2d15","title":"BARTScore: Evaluating Generated Text as Text Generation"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"0b077c9577f4297dcf3da835e253d21965bbc6e0","title":"CoTexT: Multi-task Learning with Code-Text Transformer"},{"paperId":"26e3d58181724f9ef77973ff0f65bac06e499fec","title":"ProphetNet-X: Large-Scale Pre-training Models for English, Chinese, Multi-lingual, Dialog, and Code Generation"},{"paperId":"5bfb0cc16b871c75e32a6a9d54dd7db225260e04","title":"CodeTrans: Towards Cracking the Language of Silicone's Code Through Self-Supervised Deep Learning and High Performance Computing"},{"paperId":"05e396e79a2f88f0b8f8d99f5ab36ab3efa95c14","title":"Language-Agnostic Representation Learning of Source Code from Structure and Context"},{"paperId":"2e05413a737a7fe823c97e12c2ddc10d4a4c9dc0","title":"Generating Adversarial Computer Programs using Optimized Obfuscations"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"77f9b54da444f49b2436712abe932627f16cbf95","title":"Code Summarization with Structure-induced Transformer"},{"paperId":"be910753f93c24712942536c8dc69e320247c680","title":"On the generalizability of Neural Program Models with respect to semantic-preserving program transformations"},{"paperId":"ceaff192479db6faee58ae88e053b0b319cf1893","title":"Retrieval-Augmented Generation for Code Summarization via Hybrid GNN"},{"paperId":"c58ad64a5774023884f5aef01106a419a8e40463","title":"Learning Sequential and Structural Information for Source Code Summarization"},{"paperId":"4614f69d7f831fbb93c997fc4a9c62a0ae1d8db0","title":"A Human Study of Comprehension and Code Summarization"},{"paperId":"bd47bb8cdd749a3356149da6155d2dcd7458779f","title":"Retrieval-based Neural Source Code Summarization"},{"paperId":"805a6d1df9f460abfcea3d51d181cf1e80680be4","title":"A Transformer-based Approach for Source Code Summarization"},{"paperId":"c345b74be9f98cca9592cc376465118df5c9f2da","title":"Improved Code Summarization via a Graph Neural Network"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"60739c6a62f86d274d7e6a0565acc78e6f319ab1","title":"Adversarial Robustness for Code"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"2ab44880e1763baf3d8753ccb43ad3bd5f122b70","title":"Adversarial examples for models of code"},{"paperId":"295065d942abca0711300b2b4c39829551060578","title":"BERTScore: Evaluating Text Generation with BERT"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"4480ad82baf3028d89361dbedadae073e21b90cf","title":"Automatic Code Summarization: A Systematic Literature Review"},{"paperId":"3092325d55f6aa9ba28b0841bdcfd61991a38d48","title":"A Neural Model for Generating Natural Language Summaries of Program Subroutines"},{"paperId":"98d1307bed619b58b4a44acd8e65ac58495776c2","title":"code2seq: Generating Sequences from Structured Representations of Code"},{"paperId":"077ea3a496bd76bd22d76c432b79b9fa40e136e8","title":"Improving Automatic Source Code Summarization via Deep Reinforcement Learning"},{"paperId":"49164d216b7e7968ded2d9863af161191f2c32e5","title":"Summarizing Source Code with Transferred API Knowledge"},{"paperId":"15f16252af84a630f1f9442d4e0367b6fba089cf","title":"Deep Code Comment Generation"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"}],"id":"6f84d0cc33c7c58f74b28ddcc1cbda91ea608c9f","summary":"Overall, the quality of the generated summaries even from state-of-the-art (SOTA) models is quite poor, raising questions about the utility of current approaches and datasets."},{"url":"https://www.semanticscholar.org/paper/3b0cf543a730e674d4213d344ebc857fada76ead","title":"Understanding High-Level Properties of Low-Level Programs Through Transformers","venue":"","year":2022,"referenceCount":63,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"William S. Moses","citations":[],"references":[{"paperId":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"775a9c722262c7b656876a5fef20f4577afd8981","title":"Multilingual training for Software Engineering"},{"paperId":"1aed58bd07026492194672adec494dc37c894a28","title":"Leveraging Automated Unit Tests for Unsupervised Code Translation"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"6c2d43e71e240e354b5790a38da78a291ceffe7c","title":"Learning to Superoptimize Real-world Programs"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"3ad7b08331be33e7ff59f75420b8942b9181c738","title":"Towards Compile-Time-Reducing Compiler Optimization Selection via Machine Learning"},{"paperId":"3456cf5138c442f8271dd0e05ce1c6097abae917","title":"Network planning with deep reinforcement learning"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"0b077c9577f4297dcf3da835e253d21965bbc6e0","title":"CoTexT: Multi-task Learning with Code-Text Transformer"},{"paperId":"3a863672b072a37ac8d2eb6b780234f6b748a2d9","title":"Generating bug-fixes using pretrained transformers"},{"paperId":"4540f10718ef31c1643104058e9261c9f2fc4834","title":"ANGHABENCH: A Suite with One Million Compilable C Benchmarks for Code-Size Reduction"},{"paperId":"3a1efac4a8aa2f40a8128e034c01f18660652dd5","title":"DOBF: A Deobfuscation Pre-Training Objective for Programming Languages"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"021bbcefc993c389bad6c1daefd8ff92d0fc2441","title":"Contrastive Code Representation Learning"},{"paperId":null,"title":"Dobf: A deobfuscation pretraining objective for programming languages, 2021"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"15a60130ea9cb4a00e0dac6d11a368894974cedf","title":"Unit Test Case Generation with Transformers and Focal Context"},{"paperId":"5bbf22fa3b9b7bd7c55a96fdce249fdf9bdfdacb","title":"Static Neural Compiler Optimization via Deep Reinforcement Learning"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"6aa41f4389de6761f7c600285be9b5518271fc32","title":"Deep Symbolic Superoptimization Without Human Knowledge"},{"paperId":"9aaa11190026766bab8a5b6b7be9f50efb4d24a2","title":"AutoPhase: Juggling HLS Phase Orderings in Random Forests with Deep Reinforcement Learning"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"938a603d320d580aab79715abd0c0dcd3093db4b","title":"NeuroVectorizer: end-to-end vectorization with deep reinforcement learning"},{"paperId":null,"title":"A Deep Learning Based Cost Model for Automatic Code Optimization in Tiramisu"},{"paperId":null,"title":"Codebert: A pretrained model for programming and natural languages, 2020"},{"paperId":null,"title":"Gaussian error linear units (gelus), 2020"},{"paperId":"6456d905a63d2741c6d896f59eb6372ceedcecce","title":"Solving Arithmetic Word Problems Automatically Using Transformer and Unambiguous Representations"},{"paperId":"9bdf9f601204754845ff007bbc53d344d7bc07d8","title":"BHive: A Benchmark Suite and Measurement Framework for Validating x86-64 Basic Block Performance Models"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"f90a7bc396e205b204d5d6066a10162f84b128f9","title":"Learning to optimize halide with tree search and random programs"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"516497e7fc39ebb126bed28dcc5c6129e500acb8","title":"An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation"},{"paperId":"e0a3db9884fee0e7b7ae7e48cd9992866fa8d300","title":"Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation using Deep Neural Networks"},{"paperId":"e1d7e18915bd8991373404ea157c3599e493122f","title":"A Survey on Compiler Autotuning using Machine Learning"},{"paperId":null,"title":"Cross-lingual language model pretraining, 2019"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"48925fef94500cf19ee220ed74217816f1ab5e60","title":"Phrase-Based & Neural Unsupervised Machine Translation"},{"paperId":"29064c333a95a40854172e60440168777f87994c","title":"Learning to superoptimize programs"},{"paperId":null,"title":"Attention is all you need, 2017"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":"9c78fd9c0fd24bf1f749c45ac54587638b78e20e","title":"Automatic construction of inlining heuristics using machine learning"},{"paperId":"7177edea5d1116fe5473c3e6ff41a48c91f402aa","title":"Mitigating the compiler optimization phase-ordering problem using machine learning"},{"paperId":"bc4638f55f6ec57e37ac201ec3a61fdf58540aca","title":"Using graph-based program characterization for predictive modeling"},{"paperId":"f0d19c7e123c0160f2535e561c2955df99d4f993","title":"Transparent dynamic instrumentation"},{"paperId":"3b1bae14269d1e3bbb45f79bb471af3bd0bf4e1e","title":"Finding and understanding bugs in C compilers"},{"paperId":"6910d193282b4f00a0a81fdfb0e7dd8ae50648cd","title":"Milepost GCC: Machine Learning Enabled Self-tuning Compiler"},{"paperId":"6a134afcbe749b5cbf5c185831e50e660f786a91","title":"Rapidly Selecting Good Compiler Optimizations using Performance Counters"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"9364b48b83244e432d970414cb3659cd0d0a23da","title":"Advanced Compiler Design and Implementation"},{"paperId":null,"title":"Long shortterm memory"},{"paperId":null,"title":"Gomez , Lukasz Kaiser , and Illia Polosukhin"},{"paperId":null,"title":", Gianluca Palermo , and Cristina Silvano . A survey on compiler autotuning using machine learning"}],"id":"3b0cf543a730e674d4213d344ebc857fada76ead","summary":"It is shown that Transformer models can translate C to LLVM-IR with high accuracy, by training on a parallel corpus of functions extract from 1 million compilable, open-sourced C programs and its corresponding LL VM-IR after compiling with Clang."},{"url":"https://www.semanticscholar.org/paper/d6b7bc4e7968398101d8d9b4da7d4a0186763ff3","title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages","venue":"ArXiv","year":2022,"referenceCount":60,"citationCount":9,"influentialCitationCount":1,"publicationDate":"16/03/2022","authors":"Zhiruo Wang,Grace Cuenca,Shuyan Zhou,Frank F. Xu,Graham Neubig","citations":[{"paperId":"8a2a2006e371ce07e0b29538cd11337a5c977caf","title":"“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"27c2ddc921befc44f7b2cdb8bb39facdd8b0b188","title":"Measuring The Impact Of Programming Language Distribution"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"6bc87e51018d6de55011e95a0d43c588dd44a1e8","title":"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages"},{"paperId":"13ea8689a4fe12e7443f5a3a0a25c5337a2f4cd8","title":"MarianCG: a code generation transformer model inspired by machine translation"},{"paperId":"2577d053f8aab912d29b424e1f09133d83740fd2","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"dda0f7f086fc875d583604f8b0cf4a8678bc4de4","title":"Bootstrapping Multilingual Semantic Parsers using Large Language Models"},{"paperId":"780f7eebde16b1ae5843df3a79a7772899ef6a71","title":"MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation"}],"references":[{"paperId":"7ad2b12526e77badd629b10880db147afce4864a","title":"Lyra: A Benchmark for Turducken-Style Code Generation"},{"paperId":"789b8487da7188442085983caba3ffaae05531e9","title":"The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation"},{"paperId":"850232f59e25fc9a37ba72f0738126503c4040bb","title":"Zero-Shot Cross-lingual Semantic Parsing"},{"paperId":"92d211c65ae8c8006c07d34dfa0587e278d0ac00","title":"An Empirical Study of C++ Vulnerabilities in Crowd-Sourced Code Examples"},{"paperId":"66ee9661ebcdbd0a171c9ee1fc17c26778789c67","title":"Cross-Lingual Training with Dense Retrieval for Document Retrieval"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"0258f9ffd26903fbcc50be4eb0e3f502dc556962","title":"SGL: Speaking the Graph Languages of Semantic Parsing via Multilingual Translation"},{"paperId":"7571ed4cf1bbdcf891b576a0da12c910b1f0c72f","title":"Concealed Data Poisoning Attacks on NLP Models"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"268251591a4d8a64ab70b7e01a618deaab9a5993","title":"Code Generation from Natural Language with Less Prior and More Monolingual Data"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"687b13c44f849d23c2496996b5da83e706094db9","title":"Beyond English-Centric Multilingual Machine Translation"},{"paperId":"a8a168d53e01b0c35d626cfced103656e22b8343","title":"MTOP: A Comprehensive Multilingual Task-Oriented Semantic Parsing Benchmark"},{"paperId":"27b2bff2dce12c0edc1ff56109335f494424a87d","title":"Multi-Domain Multilingual Question Answering"},{"paperId":"3f97c2067cde9377e50b3160bbd7982c94abd88a","title":"An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions"},{"paperId":"b029346bfa85fa2fc7a12498ae5f018922ce55f9","title":"Code Generation from Natural Language with Less Prior Knowledge and More Monolingual Data"},{"paperId":null,"title":"2018), and CoNaLa (Yin et al., 2018a). Other examples include datasets for problem solving, such as HumanEval (Chen et al., 2021)"},{"paperId":"f4af3fe736b616452424d50cbd47d52f0a210582","title":"OPUS-MT – Building open translation services for the World"},{"paperId":"0c0e8d3f1e17d2b15d07d0a37f8a55c70b29976d","title":"Localizing Open-Ontology QA Semantic Parsers in a Day Using Machine Translation"},{"paperId":"5967a76ecb0f15de1da1a2cc869be581656411b1","title":"Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation"},{"paperId":"805a6d1df9f460abfcea3d51d181cf1e80680be4","title":"A Transformer-based Approach for Source Code Summarization"},{"paperId":"77910e51a40d17157fc798325d06edfa6cff18d6","title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation"},{"paperId":"bd2a84a08f032179b60deb286543004c89e652fb","title":"Bootstrapping a Crosslingual Semantic Parser"},{"paperId":"ba4a34680e09e77984624c95f5245d91b54373f6","title":"XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization"},{"paperId":"83a820fe19944a7621238b8cfcc0b8a0cbc0f4b6","title":"TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages"},{"paperId":"495da6f19baa09c6db3697d839e10432cdc25934","title":"Multilingual Denoising Pre-training for Neural Machine Translation"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"17dbd7b72029181327732e4d11b52a08ed4630d0","title":"Natural Questions: A Benchmark for Question Answering Research"},{"paperId":"595306f993993e44e2c2f674367103f44df03d9b","title":"Generalized Data Augmentation for Low-Resource Translation"},{"paperId":"649c1148439a4e875dab414ba67bf8c80350af4a","title":"TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"807370352540f171685998aec7a5701f7110f147","title":"Understanding Back-Translation at Scale"},{"paperId":"49164d216b7e7968ded2d9863af161191f2c32e5","title":"Summarizing Source Code with Transferred API Knowledge"},{"paperId":"d10df96b3fb0ab5c6b1d0cc22c7400d0acccc3cc","title":"The Hitchhiker’s Guide to Testing Statistical Significance in Natural Language Processing"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"dc030c2e55b266c029356a54bb444b7d9b1f2abc","title":"StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow"},{"paperId":null,"title":"Multilingual bert readme"},{"paperId":"116a877c774969b53399b1ccaa34d51bfb492ee4","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"2c1e874c3b67510a3215e535f5646b362de5bc89","title":"Abstract Syntax Networks for Code Generation and Semantic Parsing"},{"paperId":"32aa2517b03c871c11e521c2a3406f457833e2c3","title":"Summarizing Source Code using a Neural Attention Model"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"484ed558a5863060b373e8a2cb7cb302b8c36116","title":"A Convolutional Attention Network for Extreme Summarization of Source Code"},{"paperId":"f3b96ef2dc1fc5e14982f1b963db8db6a54183bb","title":"Improving Neural Machine Translation Models with Monolingual Data"},{"paperId":"27e1dbe9f7c71cd6cc1b0357f49aef497e572d09","title":"Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"},{"paperId":"abb80b4a82f3455f6b271f8384d3affe464f5dba","title":"CloCom: Mining existing source code for automatic comment generation"},{"paperId":"d4ab3e01c4d1308371c76fbc9665701100461e88","title":"Language to Code: Learning Semantic Parsers for If-This-Then-That Recipes"},{"paperId":"600586ed0ca601a03f754463639b737861c5c994","title":"AutoComment: Mining question and answer sites for automatic comment generation"},{"paperId":"9d555ed29496850c4ef8a3facd7dce734c86aae7","title":"Natural Language Models for Predicting Programming Comments"},{"paperId":"696b505083d34c6f995aef88d0352d70d7f7e8c8","title":"Learning multilingual named entity recognition from Wikipedia"},{"paperId":"7cf82b201c28d4bdef0f88ffef4f287f454e53f8","title":"Mining source code descriptions from developer communications"},{"paperId":"77fe510b8f30533e92b9b03651168a00e5638e79","title":"Example Overflow: Using social media for code recommendation"},{"paperId":"c5fdd3810288aa4d36de28fe4814038f408f6c63","title":"Description of the NTOU Complex QA System"},{"paperId":"aca7fb8d5aeb9fff60279a0076efc0c892ce735a","title":"Bootstrap Pattern Learning for Open-Domain CLQA*"},{"paperId":"3b44f86ff382586d05699f3ce47063d2f1a5c53e","title":"SNIFF: A Search Engine for Java Using Free-Form Queries"},{"paperId":null,"title":"A number of methods have been proposed to mine intent-snippet pairs for the purpose of code search, summarization, or generation. While our work falls in the line of mining from SO"},{"paperId":"030748679e9806b6f3eebcd4f87971a34eaf3da4","title":"University of Hagen at QA@CLEF 2008: Efficient Question Answering with Question Decomposition and Multiple Answer Streams"},{"paperId":null,"title":"2020) or generalized data augmentation (Xia et al., 2019), also inspired our experiments. However, these techniques have rarely been utilized for NL-conditioned code generation"}],"id":"d6b7bc4e7968398101d8d9b4da7d4a0186763ff3","summary":"A multilingual dataset is proposed to benchmark code generation from natural language commands extending beyond English, modeled off of the methodology from the English Code/Natural Language Challenge (CoNaLa) dataset, and a quantitative evaluation of performance is presented by testing with state-of-the-art code generation systems."},{"url":"https://www.semanticscholar.org/paper/8a854331c593c6a766fa3b8037fb2ad1b95a6f06","title":"An Empirical Study of Code Smells in Transformer-based Code Generation Techniques","venue":"IEEE Working Conference on Source Code Analysis and Manipulation","year":2022,"referenceCount":57,"citationCount":6,"influentialCitationCount":1,"publicationDate":"01/10/2022","authors":"Mohammed Latif Siddiq,Shafayat H. Majumder,Maisha R. Mim,Sourov Jajodia,Joanna C. S. Santos","citations":[{"paperId":"9b1367167e45bac5e039ba0407cb3329125e9ff2","title":"Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?"},{"paperId":"7677e9ead35f8f2578acbf5ad15fb330c83762c4","title":"Systematically Finding Security Vulnerabilities in Black-Box Code Generation Models"},{"paperId":"71280dba5bda65c162f9deaffed7d3d20692ca0a","title":"SecurityEval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques"},{"paperId":"9afab8dc694269d205d769eaea549d8f7558d776","title":"Exploring the Verifiability of Code Generated by GitHub Copilot"},{"paperId":"a889a606734cd47f802765866487c677497a70d6","title":"Lost at C: A User Study on the Security Implications of Large Language Model Code Assistants"},{"paperId":"1c8e15f15d67c5974445634bb971e2275e957aff","title":"Security Implications of Large Language Model Code Assistants: A User Study"}],"references":[{"paperId":"9d6e411065cd65b4291955c47b3f255ae668b81a","title":"StructCoder: Structure-Aware Transformer for Code Generation"},{"paperId":"ec229b004ebaee27b9efa84b834825dbc2937538","title":"Is GitHub's Copilot as Bad As Humans at Introducing Vulnerabilities in Code?"},{"paperId":"4b3e61d64f27bd05e1af781ea4f95ac31c280e8d","title":"M2TS: Multi-Scale Multi-Modal Approach Based on Transformer for Source Code Summarization"},{"paperId":"d407aa1cc3f9ce8478d3052344947fb49baa04d4","title":"CodeFill: Multi-token Code Completion by Jointly learning from Structure and Naming Sequences"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"},{"paperId":null,"title":"CWE - Common Weakness Enumeration"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"939c0c833a4864be9bdac99f630c9ea96caacc82","title":"The Prevalence of Code Smells in Machine Learning projects"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"04413d13c76c4eadf1adcbbe88c3a72e6462f166","title":"Fast and Memory-Efficient Neural Code Completion"},{"paperId":"1311adebe439ccd9e373d594a8df1d720a685190","title":"Code Prediction by Feeding Trees to Transformers"},{"paperId":null,"title":"Code Clippy Data: A large dataset of code data from Github for research into code language models"},{"paperId":null,"title":"OpenAI"},{"paperId":null,"title":"GPT Code Clippy: The Open Source version of GitHub Copilot"},{"paperId":"542b9f6b98b19f355c3c9672e6601b4f92f2708f","title":"Code Smells Detection and Visualization: A Systematic Literature Review"},{"paperId":"aa44017cf1f1bcf8f0b60439ad02758b9a467501","title":"A large-scale comparative analysis of Coding Standard conformance in Open-Source Data Science projects"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"51a920c3d201eec57bcc2e97e0268304f53b5161","title":"TreeGen: A Tree-Based Transformer Architecture for Code Generation"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"4c051b40c060502339b4a851ea550476baac536f","title":"Share, But be Aware: Security Smells in Python Gists"},{"paperId":"1cc47e5aea8d6d71817c438e4e790972c04722e2","title":"The Seven Sins: Security Smells in Infrastructure as Code Scripts"},{"paperId":"71c5f6c28e2e2bf32923a0eb0fadd01f6074f572","title":"Machine Learning Techniques for Code Smells Detection: A Systematic Mapping Study"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"b4be98bf9a1a635cd9ab8d14560c8371ca12eaaa","title":"Detecting code smells using machine learning techniques: Are we there yet?"},{"paperId":"f707407d6635aab658f3c6e67e67cf526be6b7f9","title":"Understanding metric-based detectable smells in Python software: A comparative study"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"4441b135aa0897a716d70c483c7a372ca843d1e9","title":"House of Cards: Code Smells in Open-Source C# Repositories"},{"paperId":"ca238987d2e99d99cbd60897c3134186990ccc9e","title":"Refactoring improving the design of existing code"},{"paperId":"00ab3faccde927281a07764f229afd7be736138c","title":"Security Smells in Android"},{"paperId":"82e00f9782346edcf08cac243bd5cfa1794e3ba9","title":"On the evaluation of code smells and detection tools"},{"paperId":"a99929c030d6f5dfe8ac31c72ea76e580e874b73","title":"On the diffuseness and the impact on maintainability of code smells: a large scale empirical investigation"},{"paperId":"3807a517403cfc7ed67fdfea64baf62b9948bcfe","title":"A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"625a9e9822603b79f754c4ce044760f7363b5eb6","title":"Data Quality Considerations for Big Data and Machine Learning: Going Beyond Data Cleaning and Transformations"},{"paperId":"00da5de02e2375b5193d842c677a6c2aedab1dc3","title":"Code Quality: Examining the Efficacy of Automated Tools"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"4d9b2f1939c1e697b1cef3fb4cc90ccb7172b7d1","title":"Detecting Code Smells in Python Programs"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"abdffea29bfddf10ae04f38c9428c11feb81df83","title":"A Cooperative Parallel Search-Based Software Engineering Approach for Code-Smells Detection"},{"paperId":"17274fef400424fe4876cd23edb8318e4944b203","title":"A Study of Different Coding Styles Affecting Code Readability"},{"paperId":"83417a8d2af07da97651e909223d825beeda946b","title":"Object-Oriented Metrics in Practice - Using Software Metrics to Characterize, Evaluate, and Improve the Design of Object-Oriented Systems"},{"paperId":"bf089bc9897253f5d92202a5986669365ee5e9bc","title":"The Adherence of Open Source Java Programmers to Standard Coding Practices"},{"paperId":"2fb9729c0a3b046c390ad6748773527eae6cfc84","title":"What Is Program Synthesis?"},{"paperId":"94d9cbfc474cb6ce961de45a06233b2853ca6724","title":"Toward automatic program synthesis"},{"paperId":"1beae9ef432a57bb5ec0c43944a07182814ab443","title":"Application of Theorem Proving to Problem Solving"},{"paperId":null,"title":"Pep 8 -the style guide for python code"},{"paperId":null,"title":"CodeSmell"},{"paperId":null,"title":"Github copilot : Your ai pair programmer"},{"paperId":null,"title":"How to ensure entropy and proper random numbers generation in virtual machines"}],"id":"8a854331c593c6a766fa3b8037fb2ad1b95a6f06","summary":"This study investigates to what extent code smells are present in the datasets of coding generation techniques and verify whether they leak into the output of these techniques."},{"url":"https://www.semanticscholar.org/paper/6bc87e51018d6de55011e95a0d43c588dd44a1e8","title":"ERNIE-Code: Beyond English-Centric Cross-lingual Pretraining for Programming Languages","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/12/2022","authors":"Yekun Chai,Shuohuan Wang,Chao Pang,Yu Sun,Hao Tian,Hua Wu","citations":[{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"}],"references":[{"paperId":"d6b7bc4e7968398101d8d9b4da7d4a0186763ff3","title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"789b8487da7188442085983caba3ffaae05531e9","title":"The Flores-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":null,"title":"2021), casual transformer decoders (Chen et al., 2021"},{"paperId":"23eec8ae5e1091f58a979d7ff179b9a628cbcd20","title":"CodeRetriever: Unimodal and Bimodal Contrastive Learning for Code Search"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"74276a37bfa50f90dfae37f767b2b67784bd402a","title":"mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer"},{"paperId":"f48f90464d9694e2ea18767f14842c64c9a1e8fb","title":"WikiMatrix: Mining 135M Parallel Sentences in 1620 Language Pairs from Wikipedia"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"bccfdbf542bc1d0d7172906db36dce542805d101","title":"Highway Transformer: Self-Gating Enhanced Self-Attentive Networks"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"bdbf780dfd6b3eb0c9e980887feae5f23af15bc4","title":"GLU Variants Improve Transformer"},{"paperId":"495da6f19baa09c6db3697d839e10432cdc25934","title":"Multilingual Denoising Pre-training for Neural Machine Translation"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"00c957711b12468cb38424caccdf5291bb354033","title":"ZeRO: Memory optimizations Toward Training Trillion Parameter Models"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"516497e7fc39ebb126bed28dcc5c6129e500acb8","title":"An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"212e10d10dc3abe680ea9db10aae44b966992236","title":"Non-Native English Speakers Learning Computer Programming: Barriers, Desires, and Design Opportunities"},{"paperId":"54a13bcc9613dcaa76fb25fbe96572f376cfcca9","title":"Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"},{"paperId":"25c0e5ad89b77ae9c7ff91765ebd4e1e21abdcb3","title":"The IIT Bombay English-Hindi Parallel Corpus"},{"paperId":"892e53fe5cd39f037cb2a961499f42f3002595dd","title":"Bag of Tricks for Efficient Text Classification"},{"paperId":"800366078f063a637e6a4880c0c49c217c7905ea","title":"The United Nations Parallel Corpus v1.0"},{"paperId":"285c165c81fc9275955147a892b9a039ec8b1052","title":"chrF: character n-gram F-score for automatic MT evaluation"},{"paperId":"34f25a8704614163c4095b3ee2fc969b60de4698","title":"Dropout: a simple way to prevent neural networks from overfitting"},{"paperId":"25ca4a36df2955b345634b5f8a6b6bb66a774b3c","title":"Parallel Data, Tools and Interfaces in OPUS"},{"paperId":"fe620306c0278e8c9ccd9519377ce26a70400aaf","title":"A Comparison of Pivot Methods for Phrase-Based Statistical Machine Translation"},{"paperId":"b69adc989be4d05c5b8b0f95e84caf4022d65fcc","title":"Catalan-English Statistical Machine Translation without Parallel Corpus : Bridging through Spanish"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"}],"id":"6bc87e51018d6de55011e95a0d43c588dd44a1e8","summary":"This work releases ERNIE-Code, a uniﬁed pre-trained language model for 116 NLs and 6 PLs, and employs two methods for universal cross-lingual pre-training: span-corruption language modeling that learns patterns from monolingual NL or PL; and pivot-based translationlanguage modeling that re-lies on parallel data of manyNLs and PLs."},{"url":"https://www.semanticscholar.org/paper/4ddc26b3a5fe9044b97b408d163f7464d769ebbf","title":"CODEP: Grammatical Seq2Seq Model for General-Purpose Code Generation","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":5,"influentialCitationCount":0,"publicationDate":2022,"authors":"Yihong Dong,Ge Li,Zhi Jin","citations":[{"paperId":"dd0960a8ffb6a8baff69ec96c3f67deb2912b53a","title":"Self-collaboration Code Generation via ChatGPT"},{"paperId":"f7e41afdd7b54af3ddb2bff8005efbce4de87d38","title":"Constructing Effective In-Context Demonstration for Code Intelligence Tasks: An Empirical Study"},{"paperId":"d39db76fb80c1f74455a0f6432c2242b727389da","title":"Self-planning Code Generation with Large Language Model"},{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"},{"paperId":"b109588511459bf46e94cb4eb68b5cf79b092795","title":"Antecedent Predictions Are More Important Than You Think: An Effective Method for Tree-Based Code Generation"}],"references":[{"paperId":"6e53a4e0f38d4be2dcaa2625a2a2a9332d338d05","title":"Incorporating domain knowledge through task augmentation for front-end JavaScript code generation"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"7a1069dafaeb484e22f2473d5545f1e45ce30656","title":"Compilable Neural Code Generation with Compiler Feedback"},{"paperId":"4b27f18bff43d605805c92696a979714ced0b805","title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"02da1f11b2620ee4a6c8b010fd1c8fe6f5ab5119","title":"Antecedent Predictions Are Dominant for Tree-Based Code Generation"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"ab7525d6b4fee650fccb8d6d44f6accc41e981a6","title":"An AST Structure Enhanced Decoder for Code Generation"},{"paperId":"5fbcfccd3736969d95ed660d8e6962c86b7a9113","title":"PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"3ca1430fb5bbf6ffa8f377f6b603648268f7546e","title":"Exploring Dynamic Selection of Branch Expansion Orders for Code Generation"},{"paperId":"9ba5501a22f8e2207280abb997bbea12fbf9027a","title":"Improving Tree-Structured Decoder Training for Code Generation via Mutual Learning"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"51a920c3d201eec57bcc2e97e0268304f53b5161","title":"TreeGen: A Tree-Based Transformer Architecture for Code Generation"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"b7af363ff612f41e0c1071ae24ce68a836aaa678","title":"Code Generation as a Dual Task of Code Summarization"},{"paperId":"5301f6320300ac87b42018da171f17e6b7842486","title":"Semantic Parsing with Dual Learning"},{"paperId":"3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c","title":"Reranking for Neural Semantic Parsing"},{"paperId":"9c2ab4707e902f6174910847e3f94f3b85017ec3","title":"A Grammar-Based Structural CNN Decoder for Code Generation"},{"paperId":"649c1148439a4e875dab414ba67bf8c80350af4a","title":"TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"2c1e874c3b67510a3215e535f5646b362de5bc89","title":"Abstract Syntax Networks for Code Generation and Semantic Parsing"},{"paperId":null,"title":"A Syntactic Neural Model for General- Purpose Code Generation"},{"paperId":"b7eac64a8410976759445cce235469163d23ee65","title":"Data Recombination for Neural Semantic Parsing"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"558ac446dc26bee9789d660a251b75728cb6eeb2","title":"Language to Logical Form with Neural Attention"},{"paperId":"27e1dbe9f7c71cd6cc1b0357f49aef497e572d09","title":"Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"},{"paperId":"9653d5c2c7844347343d073bbedd96e05d52f69b","title":"Pointer Networks"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"f9199e94e97ca68f1fc6d8758b53bb34bdb353fe","title":"A Half-Century of Automata Theory: Celebration and Inspiration"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"2be51657e5ca6f2777d7c9e95f120bbbadbcd4bd","title":"On Context-Free Languages and Push-Down Automata"},{"paperId":"7a93ad4af156636f5ce6a079e0f10d5c48a5cc95","title":"Application of pushdown-store machines"}],"id":"4ddc26b3a5fe9044b97b408d163f7464d769ebbf","summary":"This paper proposes CODEP, a grammatical Seq2Seq code generation framework equipped with a Pushdown automaton (PDA) module, and constructs the DPA for the most popular GPL Python and conducts extensive experiments to evaluate the effectiveness."},{"url":"https://www.semanticscholar.org/paper/3cba16fc46ac5b35c1cc72a822208aa0097384cc","title":"CodePAD: Sequence-based Code Generation with Pushdown Automaton","venue":"","year":2022,"referenceCount":40,"citationCount":1,"influentialCitationCount":0,"publicationDate":"02/11/2022","authors":"Yihong Dong,Xue Jiang,Yuchen Liu,Ge Li,Zhi Jin","citations":[{"paperId":"372dd962ff8c92ac5cf9a195fd52b4b79372c149","title":"A Syntax-Guided Multi-Task Learning Approach for Turducken-Style Code Generation"}],"references":[{"paperId":"6e53a4e0f38d4be2dcaa2625a2a2a9332d338d05","title":"Incorporating domain knowledge through task augmentation for front-end JavaScript code generation"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"7a1069dafaeb484e22f2473d5545f1e45ce30656","title":"Compilable Neural Code Generation with Compiler Feedback"},{"paperId":"4b27f18bff43d605805c92696a979714ced0b805","title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"02da1f11b2620ee4a6c8b010fd1c8fe6f5ab5119","title":"Antecedent Predictions Are Dominant for Tree-Based Code Generation"},{"paperId":"ab7525d6b4fee650fccb8d6d44f6accc41e981a6","title":"An AST Structure Enhanced Decoder for Code Generation"},{"paperId":"5fbcfccd3736969d95ed660d8e6962c86b7a9113","title":"PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"3ca1430fb5bbf6ffa8f377f6b603648268f7546e","title":"Exploring Dynamic Selection of Branch Expansion Orders for Code Generation"},{"paperId":"9ba5501a22f8e2207280abb997bbea12fbf9027a","title":"Improving Tree-Structured Decoder Training for Code Generation via Mutual Learning"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"51a920c3d201eec57bcc2e97e0268304f53b5161","title":"TreeGen: A Tree-Based Transformer Architecture for Code Generation"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"b7af363ff612f41e0c1071ae24ce68a836aaa678","title":"Code Generation as a Dual Task of Code Summarization"},{"paperId":"5301f6320300ac87b42018da171f17e6b7842486","title":"Semantic Parsing with Dual Learning"},{"paperId":"3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c","title":"Reranking for Neural Semantic Parsing"},{"paperId":"9c2ab4707e902f6174910847e3f94f3b85017ec3","title":"A Grammar-Based Structural CNN Decoder for Code Generation"},{"paperId":"649c1148439a4e875dab414ba67bf8c80350af4a","title":"TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"43428880d75b3a14257c3ee9bda054e61eb869c0","title":"Convolutional Sequence to Sequence Learning"},{"paperId":"2c1e874c3b67510a3215e535f5646b362de5bc89","title":"Abstract Syntax Networks for Code Generation and Semantic Parsing"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"b7eac64a8410976759445cce235469163d23ee65","title":"Data Recombination for Neural Semantic Parsing"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"558ac446dc26bee9789d660a251b75728cb6eeb2","title":"Language to Logical Form with Neural Attention"},{"paperId":"27e1dbe9f7c71cd6cc1b0357f49aef497e572d09","title":"Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"},{"paperId":"9653d5c2c7844347343d073bbedd96e05d52f69b","title":"Pointer Networks"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"89b1f4740ae37fd04f6ac007577bdd34621f0861","title":"Generating Sequences With Recurrent Neural Networks"},{"paperId":"f9199e94e97ca68f1fc6d8758b53bb34bdb353fe","title":"A Half-Century of Automata Theory: Celebration and Inspiration"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"2be51657e5ca6f2777d7c9e95f120bbbadbcd4bd","title":"On Context-Free Languages and Push-Down Automata"},{"paperId":"7a93ad4af156636f5ce6a079e0f10d5c48a5cc95","title":"Application of pushdown-store machines"}],"id":"3cba16fc46ac5b35c1cc72a822208aa0097384cc","summary":"This paper devise a pushdown automaton (PDA)-based methodology to address the problem of grammar constraints of programming language (PL), and proposes CodePAD, a sequence-based code generation framework equipped with a PDA module, to integrate the deduction of PDA into deep learning."},{"url":"https://www.semanticscholar.org/paper/5ff9032d0f7f246d01ae7b2c231ab03469a7344a","title":"Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?","venue":"ArXiv","year":2021,"referenceCount":56,"citationCount":25,"influentialCitationCount":1,"publicationDate":2021,"authors":"H. Pearce,B. Tan,Baleegh Ahmad,R. Karri,Brendan Dolan-Gavitt","citations":[{"paperId":"c6808575096a6e4f3cbdc5f893384bc5a01cc6f8","title":"A study on Prompt Design, Advantages and Limitations of ChatGPT for Deep Learning Program Repair"},{"paperId":"491c17efd2e70c7efc6f2e1ba554d5caee07af0c","title":"Challenging the appearance of machine intelligence: Cognitive bias in LLMs"},{"paperId":"3e7055980da1853d196fcee0d3b7f390fb518d62","title":"A Survey on Automated Program Repair Techniques"},{"paperId":"645960e8bcb2a55bc7e8816c49e60e8f98303acb","title":"Combining Contexts from Multiple Sources for Documentation-Specific Code Example Generation"},{"paperId":"af622206f1e5a6632600075897385cdabcae463d","title":"Large Language Models and Simple, Stupid Bugs"},{"paperId":"270b015093073d3ba254928b6d736a59870d3fb1","title":"Chatbots As Fluent Polyglots: Revisiting Breakthrough Code Snippets"},{"paperId":"3b8ccc7ec80b8775de603e248ac1ca2b919d6b70","title":"Chatbots in a Botnet World"},{"paperId":"8175ce2cbb99b6a394bdac152ae39d413f4f1380","title":"Codex Hacks HackerRank: Memorization Issues and a Framework for Code Synthesis Evaluation"},{"paperId":"5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","title":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming"},{"paperId":"825333b7efe2cade106eaf36c7e731f757974806","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot"},{"paperId":"9360390b02b9a09ece9a2486055b17e18dc5d3f6","title":"Automatic Code Documentation Generation Using GPT-3"},{"paperId":"0509c25103939d59ed4e27b1393e74ad5734c453","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot"},{"paperId":"99f85119f113b5498517928eff74a904b69e37b7","title":"CCTEST: Testing and Repairing Code Completion Systems"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"0d08ffccc982781e310bb184397bbe64b9aef157","title":"Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models"},{"paperId":"6074c7b75f27ca9adb6d74b080c07d5d079c3ea0","title":"Improving automatically generated code from Codex via Automated Program Repair"},{"paperId":"0bcd59da541fdae66884afba8d25475a54a9da1a","title":"Automated Repair of Programs from Large Language Models"},{"paperId":"1f3a97d255a6d0b2d5535af75737ed43b1e96fc4","title":"Is GitHub Copilot a Substitute for Human Pair-programming? An Empirical Study"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"ec229b004ebaee27b9efa84b834825dbc2937538","title":"Is GitHub's Copilot as Bad As Humans at Introducing Vulnerabilities in Code?"},{"paperId":"9ac5e1ffa8f837671a89354d4e298b1adeb08d79","title":"Enabling Automatic Repair of Source Code Vulnerabilities Using Data-Driven Methods"},{"paperId":"763792c655e591c5d61f67d7ac9cbecbcb5f4508","title":"Pop Quiz! Can a Large Language Model Help With Reverse Engineering?"},{"paperId":null,"title":"Materials ExerciseTeacher Student Attempt Feedback"},{"paperId":"6df98ac2300c6e9c232440147ba976b4f501ca67","title":"C ODEX HACKS H ACKER R ANK : B ENEFITS AND R ISKS OF L ARGE -S CALE S OURCE C ODE M ODELS"},{"paperId":"b2c4fdb49bdb23e6ad0ac3272029324157046ea7","title":"Automated Repair of Code from Language Models"}],"references":[{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"fc70c090ce5df3672f47e4ec8a4ae40bdab1a5c1","title":"Automated Bug Hunting With Data-Driven Symbolic Root Cause Analysis"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"3a863672b072a37ac8d2eb6b780234f6b748a2d9","title":"Generating bug-fixes using pretrained transformers"},{"paperId":"171440398a1c0f43063a7689e3b385280336fb68","title":"CURE: Code-Aware Neural Machine Translation for Automatic Program Repair"},{"paperId":"59cc0871534a20ac98db0663539c6675ab63566f","title":"SequenceR: Sequence-to-Sequence Learning for End-to-End Program Repair"},{"paperId":null,"title":"CodeQL documentation"},{"paperId":null,"title":"Jurassic-1: Technical Details and Evaluation"},{"paperId":null,"title":"2021 CWE Top 25 Most Dangerous Software Weaknesses"},{"paperId":"3f97c2067cde9377e50b3160bbd7982c94abd88a","title":"An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions"},{"paperId":"50697de979081a2d605d63b490568b1dd76398e1","title":"ARCUS: Symbolic Root Cause Analysis of Exploits in Production Systems"},{"paperId":null,"title":"Jurassic-1 Language Models - AI21 Studio Docs"},{"paperId":null,"title":"OpenAI Codex"},{"paperId":null,"title":"CWE-1194: CWE VIEW: Hardware Design"},{"paperId":"fec5fd0d466de52e9fe03d8efffba47ee70fbb11","title":"Fix that Fix Commit: A real-world remediation analysis of JavaScript projects"},{"paperId":"e0d4587181a8848e73612e8a32b02bd9cc82b595","title":"CoCoNuT: combining context-aware neural translation models using ensemble for program repair"},{"paperId":"7992dd60f4edec88eb20af0d1648ca1a6aea8935","title":"The Secret Life of Commented-Out Source Code"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0d3e7ed508876d8bc54a52570d66740228f64e12","title":"Is there a correlation between code comments and issues?: an exploratory study"},{"paperId":"9a08932eb095cbf78392f116d7fa3f06e154540d","title":"Beyond Tests: Program Vulnerability Repair via Crash Constraint Extraction"},{"paperId":null,"title":"CWE - CWE-Compatible Products and Services"},{"paperId":"deabe3945e100ec42c80582652aa149c2e1b66b0","title":"Automatic Software Repair: a Bibliography"},{"paperId":"516497e7fc39ebb126bed28dcc5c6129e500acb8","title":"An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"59d56bcf8ce9c77399b42747813696cb2cf97f8b","title":"An Empirical Investigation into Learning Bug-Fixing Patches in the Wild via Neural Machine Translation"},{"paperId":"0721fa2c9cde26ad79cf4a34c55e09f02e786bda","title":"Fuzzing: a survey"},{"paperId":"256d5f9dcccb2d2f6f6b7dc541f2e95b9037d810","title":"A Large-Scale Empirical Study of Security Patches"},{"paperId":"27f0d1b0a00e534f7808aa592d1d264a0e9b6ce9","title":"Classifying Code Comments in Java Open-Source Software Systems"},{"paperId":"eed74a8f02101566f5ee0b9a66e59bb508b32f5c","title":"Failure sketching: a technique for automated root cause diagnosis of in-production failures"},{"paperId":"193136b86539bd6df3f57a3685629c049a037418","title":"An analysis of patch plausibility and correctness for generate-and-validate patch generation systems"},{"paperId":null,"title":"GCC Undefined Behavior Sanitizer - ubsan"},{"paperId":"701c50a6d273f71951da833394a99e9132a7b768","title":"Quality analysis of source code comments"},{"paperId":"c8349d8727c474b16126cd4386aacc48e8d74ce4","title":"Using likely invariants for automated software fault localization"},{"paperId":"db734a0e1dc65fe3fe2eef474aefba6d083f54dd","title":"A New Algorithm For Data Compression"},{"paperId":"71a27d378704cac951bfeac1d61daa853fee1cf0","title":"Generating formal system models from natural language descriptions"},{"paperId":"29146d2b80bc8c56f7e18efe8d2c92354254e947","title":"AddressSanitizer: A Fast Address Sanity Checker"},{"paperId":"3aa35c213730e688fa191a1ce241db31087956f0","title":"GenProg: A Generic Method for Automatic Software Repair"},{"paperId":"3e264b8bd4dacb78537347616565a3805a44b047","title":"What would other programmers do: suggesting solutions to error messages"},{"paperId":"dd892c577eb45f3237c1e4d2513b1674cbc2043c","title":"BugFix: A learning-based tool to assist developers in fixing bugs"},{"paperId":"d8138bcf285547b1b0939c9ad86ecd97a1b82621","title":"/*icomment: bugs or bad comments?*/"},{"paperId":"b1a6ec04c658e0069def94f04c3f2454b9a0f2f9","title":"NLP (Natural Language Processing) for NLP (Natural Language Programming)"},{"paperId":"857fc1be23924b3ca08c320390cdfc38bddee543","title":"Isolating failure-inducing thread schedules"},{"paperId":"a397c5d4fadcc0ce29123b78815987b543417ba4","title":"Simplifying and Isolating Failure-Inducing Input"},{"paperId":"1aa9c0045f1fe8c79cce03c7c14ef4b4643a21f8","title":"A new algorithm for data compression"},{"paperId":null,"title":"APPENDIX Source and Dataset Access We provide the current version of the data and our scripts at the following Google Drive link"},{"paperId":null,"title":"Discover Use Cases for AI21 Studio and Jurassic-1"},{"paperId":null,"title":"How to generate text : using different decoding methods for language generation with Transformers , ” Mar . 2020"},{"paperId":null,"title":"OWASP , “ Source Code Analysis Tools . ” [ Online ]"},{"paperId":null,"title":"List of tools for static code analysis"},{"paperId":null,"title":"Completion -OpenAI API"},{"paperId":null,"title":"OpenAI ’ s API Now Available with No Waitlist , ” Nov . 2021 . [ Online ]"},{"paperId":null,"title":"OpenAI API | Tokenizer . ” [ Online ]"},{"paperId":null,"title":"Verilator User ’ s Guide — Verilator 4 . 202 documentation . ” [ Online ]"},{"paperId":null,"title":"GitHub Copilot · Your AI pair programmer"},{"paperId":null,"title":"Available: https"}],"id":"5ff9032d0f7f246d01ae7b2c231ab03469a7344a","summary":"This work examines the use of large language models for code (such as OpenAI's Codex and AI21's Jurassic J-1) for zero-shot vulnerability repair, and investigates challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code."},{"url":"https://www.semanticscholar.org/paper/a1ef81e17a9ca41e09aba802040a2eca2744716f","title":"Generation Probabilities are Not Enough: Improving Error Highlighting for AI Code Suggestions","venue":"","year":2022,"referenceCount":15,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"Helena Vasconcelos","citations":[{"paperId":"57ffd52abfbba0eb76c8406c7331f1f8ca29aaff","title":"R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents"},{"paperId":"6c0f2c8b6dbfbc0ea7704ea0e519b7e6aef72855","title":"Designing AI for Appropriation Will Calibrate Trust"},{"paperId":"6257d48954680fb4e7df9b663e0ac50db4404046","title":"On the Security Vulnerabilities of Text-to-SQL Models"}],"references":[{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","title":"Investigating Explainability of Generative AI for Code through Scenario-based Design"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"},{"paperId":"c32cd2f5ed69279ca3d562213ca9d41c47235e4f","title":"When Do XAI Methods Work? A Cost-Benefit Approach to Human-AI Collaboration"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"3973e0fab69a00f5ed6a81ca408f60c420fa6e61","title":"Uncertainty as a Form of Transparency: Measuring, Communicating, and Using Uncertainty"},{"paperId":"1109f787fc8d51feb3bae9bf6e1945dc4a1191e7","title":"Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance"},{"paperId":"19e074731a7e337e8cc6453a73fec44c0ae90b02","title":"\"Hello AI\": Uncovering the Onboarding Needs of Medical Practitioners for Human-AI Collaborative Decision-Making"},{"paperId":"53eef24a59b12107e6188e121c85f85fa8a78100","title":"Explainable machine-learning predictions for the prevention of hypoxaemia during surgery"},{"paperId":"cc187b7c086934cbdef4576b4cd20917770c6ded","title":"Can AI become Reliable Source to Support Human Decision Making in a Court Scene?"},{"paperId":null,"title":"Machine bias: There’s software across the country to predict future criminals and it’s biased against blacks"},{"paperId":"40debbfb0e5c2e76e546d7f53f86cfdd1bcb9701","title":"Complacency and Automation Bias in the Use of Imperfect Automation"},{"paperId":null,"title":"The world's leading online programming learning platform"},{"paperId":"b0e5a85803bb959ed2cbd47c51009cc48059c02c","title":"Complacency and Bias in Human Use of Automation: An Attentional Integration"},{"paperId":null,"title":"GitHub Copilot -Your AI pair programmer"}],"id":"a1ef81e17a9ca41e09aba802040a2eca2744716f","summary":"It is unclear how best to convey the uncertainty of generative models to human operators or if doing so will positively impact human-AI collaboration."},{"url":"https://www.semanticscholar.org/paper/bb7e46f316d319f9819c3554c99995ef8361ae9c","title":"CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":3,"influentialCitationCount":0,"publicationDate":"19/04/2022","authors":"Immanuel Trummer","citations":[{"paperId":"8a2a2006e371ce07e0b29538cd11337a5c977caf","title":"“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models"},{"paperId":"af622206f1e5a6632600075897385cdabcae463d","title":"Large Language Models and Simple, Stupid Bugs"},{"paperId":"1b45949be1b203f096d6451d88cf91174c620be7","title":"From Natural Language to Simulations: Applying GPT-3 Codex to Automate Simulation Modeling of Logistics Systems"}],"references":[{"paperId":"51000d9f79be0eefd7972fe94e3c71dddc90d2c6","title":"Evaluating the Text-to-SQL Capabilities of Large Language Models"},{"paperId":"2236b6036cd1ecba1792d5e1fedbae7cefc3d43b","title":"Can we generate shellcodes via natural language? An empirical study"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis"},{"paperId":"6fa43591e6da321e43722dac609f6b8ef8204768","title":"SeaD: End-to-end Text-to-SQL Generation with Schema-aware Denoising"},{"paperId":"5fbcfccd3736969d95ed660d8e6962c86b7a9113","title":"PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"fe935caed47ef090a306d6d09240f76adc43a420","title":"Ember: No-Code Context Enrichment via Similarity-Based Keyless Joins"},{"paperId":"bfe6cce534d76df1aa7ebd629c09e30d6b8abdd4","title":"RPT: Relational Pre-trained Transformer Is Almost All You Need towards Democratizing Data Preparation"},{"paperId":null,"title":"Fotios Chantzis"},{"paperId":"43a712e8b9d2db596abd6dc2ca0ffedb8878cde3","title":"ATHENA++: Natural Language Querying for Complex Nested SQL Queries"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"5452cd8a1ebb941e473f0e5c8ba6cc7359e40b24","title":"DBPal: Weak Supervision for Learning a Natural Language Interface to Databases"},{"paperId":"157a7ae44613a1fcf34e2be8c1e19a4f6e3c50e3","title":"Transfer Learning in Natural Language Processing"},{"paperId":"5ba52bbe1101939c490a06cc0cf316a09000834e","title":"Neo: A Learned Query Optimizer"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"287050dc91b146768c9d4435e5582fc9975ba84c","title":"Learned Cardinalities: Estimating Correlated Joins with Deep Learning"},{"paperId":"585b91181b5eff055f87bce084e45c448524ca3d","title":"SkinnerDB: Regret-Bounded Query Evaluation via Reinforcement Learning"},{"paperId":"f6eee02a2f4c74c2b543bf419f76cef60d5752f8","title":"The Case for Learned Index Structures"},{"paperId":"116a877c774969b53399b1ccaa34d51bfb492ee4","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"ac67d5f9c89d8d72fbd074f94079608220348f3f","title":"ATHENA: An Ontology-Driven System for Natural Language Querying over Relational Data Stores"},{"paperId":"1c39f884bc53d7d5465d0f56c4b432a9352afbe7","title":"Understanding Natural Language Queries over Relational Databases"},{"paperId":"888764f05a60d770cfc0b49944308fd92ed45ee5","title":"How Good Are Query Optimizers, Really?"},{"paperId":"42414b70fc61def8adcf5c159604c72e4508e9c1","title":"NaLIR: an interactive natural language interface for querying relational databases"},{"paperId":"bac4169d6b6f713c76271b5ccf3d45293351f785","title":"Runtime Code Generation in Cloudera Impala"},{"paperId":"56eeedcd45e384f1d73cd27b4aa426c154554bbb","title":"Generating code for holistic query evaluation"},{"paperId":null,"title":"Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-Art Natural Language Processing"}],"id":"bb7e46f316d319f9819c3554c99995ef8361ae9c","summary":"CodexDB is a framework on top of GPT-3 Codex that decomposes complex SQL queries into a series of simple processing steps, described in natural language, enriched with user-provided instructions and descriptions of database properties."},{"url":"https://www.semanticscholar.org/paper/1d160123cbbef972ea151a641dd435d57c727de8","title":"AixBench: A Code Generation Benchmark Dataset","venue":"ArXiv","year":2022,"referenceCount":4,"citationCount":6,"influentialCitationCount":1,"publicationDate":"27/06/2022","authors":"Yiyang Hao,Ge Li,Yongqiang Liu,Xiaowei Miao,He Zong,Siyuan Jiang,Yang Liu,He Wei","citations":[{"paperId":"3b01b5b497e1f359b11da45af029281ee6f64c2c","title":"Towards Enhancing In-Context Learning for Code Generation"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"13a66fc8689724e295548ceac9e5425fc46cc093","title":"SkCoder: A Sketch-based Approach for Automatic Code Generation"},{"paperId":"a764d4f28612a7b5c4767ea6a2540b169fdb052c","title":"CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models"},{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"},{"paperId":"46d0a832fada6147bceb0bd4e39928e482733246","title":"How Important are Good Method Names in Neural Code Generation? A Model Robustness Perspective"}],"references":[{"paperId":"a08a3b08a5a1de6462a7da2906b1cd81691d6c18","title":"CERT: Continual Pre-Training on Sketches for Library-Oriented Code Generation"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"}],"id":"1d160123cbbef972ea151a641dd435d57c727de8","summary":"A benchmark dataset for evaluating method-level code generation task and a new metric for automatically evaluating the correctness of the generated code, and a set of criteria to manually evaluating the overall quality of thegenerated code are presented."},{"url":"https://www.semanticscholar.org/paper/114eb5a35a3cd802cd1f46fff35c284e32ef6c54","title":"GitHub Copilot AI pair programmer: Asset or Liability?","venue":"ArXiv","year":2022,"referenceCount":56,"citationCount":15,"influentialCitationCount":1,"publicationDate":"30/06/2022","authors":"Arghavan Moradi Dakhel,Vahid Majdinasab,Amin Nikanjam,F. Khomh,M. Desmarais,Z. Jiang","citations":[{"paperId":"0115e7945158195d3d159370dabd121e074d8142","title":"A Case Study on Scaffolding Exploratory Data Analysis for AI Pair Programmers"},{"paperId":"c2dcbf6a879ee1054e103beceb64825611a3fd9a","title":"LLM-based Interaction for Content Generation: A Case Study on the Perception of Employees in an IT department"},{"paperId":"dca09e13fe9b4f46b1e10b966f3ede63f63f9489","title":"Understanding the Usability of AI Programming Assistants"},{"paperId":"e1a17cb16742cf484aedc9ae106363d82454172e","title":"Practices and Challenges of Using GitHub Copilot: An Empirical Study"},{"paperId":"f0ea9e2d3889d37f34743ed1dc64f11e8e0484de","title":"Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models"},{"paperId":"c532f1df90925e5c69789f0cd99248d8a2a2e5bc","title":"My AI Wants to Know if This Will Be on the Exam: Testing OpenAI’s Codex on CS2 Programming Exercises"},{"paperId":"dca3bc28a7d404b28780a813ea7072eda809e6c0","title":"Programming Is Hard - Or at Least It Used to Be: Educational Opportunities and Challenges of AI Code Generation"},{"paperId":"0566c1c3eeeef5c968fced6d80b77fe22d02bbd9","title":"Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language"},{"paperId":"c2a259bb42cedad59469797b2175f7ca062c2bd7","title":"User-Driven Support for Rapid Visualization Prototyping in D3"},{"paperId":"9bed7e0205963eaa332721b25f024cd1e876c30d","title":"User-Driven Support for Visualization Prototyping in D3"},{"paperId":"3078a916b57e465614efea50628c889799e32289","title":"\"It would work for me too\": How Online Communities Shape Software Developers' Trust in AI-Powered Code Generation Tools"},{"paperId":"5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","title":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming"},{"paperId":"9afab8dc694269d205d769eaea549d8f7558d776","title":"Exploring the Verifiability of Code Generated by GitHub Copilot"},{"paperId":"a889a606734cd47f802765866487c677497a70d6","title":"Lost at C: A User Study on the Security Implications of Large Language Model Code Assistants"},{"paperId":"1c8e15f15d67c5974445634bb971e2275e957aff","title":"Security Implications of Large Language Model Code Assistants: A User Study"}],"references":[{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"cdfe9580f63070f311151444f9df32818cc858bf","title":"An Empirical Evaluation of GitHub Copilot's Code Suggestions"},{"paperId":"1f3a97d255a6d0b2d5535af75737ed43b1e96fc4","title":"Is GitHub Copilot a Substitute for Human Pair-programming? An Empirical Study"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"ec229b004ebaee27b9efa84b834825dbc2937538","title":"Is GitHub's Copilot as Bad As Humans at Introducing Vulnerabilities in Code?"},{"paperId":"28c7e583d90ccfc5c3078dfc1d6b80a9ad90248d","title":"Quantifying Memorization Across Neural Language Models"},{"paperId":"979eb5c97c49d7979447ed684500895a24d75ac4","title":"The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"1bc8906fbdd719ec6bf438897c9f5d6a432703ec","title":"The Potential of Artificial Intelligence as a Method of Software Developer's Productivity Improvement"},{"paperId":"8417d83e74a63b9d3874cd7695477ebad8b98f8a","title":"Choose your programming copilot: a comparison of the program synthesis performance of github copilot and genetic programming"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"},{"paperId":null,"title":"and N"},{"paperId":"f7987fa2aadc0b368c185dc4d2fdb1337a202c32","title":"Solving Probability and Statistics Problems by Program Synthesis"},{"paperId":"4e40595d6ecba027cebb4f2e3b43ae44bcf51daf","title":"Solving Linear Algebra by Program Synthesis"},{"paperId":"040e38582de947385c0515db40a14742aa19d113","title":"Recent Developments in Program Synthesis with Evolutionary Algorithms"},{"paperId":"830ecd0005a744cdff4117bb204eb5e60c7e628b","title":"Readability and Understandability Scores for Snippet Assessment: an Exploratory Study"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"d854b0b3afd652882a65111260dfe5c1ab2b89b9","title":"Assessing Developer Expertise from the Statistical Distribution of Programming Syntax Patterns"},{"paperId":"ae6496ec4c6d34fec0c044187cc07c91c923f848","title":"Automatically Assessing Code Understandability"},{"paperId":"2a074e2ac78ebb6158457aa4045fd92aeaba1ac1","title":"Introduction to Algorithms"},{"paperId":"28797c288403828f29d3faca6ab9d64d134a4ea3","title":"The SPACE of Developer Productivity: There's more to it than you think"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"195eb3fdb88a255dcb6d133728872bf09faa27bc","title":"Characterizing the Pedagogical Benefits of Adaptive Feedback for Compilation Errors by Novice Programmers"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"4b01fa22d8ae8200e4ee2430b13fc0e0fb0eb564","title":"The Amazing World of Quantum Computing"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"044d43edb63a7119efa6e1ba99b2ee64aa8c4886","title":"Replication package for"},{"paperId":"edb1798a5eea12893641c6de70afc67b2bb86894","title":"Fundamental Limits to Computing"},{"paperId":null,"title":"Comparing python programs using abstract syntax trees"},{"paperId":"325aecaa34bcf6e5f0e6683e526dc9946a6c1f4f","title":"Re-Factoring Based Program Repair Applied to Programming Assignments"},{"paperId":"9e3e00d59e3673c4a17f94a42c41fb813165e460","title":"Machine Translation from Natural Language to Code using Long-Short Term Memory"},{"paperId":"9e955da40c62b2543b963454b90928ad6cbd5f8a","title":"Does BLEU Score Work for Code Migration?"},{"paperId":"ae42db09690d9dad7f647e8fb1290ba3b0f92c0b","title":"Improving Source Code Readability: Theory and Practice"},{"paperId":"bea4d272b2f9a3d0869be9db5ac0b1f60986a3e7","title":"Impacts of coding practices on readability"},{"paperId":"30a331410b32cb1a08e2906ac57eb4e22069e664","title":"Automated clustering and program repair for introductory programming assignments"},{"paperId":"3c94afaf725b2733d0a161f7807b7815239d1cbb","title":"Cyclomatic Complexity"},{"paperId":"46b91efc27f7c227b50e85de08b247ec9dce3626","title":"GLAsT: Learning formal grammars to translate natural language specifications into hardware assertions"},{"paperId":"f868f0a129e95b964c3cd905a25b71435919385c","title":"Knowledge transfer in pair programming: An in-depth analysis"},{"paperId":"f7ba81057c8ec4f1c039027232034143a5afe6bf","title":"Syntax-guided synthesis"},{"paperId":"6d1662b1ca566631447c301c20ba1e3570a58c85","title":"Cyclomatic complexity: The nesting problem"},{"paperId":"71a27d378704cac951bfeac1d61daa853fee1cf0","title":"Generating formal system models from natural language descriptions"},{"paperId":"7646239afa9ecdfbe09ac380989017eb173f4349","title":"An Empirical Study on Factors Impacting Bug Fixing Time"},{"paperId":"bc6dff14a130c57a91d5a21339c23471faf1d46f","title":"Et al"},{"paperId":"18b8ef71bc01b8658b4ef2c8b9a9e4e6e5c2a07b","title":"Dimensions in program synthesis"},{"paperId":"6127e121a5a4f7caf0cd1a1eb9caf3e5e158404b","title":"An interpretation of the results of the analysis of pair programming during novices integration in a team"},{"paperId":"7d12562a96f857d2ffc99acd1006fced59a6e7ab","title":"Role of collective ownership and coding standards in coordinating expertise in software project teams"},{"paperId":"ab2648283e6bf167d6b6ff843fe80ed4b428ceed","title":"On the automation of fixing software bugs"},{"paperId":"47f618081e945dd27a0d76012181ae8e367679ea","title":"Pair programming productivity: Novice-novice vs. expert-expert"},{"paperId":"892c20d3c61868a1f3dec93053eb513e89700ef5","title":"How long did it take to fix bugs?"},{"paperId":"b1a6ec04c658e0069def94f04c3f2454b9a0f2f9","title":"NLP (Natural Language Processing) for NLP (Natural Language Programming)"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"0a5d358e643f46f5a5d20892417260800cccc345","title":"Experimentation in Software Engineering"},{"paperId":"4719c7a6acd699c824026c82e5de5f8fe386fa99","title":"Algorithms & data structures"},{"paperId":"ceb3163c56465fda5fef591d0ff0a6c7f434a04d","title":"A Deductive Approach to Program Synthesis"},{"paperId":"9e463eefadbcd336c69270a299666e4104d50159","title":"A Coefficient of Agreement for Nominal Scales"}],"id":"114eb5a35a3cd802cd1f46fff35c284e32ef6c54","summary":"Comparing Copilot to humans, the results show that the correct ratio of human solutions is greater than Copilot’s correct ratio, while the buggy solutions generated by Copilot require less time to be repaired."},{"url":"https://www.semanticscholar.org/paper/8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models","venue":"ArXiv","year":2022,"referenceCount":66,"citationCount":23,"influentialCitationCount":2,"publicationDate":"30/06/2022","authors":"Shraddha Barke,M. James,N. Polikarpova","citations":[{"paperId":"78602a7112b6ad1e6150ad04023a3b96636425e1","title":"GenAICHI 2023: Generative AI and HCI at CHI 2023"},{"paperId":"72888350e9f175c2a85cb248a6084f7b868b5121","title":"Whose Text Is It Anyway? Exploring BigCode, Intellectual Property, and Ethics"},{"paperId":"f352a968c8735fac58912870a7bde57fcfc2e6bd","title":"\"It's Weird That it Knows What I Want\": Usability and Interactions with Copilot for Novice Programmers"},{"paperId":"dca09e13fe9b4f46b1e10b966f3ede63f63f9489","title":"Understanding the Usability of AI Programming Assistants"},{"paperId":"9b1367167e45bac5e039ba0407cb3329125e9ff2","title":"Can Generative Pre-trained Transformers (GPT) Pass Assessments in Higher Education Programming Courses?"},{"paperId":"57ffd52abfbba0eb76c8406c7331f1f8ca29aaff","title":"R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents"},{"paperId":"f5d93cf5d81575aeee61faeddde4437fbcb74cd0","title":"The Programmer’s Assistant: Conversational Interaction with a Large Language Model for Software Development"},{"paperId":"038f249ab708cebae2a58265b768b9b1cbadad3a","title":"The Impact of AI on Developer Productivity: Evidence from GitHub Copilot"},{"paperId":"b0335eb2b9a1684fc16ff79234f0b206b30da169","title":"A Study of Editor Features in a Creative Coding Classroom"},{"paperId":"0192638593c430e15e2804a74a3e8a7ecb52d435","title":"On the Design of AI-powered Code Assistants for Notebooks"},{"paperId":"2abed82162c47a0cc32cd62afcf46b0745541017","title":"Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book"},{"paperId":"0566c1c3eeeef5c968fced6d80b77fe22d02bbd9","title":"Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language"},{"paperId":"cbbacf9ba52942fd6a43588295385e2ab39545b0","title":"Interacting with Next-Phrase Suggestions: How Suggestion Systems Aid and Influence the Cognitive Processes of Writing"},{"paperId":"433def684b5a9de5a9163f50b9004a44a11128b1","title":"CoCoMIC: Code Completion By Jointly Modeling In-file and Cross-file Context"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"3078a916b57e465614efea50628c889799e32289","title":"\"It would work for me too\": How Online Communities Shape Software Developers' Trust in AI-Powered Code Generation Tools"},{"paperId":"ce3f027b68dad014a58aa35f52380932c8d0b209","title":"Do Users Write More Insecure Code with AI Assistants?"},{"paperId":"2577d053f8aab912d29b424e1f09133d83740fd2","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","title":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming"},{"paperId":"e73a16490c29530c37b49f6a30592790e7caaaa4","title":"Don't Complete It! Preventing Unhelpful Code Completion for Productive and Sustainable Neural Code Completion Systems"},{"paperId":"ef29fb8cc6bdda4b011288f51da521d3c25fc53d","title":"Learning to Prevent Profitless Neural Code Completion"},{"paperId":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?"},{"paperId":"ec229b004ebaee27b9efa84b834825dbc2937538","title":"Is GitHub's Copilot as Bad As Humans at Introducing Vulnerabilities in Code?"}],"references":[{"paperId":null,"title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"99323bd786ee5be1e1aa589858e14e89630f207b","title":"Exploring the Learnability of Program Synthesizers by Novice Programmers"},{"paperId":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?"},{"paperId":"ec4c8d99eb1c028c43af6d8bbf727392d351cb59","title":"Efficient Training of Language Models to Fill in the Middle"},{"paperId":"4bc040835fbff57ce6612306d794b8c6c8226086","title":"Discovering the Syntax and Strategies of Natural Language Programming with Generative Language Models"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"14d7f1f68c17c4fde68679f40d035cbb011cfc7c","title":"Synthesizing analytical SQL queries from computation demonstration"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"},{"paperId":"3f791ea6584b78fdf0ed80560d09d9496cf5a353","title":"Learning to Complete Code with Sketches"},{"paperId":"4a160efbe80c38cd5eb2f92c7c095b49b113397d","title":"In-IDE Code Generation from Natural Language: Promise and Challenges"},{"paperId":null,"title":"Association for Computing Machinery, New York, NY, USA, 627–648"},{"paperId":"3fc1c9801682d97c6bba6ea4fd73043b30d9c45e","title":"How statically-typed functional programmers write code"},{"paperId":"ba919b8458b09e19c2b7cb9b98d362afe355b28a","title":"LooPy: interactive program synthesis with control structures"},{"paperId":"acdeb7c703fe436c8285a563ae6a6f6b4c9a6345","title":"reCode : A Lightweight Find-and-Replace Interaction in the IDE for Transforming Code by Example"},{"paperId":"942298ee6e7a20f36730c6d6ba240a4e20ba7508","title":"A rational reinterpretation of dual-process theories"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"65f6229f1c9a1cf17dad0a288869c2e5e1ebedb8","title":"Interpretable Program Synthesis"},{"paperId":"2a36402451c4fcab33c9e7f59fc027167c070aa6","title":"Perfection Not Required? Human-AI Partnerships in Code Translation"},{"paperId":"37808b916d4f9609f03d5c837fe983966976ce94","title":"Falx: Synthesis-Powered Visualization Authoring"},{"paperId":"6d064fa53bdf1c0085267b6d7ab88276e19bb3c2","title":"PLIERS: A Process that Integrates User-Centered Methods into Programming Language Design"},{"paperId":null,"title":"Advent of Code"},{"paperId":null,"title":"Generation with AlphaCode"},{"paperId":"2a33175ef68d6e114c78bef7e7a5572b8a3642a7","title":"Programming with a read-eval-synth loop"},{"paperId":"18df811ca0f5346164a5de0d9842fc32e96929de","title":"Digging for fold: synthesis-aided API discovery for Haskell"},{"paperId":"cb7ecee937f3aed98a9ce50e0881e43a60270466","title":"Small-Step Live Programming by Example"},{"paperId":"60ec806636e4781dec5a848127062ff911f7f224","title":"Interactive Program Synthesis by Augmented Examples"},{"paperId":"7f396e4fee019c1df42d6da93408fc2aff755d2b","title":"Detecting Affective Flow States of Knowledge Workers Using Physiological Sensors"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"b01ac6b990770092c6784f6eda8f3e94e2feb5a8","title":"Wrex: A Unified Programming-by-Example Interaction for Synthesizing Readable Code for Data Scientists"},{"paperId":"57e3ad7c294490947caf75cc19070453019f78f7","title":"Projection Boxes: On-the-fly Reconfigurable Visualization for Live Programming"},{"paperId":"77910e51a40d17157fc798325d06edfa6cff18d6","title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation"},{"paperId":null,"title":"Kite: AI-Powered Completions for JupyterLab"},{"paperId":"d2d44be771d01e277a9912249f2f7c211c393fee","title":"On the fly synthesis of edit suggestions"},{"paperId":"0883204dc486793e777a025a7bc129da886a54b3","title":"Live functional programming with typed holes"},{"paperId":"f7c9e00b3339cfc209b4402107e6c40ca0c18384","title":"Rousillon: Scraping Distributed Hierarchical Web Data"},{"paperId":"85bdb70962147cd3586d99f50328025c82c9ac8e","title":"Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples"},{"paperId":"3bc43349efae4ab2f2311280e5a47d0d63309b9e","title":"Programming Not Only by Example"},{"paperId":null,"title":"TabNine: AI Assistant for Development Teams"},{"paperId":"f1d58000dfb9646d84f39fc1ba451b08c1fb18fb","title":"Dual-process theories"},{"paperId":"8165d6217a2f623f7d9e613c791e94102921cd3b","title":"Thinking Fast and Slow"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"8a25c9403d8a0e2fb8ca362a1b26262afd57417f","title":"DeepCoder: Learning to Write Programs"},{"paperId":"06a90d38fd239af1e76471b2e2f4b24644d347ea","title":"Programmers Are Users Too: Human-Centered Methods for Improving Programming Tools"},{"paperId":"8fe2f59ff3733f9ee50ffa295beda502f4e268e2","title":"Grounded Theory in Software Engineering Research: A Critical Review and Guidelines"},{"paperId":"44392388300d2f3c792300c3f88b71d9b17a88c0","title":"Programmers are Users Too : Human Centered Methods for Improving Tools for Programming"},{"paperId":"d709942158bb9c28c18cc74eb99a95db9c8a834c","title":"Introduction to Qualitative Research Methods: A Guidebook and Resource"},{"paperId":"3c938b79d1a8f73e4edda5a9ffb4fe2cdf03a2ed","title":"OverCode"},{"paperId":"71767eb25146b8e79925361b3caf54c36cb5ac7e","title":"OverCode: visualizing variation in student solutions to programming problems at scale"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"2244595cc05a687ff04390f63a4443e1cd0cb3b0","title":"Flow and the Foundations of Positive Psychology"},{"paperId":"09f8e633eaf825d740c66725651aebb9c49f7e1f","title":"Curiosity, Creativity, and Surprise as Analytic Tools: Grounded Theory Method"},{"paperId":"1ca194822bbc0ceb764937c63248c09254ef663b","title":"Program sketching"},{"paperId":"e2d3f4ef30652b36145cbecfcd1f50d9f69351f3","title":"Automating string processing in spreadsheets using input-output examples"},{"paperId":"af02f111644a38edec727e5891256fd811e08bdc","title":"Codex"},{"paperId":"4d8542b3ad8d417aac72852be17ed056a5e7f7b0","title":"The sage handbook of grounded theory"},{"paperId":"870768ad418746754971363daf51142415d83387","title":"Motivational Effects on Self-Regulated Learning with Different Tasks"},{"paperId":"b074e9892852f61a850fc7553a30c8688e98981c","title":"The Impact of Background and Experience on Software Inspections"},{"paperId":"9f82946d076094e1d4013e616d7a1cdd1bb94ebf","title":"Software Design — Cognitive Aspects"},{"paperId":"75d28729e96691eb85ae2b34e791473a24062ce5","title":"QuickCheck: a lightweight tool for random testing of Haskell programs"},{"paperId":"adc478b397f59a8613727898e40cc37b929ea164","title":"Basics of qualitative research: Grounded theory procedures and techniques."},{"paperId":"3036e10df2e4855c56bf174fc1e00f6dd4100f55","title":"Stimulus structures and mental representations in expert comprehension of computer programs"},{"paperId":"e890a12f7daaea0f23d189acb61e2eda160d5c42","title":"The Discovery of Grounded Theory: Strategies for Qualitative Research"},{"paperId":null,"title":"More generally, qualitative methods appear to be making a come-back in the programming languages field. The PLIERS work"},{"paperId":null,"title":"Article 78. Publication date"},{"paperId":null,"title":"Myers et al. 2016] describe other ways to incorporate qualitative methods into the tool-building workflow. While neither of these explicitly mentions GT, they do recommend contextual inquiry"}],"id":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","summary":"Interactions with programming assistants are bimodal : in acceleration mode , the programmer knows what to do next and uses Copilot to get there faster; in exploration mode, the programmer is unsure how to proceed and usesCopilot to explore their options."},{"url":"https://www.semanticscholar.org/paper/654cd297b307c0c6fa08732511ba852b8dce1977","title":"Out of the BLEU: how should we assess quality of the Code Generation models?","venue":"SSRN Electronic Journal","year":2022,"referenceCount":43,"citationCount":12,"influentialCitationCount":0,"publicationDate":"05/08/2022","authors":"Mikhail Evtikhiev,Egor Bogomolov,Yaroslav Sokolov,T. Bryksin","citations":[{"paperId":"8a2a2006e371ce07e0b29538cd11337a5c977caf","title":"“What It Wants Me To Say”: Bridging the Abstraction Gap Between End-User Programmers and Code-Generating Large Language Models"},{"paperId":"3e7055980da1853d196fcee0d3b7f390fb518d62","title":"A Survey on Automated Program Repair Techniques"},{"paperId":"66718e87b70de80cbc2a4120050ca36fda49f8d6","title":"Exploring Distributional Shifts in Large Language Models for Code Analysis"},{"paperId":"13a66fc8689724e295548ceac9e5425fc46cc093","title":"SkCoder: A Sketch-based Approach for Automatic Code Generation"},{"paperId":"535a3d95a743e1f4b591b5b2af3e778a6347158a","title":"CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code"},{"paperId":"0bc9cedda48551847cc741b74c1fc299c5a9eed2","title":"Who Evaluates the Evaluators? On Automatic Metrics for Assessing AI-based Offensive Code Generators"},{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"5f86bbe35dbca8d932b3110f0c98a6b2d06a0b5b","title":"Reading Between the Lines: Modeling User Behavior and Costs in AI-Assisted Programming"},{"paperId":"ef29fb8cc6bdda4b011288f51da521d3c25fc53d","title":"Learning to Prevent Profitless Neural Code Completion"},{"paperId":"e73a16490c29530c37b49f6a30592790e7caaaa4","title":"Don't Complete It! Preventing Unhelpful Code Completion for Productive and Sustainable Neural Code Completion Systems"},{"paperId":"e772bf6ad9b87a333291f427d328e24cc57f23a1","title":"Human perceiving behavior modeling in evaluation of code generation models"}],"references":[{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"8e136c0944c9181b4b853df4f642d8bb7afddecb","title":"Reassessing automatic evaluation metrics for code summarization tasks"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"4c67c129dab9805ab248407b77a6d542c2e40d41","title":"Adversarial Crowdsourcing Through Robust Rank-One Matrix Completion"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"868207797e69df5055f5c3fd4aa78a33e5a7ca45","title":"Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics"},{"paperId":"b7af363ff612f41e0c1071ae24ce68a836aaa678","title":"Code Generation as a Dual Task of Code Summarization"},{"paperId":"fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation"},{"paperId":"37c035a8e6ad1da2a8b22d5f2951f01f1f309632","title":"Results of the WMT19 Metrics Shared Task: Segment-Level and Strong MT Systems Pose Big Challenges"},{"paperId":"3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c","title":"Reranking for Neural Semantic Parsing"},{"paperId":"9e955da40c62b2543b963454b90928ad6cbd5f8a","title":"Does BLEU Score Work for Code Migration?"},{"paperId":"e3ef11877bdd08140fcabf358dd9fc5bef6b15e0","title":"Recommendations for Datasets for Source Code Summarization"},{"paperId":"9c2ab4707e902f6174910847e3f94f3b85017ec3","title":"A Grammar-Based Structural CNN Decoder for Code Generation"},{"paperId":"649c1148439a4e875dab414ba67bf8c80350af4a","title":"TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation"},{"paperId":"b4e59a9926362fd18ce39c64bb42fe874b4de8f9","title":"Results of the WMT18 Metrics Shared Task: Both characters and embeddings achieve good performance"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"814db99ccf6b88d6af5b406b0c344b64c0a710b7","title":"A Structured Review of the Validity of BLEU"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"b4bfadfca9742bb3ee98a0cd322d5ce4e59a3ceb","title":"A Call for Clarity in Reporting BLEU Scores"},{"paperId":"3807a517403cfc7ed67fdfea64baf62b9948bcfe","title":"A Parallel Corpus of Python Functions and Documentation Strings for Automated Code Documentation and Code Generation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2c1e874c3b67510a3215e535f5646b362de5bc89","title":"Abstract Syntax Networks for Code Generation and Semantic Parsing"},{"paperId":null,"title":"A syntactic neural model for generalpurpose code generation"},{"paperId":"14c1c84f8537186fcf147d1f95cae3c8133b143a","title":"chrF deconstructed: beta parameters and n-gram weights"},{"paperId":"129cbad01be98ee88a930e31898cb76be79c41c1","title":"How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"27e1dbe9f7c71cd6cc1b0357f49aef497e572d09","title":"Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"},{"paperId":"285c165c81fc9275955147a892b9a039ec8b1052","title":"chrF: character n-gram F-score for automatic MT evaluation"},{"paperId":"26adb749fc5d80502a6d889966e50b31391560d3","title":"Meteor Universal: Language Specific Translation Evaluation for Any Target Language"},{"paperId":"8bc150dc49fc81c7c4dacd35a2b8b1afe1a1692a","title":"A Systematic Comparison of Smoothing Techniques for Sentence-Level BLEU"},{"paperId":"0236729c7077cd894ef95fe1206d125f8898f8c6","title":"Randomized Significance Tests in Machine Translation"},{"paperId":"0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7","title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"4774432f02ef4c5285952dd8c7daff0852c3a601","title":"Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":null,"title":"Long short-termmemory"},{"paperId":"d0be39ee052d246ae99c082a565aba25b811be2d","title":"Learning long-term dependencies with gradient descent is difficult"},{"paperId":"25578d4a1c760e4bb1af20210dd93abe24290575","title":"A 15 Year Perspective on Automatic Programming"},{"paperId":"6957cb5b8335a27e7bcccad134ae730523e5de8e","title":"Estimating the Error Rate of a Prediction Rule: Improvement on Cross-Validation"},{"paperId":"59d823fc9877f8d448ac4c2d90e38051026e3201","title":"Individual Comparisons by Ranking Methods"},{"paperId":null,"title":"earthbreaker -open source Hearthstone simulator"},{"paperId":null,"title":"Rouge 1.5.5 Perl script"}],"id":"654cd297b307c0c6fa08732511ba852b8dce1977","summary":"A study on applicability of six metrics— BLEu, ROUGE-L, METEOR, ChrF, CodeBLEU, RUBY—for evaluation of the code generation models is presented and several recommendations on using metrics to estimate the model performance on the codegeneration tasks are derived."},{"url":"https://www.semanticscholar.org/paper/0509c25103939d59ed4e27b1393e74ad5734c453","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot","venue":"International Conference on Automated Software Engineering","year":2022,"referenceCount":28,"citationCount":4,"influentialCitationCount":0,"publicationDate":"31/08/2022","authors":"N. A. Madi","citations":[{"paperId":"e1a17cb16742cf484aedc9ae106363d82454172e","title":"Practices and Challenges of Using GitHub Copilot: An Empirical Study"},{"paperId":"57ffd52abfbba0eb76c8406c7331f1f8ca29aaff","title":"R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents"},{"paperId":"3078a916b57e465614efea50628c889799e32289","title":"\"It would work for me too\": How Online Communities Shape Software Developers' Trust in AI-Powered Code Generation Tools"},{"paperId":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?"}],"references":[{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"8417d83e74a63b9d3874cd7695477ebad8b98f8a","title":"Choose your programming copilot: a comparison of the program synthesis performance of github copilot and genetic programming"},{"paperId":"4a160efbe80c38cd5eb2f92c7c095b49b113397d","title":"In-IDE Code Generation from Natural Language: Promise and Challenges"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"c6c45ee81950c8d135fc3b4257839a46bc83fa37","title":"EMIP Toolkit: A Python Library for Customized Post-processing of the Eye Movements in Programming Dataset"},{"paperId":"9734459e288c45c1730ed996e426286034dbb492","title":"From Novice to Expert: Analysis of Token Level Effects in a Longitudinal Eye Tracking Study"},{"paperId":"70c3be778323b896f5575a5770f934ee6f399441","title":"Program Comprehension and Code Complexity Metrics: An fMRI Study"},{"paperId":"5ff9032d0f7f246d01ae7b2c231ab03469a7344a","title":"Can OpenAI Codex and Other Large Language Models Help Us Fix Security Bugs?"},{"paperId":null,"title":"n.d.]. Program in Pairs"},{"paperId":"f2a621a360a13211a877923b68af3c147155c9a6","title":"Applications of AI in classical software engineering"},{"paperId":"4644afca80b98dc39ee8ad4cf1ce1e940214c366","title":"A practical guide on conducting eye tracking studies in software engineering"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"94b97c2500bcd635b425159faddbd6bd4dc80668","title":"Automatically assessing code understandability: How far are we?"},{"paperId":"726488a7458b44f998e1190fc19c612c5c58d100","title":"Source code metrics: A systematic mapping study"},{"paperId":"113c14450b992e4a7c269545f4255adc80ebad7e","title":"iTrace: Overcoming the Limitations of Short Code Examples in Eye Tracking Experiments"},{"paperId":"b1cbae5f79e498ef436a2ef5b9dca31ec500b328","title":"Analyzing individual performance of source code review using reviewers' eye movement"},{"paperId":"7e63e256311e956093f9bea27456bc4e7206325a","title":"Software Engineering Metrics: What Do They Measure and How Do We Know?"},{"paperId":"c771ed2164253ebea25b1ef4c4a2870683090a3b","title":"The E-Z Reader model of eye-movement control in reading: Comparisons to other models"},{"paperId":"8d7013c1fb7fdb5b3ed638022a9d0048dd01e8fd","title":"The Roles Beacons Play in Comprehension for Novice and Expert Programmers"},{"paperId":null,"title":"In search of a software engineering profession'(Cat. No. PR01059)"},{"paperId":null,"title":"PEP 8: style guide for Python code"},{"paperId":"87c8a7be8d5e2e2209e766c3e28a3e8ee5babb64","title":"Eye movements in reading and information processing: 20 years of research."},{"paperId":"b24b0b78978d744df9798575b4ab9b65cf1cff02","title":"How do we read algorithms? A case study"},{"paperId":"72910077a29caf411dbb03148997c72b47e65ab0","title":"Software Engineering Economics"},{"paperId":"94d9cbfc474cb6ce961de45a06233b2853ca6724","title":"Toward automatic program synthesis"}],"id":"0509c25103939d59ed4e27b1393e74ad5734c453","summary":"The results suggest that model generated code is comparable in complexity and readability to code written by human pair programmers, and eye tracking data suggests, to a statistically significant level, that programmers direct less visual attention to model generate code."},{"url":"https://www.semanticscholar.org/paper/46d0a832fada6147bceb0bd4e39928e482733246","title":"How Important are Good Method Names in Neural Code Generation? A Model Robustness Perspective","venue":"ArXiv","year":2022,"referenceCount":75,"citationCount":1,"influentialCitationCount":0,"publicationDate":"29/11/2022","authors":"Guang Yang,Yu Zhou,Wenhua Yang,Tao Yue,Xiang Chen,Taolue Chen","citations":[{"paperId":"921dace8bf038a34cba5473a72abc8cf65d61e03","title":"Large Language Models (GPT) Struggle to Answer Multiple-Choice Questions about Code"}],"references":[{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"e9fc39f56abbc6b8aed1e05496d985e70345a95a","title":"An extensive study on pre-trained models for program understanding and generation"},{"paperId":"1d160123cbbef972ea151a641dd435d57c727de8","title":"AixBench: A Code Generation Benchmark Dataset"},{"paperId":"45263786d07f5751f7494fdeee3c8764836d02c4","title":"NatGen: generative pre-training by “naturalizing” source code"},{"paperId":"08433ddb8c799c00008bc71a6252ee473585f7e3","title":"Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data"},{"paperId":"4b27f18bff43d605805c92696a979714ced0b805","title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation"},{"paperId":"e112eede2811be3c07918ca12fc230771d2c222a","title":"Method Name Generation Based on Code Structure Guidance"},{"paperId":"2236b6036cd1ecba1792d5e1fedbae7cefc3d43b","title":"Can we generate shellcodes via natural language? An empirical study"},{"paperId":"e6770e3f5e74210c6863aaeed527ac4c1da419d7","title":"A Survey on Retrieval-Augmented Text Generation"},{"paperId":"2c3111cbd1327e9b8616082eef39dc0a3a516b2a","title":"Natural Attack for Pre-trained Models of Code"},{"paperId":"844f51f17a576551fea57187163c35ee3e55246b","title":"CCGIR: Information retrieval-based code comment generation method for smart contracts"},{"paperId":"6f32b9cc23eaf7a531918c00e36e788304ef0fd5","title":"Adversarial Robustness of Deep Code Comment Generation"},{"paperId":"529b463995b2729c1d6116e2a3938c107bf6eec8","title":"Generating Adversarial Examples of Source Code Classification Models via Q-Learning-Based Markov Decision Process"},{"paperId":"80dada74cc8b974d92f2b1be0dd4cde47fedc1d3","title":"Assessing Robustness of ML-Based Program Analysis Tools using Metamorphic Program Transformations"},{"paperId":"46104f79702e959798ebfa7e07bfe22f22c1096b","title":"PyTorrent: A Python Library Corpus for Large-scale Language Models"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"69acc5f67ea7dba13c58d7281b8f0a25ed64f0e8","title":"EVIL: Exploiting Software via Natural Language"},{"paperId":"c56aced0f0c5cfebefadb530cb08d736c3ac5c05","title":"Retrieval Augmented Code Generation and Summarization"},{"paperId":"e9f79bae3269e8c3aa14a76e6b6ed6acd0d36e3e","title":"Lightweight global and local contexts guided method name recommendation with prior knowledge"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"0b077c9577f4297dcf3da835e253d21965bbc6e0","title":"CoTexT: Multi-task Learning with Code-Text Transformer"},{"paperId":"0cd5440f5e05016563767d04a633a64dc69442de","title":"Keywords Guided Method Name Generation"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"fb6969feb0b772998bfdb029a7c950c4d892679e","title":"RobOT: Robustness-Oriented Testing for Deep Learning Systems"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"6110581bd5650ebdf777f008b10e1db6a6e03bc0","title":"Named Entity Recognition via Noise Aware Training Mechanism with Data Filter"},{"paperId":"244599ac044f3e931e3b243491cf05cb8d23e496","title":"Training Deep Code Comment Generation Models via Data Augmentation"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"4614f69d7f831fbb93c997fc4a9c62a0ae1d8db0","title":"A Human Study of Comprehension and Code Summarization"},{"paperId":"bd47bb8cdd749a3356149da6155d2dcd7458779f","title":"Retrieval-based Neural Source Code Summarization"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"b8216617cec1c7bb73e962773c4458d16cbf8700","title":"Generating Adversarial Examples for Holding Robustness of Source Code Processing Models"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"f661e8d9e676319dac9a1109803e941b8fc1be11","title":"An Analysis of Adversarial Attacks and Defenses on Autonomous Driving Models"},{"paperId":"51a920c3d201eec57bcc2e97e0268304f53b5161","title":"TreeGen: A Tree-Based Transformer Architecture for Code Generation"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"2ab44880e1763baf3d8753ccb43ad3bd5f122b70","title":"Adversarial examples for models of code"},{"paperId":"c33e4ebe6b88a5958c4f8a1bec3e78e1a33a5e19","title":"Retrieval-guided Dialogue Response Generation via a Matching-to-Generation Framework"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"0073547de99221662cc1dd3ea4babab91a2512f6","title":"Learning to Spot and Refactor Inconsistent Method Names"},{"paperId":"7f21c2bb0acc70e957d26be525e0460c82dae747","title":"A Neural Model for Method Name Generation from Functional Description"},{"paperId":"a9a113171a013e680fcfc368b6cdffde052f3b60","title":"Method name suggestion with hierarchical attention networks"},{"paperId":"1e7ea2465753c5186e5fcc9e1a64cdc522cf4de4","title":"TextBugger: Generating Adversarial Text Against Real-world Applications"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"f6335a158a3f4fcfffe74f2df1d55d835bf95095","title":"A Retrieve-and-Edit Framework for Predicting Structured Outputs"},{"paperId":"5cca64b6b3d3c1f010f70f0e403991f19175cf00","title":"Neural-Machine-Translation-Based Commit Message Generation: How Far Are We?"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"d3e13d2514edaf74b863bfbe45a739c32a7689e1","title":"Retrieval-Based Neural Code Generation"},{"paperId":"d05e553f9bc3eb11d6a4a17766fb65d595fc7768","title":"Search Engine Guided Neural Machine Translation"},{"paperId":"429b65937d4922578a81e1f0ef5aeab7361ae36b","title":"NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System"},{"paperId":"0b0d518eda02564a720298ddea04edb509331672","title":"Unravelling Robustness of Deep Learning based Face Recognition Against Adversarial Attacks"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"442e10a3c6640ded9408622005e3c2a8906ce4c2","title":"A Unified Approach to Interpreting Model Predictions"},{"paperId":"668db48c6a79826456341680ee1175dfc4cced71","title":"Get To The Point: Summarization with Pointer-Generator Networks"},{"paperId":"de8e9b53057fd3869d1cd062e01f37a2d17a32be","title":"Shorter identifier names take longer to comprehend"},{"paperId":"df40ce107a71b770c9d0354b78fdd8989da80d2f","title":"Towards Evaluating the Robustness of Neural Networks"},{"paperId":null,"title":"A Syntactic Neural Model for General-Purpose Code Generation"},{"paperId":"74157ae408173bf713f1e94f15aca1475c43bd74","title":"Neural Generation of Regular Expressions from Natural Language with Minimal Domain Knowledge"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"964e518d8f93b2d4549e64365ed302f283d7513c","title":"On End-to-End Program Generation from User Intention by Deep Neural Networks"},{"paperId":"a70d55a840866b7d0bb7d0644b71c27bab5aac1b","title":"De-anonymizing Programmers via Code Stylometry"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"49acfc8071e322c0001b31ea42c51f8c12433fc9","title":"How we refactor, and how we know it"},{"paperId":"47ced790a563344efae66588b5fb7fe6cca29ed3","title":"The Probabilistic Relevance Framework: BM25 and Beyond"},{"paperId":"835ec26481c12e4b0961ca3f77b84b7b23e9bf7a","title":"The Significance of Letter Position in Word Recognition"},{"paperId":"5e2799e1b58e93fbf465e153157ace7993450fe4","title":"An alternative approach for neural network evolution with a genetic algorithm: Crossover by combinatorial optimization"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"9b9a7642c9018deb8d66455f14ed38930b7d2e59","title":"An information-theoretic perspective of tf-idf measures"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":null,"title":"IEEE standard glossary of software engineering terminology (IEEE Std 610.12-1990)"}],"id":"46d0a832fada6147bceb0bd4e39928e482733246","summary":"The potential of benefiting from method names to enhance the performance of PCGMs, from a model robustness perspective, is studied and a novel approach is proposed, named RADAR (neuRAl coDe generAtor Robustifier)."},{"url":"https://www.semanticscholar.org/paper/0f38267a8ba32789f5d3b1b19820f86940fea052","title":"Generation-Augmented Query Expansion For Code Retrieval","venue":"ArXiv","year":2022,"referenceCount":25,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Dong Li,Yelong Shen,Ruoming Jin,Yi Mao,Kuan Wang,Weizhu Chen","citations":[{"paperId":"349ec9ca2ff17dbc1aa1caf27f98b7cd85a4747e","title":"RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation"}],"references":[{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"2b556fcf2ac634f03c1fb0ace5e602e829418e65","title":"ReACC: A Retrieval-Augmented Code Completion Framework"},{"paperId":"4b27f18bff43d605805c92696a979714ced0b805","title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":null,"title":"Adversarial retriever-ranker model for dense retrieval"},{"paperId":"23eec8ae5e1091f58a979d7ff179b9a628cbcd20","title":"CodeRetriever: Unimodal and Bimodal Contrastive Learning for Code Search"},{"paperId":"c56aced0f0c5cfebefadb530cb08d736c3ac5c05","title":"Retrieval Augmented Code Generation and Summarization"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"8d1369a218a39214d82ea77ff964570eca057c15","title":"Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"7d429ad73fc311a0a29ab9d02482b4e6b059d81f","title":"Generation-Augmented Retrieval for Open-Domain Question Answering"},{"paperId":null,"title":"GPT-J6B: A 6 Billion Parameter Autoregressive Language Model"},{"paperId":"58ed1fbaabe027345f7bb3a6312d41c5aac63e22","title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"paperId":"79cd9f77e5258f62c0e15d11534aea6393ef73fe","title":"Dense Passage Retrieval for Open-Domain Question Answering"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"f6335a158a3f4fcfffe74f2df1d55d835bf95095","title":"A Retrieve-and-Edit Framework for Predicting Structured Outputs"},{"paperId":"d3e13d2514edaf74b863bfbe45a739c32a7689e1","title":"Retrieval-Based Neural Code Generation"},{"paperId":"e033a0b29af2939fd44e5765d03380c08897a9e8","title":"Deep Code Search"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"c42bef8bb338aa931b48e5b65e29a82e9f722cbd","title":"What do developers search for on the web?"},{"paperId":"294be1a0a18aa158fb99836c68b8cd0b25dbc562","title":"What do developers search for on the web?"},{"paperId":"a60b213b7488bd9f13d1b8fa9ab5139c941241d7","title":"ANNE: Improving Source Code Search using Entity Retrieval Approach"},{"paperId":"7e8d5a108c28cdfb92f419ce919fbf7993dfebfc","title":"A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval"}],"id":"0f38267a8ba32789f5d3b1b19820f86940fea052","summary":"This paper proposes a generation-augmented query expansion framework that leverages the code generation model to enhance the code retrieval task and achieves new state-of-the-art results on the CodeSearchNet benchmark and surpass the baselines signiﬁcantly."},{"url":"https://www.semanticscholar.org/paper/269df328eec08b56b7b1f38a7555797fe2b999b6","title":"ReCode: Robustness Evaluation of Code Generation Models","venue":"ArXiv","year":2022,"referenceCount":35,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Shiqi Wang,Zheng Li,Haifeng Qian,Cheng Yang,Zijian Wang,Mingyue Shang,Varun Kumar,Samson Tan,Baishakhi Ray,Parminder Bhatia,Ramesh Nallapati,M. Ramanathan,D. Roth,Bing Xiang","citations":[{"paperId":"27c2ddc921befc44f7b2cdb8bb39facdd8b0b188","title":"Measuring The Impact Of Programming Language Distribution"}],"references":[{"paperId":"89daa253cfd707958b1539ec4d8ea9664e8ceb7d","title":"NL-Augmenter: A Framework for Task-Sensitive Natural Language Augmentation"},{"paperId":"5032c0946ee96ff11a292762f23e6377a6cf2731","title":"Holistic Evaluation of Language Models"},{"paperId":"45263786d07f5751f7494fdeee3c8764836d02c4","title":"NatGen: generative pre-training by “naturalizing” source code"},{"paperId":"34503c0b6a615124eaf82cb0e4a1dab2866e8980","title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models"},{"paperId":"d18287d5ef8653aa1276a11957f2b3934c7c93e1","title":"CodeAttack: Code-based Adversarial Attacks for Pre-Trained Programming Language Models"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"2c3111cbd1327e9b8616082eef39dc0a3a516b2a","title":"Natural Attack for Pre-trained Models of Code"},{"paperId":"6f32b9cc23eaf7a531918c00e36e788304ef0fd5","title":"Adversarial Robustness of Deep Code Comment Generation"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":null,"title":"Competition-level code generation"},{"paperId":"8436897e713c2242d6291df9a6a33c1544d4dd39","title":"Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"c3aebfe0cc75091ab8b868c80cfcb07f4d934ed5","title":"Automatic Construction of Evaluation Suites for Natural Language Generation Datasets"},{"paperId":"77a096d80eb4dd4ccd103d1660c5a5498f7d026b","title":"Dynabench: Rethinking Benchmarking in NLP"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":null,"title":"GPT-J6B: A 6 Billion Parameter Autoregressive Language Model"},{"paperId":"0e002114cd379efaca0ec5cda6d262b5fe0be104","title":"MPNet: Masked and Permuted Pre-training for Language Understanding"},{"paperId":"35e6783307f82d1faa39be0653431305abec7271","title":"Evaluating Models’ Local Decision Boundaries via Contrast Sets"},{"paperId":"dc0ce66f5ab4c5173cdef951649044e4c4c05076","title":"BERT-ATTACK: Adversarial Attack against BERT Using BERT"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"60739c6a62f86d274d7e6a0565acc78e6f319ab1","title":"Adversarial Robustness for Code"},{"paperId":"207da6d2c07289bf72a2b5974bb3f011ebb5dd0d","title":"Adversarial NLI: A New Benchmark for Natural Language Understanding"},{"paperId":"309b906fed883e5efe4acf676c655ead21f6c17b","title":"Word-level Textual Adversarial Attacking as Combinatorial Optimization"},{"paperId":"ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2","title":"Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment"},{"paperId":"652107ea8161f607e3bdabc89199e9ff2fdfd015","title":"Adversarial Attacks on Deep-learning Models in Natural Language Processing"},{"paperId":"c6801d553a43530b192309ef4364a43e33e4067f","title":"Data augmentation using back-translation for context-aware neural machine translation"},{"paperId":"0d1f3f5a6bf3393c517f07f3fb2107e4ad5a7e85","title":"Improving Neural Machine Translation Robustness via Data Augmentation: Beyond Back-Translation"},{"paperId":"42b49971a1bc73088ec3317222e7486db82bb4cd","title":"Handbook of Inter-Rater Reliability: The Definitive Guide to Measuring the Extent of Agreement Among Raters"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"},{"paperId":"0b44fcbeea9415d400c5f5789d6b892b6f98daff","title":"Building a Large Annotated Corpus of English: The Penn Treebank"},{"paperId":"68c03788224000794d5491ab459be0b2a2c38677","title":"WordNet: A Lexical Database for English"}],"id":"269df328eec08b56b7b1f38a7555797fe2b999b6","summary":"This paper proposes ReCode, a comprehensive robustness evaluation benchmark for code generation models, and customizable over 30 transformations for code on docstrings, function and variable names, code syntax, and code format, which provide multifaceted assessments of a model’s robustness performance."},{"url":"https://www.semanticscholar.org/paper/51d253814e85249a84bbe634b4a80d306b74fbd0","title":"\"It would work for me too\": How Online Communities Shape Software Developers' Trust in AI-Powered Code Generation Tools","venue":"ArXiv","year":2022,"referenceCount":68,"citationCount":3,"influentialCitationCount":1,"publicationDate":"07/12/2022","authors":"Ruijia Cheng,Ruotong Wang,T. Zimmermann,Denae Ford","citations":[{"paperId":"dca09e13fe9b4f46b1e10b966f3ede63f63f9489","title":"Understanding the Usability of AI Programming Assistants"},{"paperId":"24c5450d8fa785e5f85d9427d2d65cf66476ac3a","title":"Toward General Design Principles for Generative AI Applications 130-144"},{"paperId":"270b015093073d3ba254928b6d736a59870d3fb1","title":"Chatbots As Fluent Polyglots: Revisiting Breakthrough Code Snippets"}],"references":[{"paperId":"8dd412cd31592ba633b5dac8b2e7b4c679ec1c0a","title":"Grounded Copilot: How Programmers Interact with Code-Generating Models"},{"paperId":"0509c25103939d59ed4e27b1393e74ad5734c453","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot"},{"paperId":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?"},{"paperId":"d6954c43aa1ca197319c45d3988bc8fcec3de976","title":"GitHub Copilot AI pair programmer: Asset or Liability?"},{"paperId":"903ce14de7226276a66e1bd3e3d3d97934d2dfd2","title":"Mapping the Design Space of Human-AI Interaction in Text Summarization"},{"paperId":"a4374578c7a7226c159c77d7cd98b8a9d293c184","title":"Limits and Possibilities for “Ethical AI” in Open Source: A Study of Deepfakes"},{"paperId":"3fbc8d04a1f3dba58bdaada1924ee132512e98be","title":"Productivity assessment of neural code completion"},{"paperId":"8dec8614a5f280589e4cd6fbf12672a18eedfddc","title":"Designing for Responsible Trust in AI Systems: A Communication Perspective"},{"paperId":"24fcd10397aab543513c06deac6243c713affff0","title":"To Self-Persuade or be Persuaded: Examining Interventions for Users’ Privacy Setting Selection"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"908f3697b3047c9ad6c3d39e0752f11383c12e18","title":"Are Deepfakes Concerning? Analyzing Conversations of Deepfakes on Reddit and Exploring Societal Implications"},{"paperId":"463c6944ddcb43280e1c9765c2245bc7b764ab68","title":"AI-Driven Development Is Here: Should You Worry?"},{"paperId":"7438626a757c5442b9c0fb37b54ec0fe7e1889c3","title":"Better Together? An Evaluation of AI-Supported Code Translation"},{"paperId":"11fede75aa15578b42a5b0e5152dbe2ea74fcbe9","title":"Shifting Trust: Examining How Trust and Distrust Emerge, Transform, and Collapse in COVID-19 Information Seeking"},{"paperId":"76ebf56d6ebb833d8c8c4124f7b2f15771a4997c","title":"Investigating Explainability of Generative AI for Code through Scenario-based Design"},{"paperId":"1cc696e31a3ae3d037730df262fce681bfec2f02","title":"What Makes Online Communities 'Better'? Measuring Values, Consensus, and Conflict across Thousands of Subreddits"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"},{"paperId":"4a160efbe80c38cd5eb2f92c7c095b49b113397d","title":"In-IDE Code Generation from Natural Language: Promise and Challenges"},{"paperId":"f371fc933d25ac441073628541a9f2a403da2e6b","title":"Situated Knowledges: The Science Question in Feminism and the Privilege of Partial Perspective"},{"paperId":null,"title":"2022. GitHub Copilot is generally available to all developers"},{"paperId":"3041b15b1f2f08426585440eb52d2ea3156287bb","title":"How to Evaluate Trust in AI-Assisted Decision Making? A Survey of Empirical Methodologies"},{"paperId":"9a02c7ec59c3dc42bc547d6e6d5200c2bb6576d7","title":"Developers Who Vlog: Dismantling Stereotypes through Community and Identity"},{"paperId":"2bf2f45f6bcc82fa573565c457661f3f0a464af6","title":"Yes: Affirmative Consent as a Theoretical Framework for Understanding and Imagining Social Platforms"},{"paperId":"2a36402451c4fcab33c9e7f59fc027167c070aa6","title":"Perfection Not Required? Human-AI Partnerships in Code Translation"},{"paperId":"0405474dc91d8232b287c78698e067009353fa8f","title":"How WEIRD is CHI?"},{"paperId":"c8965761083d80ff762ce76c08df92d66e01f37d","title":"Expanding Explainability: Towards Social Transparency in AI systems"},{"paperId":"14dddd1d8cb2e8c5f4e9998fef84e715cb321ac9","title":"Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI"},{"paperId":"81ca863e4d2007d5f8105887806a328dfa15171a","title":"Towards a Theory of Software Developer Job Satisfaction and Perceived Productivity"},{"paperId":"494e91cca437cb791e5955c82d1899002e5272e2","title":"What Predicts Software Developers’ Productivity?"},{"paperId":"3d052b552be2d4e6989880906f9fc8cfa67a56a9","title":"Building Community Knowledge In Online Competitions"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"71960b10294f741fdcb1c339a218c378eba6bc03","title":"Trust in AutoML: exploring information needs for establishing trust in automated machine learning systems"},{"paperId":"5cc4100a67fd6f2ce3c760655ba7a12f358c7950","title":"Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making"},{"paperId":"87448d72d0cd8a0e5e6b0babe78653b72e4c452a","title":"Cognitive Load Drivers in Large Scale Software Development"},{"paperId":"7bbceb9cb7ca579082699a00364d6314c87fe846","title":"Measuring the Cognitive Load of Software Developers: A Systematic Mapping Study"},{"paperId":"b4b1cbd74029f46ef9b462290a46111217552761","title":"Understanding the Effect of Accuracy on Trust in Machine Learning Models"},{"paperId":"b559ad3ebbb9b61e75742dc9634ba27cb14b2a91","title":"r/science: Challenges and Opportunities in Online Science Communication"},{"paperId":"ad3cf68bae32d21f25ac142287d4a556155619d2","title":"Guidelines for Human-AI Interaction"},{"paperId":"16914d2e559dcfbb00717faa3bfb99c35c5e484b","title":"Do I trust my machine teammate?: an investigation from perception to decision"},{"paperId":"2997d34e7359864e7c83f99f7e0a8d2cbb1ef007","title":"Situated Trust in a Physician: Patient Health Characteristics and Trust in Physician Confidentiality"},{"paperId":"e38df7e177db9493e6961eff1c2194dbf309278f","title":"\"We Don't Do That Here\": How Collaborative Editing with Mentors Improves Engagement in Social Q&A Communities"},{"paperId":"5c6de0c4f044f4ea6a5a93973b323f2c02211e4c","title":"User experience sharing: Understanding customer initiation of value co-creation in online communities"},{"paperId":null,"title":"Watch Me Code: Programming Mentorship Communities on Twitch.Tv"},{"paperId":"239b9bc3f4c77639ca6c7e071dd4be2ce062364e","title":"Paradise unplugged: identifying barriers for female participation on stack overflow"},{"paperId":"00636b216b4526d8824c4851bff3447e5a328e5a","title":"How the R Community Creates and Curates Knowledge: A Comparative Study of Stack Overflow and Mailing Lists"},{"paperId":"27422c3a5c5e7ab1f982119252d81eaa61dec2ea","title":"Quantifying developers' adoption of security tools"},{"paperId":"71d9ea65ea1760e4486bf7bd783e910c5fcb949f","title":"One-Day Flies on StackOverflow - Why the Vast Majority of StackOverflow Users Only Posts Once"},{"paperId":"18163f9f03c51a0f68d87523b52238680273acaf","title":"Social Barriers Faced by Newcomers Placing Their First Contribution in Open Source Software Projects"},{"paperId":"f1e4a993f6018ff346175b6cfbd784d8874e53dc","title":"The Matter of Heartbleed"},{"paperId":"be1069cec80b38603d4b800fe4ffaea760d61e1f","title":"Collaborative problem solving: a study of MathOverflow"},{"paperId":"a7bfa69c58ae57ea49cc210120b37bf257d228c0","title":"Social influences on secure development tool adoption: why security tools spread"},{"paperId":"4644f1b59d33fac53100aed1174bfe00966c93ce","title":"What are developers talking about? An analysis of topics and trends in Stack Overflow"},{"paperId":"533124473315ab7961c399e6229819272c3f75a6","title":"Harnessing Stack Overflow for the IDE"},{"paperId":"32f56543d02b22ecd167334aa046b5dbdbb89f79","title":"Building Successful Online Communities: Evidence-Based Social Design"},{"paperId":"f783cb2549f75143c8e4cc92ce5dc9bc76bfb5dc","title":"How do programmers ask and answer questions on the web?: NIER track"},{"paperId":"c426ab1cb2e12a5d09110866fb0e77cc6fcfa632","title":"Design lessons from the fastest q&a site in the west"},{"paperId":"b77947bc41aad36856efeff4a6ede2c107b92b2f","title":"Why Software Is Eating the World"},{"paperId":"4fac0babc24218cb8a167de8416f11134cc01e43","title":"Social network sites: definition, history, and scholarship"},{"paperId":"5ae3e19913ee57ab45034c51dee89cc83127cdea","title":"Changing Software Development: Learning to Become Agile"},{"paperId":"dc436f4d1591410ade952f8116093539d678c33d","title":"Toward trustworthy software systems"},{"paperId":"e5136e9306bf1b0e3d4be0cea384ee9a969a44fa","title":"Using thematic analysis in psychology"},{"paperId":"b038d945d5f083df0ad2d7cf74a8a1d20d2d6c46","title":"The trustworthy computing security development lifecycle"},{"paperId":"7dd86508438657ac7a704a5d952a2a4422808975","title":"Trust in Automation: Designing for Appropriate Reliance"},{"paperId":"b0e89da3eb67767b63ca5c23c233238f70263ae7","title":"The role of trust in automation reliance"},{"paperId":"d995e99843b3b72fd787d7f9c49186e8c9a398a5","title":"Trust—the fundamental bond in global collaboration"},{"paperId":"b8775b8ead06b2f683a7ed21384d50d5da34d3a8","title":"An Integrative Model Of Organizational Trust"},{"paperId":"d9c1d8c22fb83b8d632d6764abaced3228153565","title":"Situated Learning: Legitimate Peripheral Participation"},{"paperId":"2f4010c6f7248d9cd1e43bd5985a26fe2f068211","title":"The reflective practitioner: How professionals think in action"}],"id":"51d253814e85249a84bbe634b4a80d306b74fbd0","summary":"This study unpack how developers in online communities collectively make sense of AI code generation tools by developing proper expectation, understanding, strategies, and awareness of broader implications, as well as how they leverage community signals to evaluate AI suggestions."},{"url":"https://www.semanticscholar.org/paper/13ea8689a4fe12e7443f5a3a0a25c5337a2f4cd8","title":"MarianCG: a code generation transformer model inspired by machine translation","venue":"Journal of engineering and applied sciences","year":2022,"referenceCount":41,"citationCount":1,"influentialCitationCount":0,"publicationDate":"22/11/2022","authors":"Ahmed S. Soliman,M. Hadhoud,S. Shaheen","citations":[{"paperId":"6b9a79c49c684003254c933a08855bc62091974f","title":"Improving Text-to-SQL with a Hybrid Decoding Method"}],"references":[{"paperId":"dec263e87f0c049d584b4948cfbcd0c47f345a52","title":"Formal Algorithms for Transformers"},{"paperId":"d6b7bc4e7968398101d8d9b4da7d4a0186763ff3","title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages"},{"paperId":"8c62277dada489904a63de4dd87336c27c68fb5e","title":"Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models"},{"paperId":"bc7984bfcfae537dbe633eeeb8d69c42a994c724","title":"ELLE: Efficient Lifelong Pre-training for Emerging Data"},{"paperId":"1f11601f9eb8c0bee1d1d2cc7f4fa187e8c5e525","title":"The impact of lexical and grammatical processing on generating code from natural language"},{"paperId":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective"},{"paperId":"f5b3be8b0f06eba6d26ed02656fac82928b1ae05","title":"Natural Language to Code Using Transformers"},{"paperId":"e553407be283d018e275f472d4d2fd709a6c9248","title":"PPT: Pre-trained Prompt Tuning for Few-shot Learning"},{"paperId":"420c897bc67e6f438db522d919d925df1a10aa8c","title":"AMMU - A Survey of Transformer-based Biomedical Pretrained Language Models"},{"paperId":"6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"},{"paperId":"feba0c47bf12a02c3a725174bb53df78658a72a8","title":"Pre-Trained Models: Past, Present and Future"},{"paperId":"cc9ad384ec0d0176ce05865d3866b44d2519bd68","title":"Reading StackOverflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation"},{"paperId":"64a1dbdd7653eaca25c78e87335ee156b6f6959e","title":"Constrained Language Models Yield Few-Shot Semantic Parsers"},{"paperId":"010c9f63c51ccc504de36d2d1693e0ea9d525da1","title":"Deep Learning for Source Code Modeling and Generation"},{"paperId":"e13d317fe0178a8b8b67f4af995e7fac12c35014","title":"Analysis of Tree-Structured Architectures for Code Generation"},{"paperId":"b029346bfa85fa2fc7a12498ae5f018922ce55f9","title":"Code Generation from Natural Language with Less Prior Knowledge and More Monolingual Data"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"77910e51a40d17157fc798325d06edfa6cff18d6","title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation"},{"paperId":"51a920c3d201eec57bcc2e97e0268304f53b5161","title":"TreeGen: A Tree-Based Transformer Architecture for Code Generation"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c","title":"Reranking for Neural Semantic Parsing"},{"paperId":"735ce0447e459e13f89ae751b19323d76a2af786","title":"Program Synthesis and Semantic Parsing with Learned Code Idioms"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"649c1148439a4e875dab414ba67bf8c80350af4a","title":"TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"e73bd7f9bdc262b9b7fb60ca0d5230d3ab0fad5e","title":"Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates"},{"paperId":"302c56e718d449b25cd3d6873f2e58078c584617","title":"Marian: Fast Neural Machine Translation in C++"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2c1e874c3b67510a3215e535f5646b362de5bc89","title":"Abstract Syntax Networks for Code Generation and Semantic Parsing"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"e957747f4f8600940be4c5bb001aa70c84e53a53","title":"Latent Predictor Networks for Code Generation"},{"paperId":"558ac446dc26bee9789d660a251b75728cb6eeb2","title":"Language to Logical Form with Neural Attention"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"27e1dbe9f7c71cd6cc1b0357f49aef497e572d09","title":"Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"bbbe3fb9f49528ad4cff91d4cb7b1285266fe051","title":"Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"},{"paperId":null,"title":"The illustrated transformer"}],"id":"13ea8689a4fe12e7443f5a3a0a25c5337a2f4cd8","summary":"MarianCG, a code generation Transformer model used to tackle the code generation challenge of generating python code from natural language descriptions is presented, based on fine-tuning a machine translation pre-trained language model."},{"url":"https://www.semanticscholar.org/paper/0121c151c96b32cd7851e4bcda2a468b279c1e6f","title":"CODE-MVP: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training","venue":"NAACL-HLT","year":2022,"referenceCount":65,"citationCount":8,"influentialCitationCount":1,"publicationDate":"04/05/2022","authors":"Xin Wang,Yasheng Wang,Yao Wan,Jiawei Wang,Pingyi Zhou,Li Li,Hao Wu,Jin Liu","citations":[{"paperId":"9ed21b38ed48773048a6749c48748d3b88974a17","title":"A large-scale empirical study of commit message generation: models, datasets and evaluation"},{"paperId":"8ffe7b7eeddbd4c22d642b0a48379d17e61c3bab","title":"Soft-Labeled Contrastive Pre-training for Function-level Code Representation"},{"paperId":"045a6f92b58f08d5b305dd5d661316a506ee4d43","title":"Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation"},{"paperId":"22df866f9605d27d1e5cca9b3ab721f33673e158","title":"ProgramTransformer: A tool for generating semantically equivalent transformed programs"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"ebc08a933e053e57bc2102848efa1119c0ce2bd9","title":"NS3: Neuro-Symbolic Semantic Code Search"},{"paperId":"74f57a9ffa73e1bee99300b177904c06199840aa","title":"CodeRetriever: A Large Scale Contrastive Pre-Training Method for Code Search"},{"paperId":"d388b3cbc820975da21c57c9fb9cef2d9c6d08f0","title":"CodeRetriever: Large-scale Contrastive Pre-training for Code Search"}],"references":[{"paperId":"38c198185f216df2b9ea93454daf4466241f4805","title":"NaturalCC: An Open-Source Toolkit for Code Intelligence"},{"paperId":"2c71912f388811abd0a820b58b13dffde889f3bc","title":"Graph4Web: A relation-aware graph attention network for web service classification"},{"paperId":"7a1069dafaeb484e22f2473d5545f1e45ce30656","title":"Compilable Neural Code Generation with Compiler Feedback"},{"paperId":"a4c11b9e43c8ea9a2c3d7733dd73f44ea189bdf3","title":"Scalpel: The Python Static Analysis Framework"},{"paperId":"9e92039847c5ce7186ca579dc863e887601b38ca","title":"What Do They Capture? - A Structural Analysis of Pre-Trained Language Models for Source Code"},{"paperId":"f29599399def2cadb8e9a7fafec5b7bdf58382aa","title":"Exploiting gated graph neural network for detecting and explaining self-admitted technical debts"},{"paperId":"cbd0d673a521711105355b323752e99d8c67d553","title":"Mashup-Oriented Web API Recommendation via Multi-Model Fusion and Multi-Task Learning"},{"paperId":"28284c0109ef7e627bf60f37b211c07f9f65c8d1","title":"A comprehensive investigation of the impact of feature selection techniques on crashing fault residence prediction models"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"0dc49561ec7a8f2ce51269a0d00e7c5170f79a25","title":"Relational Graph Neural Network with Neighbor Interactions for Bundle Recommendation Service"},{"paperId":"a9341943cef6a10067fd0f8a96256990bf123425","title":"Time-aware User Modeling with Check-in Time Prediction for Next POI Recommendation"},{"paperId":"14b104d1a1c0b784e4e74454d809455cc47d0093","title":"SynCoBERT: Syntax-Guided Multi-Modal Contrastive Pre-Training for Code Representation"},{"paperId":"0c21334be0228431d619a180c809b43be0065bdd","title":"CoSQA: 20,000+ Web Queries for Code Search and Question Answering"},{"paperId":"b5f9c1cc4c74d973986bc4b352b85a6ee2f475d6","title":"TreeBERT: A Tree-Based Pre-Trained Model for Programming Language"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"0b077c9577f4297dcf3da835e253d21965bbc6e0","title":"CoTexT: Multi-task Learning with Code-Text Transformer"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"acd8e24eda17808e3528d2dfd52de1619d13241a","title":"InferCode: Self-Supervised Learning of Code Representations by Predicting Subtrees"},{"paperId":"8b4c857311c001f6ed0cd790cce4af4dfcfb6533","title":"Deep Graph Matching and Searching for Semantic Code Retrieval"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"10d5a71da740e4d709914c450fc70fee1959b196","title":"Self-Supervised Contrastive Learning for Code Retrieval and Summarization via Semantic-Preserving Transformations"},{"paperId":"021bbcefc993c389bad6c1daefd8ff92d0fc2441","title":"Contrastive Code Representation Learning"},{"paperId":"1311adebe439ccd9e373d594a8df1d720a685190","title":"Code Prediction by Feeding Trees to Transformers"},{"paperId":"f44ae7cf4396d1d2837347d9aa705eb791902848","title":"ServiceBERT: A Pre-trained Model for Web Service Tagging and Recommendation"},{"paperId":"61a3de09c3430eb1a7da2cf5836c66e4520bb674","title":"Contrastive Learning for Source Code with Structural and Functional Properties"},{"paperId":null,"title":"2021a. A comprehensive investi"},{"paperId":null,"title":"2021b. Codet5: Identifier-aware"},{"paperId":null,"title":"2021a. Servicebert: A pre-trained"},{"paperId":"6765192dbf76a7876dfaf22ee35ff1b1ba32bf09","title":"Simplified Deep Forest Model based Just-In-Time Defect Prediction for Android Mobile Apps"},{"paperId":"4f09c97321628688c7f3cbaa76ef132656a4b830","title":"Detecting and Explaining Self-Admitted Technical Debts with Attention-based Neural Networks"},{"paperId":"581151d7bab87fc3f0bf04c9f91599ce994ca8c7","title":"On the Generalizability of Neural Program Analyzers with respect to Semantic-Preserving Program Transformations"},{"paperId":"7354e34cf3c495cfe2d8792bd9d8b2a55af709b1","title":"Deep Learning for Software Defect Prediction: A Survey"},{"paperId":"d6d76d2eeaf07bfa02554252026a2de9ef6166d5","title":"Blended, precise semantic program embeddings"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"08066c80919620397e8e4e5372ff84caf401e675","title":"Global Relational Models of Source Code"},{"paperId":"7660d5b42ac6769c6c61bf12aa9b8d320b864b30","title":"Software Language Comprehension using a Program-Derived Semantic Graph"},{"paperId":"756810258e3419af76aff38c895c20343b0602d0","title":"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"},{"paperId":"73e8fc4292ecbd271db9c4229c1cc9736b053760","title":"ProGraML: Graph-based Deep Learning for Program Optimization and Analysis"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"34733eaf66007516347a40ad5d9bbe1cc9dacb6b","title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"ee0aa02ed9e424364b2b5c1403026ccff409807b","title":"A Spatial and Sequential Combined Method for Web Service Classification"},{"paperId":"0998353a6a342143c08e152650c0146adc61d94b","title":"Multi-modal Attention Network Learning for Semantic Source Code Retrieval"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"3b99ae042727b7bf679a8e4120b28999d6530ff6","title":"Graph Matching Networks for Learning the Similarity of Graph Structured Objects"},{"paperId":"e0a3db9884fee0e7b7ae7e48cd9992866fa8d300","title":"Ithemal: Accurate, Portable and Fast Basic Block Throughput Estimation using Deep Neural Networks"},{"paperId":"73bade9162bc4df6ca6c9ab8a7eb2d54d35acad2","title":"code2vec: learning distributed representations of code"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"077ea3a496bd76bd22d76c432b79b9fa40e136e8","title":"Improving Automatic Source Code Summarization via Deep Reinforcement Learning"},{"paperId":"41c8cabe475d85d7548c25935da19ea5b96dfe06","title":"Neural Code Comprehension: A Learnable Representation of Code Semantics"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"62e176977d439aac2e2d7eca834a7a99016dfcaf","title":"Probabilistic model for code with decision trees"},{"paperId":"775dd65fd70c13bc4144c28b25aa3376bbab3254","title":"Deep learning code fragments for code clone detection"},{"paperId":"32aa2517b03c871c11e521c2a3406f457833e2c3","title":"Summarizing Source Code using a Neural Attention Model"},{"paperId":"1af68821518f03568f913ab03fc02080247a27ff","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"5a228775e3811c109a53ee77fea0a99c51899f6e","title":"Dynamic inference of static types for ruby"},{"paperId":"7554198284731daac1e6115fed8694cd40a5a292","title":"Context-free grammar induction using genetic programming"},{"paperId":"5531807b753a46a6b7f945cb6bd6fb898e097e18","title":"Attribute grammar paradigms—a high-level methodology in language implementation"}],"id":"0121c151c96b32cd7851e4bcda2a468b279c1e6f","summary":"This paper proposes to integrate different views with the natural-language description of source code into a unified framework with Multi-View contrastive Pre-training, and names the model as CODE-MVP."},{"url":"https://www.semanticscholar.org/paper/dae74645479f7c1fa3671066f9e24ec6c20c17ec","title":"TrojanPuzzle: Covertly Poisoning Code-Suggestion Models","venue":"ArXiv","year":2023,"referenceCount":57,"citationCount":1,"influentialCitationCount":0,"publicationDate":"06/01/2023","authors":"H. Aghakhani,Wei Dai,Andre Manoel,Xavier Fernandes,Anant Kharkar,Christopher Kruegel,Giovanni Vigna,David Evans,B. Zorn,Robert Sim","citations":[{"paperId":"1786a2f9140ed7211b21302977de64e948b92308","title":"Learning Performance-Improving Code Edits"}],"references":[{"paperId":"370b680057a6e324e67576a6bf1bf580af9fdd74","title":"Self-Supervised Learning: Generative or Contrastive"},{"paperId":"ce3f027b68dad014a58aa35f52380932c8d0b209","title":"Do Users Write More Insecure Code with AI Assistants?"},{"paperId":"08aaaf7ae3d61875e7bcf5c1bb0df4f17066e300","title":"Piccolo: Exposing Complex Backdoors in NLP Transformer Models"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":null,"title":"Natural language processing with transformers"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"592c7349e7ee1a7d6f2d1ddd468b6824b2d2217c","title":"Data Poisoning Attack against Recommender System Using Incomplete and Perturbed Data"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"feba0c47bf12a02c3a725174bb53df78658a72a8","title":"Pre-Trained Models: Past, Present and Future"},{"paperId":"d7c0aff686894b370a13840890d42fe7992d6b95","title":"Turn the Combination Lock: Learnable Textual Backdoor Attacks via Word Substitution"},{"paperId":"7571ed4cf1bbdcf891b576a0da12c910b1f0c72f","title":"Concealed Data Poisoning Attacks on NLP Models"},{"paperId":"ac6d17a1e4345b6699965fca636590edb91f10a8","title":"Hidden Killer: Invisible Textual Backdoor Attacks with Syntactic Trigger"},{"paperId":"3ab9145d5134e4e89bcceb1c8a95f9f98c98c5ff","title":"T-Miner: A Generative Approach to Defend Against Trojan Attacks on DNN-based Text Classification"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"62d1a3137b01a69443bebf4d92c1990ec512a6a1","title":"Extracting Training Data from Large Language Models"},{"paperId":"8dc712493df0a46fef830a6c3be64899880100c4","title":"Witches' Brew: Industrial Scale Data Poisoning via Gradient Matching"},{"paperId":"11fe33206746251656698bf5188fc622aea7fc21","title":"Trojaning Language Models for Fun and Profit"},{"paperId":"1ec69f1a1a9d4ff5c5bc70db0e5087157b620570","title":"You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion"},{"paperId":"218bd4129c8399ae48eaeba9a64a71e6f1e8cfea","title":"Bullseye Polytope: A Scalable Clean-Label Poisoning Attack with Improved Transferability"},{"paperId":"acce6b9598a7be1abfd7cf34a512059c91762ca0","title":"Detecting AI Trojans Using Meta Neural Analysis"},{"paperId":"478141a86e99fcd46fa4c91e68500e7198819d39","title":"Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers"},{"paperId":null,"title":"Gpt-j-6b: A 6 billion parameter autoregressive language model, 2021"},{"paperId":"2689649467774e024f4ee95fc0f626d551497e81","title":"VenoMave: Targeted Poisoning Against Speech Recognition"},{"paperId":"300343abf6440b33c8e5984f5864be566aab7661","title":"BAAAN: Backdoor Attacks Against Autoencoder and GAN-Based Machine Learning Models"},{"paperId":"cc3725e66aa600eb12b244e82880f8e0bd225065","title":"Poison Attacks against Text Datasets with Conditional Adversarially Regularized Autoencoder"},{"paperId":"df7336844a31165db0ae08f1cd0f560c9e3faeea","title":"BadNL: Backdoor Attacks Against NLP Models"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"8465338724f00a1f57a86717e4c898256c522be0","title":"MetaPoison: Practical General-purpose Clean-label Data Poisoning"},{"paperId":"3bcb17559ce96eb20fa79af8194f4af0380d194a","title":"Pre-trained models for natural language processing: A survey"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"3eb594bdc7057858a7bcd6243947c1944e89e2e3","title":"Adversarial Machine Learning - Industry Perspectives"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"31675c0dea68199af03a8c58b638b89df4b9db3f","title":"Hidden Trigger Backdoor Attacks"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"afbddbc750a003703ebd30090eb8c13a827dca69","title":"Trojan Attack on Deep Generative Models in Autonomous Driving"},{"paperId":"a94eb8f40c02fa7e30e3c92b331aadc1cd3dd944","title":"DeepInspect: A Black-box Trojan Detection and Mitigation Framework for Deep Neural Networks"},{"paperId":"79b7a61abd4a4b4cf62c26cec815b55b0778f48d","title":"Pythia: AI-assisted Code Completion System"},{"paperId":"f182ccbc90c1d20d358e3d197b340691f277428f","title":"A Backdoor Attack Against LSTM-Based Text Classification Systems"},{"paperId":"d9c14b1a705a3ac55e346a921189ef0fd2292fcb","title":"Transferable Clean-Label Poisoning Attacks on Deep Neural Nets"},{"paperId":"42658c812d60d26a0bdad91b4d81e8620b994bf6","title":"Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks"},{"paperId":"633ccadcde3bfca87f91bfe5ef4aa297fb2da2f4","title":"Detecting Backdoor Attacks on Deep Neural Networks by Activation Clustering"},{"paperId":"520ec00dc35475e0554dbb72f27bd2eeb6f4191d","title":"The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"71f212b84e8f784bb2c17bf9e2415b0b780f2e73","title":"Spectral Signatures in Backdoor Attacks"},{"paperId":"3db0e7710730707564bc1d335d2dcf7f9e70a011","title":"Clean-Label Backdoor Attacks"},{"paperId":"790ec1befba47991e8fd50a24d13be6094253f93","title":"Fine-Pruning: Defending Against Backdooring Attacks on Deep Neural Networks"},{"paperId":"4cdf26ce5e924a1d55b044626e41531a9e501e7a","title":"Automated poisoning attacks and defenses in malware detection systems: An adversarial machine learning approach"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"cb4c2a2d7e50667914d1a648f1a9134056724780","title":"Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":null,"title":"Github copilot -your ai pair programmer"},{"paperId":null,"title":"Amazon codewhisperer, ml-powered coding companion"}],"id":"dae74645479f7c1fa3671066f9e24ec6c20c17ec","summary":"Two novel data poisoning attacks are demonstrated, C OVERT and T ROJAN P UZZLE, that can bypass static analysis by planting malicious poisoning data in out-of-context regions such as docstrings and have implications for how practitioners should select code used to be coded."},{"url":"https://www.semanticscholar.org/paper/bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution","venue":"ArXiv","year":2023,"referenceCount":41,"citationCount":2,"influentialCitationCount":0,"publicationDate":"22/01/2023","authors":"Yihong Dong,J. Ding,Xue Jiang,Zhuo Li,Ge Li,Zhi Jin","citations":[{"paperId":"dd0960a8ffb6a8baff69ec96c3f67deb2912b53a","title":"Self-collaboration Code Generation via ChatGPT"},{"paperId":"d39db76fb80c1f74455a0f6432c2242b727389da","title":"Self-planning Code Generation with Large Language Model"}],"references":[{"paperId":"6e53a4e0f38d4be2dcaa2625a2a2a9332d338d05","title":"Incorporating domain knowledge through task augmentation for front-end JavaScript code generation"},{"paperId":"876eb375cb7b365475040046df669c039ad54202","title":"CodeT: Code Generation with Generated Tests"},{"paperId":"62851a515ea0ee3a547d94e8a493d978c22d0be9","title":"Multilingual Code Snippets Training for Program Translation"},{"paperId":"1d160123cbbef972ea151a641dd435d57c727de8","title":"AixBench: A Code Generation Benchmark Dataset"},{"paperId":"075b6fb7d3787953164eecc1bd2e13f97c9f3c44","title":"Fault-Aware Neural Code Rankers"},{"paperId":"ebc08a933e053e57bc2102848efa1119c0ce2bd9","title":"NS3: Neuro-Symbolic Semantic Code Search"},{"paperId":"1ff57e4c588173afeaacea553cd0908cadc3c8c8","title":"UniTE: Unified Translation Evaluation"},{"paperId":"47e15941c8b157873c8264e4bf50318d1ba5cd18","title":"Natural Language to Code Translation with Execution"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"2b556fcf2ac634f03c1fb0ace5e602e829418e65","title":"ReACC: A Retrieval-Augmented Code Completion Framework"},{"paperId":"4b27f18bff43d605805c92696a979714ced0b805","title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation"},{"paperId":"287e46bbc3d070fb82836cf706803c8a806b75e2","title":"Code Search based on Context-aware Code Translation"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"3f791ea6584b78fdf0ed80560d09d9496cf5a353","title":"Learning to Complete Code with Sketches"},{"paperId":"4ddc26b3a5fe9044b97b408d163f7464d769ebbf","title":"CODEP: Grammatical Seq2Seq Model for General-Purpose Code Generation"},{"paperId":"02da1f11b2620ee4a6c8b010fd1c8fe6f5ab5119","title":"Antecedent Predictions Are Dominant for Tree-Based Code Generation"},{"paperId":"570a6a5b8ec2827c3f33bb1b1bd027190a0d3e07","title":"Neural Program Generation Modulo Static Analysis"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"a6a7724763d8adba466519489b0b9d209e7f2d15","title":"BARTScore: Evaluating Generated Text as Text Generation"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"10d83b2590fd444d7dfa6344f051c1870d1f8398","title":"TransQuest: Translation Quality Estimation with Cross-lingual Transformers"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"9e67b9758520e49016ab66bafb974d2e1ed762d1","title":"COMET: A Neural Framework for MT Evaluation"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"4ae52766028e69186052ea8f33a137fbbbdb986a","title":"BLEURT: Learning Robust Metrics for Text Generation"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"295065d942abca0711300b2b4c39829551060578","title":"BERTScore: Evaluating Text Generation with BERT"},{"paperId":"635cb6fb865e86c108c5d1d895aeac0e759eb199","title":"MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"649c1148439a4e875dab414ba67bf8c80350af4a","title":"TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"d5516d19ed1cd91cdfd562a0339fb27eb1570b9e","title":"Bravais (1846) による2変量正規分布と相関係数の発見 -Analyse mathematique sur les probabilites des erreurs de situation d'un point 解題-"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"d7b7f16ec98591b535f7b03c5e7d3327a4b2cac8","title":"A NEW MEASURE OF RANK CORRELATION"},{"paperId":"3da8684050df72c07731c030f7669a920ccfffde","title":"An Introduction to the Theory of Statistics"},{"paperId":null,"title":"Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba"}],"id":"bf8d5f801237f4b9502efcc0528be3d27978bf59","summary":"CodeScore is proposed, an efﬁcient and effective CEM for code generation, which estimates test case PassRatio of generated code without executing code, and a framework named UniCE for training code evaluation models by learning code execution, i.e., learning Pass Ratio and Executability ofgenerated code."},{"url":"https://www.semanticscholar.org/paper/9afab8dc694269d205d769eaea549d8f7558d776","title":"Exploring the Verifiability of Code Generated by GitHub Copilot","venue":"ArXiv","year":2022,"referenceCount":14,"citationCount":1,"influentialCitationCount":0,"publicationDate":"05/09/2022","authors":"Dakota Wong,Austin Kothig,Patrick Lam","citations":[{"paperId":"f0ea9e2d3889d37f34743ed1dc64f11e8e0484de","title":"Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models"}],"references":[{"paperId":"8a854331c593c6a766fa3b8037fb2ad1b95a6f06","title":"An Empirical Study of Code Smells in Transformer-based Code Generation Techniques"},{"paperId":"d6954c43aa1ca197319c45d3988bc8fcec3de976","title":"GitHub Copilot AI pair programmer: Asset or Liability?"},{"paperId":"cdfe9580f63070f311151444f9df32818cc858bf","title":"An Empirical Evaluation of GitHub Copilot's Code Suggestions"},{"paperId":"4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models"},{"paperId":"8417d83e74a63b9d3874cd7695477ebad8b98f8a","title":"Choose your programming copilot: a comparison of the program synthesis performance of github copilot and genetic programming"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"},{"paperId":null,"title":"Your AI pair programmer"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":null,"title":"GitHub Copilot: Copyright, Fair Use, Creativity, Transformativity, and Algorithms"},{"paperId":null,"title":"Accessed 1 September 2022"},{"paperId":null,"title":"The Apple goto fail vulnerability: lessons learned"},{"paperId":"53a027ff333e4eb1d9f76152ce294922f5cbacfd","title":"The Dafny Integrated Development Environment"},{"paperId":"052d058217ad2561d3f292175e21ae07ed9eae67","title":"Binary Heaps Formally Verified in Why3"},{"paperId":"6c725d2a7e88515c5f7c877936f90b0184c4fe8f","title":"Dafny: An Automatic Program Verifier for Functional Correctness"}],"id":"9afab8dc694269d205d769eaea549d8f7558d776","summary":"Evidence is found which corroborates the current consensus in the literature: Copilot is a powerful tool; however, it should not be “ﬂying the plane\" by itself."},{"url":"https://www.semanticscholar.org/paper/74f57a9ffa73e1bee99300b177904c06199840aa","title":"CodeRetriever: A Large Scale Contrastive Pre-Training Method for Code Search","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Xiaonan Li,Yeyun Gong,Yelong Shen,Xipeng Qiu,Hang Zhang,Bolun Yao,Weizhen Qi,Daxin Jiang,Weizhu Chen,Nan Duan","citations":[],"references":[{"paperId":"8ffe7b7eeddbd4c22d642b0a48379d17e61c3bab","title":"Soft-Labeled Contrastive Pre-training for Function-level Code Representation"},{"paperId":"0121c151c96b32cd7851e4bcda2a468b279c1e6f","title":"CODE-MVP: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training"},{"paperId":"4b27f18bff43d605805c92696a979714ced0b805","title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation"},{"paperId":"606957402ee0741ed130c6a03ec7f20d30b7083f","title":"Adversarial Retriever-Ranker for dense text retrieval"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"c56aced0f0c5cfebefadb530cb08d736c3ac5c05","title":"Retrieval Augmented Code Generation and Summarization"},{"paperId":"0c21334be0228431d619a180c809b43be0065bdd","title":"CoSQA: 20,000+ Web Queries for Code Search and Question Answering"},{"paperId":"077c713bccd9d2c7fde68d4cbde06ab0f07a6855","title":"ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer"},{"paperId":"0b077c9577f4297dcf3da835e253d21965bbc6e0","title":"CoTexT: Multi-task Learning with Code-Text Transformer"},{"paperId":"c26759e6c701201af2f62f7ee4eb68742b5bf085","title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"8d1369a218a39214d82ea77ff964570eca057c15","title":"Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup"},{"paperId":"3416e5e5694855f7175125b5fe2e0b659c3cdbfa","title":"RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"10d5a71da740e4d709914c450fc70fee1959b196","title":"Self-Supervised Contrastive Learning for Code Retrieval and Summarization via Semantic-Preserving Transformations"},{"paperId":"021bbcefc993c389bad6c1daefd8ff92d0fc2441","title":"Contrastive Code Representation Learning"},{"paperId":"c9b8593db099869fe7254aa1fa53f3c9073b0176","title":"Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval"},{"paperId":"2cbb8de53759e75411bc528518947a3094fbce3a","title":"Billion-Scale Similarity Search with GPUs"},{"paperId":"036fda02d93139ad0ef37d892d41796a2a39fdcd","title":"CLSEBERT: Contrastive Learning for Syntax Enhanced Code Pre-Trained Model"},{"paperId":"61a3de09c3430eb1a7da2cf5836c66e4520bb674","title":"Contrastive Learning for Source Code with Structural and Functional Properties"},{"paperId":"8edf070ee55db69f06b43fb46b055182837598f7","title":"On the Sentence Embeddings from Pre-trained Language Models"},{"paperId":"2dc59238ad0f010f505238fcd0dd0695681200ef","title":"Neural Code Search Revisited: Enhancing Code Snippet Retrieval through Natural Language Intent"},{"paperId":"406afe68e789fcb0d7e3d24ebea65b53d206f740","title":"Exploring Software Naturalness through Neural Language Models"},{"paperId":"9a56ab8b1aba50dc2fea3cf4b531d30891a88ba9","title":"Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere"},{"paperId":"8ee2351221b72fca5eef4c42147ed67071903d93","title":"IntelliCode compose: code generation using transformer"},{"paperId":"5d839ec37463be6998a46af8e6ca7a96c575b3ba","title":"SCELMo: Source Code Embeddings from Language Models"},{"paperId":"79cd9f77e5258f62c0e15d11534aea6393ef73fe","title":"Dense Passage Retrieval for Open-Domain Question Answering"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"463fefdbd81a4a0a32cf59bc58a9545757c8cf2e","title":"Pre-trained Contextual Embedding of Source Code"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"9656eeb6bb9fc1f35418791cf0c7310e378f2435","title":"Neural Code Search Evaluation Dataset"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"dc030c2e55b266c029356a54bb444b7d9b1f2abc","title":"StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow"},{"paperId":"b0767a0fbba9ed35e452122b6d7cc1da7e96e544","title":"Query Expansion Based on Crowd Knowledge for Code Search"},{"paperId":"49512270b39636375880d611d7b2192d324f4ba6","title":"Convolutional Neural Networks over Tree Structures for Programming Language Processing"},{"paperId":"7e8d5a108c28cdfb92f419ce919fbf7993dfebfc","title":"A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval"},{"paperId":"fdb813d8b927bdd21ae1858cafa6c34b66a36268","title":"Learning deep structured semantic models for web search using clickthrough data"},{"paperId":"1c46943103bd7b7a2c7be86859995a4144d1938b","title":"Visualizing Data using t-SNE"},{"paperId":"cfb55a09bf84f7a22740dbc25f07ff0c61ed9383","title":"Xerox TREC-8 Question Answering Track Report"}],"id":"74f57a9ffa73e1bee99300b177904c06199840aa","summary":"The proposed CodeRetriever model, which learns the function-level code semantic representations through large-scale code-text contrastive pre-training, achieves new state-of-the-art with significant improvement over existing code pre-trained models, on eleven domain/language-specific code search tasks with six programming languages in different code granularity."},{"url":"https://www.semanticscholar.org/paper/2a0456b0408cd4c33f2ff4400374e7be2497a362","title":"Repairing Bugs in Python Assignments Using Large Language Models","venue":"ArXiv","year":2022,"referenceCount":49,"citationCount":5,"influentialCitationCount":1,"publicationDate":"29/09/2022","authors":"Jialu Zhang,J. Cambronero,Sumit Gulwani,Vu Le,R. Piskac,Gustavo Soares,Gust Verbruggen","citations":[{"paperId":"3e7055980da1853d196fcee0d3b7f390fb518d62","title":"A Survey on Automated Program Repair Techniques"},{"paperId":"c72ac81c4ac414314b52cf8f5b77370f8d0875d4","title":"Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models"},{"paperId":"e9a64e58855dbbc725203d0202ceb9e7f8b7bb36","title":"A Survey of Learning-based Automated Program Repair"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"ec229b004ebaee27b9efa84b834825dbc2937538","title":"Is GitHub's Copilot as Bad As Humans at Introducing Vulnerabilities in Code?"}],"references":[{"paperId":"b78d0f729d44b54b8044afb0fbd426c2201cba02","title":"Generating Concise Patches for Newly Released Programming Assignments"},{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"453a8fac3be9282be53908f0735160d0d21e0f48","title":"Repair Is Nearly Generation: Multilingual Program Repair with LLMs"},{"paperId":"2f2750b48a6f958ff12cba90e99695123d1e2f47","title":"Using pre-trained language models to resolve textual and semantic merge conflicts (experience paper)"},{"paperId":"e37155d21818513bd40d64ee212099aac82bd6f8","title":"Less training, more repairing please: revisiting automated program repair via zero-shot learning"},{"paperId":"1100dee3fd78655cddc4b7bfaef1161351d4fab5","title":"Automated Feedback Generation for Competition-Level Code"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"979eb5c97c49d7979447ed684500895a24d75ac4","title":"The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"8c40f0259aac79db7203ffdb452fffe006cd7e18","title":"TOGA: A Neural Method for Test Oracle Generation"},{"paperId":"c7684a8a4f24dc0fc9416e3cd0782cdbd467f27c","title":"Verifix: Verified Repair of Programming Assignments"},{"paperId":null,"title":"Coping with copilot"},{"paperId":"75264d5b7d144845737faa3947b82d06c6c537aa","title":"Semantic programming by example with pre-trained models"},{"paperId":"a8863de15a5ee8eed98107f423138a1a8f5a2ba8","title":"Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis"},{"paperId":"426321fc4ae7a67f8d0f35bf4afe212da25c96f7","title":"VarFix: balancing edit expressiveness and search effectiveness in automated program repair"},{"paperId":"46ca52b858c0ac1088cdcfb5bed7006b55cadf33","title":"Context-aware and data-driven feedback generation for programming assignments"},{"paperId":"ebfcbe0a8b238d5a52286fdfaab7be0170dbc91b","title":"FAPR: Fast and Accurate Program Repair for Introductory Programming Courses"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"ab65b25165e9ac037bb161c09ec3308a8b197793","title":"Concolic program repair"},{"paperId":"a989f5daa7c438d6b8433efbb1461f1b0af1aa8e","title":"Break-It-Fix-It: Unsupervised Learning for Program Repair"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"93999f914e2a99bbbe2eab49e47f2f8358d99a09","title":"MACER: A Modular Framework for Accelerated Compilation Error Repair"},{"paperId":"6acf0d7b24a93953269a4d52c6b2f0c7d11f9ae4","title":"Hoppity: Learning Graph Transformations to Detect and Fix Bugs in Programs"},{"paperId":"b5abecf78c1bd975449e34350289a904e6ef35cc","title":"Automated program repair"},{"paperId":"325aecaa34bcf6e5f0e6683e526dc9946a6c1f4f","title":"Re-Factoring Based Program Repair Applied to Programming Assignments"},{"paperId":null,"title":"The hard part of computer science? getting into class"},{"paperId":"b599ce0f548a73c156e94c7bcca8aed5b0a9678b","title":"Compilation Error Repair: For the Student Programs, From the Student Programs"},{"paperId":"a1b9e544b74bb9e1b7ef20ee18d90644367bab17","title":"Semantic Program Repair Using a Reference Implementation"},{"paperId":"c492e49c9eb3cfd51e77ab814201e81fe33cdcc5","title":"Dynamic Neural Program Embedding for Program Repair"},{"paperId":"d591ed7f34cd0c169baf0ee75f82a11c338249ca","title":"Search, align, and repair: data-driven feedback generation for introductory programming exercises"},{"paperId":"30a331410b32cb1a08e2906ac57eb4e22069e664","title":"Automated clustering and program repair for introductory programming assignments"},{"paperId":"388f8fcb6456ca33ae3e861c06e8b978ba5914cd","title":"Leveraging syntax-related code for automated program repair"},{"paperId":"a948e30cb47360d23d22f73008880c9c27d034c6","title":"HappyFace: Identifying and predicting frustrating obstacles for learning programming at scale"},{"paperId":"6e5f3c4507aeb175da8712cff43cb8b2e60b5a12","title":"A feasibility study of using automated program repair for introductory programming assignments"},{"paperId":"bef6f21fccdcac24ebcd6b4287801ff1c9734966","title":"Automatic inference of code transforms for patch generation"},{"paperId":"cf404026f1e66dc04417d699ef2b4731cbec842c","title":"Learning Syntactic Program Transformations from Examples"},{"paperId":"5b0602f9d0f1384dc95335c6ed220fef40a7e186","title":"sk_p: a neural program corrector for MOOCs"},{"paperId":"3f215e83b39a0887257a03274002f353c5a57537","title":"Angelix: Scalable Multiline Program Patch Synthesis via Symbolic Analysis"},{"paperId":"50358e3f30ab8c99f9d383e5683b31c2311e5651","title":"Automatic patch generation by learning correct code"},{"paperId":"f2ec1f6be2e9206f5d0f9be2253f20d261cf2242","title":"Repairing Programs with Semantic Code Search (T)"},{"paperId":"010d163129deb9a5fdb611a565ed1fbb4e62a114","title":"37 Million Compilations: Investigating Novice Programming Mistakes in Large-Scale Student Data"},{"paperId":"90441b975380c806320420724fc9d0ec77dbefdb","title":"The strength of random search on automated program repair"},{"paperId":"3136ad216d30bdff223e5c3f02e07f980a6a45a5","title":"Automatic patch generation learned from human-written patches"},{"paperId":"169a72730c870f30a4452bfc0648e35ac4561f42","title":"Automated feedback generation for introductory programming assignments"},{"paperId":"3aa35c213730e688fa191a1ce241db31087956f0","title":"GenProg: A Generic Method for Automatic Software Repair"},{"paperId":"ff4324c1bb61c3903ea0d94701549a45900fdbd9","title":"Error detecting and error correcting codes"},{"paperId":null,"title":"New GPT-3 Capabilities: Edit & Insert"},{"paperId":null,"title":"The bigscience corpus a 1 . 6 tb composite multilingual dataset . ” [ 25 ] “ New GPT - 3 Capabilities : Edit & Insert , ” Mar . 2022 . [ Online ]"}],"id":"2a0456b0408cd4c33f2ff4400374e7be2497a362","summary":"This work proposes to use a large language model trained on code, such as Codex, to build an APR system – MMAPR – for introductory Python programming assignments and finds that MM APR can produce more programs and produce smaller patches on average."},{"url":"https://www.semanticscholar.org/paper/ebc08a933e053e57bc2102848efa1119c0ce2bd9","title":"NS3: Neuro-Symbolic Semantic Code Search","venue":"ArXiv","year":2022,"referenceCount":62,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/05/2022","authors":"Shushan Arakelyan,Anna Hakhverdyan,Miltiadis Allamanis,Christophe Hauser,Luis Garcia,Xiang Ren","citations":[{"paperId":"bf8d5f801237f4b9502efcc0528be3d27978bf59","title":"CodeScore: Evaluating Code Generation by Learning Code Execution"}],"references":[{"paperId":"e02e92d7da518ae8eef676341564c4b5cef11f0a","title":"GraphSearchNet: Enhancing GNNs via Capturing Global Dependencies for Semantic Code Search"},{"paperId":"6c89f8d0172d844f939289f052fbeb4df760dd59","title":"A Neural Network Architecture for Program Understanding Inspired by Human Behaviors"},{"paperId":"0121c151c96b32cd7851e4bcda2a468b279c1e6f","title":"CODE-MVP: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training"},{"paperId":"2b556fcf2ac634f03c1fb0ace5e602e829418e65","title":"ReACC: A Retrieval-Augmented Code Completion Framework"},{"paperId":"4b27f18bff43d605805c92696a979714ced0b805","title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation"},{"paperId":"4a160efbe80c38cd5eb2f92c7c095b49b113397d","title":"In-IDE Code Generation from Natural Language: Promise and Challenges"},{"paperId":"9fd854f8315bdb02449500088d3cb8e6e84a0d09","title":"Cross-Domain Deep Code Search with Few-Shot Meta Learning"},{"paperId":"367b155174e0793d23f8992ebc0664e6102f7f02","title":"Is a Single Model Enough? MuCoS: A Multi-Model Ensemble Learning Approach for Semantic Code Search"},{"paperId":"51654d8317478fe1497678b15e36ac99d73a65b2","title":"CAST: Enhancing Code Summarization with Hierarchical Splitting and Reconstruction of Abstract Syntax Trees"},{"paperId":"0c21334be0228431d619a180c809b43be0065bdd","title":"CoSQA: 20,000+ Web Queries for Code Search and Question Answering"},{"paperId":"05e396e79a2f88f0b8f8d99f5ab36ab3efa95c14","title":"Language-Agnostic Representation Learning of Source Code from Structure and Context"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"2891beec3cf455e5ef90b829c0d548884fa17956","title":"CRaDLe: Deep Code Retrieval Based on Semantic Dependency Learning"},{"paperId":"8b4c857311c001f6ed0cd790cce4af4dfcfb6533","title":"Deep Graph Matching and Searching for Semantic Code Retrieval"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"10d5a71da740e4d709914c450fc70fee1959b196","title":"Self-Supervised Contrastive Learning for Code Retrieval and Summarization via Semantic-Preserving Transformations"},{"paperId":"021bbcefc993c389bad6c1daefd8ff92d0fc2441","title":"Contrastive Code Representation Learning"},{"paperId":"c58ad64a5774023884f5aef01106a419a8e40463","title":"Learning Sequential and Structural Information for Source Code Summarization"},{"paperId":"2dc59238ad0f010f505238fcd0dd0695681200ef","title":"Neural Code Search Revisited: Enhancing Code Snippet Retrieval through Natural Language Intent"},{"paperId":"61a412f7c351a2a582f65853522ad7a632b0186d","title":"Evaluating Compositionality of Sentence Representation Models"},{"paperId":"037aa837d95b5f0edef494a2392b1788dc840a47","title":"A Multi-Perspective Architecture for Semantic Code Search"},{"paperId":"805a6d1df9f460abfcea3d51d181cf1e80680be4","title":"A Transformer-based Approach for Source Code Summarization"},{"paperId":"71b6394ad5654f5cd0fba763768ba4e523f7bbca","title":"Longformer: The Long-Document Transformer"},{"paperId":"21d9d58ab9537d10927b749eaa1c8b8c5a970579","title":"Leveraging Code Generation to Improve Code Retrieval and Summarization via Dual Learning"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":null,"title":"spaCy: Industrialstrength Natural Language Processing in Python"},{"paperId":"0998353a6a342143c08e152650c0146adc61d94b","title":"Multi-modal Attention Network Learning for Semantic Source Code Retrieval"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"2dce4f81e601a2ba8a939e91fa45119d7eddafb2","title":"SCOR: Source Code Retrieval with Semantics and Order"},{"paperId":"1432c8378b1cafa3f91f09fa743082d154fdab92","title":"A Novel Neural Source Code Representation Based on Abstract Syntax Tree"},{"paperId":"0dc092d33f7c71bc9d8d42b53ebb1fad101db4c8","title":"Linguistic generalization and compositionality in modern artificial neural networks"},{"paperId":"b9700322c4ccbf441373973f133e2aa95e5d73b2","title":"CoaCor: Code Annotation for Code Retrieval with Reinforcement Learning"},{"paperId":"73bade9162bc4df6ca6c9ab8a7eb2d54d35acad2","title":"code2vec: learning distributed representations of code"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"bbe8c5e53ca6e4db115afeaaad2be268f039f10d","title":"DeepSim: deep learning code functional similarity"},{"paperId":"073bbfc1fd41a9d67dd6f568ac7e8dff5b2c9c95","title":"A Neural Framework for Retrieval and Summarization of Source Code"},{"paperId":"b227f3e4c0dc96e5ac5426b85485a70f2175a205","title":"Representation Learning with Contrastive Predictive Coding"},{"paperId":"e033a0b29af2939fd44e5765d03380c08897a9e8","title":"Deep Code Search"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"21c99706bb26e9012bfb4d8d48009a3d45af59b2","title":"Neural Module Networks"},{"paperId":"9808b8d09ecb18762b760f709f58b1614f224352","title":"Broad-coverage CCG Semantic Parsing with AMR"},{"paperId":"598aef1340892256ed43a5f3cd2022671c88d99b","title":"Query expansion via WordNet for effective code search"},{"paperId":"e3ce36b9deb47aa6bb2aa19c4bfa71283b505025","title":"Noise-contrastive estimation: A new estimation principle for unnormalized statistical models"},{"paperId":"181d81234e12834e3f18e4c8e4ea16286839f074","title":"Semantics-based code search demonstration proposal"},{"paperId":"74fe7ec751cd50295b15cfd46389a8fefb37c414","title":"Learning to Map Sentences to Logical Form: Structured Classification with Probabilistic Categorial Grammars"},{"paperId":"c6e234e41637656ba43762e52e00a0d1072dfbeb","title":"Semantic grep: regular expressions + relational abstraction"},{"paperId":null,"title":"Virtual Event / Punta Cana, Dominican Republic"},{"paperId":null,"title":"code, data, models) or curating/releasing new assets... (a) If your work uses existing assets"},{"paperId":null,"title":"Did you state the full set of assumptions of all theoretical results"},{"paperId":null,"title":"If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots"},{"paperId":null,"title":"Did you report error bars (e.g., with respect to the random seed after running experiments multiple times"},{"paperId":null,"title":"Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content"},{"paperId":null,"title":"Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?"},{"paperId":null,"title":"(c) Did you discuss any potential negative societal impacts of your work"},{"paperId":null,"title":"Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?"},{"paperId":null,"title":"Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Code and data in supplemental material"},{"paperId":null,"title":"Did you include any new assets either in the supplemental material or as a URL? [No] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating?"},{"paperId":null,"title":"Have you read the ethics review guidelines and ensured that your paper conforms to them"},{"paperId":null,"title":"Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation"}],"id":"ebc08a933e053e57bc2102848efa1119c0ce2bd9","summary":"This work proposes supplementing the query sentence with a layout of its semantic structure, using a Neural Module Network architecture to implement this idea, and demonstrates that this approach results in more precise code retrieval."},{"url":"https://www.semanticscholar.org/paper/bd5d3022dc395ca85f72e346022ed6175e13a278","title":"A Transformer-based Approach for Translating Natural Language to Bash Commands","venue":"International Conference on Machine Learning and Applications","year":2021,"referenceCount":50,"citationCount":6,"influentialCitationCount":0,"publicationDate":"01/12/2021","authors":"Quchen Fu,Zhongwei Teng,Jules White,Douglas C. Schmidt","citations":[{"paperId":"7cb3ced99a7afb030961f3534721c0bf94969f21","title":"CitySpec with Shield: A Secure Intelligent Assistant for Requirement Formalization"},{"paperId":"0bc2753f59e653de718b5c7a2a0a7e00d13778c7","title":"NL2CMD: An Updated Workflow for Natural Language to Bash Commands Translation"},{"paperId":"199e744034a49edbb616dc5f003bb7ebe09bea3c","title":"Deep Learning Models on CPUs: A Methodology for Efficient Training"},{"paperId":"8c7fd98b6bc4f32772e76471afe8babc323f10d3","title":"CitySpec: An Intelligent Assistant System for Requirement Specification in Smart Cities"},{"paperId":"134a83d99b4913af4ed265310dfaa670e2689ce0","title":"Translating Natural Language to Bash Commands using Deep Neural Networks"},{"paperId":"430aaacce4147faef7940d1908cf715c3938e51f","title":"Using Language Models to Convert Between Natural Language and Game Commands"}],"references":[{"paperId":"642e280df732665249315d6c144871f0e2ceeae6","title":"NeurIPS 2020 NLC2CMD Competition: Translating Natural Language to Bash Commands"},{"paperId":"548c05e30b02aebb90264cc5b4bf031758ab1c3f","title":"Towards Accurate and Reliable Energy Measurement of NLP Models"},{"paperId":"0544af8d6b6b6e9ce14be9c6425e520a638be380","title":"Photon: A Robust Cross-Domain Text-to-SQL System"},{"paperId":"fea343ea0aefcafc4dbb87b710d47c329df83bc5","title":"ValueNet: A Neural Text-to-SQL Architecture Incorporating Values"},{"paperId":"77910e51a40d17157fc798325d06edfa6cff18d6","title":"Incorporating External Knowledge through Pre-training for Natural Language to Code Generation"},{"paperId":"3c9fa66674e1053eb9de12076c48f13e46422c29","title":"CLAI: A Platform for AI Skills on the Command Line"},{"paperId":"74b4f16c5ac91e3e7c88ae81cc8c91416b71d151","title":"Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning"},{"paperId":"545e60873b0fad25407bb6d647f92c905bd2483d","title":"CoSQL: A Conversational Text-to-SQL Challenge Towards Cross-Domain Natural Language Interfaces to Databases"},{"paperId":"0de0a44b859a3719d11834479112314b4caba669","title":"A Multiscale Visualization of Attention in the Transformer Model"},{"paperId":"d6a083dad7114f3a39adc65c09bfbb6cf3fee9ea","title":"Energy and Policy Considerations for Deep Learning in NLP"},{"paperId":"9e955da40c62b2543b963454b90928ad6cbd5f8a","title":"Does BLEU Score Work for Code Migration?"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"0e37edd9c8dd7b7b9a9665501b1926e3a9f58786","title":"Neural Machine Translation Advised by Statistical Machine Translation: The Case of Farsi-Spanish Bilingually Low-Resource Scenario"},{"paperId":"649c1148439a4e875dab414ba67bf8c80350af4a","title":"TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation"},{"paperId":"97685859d4bcbc3b893425e6cb8fda8e9c15cfcb","title":"Hallucinations in Neural Machine Translation"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"bb669de2fce407df2f5cb2f8c51dedee3f467e04","title":"The Best of Both Worlds: Combining Recent Advances in Neural Machine Translation"},{"paperId":"c68fbc1f4aa72d30974f8a3071054e3b227137fd","title":"Generating Natural Language Adversarial Examples"},{"paperId":"d3707cf521e3596313af1f53acba6413d0d528a6","title":"Training Tips for the Transformer Model"},{"paperId":"7123904795de87527ab86f8b44227e7c6660ff42","title":"Mining stackoverflow for program repair"},{"paperId":"429b65937d4922578a81e1f0ef5aeab7361ae36b","title":"NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System"},{"paperId":"116a877c774969b53399b1ccaa34d51bfb492ee4","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"de5168baf710e922bb042043c3cd8d2feb411aad","title":"Checkpoint Ensembles: Ensemble Methods from a Single Training Process"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"7f8f20bf86d3fdcdd87f260c64569a355784f3be","title":"The Free Software Foundation"},{"paperId":"668db48c6a79826456341680ee1175dfc4cced71","title":"Get To The Point: Summarization with Pointer-Generator Networks"},{"paperId":"aab5002a22b9b4244a8329b140bd0a86021aa2d1","title":"OpenNMT: Open-Source Toolkit for Neural Machine Translation"},{"paperId":"8ec6abfdc5009b4e490e975991c871dfeec05434","title":"Program Synthesis from Natural Language Using Recurrent Neural Networks"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"5507dc32b368c8afd3b9507e9b5888da7bd7d7cd","title":"Sequence-to-Sequence Learning as Beam-Search Optimization"},{"paperId":"ba30df190664193514d1d309cb673728ed48f449","title":"Incorporating Copying Mechanism in Sequence-to-Sequence Learning"},{"paperId":"9653d5c2c7844347343d073bbedd96e05d52f69b","title":"Pointer Networks"},{"paperId":"adfcf065e15fd3bc9badf6145034c84dfb08f204","title":"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling"},{"paperId":"cea967b59209c6be22829699f05b8b1ac4dc092d","title":"Sequence to Sequence Learning with Neural Networks"},{"paperId":"6d334cb557f5addd3fc080134483125eea0b7a90","title":"On our best behaviour"},{"paperId":null,"title":"Bashlex"},{"paperId":"b29447ba499507a259ae9d8f685d60cc1597d7d3","title":"Semantic Parsing on Freebase from Question-Answer Pairs"},{"paperId":"128cb6b891aee1b5df099acb48e2efecfcff689f","title":"The Winograd Schema Challenge"},{"paperId":"9819b600a828a57e1cde047bbe710d3446b30da5","title":"Recurrent neural network based language model"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"a0456c27cdd58f197032c1c8b4f304f09d4c9bc5","title":"Ensemble Methods in Machine Learning"},{"paperId":"e23c34414e66118ecd9b08cf0cd4d016f59b0b85","title":"Bidirectional recurrent neural networks"},{"paperId":"ffb87ecc6bb5ac8423ab469167708793c479e5fa","title":"Usability engineering"},{"paperId":"ab7b5917515c460b90451e67852171a531671ab8","title":"The Mathematics of Statistical Machine Translation: Parameter Estimation"},{"paperId":"b0e24d152ee16920a095334cf5913e7c471a4da4","title":"Redundancy in Natural Language Processing"},{"paperId":"b3d2c98c190a3b54bda9ce3ab670549d25579e6e","title":"Hints on programming language design."},{"paperId":"50f06becb19bb2f5056d987037691153a64ed0d4","title":"The use of English as a programming language"},{"paperId":null,"title":"Recent advances in google translate , ” 2020 . [ Online ]"},{"paperId":null,"title":"Notes on ambiguity"}],"id":"bd5d3022dc395ca85f72e346022ed6175e13a278","summary":"The approach presented in this paper is the best performing architecture on this problem to date and improves the current state-of-the-art accuracy on this translation task from 13.8% to 53.2%."},{"url":"https://www.semanticscholar.org/paper/538288d24bdad73d831dfed44b706958287ed318","title":"Generating Sequences by Learning to Self-Correct","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":12,"influentialCitationCount":0,"publicationDate":"31/10/2022","authors":"S. Welleck,Ximing Lu,Peter West,Faeze Brahman,T. Shen,Daniel Khashabi,Yejin Choi","citations":[{"paperId":"eb55442d940accc67d6790e66d3196e1941fc7a4","title":"Teaching Large Language Models to Self-Debug"},{"paperId":"c715914c388fa64dd8686cd8755e5adfebbf2388","title":"REFINER: Reasoning Feedback on Intermediate Representations"},{"paperId":"762b4b11220d2d8a9c57f0a3af327840a67e7284","title":"Self-Refine: Iterative Refinement with Self-Feedback"},{"paperId":"f49efda9fca564b66bbb8c07214166715bfd79b7","title":"Extrapolative Controlled Sequence Generation via Iterative Refinement"},{"paperId":"6d269364de402d4a72ac30b0c8d81324f6849807","title":"LEVER: Learning to Verify Language-to-Code Generation with Execution"},{"paperId":"a93dab80894abc5116b95782db5871e6288812b6","title":"NASA is launching a spacecraft to Mars in 2020 ... NLG model NASA launching spacecraft"},{"paperId":null,"title":"NLG model user input NASA is launching a spacecraft to the moon on Monday ... NLG model NLG model"},{"paperId":"d3a7a4543d83f568f79d1febe8379465ff0140c9","title":"A Survey of Deep Learning for Mathematical Reasoning"},{"paperId":"bbe93c90b7b87939cd064c805858feca61a3234d","title":"Self-Instruct: Aligning Language Model with Self Generated Instructions"},{"paperId":"6db13f58ff662eefa823a660fa86faf8ddf75533","title":"Controllable Text Generation with Language Constraints"},{"paperId":"6d4e540e1bed26679097139bf90c8652919e4e5c","title":"Explanation Regeneration via Information Bottleneck"},{"paperId":"1586dd245ae4cca00ea519fb35f27e2a0980476d","title":"Teaching Structured Vision&Language Concepts to Vision&Language Models"}],"references":[{"paperId":"a630c70aed27b52f6d04d1e772b153c5a7b6f6fe","title":"LILA: A Unified Benchmark for Mathematical Reasoning"},{"paperId":"a938ff4539b09a785a66669844f1a35f76169218","title":"PEER: A Collaborative Language Model"},{"paperId":"29acc890e521f7a6415666ab9eb3432c49b4587a","title":"Self-critiquing models for assisting human evaluators"},{"paperId":"35afb74de9660962ebac2843d26de22a6fac2ef6","title":"Learning from Self-Sampled Correct and Partially-Correct Programs"},{"paperId":"1386b8a11929cf02da291c56aca353e33bbc22ed","title":"Diffusion-LM Improves Controllable Text Generation"},{"paperId":"60bff5a4527141599d8e05904baf96410541f8a9","title":"Learning to Model Editing Processes"},{"paperId":"4a6a65968a8eb8c09ffb57a7774ddabb596565b1","title":"COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"69a500b9cc0f0755efb748cff10bb90e6facd632","title":"NeuroLogic A*esque Decoding: Constrained Text Generation with Lookahead Heuristics"},{"paperId":"d6045d2ccc9c09ca1671348de86d07da6bc28eea","title":"Training Verifiers to Solve Math Word Problems"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"02f033482b8045c687316ef81ba7aaae9f0a2e1c","title":"DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"13c4e5a6122f3fa2663f63e49537091da6532f35","title":"Are NLP Models really able to Solve Simple Math Word Problems?"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"2c5bf29079cd958a2bef150077a02a1deb300652","title":"NeuroLogic Decoding: (Un)supervised Neural Text Generation with Predicate Logic Constraints"},{"paperId":"483336e50c566c5505012d8777b97800a6386113","title":"Text Editing by Command"},{"paperId":"07bcda1dff9bb696ea9cbc69303eee8bd3d85bd6","title":"GeDi: Generative Discriminator Guided Sequence Generation"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"399e7d8129c60818ee208f236c8dda17e876d21f","title":"RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"},{"paperId":"3aba4051edc0174c37b88ea4ff6f061c4b5fdeb7","title":"Iterative Refinement in the Continuous Space for Non-Autoregressive Neural Machine Translation"},{"paperId":"053b1d7b97eb2c91fc3921d589c160b0923c70b1","title":"Learning to summarize from human feedback"},{"paperId":"e816f788767eec6a8ef0ea9eddd0e902435d4271","title":"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"},{"paperId":"0119a57cf88ef16e6dc291252fae340bb6b3953c","title":"CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"e04a80263d252a3d8a382ba37a249b9345620570","title":"Plug and Play Language Models: A Simple Approach to Controlled Text Generation"},{"paperId":"aec95d1124378f46dc17ef1d4ad07ae37ee72ffe","title":"Unsupervised Paraphrasing by Simulated Annealing"},{"paperId":null,"title":"Language models are few-shot"},{"paperId":"7a15950dc71079285a4eaf195de5aadd87c41b40","title":"Fine-Tuning Language Models from Human Preferences"},{"paperId":"efa6c8c11a59284f584fbc8caa49f1bcc68deb01","title":"CGMH: Constrained Sentence Generation by Metropolis-Hastings Sampling"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"531a7f2c659787165df4fd5b4580590b953448e4","title":"The E2E Dataset: New Challenges For End-to-End Generation"},{"paperId":"187c25de08261760cf48b4e9dabf308d2f7f15d9","title":"Reasoning about Quantities in Natural Language"},{"paperId":"a7862e14b4c20cefd6dc4f611f8aa866fabf130b","title":"Learning to Solve Arithmetic Word Problems with Verb Categorization"},{"paperId":null,"title":"Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. ArXiv, abs"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"},{"paperId":"bda499c853d6351b168cb7999286f3c7d81ac184","title":"Text generation"}],"id":"538288d24bdad73d831dfed44b706958287ed318","summary":"SELF - CORRECTION is presented, an approach that decouples an imperfect base generator from a separate corrector that learns to iteratively correct imperfect generations and improves upon the base generator in three diverse generation tasks– mathematical program synthesis, lexically-constrained generation, and toxicity control."},{"url":"https://www.semanticscholar.org/paper/535a3d95a743e1f4b591b5b2af3e778a6347158a","title":"CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code","venue":"","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/02/2023","authors":"Shuyan Zhou,Uri Alon,Sumit Agarwal,Graham Neubig","citations":[],"references":[{"paperId":"690c210564226c9307b3bab977cdc07a6a45863a","title":"Execution-Based Evaluation for Open-Domain Code Generation"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"2577d053f8aab912d29b424e1f09133d83740fd2","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"1d26c947406173145a4665dd7ab255e03494ea28","title":"GLM-130B: An Open Bilingual Pre-trained Model"},{"paperId":"654cd297b307c0c6fa08732511ba852b8dce1977","title":"Out of the BLEU: how should we assess quality of the Code Generation models?"},{"paperId":"0a39442979d6e678dd36bb443ad529c14e86a86e","title":"DocPrompting: Generating Code by Retrieving the Docs"},{"paperId":"205ac1373eb7981aca2d08f2ab651871a001271e","title":"CrystalBLEU: Precisely and Efficiently Measuring the Similarity of Code"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"1b4c19168410fb2690d285b205ab2281793db81a","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"},{"paperId":null,"title":"Natural Language Processing with Transformers"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"7f42e68c4caf74f91bf9990012efa8d7641235d6","title":"Self-Supervised Bug Detection and Repair"},{"paperId":"0505f17c4052366cbc4fad99150d3542edf85faa","title":"TFix: Learning to Fix Coding Errors with a Text-to-Text Transformer"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"27724bd19946d6a824d06cdca3cdfe5d40f71003","title":"A structural model for contextual code changes"},{"paperId":"e816f788767eec6a8ef0ea9eddd0e902435d4271","title":"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"db392858262b17aa9c8ff8659738f68fbf832ebe","title":"Structural Language Models of Code"},{"paperId":"295065d942abca0711300b2b4c39829551060578","title":"BERTScore: Evaluating Text Generation with BERT"},{"paperId":"1fa9ed2bea208511ae698a967875e943049f16b6","title":"HuggingFace's Transformers: State-of-the-art Natural Language Processing"},{"paperId":"97906df07855b029b7aae7c2a1c6c5e8df1d531c","title":"BERT Rediscovers the Classical NLP Pipeline"},{"paperId":"98d1307bed619b58b4a44acd8e65ac58495776c2","title":"code2seq: Generating Sequences from Structured Representations of Code"},{"paperId":"73bade9162bc4df6ca6c9ab8a7eb2d54d35acad2","title":"code2vec: learning distributed representations of code"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"960ba564e9e598d864dff38d2f3d0bad1b319ead","title":"A Survey of Machine Learning for Big Code and Naturalness"},{"paperId":"b4a26011eb7c26d71cd074f182c4f093da24308b","title":"Are deep neural networks the best choice for modeling source code?"},{"paperId":null,"title":"A syntactic neural model for general-purpose code generation"},{"paperId":"1e688be9f4554aa981fe3db9e2a66388b05bd167","title":"Code completion with statistical language models"},{"paperId":"110b7719f16eae8d82af0f56fca6e802c842c181","title":"Mining source code repositories at massive scale using language modeling"},{"paperId":"66b1837b047be9b76f0597550c01be4c10f51190","title":"On the naturalness of software"},{"paperId":"0cd18e4400ff75b2f8b58d60ddb9b0bc12f489e7","title":"METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"535a3d95a743e1f4b591b5b2af3e778a6347158a","summary":"This paper proposes CodeBERTScore, an automatic evaluation metric for code generation, which builds on BERTScore and achieves a higher correlation with human preference and with functional correctness than all existing metrics."},{"url":"https://www.semanticscholar.org/paper/86af60090d37ee820318910e1fea4f1784f82f41","title":"Measuring The Impact Of Programming Language Distribution","venue":"ArXiv","year":2023,"referenceCount":37,"citationCount":2,"influentialCitationCount":0,"publicationDate":"03/02/2023","authors":"Gabriel Orlanski,Kefan Xiao,Xavier García,Jeffrey Hui,Joshua Howland,J. Malmaud,Jacob Austin,Rishah Singh,Michele Catasta","citations":[{"paperId":"eb55442d940accc67d6790e66d3196e1941fc7a4","title":"Teaching Large Language Models to Self-Debug"},{"paperId":"2577d053f8aab912d29b424e1f09133d83740fd2","title":"Multi-lingual Evaluation of Code Generation Models"}],"references":[{"paperId":"ba184d335a9a08c52c5d25eabd7f4a8ea987918b","title":"UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining"},{"paperId":"1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a","title":"SantaCoder: don't reach for the stars!"},{"paperId":"269df328eec08b56b7b1f38a7555797fe2b999b6","title":"ReCode: Robustness Evaluation of Code Generation Models"},{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"2577d053f8aab912d29b424e1f09133d83740fd2","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"45263786d07f5751f7494fdeee3c8764836d02c4","title":"NatGen: generative pre-training by “naturalizing” source code"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"1ed66e048bb025e75aa5ea660545285212e5341f","title":"Scaling Up Models and Data with t5x and seqio"},{"paperId":"d6b7bc4e7968398101d8d9b4da7d4a0186763ff3","title":"MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"f40aeae3e522ada1f6a9f326841b01ef5c8657b6","title":"Unifying Language Learning Paradigms"},{"paperId":"1b4c19168410fb2690d285b205ab2281793db81a","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":null,"title":"Evaluating how finetuning on bimodal data effects code"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"a989f5daa7c438d6b8433efbb1461f1b0af1aa8e","title":"Break-It-Fix-It: Unsupervised Learning for Program Repair"},{"paperId":"f7664102a451332ed7e1286561b2f621eaff128d","title":"Programming Puzzles"},{"paperId":"cc9ad384ec0d0176ce05865d3866b44d2519bd68","title":"Reading StackOverflow Encourages Cheating: Adding Question Text Improves Extractive Code Generation"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"3a1efac4a8aa2f40a8128e034c01f18660652dd5","title":"DOBF: A Deobfuscation Pre-Training Objective for Programming Languages"},{"paperId":"1c6970dc9d4da9f5e94399e344fe8ba901d8fe81","title":"PyMT5: Multi-mode Translation of Natural Language and Python Code with Transformers"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"5e657bc8097c12649d027ca3c16ff7d37df1354d","title":"Balancing Training for Multilingual Neural Machine Translation"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"6fec3e579c7cd4f13bdabbee2b6ac2e8ff5941c6","title":"Unsupervised Cross-lingual Representation Learning at Scale"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"c17985a669522e7e85ae3d34754c7df49c7187d1","title":"Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"1a53e7446274016f737236bdd48e3ff05d966384","title":"Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow"},{"paperId":"54a13bcc9613dcaa76fb25fbe96572f376cfcca9","title":"Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"}],"id":"86af60090d37ee820318910e1fea4f1784f82f41","summary":"The BabelCode framework for execution-based evaluation of any benchmark in any language and a new code translation dataset called Translating Python Programming Puzzles (TP3) from the Python Programming puzzles (Schuster et al. 2021) benchmark that involves translating expert-level python functions to any language are presented."},{"url":"https://www.semanticscholar.org/paper/7677e9ead35f8f2578acbf5ad15fb330c83762c4","title":"Systematically Finding Security Vulnerabilities in Black-Box Code Generation Models","venue":"ArXiv","year":2023,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"Hossein Hajipour,Thorsten Holz,Lea Schonherr,Mario Fritz","citations":[],"references":[{"paperId":"71280dba5bda65c162f9deaffed7d3d20692ca0a","title":"SecurityEval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques"},{"paperId":"20fae749e3d469c331731ffa2f811079db792fdc","title":"A Simple, Yet Effective Approach to Finding Biases in Code Generation"},{"paperId":"5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"8a854331c593c6a766fa3b8037fb2ad1b95a6f06","title":"An Empirical Study of Code Smells in Transformer-based Code Generation Techniques"},{"paperId":"b8acc068f277fd491f12aa8984aa7a36dca01cb8","title":"Text Revealer: Private Text Reconstruction via Model Inversion Attacks against Transformers"},{"paperId":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code"},{"paperId":"1f3a97d255a6d0b2d5535af75737ed43b1e96fc4","title":"Is GitHub Copilot a Substitute for Human Pair-programming? An Empirical Study"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"38115e80d805fb0fb8f090dc88ced4b24be07878","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"},{"paperId":"0adec918885dff698acf359988ed79a543157f80","title":"Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity"},{"paperId":null,"title":"2022. GitHub Copilot is generally available to all developers. (June 2022)"},{"paperId":"1c8e15f15d67c5974445634bb971e2275e957aff","title":"Security Implications of Large Language Model Code Assistants: A User Study"},{"paperId":"ea06b5d1d383481fd7b203fe66cb6f4e2bc55e8f","title":"Variational Model Inversion Attacks"},{"paperId":"a5731122200fbb8b37f048010a1e1ca4474aa606","title":"Examining Zero-Shot Vulnerability Repair with Large Language Models"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"62d1a3137b01a69443bebf4d92c1990ec512a6a1","title":"Extracting Training Data from Large Language Models"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"5322e5936e4a46195b1a92001467a2350fe72782","title":"KART: Privacy Leakage Framework of Language Models Pre-trained with Clinical Records"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"0dd89a3b906e9a90a47834dcc766a0b9967d10d1","title":"Dreaming to Distill: Data-Free Knowledge Transfer via DeepInversion"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"ff4804de2b9db68a052d7113cae41ef2122a1c51","title":"AFL++ : Combining Incremental Steps of Fuzzing Research"},{"paperId":"e7fe886600399448f3282c8da8fd98ab7e50eae3","title":"Fast, Diverse and Accurate Image Captioning Guided by Part-Of-Speech"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"82247c9e74ddebb4dce65560ee69620579358f2d","title":"Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space"},{"paperId":"48895c95be782024554b249e60a71ddec4a4d663","title":"What developers want and need from program analysis: An empirical study"},{"paperId":"a500e615cc4315c5ab5bd7419dbdb36699d978b7","title":"SOK: (State of) The Art of War: Offensive Techniques in Binary Analysis"},{"paperId":"8e93557e60b8147d0b1c6bea8a633f1b9272019c","title":"Analyzing the State of Static Analysis: A Large-Scale Evaluation in Open Source Software"},{"paperId":"84f4af5b916c943fc4388ad33f43c3a7ea4327b9","title":"On the capability of static code analysis to detect security vulnerabilities"},{"paperId":"d1b9a3b11e6c9571a1553556f82b605b2b4baec3","title":"Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures"},{"paperId":"4d790c8fae40357d24813d085fa74a436847fb49","title":"Understanding deep image representations by inverting them"},{"paperId":"0ed643d2fd6dc186885f08f6ba90efed44c8e713","title":"Static Analysis: A Survey of Techniques and Tools"},{"paperId":"602e4c977664087e5629a40637a1b97a381d28c9","title":"SoK: Eternal War in Memory"},{"paperId":"d4072036b8d953ac54903b3d66188e1c3f956ea6","title":"Test-Driving Static Analysis Tools in Search of C Code Vulnerabilities"},{"paperId":"a10e525a5caebd4947eaaa7603ccb4d2b2ae74a9","title":"Using Static Analysis to Find Bugs"},{"paperId":"ab77bcedc269a0d1a2c4deb8f3e91701d8d51dda","title":"A Normalized Levenshtein Distance Metric"},{"paperId":null,"title":"ChatGPT: Optimizing Language Models for Dialogue"},{"paperId":null,"title":"Felipe Petroski Such"}],"id":"7677e9ead35f8f2578acbf5ad15fb330c83762c4","summary":"This work proposes a novel black-box inversion approach based on few-shot prompting that automatically and systematically finds 1000s of security vulnerabilities in various code generation models, including the commercial black- box model GitHub Copilot."},{"url":"https://www.semanticscholar.org/paper/a764d4f28612a7b5c4767ea6a2540b169fdb052c","title":"CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models","venue":"ArXiv","year":2023,"referenceCount":21,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"Hao Yu,Bo Shen,Dezhi Ran,Jiaxin Zhang,Qi Zhang,Yu Ma,Guangtai Liang,Ying Li,Tao Xie,Qianxiang Wang","citations":[{"paperId":"34d12432af63915caf14eab9a362f7e7d24e4c13","title":"Towards Generating Functionally Correct Code Edits from Natural Language Issue Descriptions"},{"paperId":"8475c1d1d98932111b194dc139c8f8b30818725d","title":"xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval"}],"references":[{"paperId":"618d17d60fcdd5da1fd1d1e2b7e19a47af9c9ba7","title":"What is it like to program with artificial intelligence?"},{"paperId":"1d160123cbbef972ea151a641dd435d57c727de8","title":"AixBench: A Code Generation Benchmark Dataset"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"78bd4518950e3f0bcd6aa9f7f8e09cbbf13eb11f","title":"PanGu-α: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"cf4aa38ae31b43fd07abe13b4ffdb265babb7be1","title":"The Curious Case of Neural Text Degeneration"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"fd6432fa2b032dd5f246d26460bf3353c43c257a","title":"Discrete Signal Processing on Graphs: Sampling Theory"},{"paperId":"ac862a623e52eacbc43dbb13c0253f0dfd471c26","title":"Sampling large data on graphs"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"3efcb97c1de1c87832a7a1d99e91801992a938ec","title":"Crafting Papers on Machine Learning"},{"paperId":null,"title":"Search-based pseudocode to code"}],"id":"a764d4f28612a7b5c4767ea6a2540b169fdb052c","summary":"A benchmark named CoderEval of pragmatic code generation with generative pre-trained models is proposed, compared with the widely-used HumanEval benchmark from OpenAI, which can be used to assess the performance of models against pragmatic codegeneration beyond just generating standalone functions."},{"url":"https://www.semanticscholar.org/paper/6248474933664013e5b0615dc474a7f6de5e97f4","title":"LExecutor: Learning-Guided Execution","venue":"ArXiv","year":2023,"referenceCount":92,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/02/2023","authors":"Beatriz Souza,Michael Pradel","citations":[],"references":[{"paperId":"8c13403ea4d6bea2a66359b4f83e4c058426df08","title":"DynaPyt: a dynamic analysis framework for Python"},{"paperId":"7ffb212356df9980347b3d3b9910dfba75a5d0c7","title":"Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code"},{"paperId":"2a26f75ef925818941ea6fe414e37bc57d113da8","title":"Nessie: Automatically Testing JavaScript APIs with Asynchronous Callbacks"},{"paperId":"c125b0be73c8493ebc27beb572f6c1b21d6b4ae4","title":"Static Prediction of Runtime Errors by Learning to Execute Programs with External Resource Descriptions"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"287e46bbc3d070fb82836cf706803c8a806b75e2","title":"Code Search based on Context-aware Code Translation"},{"paperId":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models"},{"paperId":"1e94ab82c1ea7f6f445b3ea4bb64db4bc0495182","title":"Nalin: learning from Runtime Behavior to Find Name-Value Inconsistencies in Jupyter Notebooks"},{"paperId":"d095f9ffcb5905bf0858ad1769d3d90e2e8737e2","title":"Jigsaw: Large Language Models meet Program Synthesis"},{"paperId":"47c740e858d3dfec0bf95104600851a8a2bec9ff","title":"VarCLR: Variable Semantic Representation Pre-training via Contrastive Learning"},{"paperId":"9426793c4eb8e582bae529e7b712b2959725482e","title":"Neural Program Repair with Execution-based Backpropagation"},{"paperId":"26f1c97c9e38ed0da329e3197ed554553b305d18","title":"Neural software analysis"},{"paperId":"92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models"},{"paperId":"ba919b8458b09e19c2b7cb9b98d362afe355b28a","title":"LooPy: interactive program synthesis with control structures"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"339a21c812db1303e3f12edeb6f245532c13bbca","title":"StateFormer: fine-grained type recovery from binaries using generative state modeling"},{"paperId":"07b3a1677ab6a0d662157a6469419aba66b02135","title":"On Multi-Modal Learning of Editing Source Code"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"a989f5daa7c438d6b8433efbb1461f1b0af1aa8e","title":"Break-It-Fix-It: Unsupervised Learning for Program Repair"},{"paperId":"4fba90f9e5f928a11afd504cbd5edfd3b13f223b","title":"IdBench: Evaluating Semantic Representations of Identifier Names in Source Code"},{"paperId":"ac4fa0172096f896a53a3bcaaa44f6fd67641534","title":"Restoring Execution Environments of Jupyter Notebooks"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"ceaff192479db6faee58ae88e053b0b319cf1893","title":"Retrieval-Augmented Generation for Code Summarization via Hybrid GNN"},{"paperId":"1311adebe439ccd9e373d594a8df1d720a685190","title":"Code Prediction by Feeding Trees to Transformers"},{"paperId":"59cc0871534a20ac98db0663539c6675ab63566f","title":"SequenceR: Sequence-to-Sequence Learning for End-to-End Program Repair"},{"paperId":"5f818ecbfce3bc44325a4f8ef2d744bc94006d6c","title":"Learning to Execute Programs with Instruction Pointer Attention Graph Neural Networks"},{"paperId":"11e8384b252a5ac5356969abdf2e1325f60e102a","title":"Automated Unit Test Generation for Python"},{"paperId":"7bdcbbee9a9f9f858ee458e24c6486f380dcf071","title":"Learning input tokens for effective fuzzing"},{"paperId":"e0d4587181a8848e73612e8a32b02bd9cc82b595","title":"CoCoNuT: combining context-aware neural translation models using ensemble for program repair"},{"paperId":"e24c70ebdcf7aba490fdbdc62bf6436ddc9721af","title":"DLFix: Context-based Code Transformation Learning for Automated Program Repair"},{"paperId":"bd47bb8cdd749a3356149da6155d2dcd7458779f","title":"Retrieval-based Neural Source Code Summarization"},{"paperId":"d6d76d2eeaf07bfa02554252026a2de9ef6166d5","title":"Blended, precise semantic program embeddings"},{"paperId":"805a6d1df9f460abfcea3d51d181cf1e80680be4","title":"A Transformer-based Approach for Source Code Summarization"},{"paperId":"08066c80919620397e8e4e5372ff84caf401e675","title":"Global Relational Models of Source Code"},{"paperId":"6acf0d7b24a93953269a4d52c6b2f0c7d11f9ae4","title":"Hoppity: Learning Graph Transformations to Detect and Fix Bugs in Programs"},{"paperId":"29fc7ff02e68537a86d32978bfd2e8e45258e77d","title":"LambdaNet: Probabilistic Type Inference using Graph Neural Networks"},{"paperId":"5d839ec37463be6998a46af8e6ca7a96c575b3ba","title":"SCELMo: Source Code Embeddings from Language Models"},{"paperId":"4ddb9135fc6fdf3f9e01fdfcd115fdbf6f7486b4","title":"Sequence Model Design for Code Completion in the Modern IDE"},{"paperId":"1e655fa69c62b430b051224153f701f1b607fd9c","title":"Typilus: neural type hints"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"a429c6c8407b10a89c9610ab5e9682ec89772356","title":"TypeWriter: neural type prediction with search-based validation"},{"paperId":"78f4de568564b6b5bb9058779ead6b3be8548e53","title":"Learning to Fix Build Errors with Graph2Diff Neural Networks"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"482a35a00cabbf480b814d56df6310693193a482","title":"Executability of Python Snippets in Stack Overflow"},{"paperId":"de53bfc6d758d5c686bc4a4fac1e7a58cca76d79","title":"Parser-directed fuzzing"},{"paperId":"8e71af3fbec666e67c0fcfefedb38881027a0254","title":"NL2Type: Inferring JavaScript Function Types from Natural Language Information"},{"paperId":"1432c8378b1cafa3f91f09fa743082d154fdab92","title":"A Novel Neural Source Code Representation Based on Abstract Syntax Tree"},{"paperId":"564bce85c8ad9a50f4652a4d05e1ed0aaa22df49","title":"Neural Program Repair by Jointly Learning to Localize and Repair"},{"paperId":"4c2d6e7037ba8c4118eb1d2fe25de910871314c9","title":"Wasabi: A Framework for Dynamically Analyzing WebAssembly"},{"paperId":"98d1307bed619b58b4a44acd8e65ac58495776c2","title":"code2seq: Generating Sequences from Structured Representations of Code"},{"paperId":"73bade9162bc4df6ca6c9ab8a7eb2d54d35acad2","title":"code2vec: learning distributed representations of code"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"99f88a55e46c3228a7d624ea9a959b9216a032ba","title":"Coverage-Based Greybox Fuzzing as Markov Chain"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"ea1b62a03ab16c4acca6f7041c6f096bedec244b","title":"Deep learning type inference"},{"paperId":"21f06378f1a9c29c07c208a94dc8990c7ad37b53","title":"Test generation for higher-order functions in dynamic languages"},{"paperId":"8b1aa4727eb2a83db1bd3ae54e078b0b7ce5eccb","title":"Retrieval on source code: a neural code search"},{"paperId":"eb563405e8c281b4756168deeb2d857dbb7af7aa","title":"Statistical Learning of API Fully Qualified Names in Code Snippets of Online Forums"},{"paperId":"e033a0b29af2939fd44e5765d03380c08897a9e8","title":"Deep Code Search"},{"paperId":"c70f11da5a8bb2df36def1d99c4c08df315e2233","title":"DeepBugs: a learning approach to name-based bug detection"},{"paperId":"89365ef9f61a62ea610f210083d14f70b8ee2972","title":"VulDeePecker: A Deep Learning-Based System for Vulnerability Detection"},{"paperId":"e92b7c6661d92dfd9da6d5e5136093538864aeef","title":"Inference of static semantics for incomplete C programs"},{"paperId":"d0f2d7236e43f129744e88130fb71f8f872d2b31","title":"Learning to Represent Programs with Graphs"},{"paperId":"f79b6150d51bbc79f459d25b6f0e7f697d46513a","title":"J-Force: Forced Execution on JavaScript"},{"paperId":"554c9b3aa8c3b7504810270b18b942a0d22156e7","title":"DeepFix: Fixing Common C Language Errors by Deep Learning"},{"paperId":"484ed558a5863060b373e8a2cb7cb302b8c36116","title":"A Convolutional Attention Network for Extreme Summarization of Source Code"},{"paperId":"49512270b39636375880d611d7b2192d324f4ba6","title":"Convolutional Neural Networks over Tree Structures for Programming Language Processing"},{"paperId":"cd7ee037c029b25e691650a26cec7538d255405c","title":"Suggesting accurate method and class names"},{"paperId":"889a6da567fc63dced1d145e0244964c1169fcb7","title":"Under-Constrained Symbolic Execution: Correctness Checking for Real Code"},{"paperId":"0d0b3ff23fbf1ec7a0dfebd923a3ef224306491a","title":"Learning to Execute"},{"paperId":"3a50e97cb2de47ca946b7002f33f3029b0fd0a4d","title":"Micro execution"},{"paperId":"72df5ef4cb7403a1ab5ab817f1ecbe16d83cd23b","title":"Jalangi: a selective record-replay and dynamic analysis framework for JavaScript"},{"paperId":"cfc10d808c906700da96d057e3f832c262fd2811","title":"Automated API Property Inference Techniques"},{"paperId":"02a65e07c55b646a4c660f0e2b001a62e89ccc05","title":"The ruby type checker"},{"paperId":null,"title":"American Fuzzy Lop (AFL). https://lcamtuf.coredump"},{"paperId":"216b98bb3d9221d5f5d261864975612e4d0faaa6","title":"EvoSuite: automatic test suite generation for object-oriented software"},{"paperId":"5a228775e3811c109a53ee77fea0a99c51899f6e","title":"Dynamic inference of static types for ruby"},{"paperId":"a95dc9eb9aa62c99ab5851bc211a05ddac71bdcc","title":"All You Ever Wanted to Know about Dynamic Taint Analysis and Forward Symbolic Execution (but Might Have Been Afraid to Ask)"},{"paperId":"e037dbd628f70b7d0324a27e70a45769e7cfddfd","title":"Automatic Generation of Object Usage Specifications from Large Method Traces"},{"paperId":"b187a252f54f4cc266cdbe5d91aa25b994176073","title":"KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs"},{"paperId":"d0ff72b60f72f133a4e45c337ed65d64c21af8ac","title":"Javert: fully automatic mining of general temporal properties from dynamic traces"},{"paperId":"ad866efbe615844b76b5df7372d62f0d6eb2ae58","title":"Enabling static analysis for partial java programs"},{"paperId":"66d00613ad789411c8a37a5392354c078d0d27b2","title":"Dytan: a generic dynamic taint analysis framework"},{"paperId":"6dc7017e357767dbc210bcb27080260fb0816701","title":"Feedback-Directed Random Test Generation"},{"paperId":"457f52e29cada51a07df6683f0811bf95575acfb","title":"Combining Static and Dynamic Reasoning for Bug Detection"},{"paperId":"98614fb574f1592b0dae3b45ca9f13506d14dd86","title":"Perracotta: mining temporal API rules from imperfect traces"},{"paperId":"398e6b26088cc62bf9bd8801146c3d6fa650b08a","title":"CUTE: a concolic unit testing engine for C"},{"paperId":"5e43bbc37d070fe854840e1700b9fde62b2b4a7e","title":"DART: directed automated random testing"},{"paperId":"208a35b667ac0b17333f593f9c68cc8d8603df1e","title":"JCrasher: an automatic robustness tester for Java"},{"paperId":"e14c3d78fac21c59639e54c7620e04464cbae6dc","title":"Static and dynamic analysis: synergy and duality"},{"paperId":"566eb7be43b8a2b2daff82b03711098a84859b2a","title":"of the Association for Computational Linguistics:"},{"paperId":"75d28729e96691eb85ae2b34e791473a24062ce5","title":"QuickCheck: a lightweight tool for random testing of Haskell programs"}],"id":"6248474933664013e5b0615dc474a7f6de5e97f4","summary":"LExecutor is presented, a learning-guided approach for executing arbitrary code snippets in an underconstrained way to let a neural model predict missing values that otherwise would cause the program to get stuck, and to inject these values into the execution."},{"url":"https://www.semanticscholar.org/paper/f0ea9e2d3889d37f34743ed1dc64f11e8e0484de","title":"Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models","venue":"ArXiv","year":2023,"referenceCount":43,"citationCount":2,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"David Noever,Forrest McKee","citations":[{"paperId":"4de290467d903b9977e31b3d4084006789bd6ebd","title":"One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era"},{"paperId":"248ee36169895ddcaf3963eb76fb24e1d8ef2f81","title":"Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models"}],"references":[{"paperId":"a8e0ba16346b72c3a04dd0b1da84bc5f28900174","title":"Using GitHub Copilot to Solve Simple Programming Problems"},{"paperId":"9f530ebf624bf58e91b2a1f20b0799a45ca48f9a","title":"An Analysis of the Automatic Bug Fixing Performance of ChatGPT"},{"paperId":"384e058c262e7f29b50cc4feeb0bd731f6e5377e","title":"Is ChatGPT A Good Translator? A Preliminary Study"},{"paperId":"cb29cf52f0f7d2e4324c68690a55b22890f2212d","title":"How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection"},{"paperId":"1f22de83d912176cb8857efa1c6d65b14d6a2f5c","title":"ChatGPT is not all you need. A State of the Art Review of large Generative AI models"},{"paperId":"c0f3072aeab373fd64c98126afe1b3964ed3438d","title":"Chatbots in a Honeypot World"},{"paperId":"270b015093073d3ba254928b6d736a59870d3fb1","title":"Chatbots As Fluent Polyglots: Revisiting Breakthrough Code Snippets"},{"paperId":"2db2ccb54f68ab20edc5ae94eceb1cc16359e142","title":"OpenAI ChatGPT Generated Results: Similarity Index of Artificial Intelligence-Based Contents"},{"paperId":null,"title":"Building Natural Language Generation and Understanding Systems in Data Constrained Settings (Doctoral dissertation"},{"paperId":"7d4867e28b02059eef4cb25bfcd304b2071b30a9","title":"How Well Does ChatGPT Do When Taking the Medical Licensing Exams? The Implications of Large Language Models for Medical Education and Knowledge Assessment"},{"paperId":"fa50f2bc03d6d53fe50f37a1978107b13af24ea7","title":"SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization"},{"paperId":"909c46441282736a0acb3df21b7c13a4e3da70ab","title":"Emergent Analogical Reasoning in Large Language Models"},{"paperId":"6756fcd998caeb7b23702e08559e63710179334c","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"6f2123d040f45eaeafda392614f6faa9e0ab6b68","title":"MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering"},{"paperId":"3b8ccc7ec80b8775de603e248ac1ca2b919d6b70","title":"Chatbots in a Botnet World"},{"paperId":"a8afd12bcd51488ba69bd838ef6dbf2728d5121a","title":"Prompting Is Programming: A Query Language For Large Language Models"},{"paperId":"66c52c6ef45cdd77c086996bcaaf01470e82dbd2","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?"},{"paperId":"9afab8dc694269d205d769eaea549d8f7558d776","title":"Exploring the Verifiability of Code Generated by GitHub Copilot"},{"paperId":"d6954c43aa1ca197319c45d3988bc8fcec3de976","title":"GitHub Copilot AI pair programmer: Asset or Liability?"},{"paperId":"401c2aaf392cfea1b8314f2c2124d596adfc5d39","title":"Boston House Price Prediction Using Regression Models"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","title":"Emergent Abilities of Large Language Models"},{"paperId":"cdfe9580f63070f311151444f9df32818cc858bf","title":"An Empirical Evaluation of GitHub Copilot's Code Suggestions"},{"paperId":"1f3a97d255a6d0b2d5535af75737ed43b1e96fc4","title":"Is GitHub Copilot a Substitute for Human Pair-programming? An Empirical Study"},{"paperId":"146e9e1238ff6caf18f0bd936ffcfbe1e65d2afd","title":"Data Distributional Properties Drive Emergent In-Context Learning in Transformers"},{"paperId":"166b64f2ae8e52f5779682fab756cbd617a6e74b","title":"A neural network solves, explains, and generates university math problems by program synthesis and few-shot learning at human level"},{"paperId":"8417d83e74a63b9d3874cd7695477ebad8b98f8a","title":"Choose your programming copilot: a comparison of the program synthesis performance of github copilot and genetic programming"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"},{"paperId":"7382e3383130298910b2d7c2fe2109e0cd13832e","title":"Investigating Math Word Problems using Pretrained Multilingual Language Models"},{"paperId":"535c8840cc00d83e3448bffa0f0dd8d05da1bc79","title":"OpenAI ChatGPT Generated Literature Review: Digital Twin in Healthcare"},{"paperId":"c8e727c4e2bbdbdfbc77610215e8c9e9b09ce63b","title":"Transformer-Encoder and Decoder Models for Questions on Math"},{"paperId":"a5808ccc50f77083bd3be926fb2af05cf34563ff","title":"Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition"},{"paperId":"15789db234a9338e57aff9709868dcbd290727d3","title":"Continual Pre-training of Language Models for Math Problem Understanding with Syntax-Aware Memory Network"},{"paperId":"24847979c4a68c347d6e4405ba31325c8b41f5d8","title":"Math Word Problem Generation with Multilingual Language Models"},{"paperId":null,"title":"Breaking ChatGPT with Dangerous Questions Understanding how ChatGPT Prioritizes Safety, Context, and Obedience"},{"paperId":"6e0cd34973f8f61e2d1dce5268722ac9417706ad","title":"A Transformer-based Math Language Model for Handwritten Math Expression Recognition"},{"paperId":"58fc4c8d3abaacff8eb387ebde2e1ba37a2d558f","title":"A survey of iris datasets"},{"paperId":"c8d22fd8e8e094d624a484d770a60873baaba5f4","title":"GPT-3: What’s it good for?"},{"paperId":"8461dde4f8644517259067de63d9c8b5e589fc70","title":"MWP-BERT: A Strong Baseline for Math Word Problems"},{"paperId":"763a2fb25d487c8a33ecc20ddeb8a70f90f4ccbc","title":"Data Mining and Exploration: A Comparison Study among Data Mining Techniques on Iris Data Set"},{"paperId":"b3774875c9105f3460c3e6c19b1f0b532af6f5e7","title":"Verbal Working Memory as Emergent from Language Comprehension and Production"},{"paperId":"c2ec0cbc5e41ae4b88bfd7bac43f39f0750b6a23","title":"Exploratory Data Analysis and Machine Learning on Titanic Disaster Dataset"},{"paperId":null,"title":"How to generate pseudo-random datasets in Python: start from scratch with Numpy & Faker"},{"paperId":"5fcac6fc943780b5ff6fa50fa77c43a28188343e","title":"Virtual assistants and self-driving cars"}],"id":"f0ea9e2d3889d37f34743ed1dc64f11e8e0484de","summary":"The work examines whether next-token prediction succeeds from sentence completion into the realm of actual numerical understanding and showcases the model's capabilities to group by or pivot categorical sums, infer feature importance, derive correlations, and predict unseen test cases using linear regression."},{"url":"https://www.semanticscholar.org/paper/782f3d43b37790a83c98d5fd3ef142b296f20616","title":"CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code Models","venue":"ArXiv","year":2023,"referenceCount":76,"citationCount":1,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"Changan Niu,Chuanyi Li,Vincent Ng,Bin Luo","citations":[{"paperId":"fbb5cc9e8a46f61d1543bd17b6eca324fd5bf55c","title":"Fully Autonomous Programming with Large Language Models"}],"references":[{"paperId":"660fde2f51e025638b8c937bf228ecaa5c5b649c","title":"XLCoST: A Benchmark Dataset for Cross-lingual Code Intelligence"},{"paperId":"2417ab25a53e97410f44a20af69b82fff077fd53","title":"Deep Learning Meets Software Engineering: A Survey on Pre-Trained Models of Source Code"},{"paperId":"ec229b004ebaee27b9efa84b834825dbc2937538","title":"Is GitHub's Copilot as Bad As Humans at Introducing Vulnerabilities in Code?"},{"paperId":"4b27f18bff43d605805c92696a979714ced0b805","title":"UniXcoder: Unified Cross-Modal Pre-training for Code Representation"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"5cbe278b65a81602a864184bbca37de91448a5f5","title":"Competition-level code generation with AlphaCode"},{"paperId":"9b1f4492a663c7f56f2b43ae1ed167d3857aacca","title":"PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts"},{"paperId":"063e410c2c52ccbaa22c62130ac2c58969bb3efa","title":"SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations"},{"paperId":"73569460b023f9ac1fe5a1876c3401460d2fc15d","title":"Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding"},{"paperId":"10bd4160b44803ada6a3d2e366c44b7e2a4ffe90","title":"An Explanation of In-context Learning as Implicit Bayesian Inference"},{"paperId":"17dd3555fd1ccf1141cf984347fa1b3fd6b009ca","title":"Multitask Prompted Training Enables Zero-Shot Task Generalization"},{"paperId":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions"},{"paperId":"c65dcc599f4539295c83b8d12f7bbf9b361e4697","title":"Making the Most of Small Software Engineering Datasets With Modern Machine Learning"},{"paperId":"cbdb45fc16b0885905b91d84281c310e6cb49e9c","title":"Cross-Task Generalization via Natural Language Crowdsourcing Instructions"},{"paperId":"ec64e324ce1210fe5245dfd0fb5a92058732e5b9","title":"Benchmarking Generalization via In-Context Instructions on 1, 600+ Language Tasks"},{"paperId":null,"title":"Patch generation with language models: Feasibility and scaling behavior"},{"paperId":"a5731122200fbb8b37f048010a1e1ca4474aa606","title":"Examining Zero-Shot Vulnerability Repair with Large Language Models"},{"paperId":"1444536496d8064f33e10b38b5820fecfab5b367","title":"Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs"},{"paperId":"21e8e76386aaaa00e0971af70ce84a8a544e1aa1","title":"Cascaded Fast and Slow Models for Efficient Semantic Code Search"},{"paperId":"42fc019b2668c9d9d984154d4c57f6c6d5a91619","title":"Language Models are Few-shot Multilingual Learners"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"70087677fd1a6309829b42968934575d05a95f92","title":"What do pre-trained code models know about code?"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"7fa273f450251523e6b7fcc2eb3fdbdfd4a30493","title":"CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP"},{"paperId":"5bfb0cc16b871c75e32a6a9d54dd7db225260e04","title":"CodeTrans: Towards Cracking the Language of Silicone's Code Through Self-Supervised Deep Learning and High Performance Computing"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"1906c757436f4829ac29a2b66921d92bfe6fcc91","title":"Why My Code Summarization Model Does Not Work"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"c2c7233293d55f201fe5b496234bed1914eea70e","title":"Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks"},{"paperId":"789b5441743c2e38cf4c38749ed820c0671d81b1","title":"Muppet: Massive Multi-task Representations with Pre-Finetuning"},{"paperId":"85e7d63f75c0916bd350a229e040c5fbb1472e7a","title":"Making Pre-trained Language Models Better Few-shot Learners"},{"paperId":"2b2711b23dd2503933b2f02a41574fc72d21aabf","title":"Empirical study of transformers for source code"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":null,"title":"Codenet: A largescale ai for code dataset for learning a diversity of coding tasks"},{"paperId":null,"title":"Javabert: Training a transformerbased model for the java programming language"},{"paperId":"016ca039d9f5220c96b26f15d90d82064c361bfa","title":"Learning from Task Descriptions"},{"paperId":"ecb2b0859bab2761be397804516b8de3983366e8","title":"The Turking Test: Can Language Models Understand Instructions?"},{"paperId":"5d639fa3df4f3a7bacd24f2651daf0b97d2b75a3","title":"Language Models as Few-Shot Learner for Task-Oriented Dialogue Systems"},{"paperId":"069a05c0ae7ac49ad7eb8e0a0744c212c58bd863","title":"Meta-learning for Few-shot Natural Language Processing: A Survey"},{"paperId":"dd1ef7e7dc6ab885d9d64218148f08354c3c6fdb","title":"CPC: Automatically Classifying and Propagating Natural Language Comments via Program Analysis"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"207da6d2c07289bf72a2b5974bb3f011ebb5dd0d","title":"Adversarial NLI: A New Benchmark for Natural Language Understanding"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"4095018e41da90f623af8be7c6f56f597b9cc136","title":"Few-Shot NLG with Pre-Trained Language Model"},{"paperId":"712f4f21b9d3e6a7f110a2ecd9b3a2f900397b9f","title":"Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples"},{"paperId":"b19729b27a1b4c24b52f87308c907653300afa7f","title":"Dota 2 with Large Scale Deep Reinforcement Learning"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"44834accf04f5c029f79e7a21f6715b74872ac94","title":"Investigating Meta-Learning Algorithms for Low-Resource Natural Language Understanding Tasks"},{"paperId":"8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad","title":"HellaSwag: Can a Machine Really Finish Your Sentence?"},{"paperId":"c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3","title":"Generalizing from a Few Examples: A Survey on Few-Shot Learning"},{"paperId":"04e65ba8493f3f01c65eda8846363ceac5edbc23","title":"Learning How to Mutate Source Code from Bug-Fixes"},{"paperId":"516497e7fc39ebb126bed28dcc5c6129e500acb8","title":"An Empirical Study on Learning Bug-Fixing Patches in the Wild via Neural Machine Translation"},{"paperId":"8ace1951c95f9bbbcf83571ff6c0579521507e2e","title":"The adverse effects of code duplication in machine learning models of code"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"b47381e04739ea3f392ba6c8faaf64105493c196","title":"Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks"},{"paperId":"a75869d69cc86f501939c237ae4711aa2885f6a6","title":"Meta-Learning for Low-Resource Neural Machine Translation"},{"paperId":"1b868f044146a91dec16a579ffb0802b431e97a3","title":"Diverse Few-Shot Text Classification with Multiple Metrics"},{"paperId":null,"title":"Alibaba ai model tops humans in reading comprehension"},{"paperId":"e38e4cde8a32df4047174054157b43d9023e44b8","title":"Detecting missing information in bug descriptions"},{"paperId":"f010affab57b5fcf1cd6be23df79d8ec98c7289c","title":"TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension"},{"paperId":"636a79420d838eabe4af7fb25d6437de45ab64e8","title":"RACE: Large-scale ReAding Comprehension Dataset From Examinations"},{"paperId":"77fb0b7aef619dfac650423d4677170df2158e0d","title":"The LAMBADA dataset: Word prediction requiring a broad discourse context"},{"paperId":"be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6","title":"Matching Networks for One Shot Learning"},{"paperId":"846aedd869a00c09b40f1f1f35673cb22bc87490","title":"Mastering the game of Go with deep neural networks and tree search"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"9277dc7dd5b348645e8fcf4a987e5b3fc9132c7c","title":"Towards a Big Data Curated Benchmark of Inter-project Code Clones"},{"paperId":"97cedf99252026f58e8154bc61d49cf885d42030","title":"Edinburgh’s Phrase-based Machine Translation Systems for WMT-14"},{"paperId":"9f2efadf66817f1b38f58b3f50c7c8f34c69d89a","title":"DeepFace: Closing the Gap to Human-Level Performance in Face Verification"},{"paperId":"ddb842f086b86003f45cd4cc91ca9b4ba3fe0a6c","title":"Qualitative data analysis"},{"paperId":"e45dcfc0a65096bdc5b19d00e4243df089b19579","title":"Understanding interobserver agreement: the kappa statistic."},{"paperId":"6ca755a0e87da3c25ac27e92b73701fe435a12d9","title":"Manual and automatic evaluation of summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"9e463eefadbcd336c69270a299666e4104d50159","title":"A Coefficient of Agreement for Nominal Scales"}],"id":"782f3d43b37790a83c98d5fd3ef142b296f20616","summary":"A large-scale benchmark that includes 216 existing code-related tasks is proposed and it is demonstrated that the cross-task generalization of models can be largely improved by in-context learning methods such as few-shot learning and learning from task instructions, which shows the promising prospects of conducting cross- task learning research on this benchmark."},{"url":"https://www.semanticscholar.org/paper/1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a","title":"SantaCoder: don't reach for the stars!","venue":"ArXiv","year":2023,"referenceCount":44,"citationCount":4,"influentialCitationCount":0,"publicationDate":"09/01/2023","authors":"Loubna Ben Allal,Raymond Li,Denis Kocetkov,Chenghao Mou,Christopher Akiki,Carlos Muñoz Ferrandis,Niklas Muennighoff,Mayank Mishra,A. Gu,Manan Dey,Logesh Kumar Umapathi,Carolyn Jane Anderson,Yangtian Zi,J. Poirier,Hailey Schoelkopf,S. Troshin,Dmitry Abulkhanov,M. Romero,M. Lappert,F. Toni,Bernardo Garc'ia del R'io,Qian Liu,Shamik Bose,Urvashi Bhattacharyya,Terry Yue Zhuo,I. Yu,Paulo Villegas,M. Zocca,Sourab Mangrulkar,D. Lansky,Huu Nguyen,Danish Contractor,Luisa Villa,Jia Li,Dzmitry Bahdanau,Yacine Jernite,S. Hughes,Daniel Fried,Arjun Guha,Harm de Vries,Leandro von Werra","citations":[{"paperId":"05ae2f22e150e47ff8030aa3024158a28c98d51d","title":"Do Machine Learning Models Produce TypeScript Types that Type Check?"},{"paperId":"27c2ddc921befc44f7b2cdb8bb39facdd8b0b188","title":"Measuring The Impact Of Programming Language Distribution"},{"paperId":"6436ff048987866680254827461dfeacca4621d4","title":"Exploring AI Ethics of ChatGPT: A Diagnostic Analysis"},{"paperId":"88a74e972898de887ad9587d4c87c3a9f03f1dc5","title":"MTEB: Massive Text Embedding Benchmark"}],"references":[{"paperId":"f3a6115e5fb2237df938976e005468f0b18da797","title":"The Stack: 3 TB of permissively licensed source code"},{"paperId":"8a4fc5f00cd4aca61e148e46a2125c3a406719f1","title":"DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation"},{"paperId":"eca35805d185374befe4da48c9f96ace6e962fad","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"2577d053f8aab912d29b424e1f09133d83740fd2","title":"Multi-lingual Evaluation of Code Generation Models"},{"paperId":"ec4c8d99eb1c028c43af6d8bbf727392d351cb59","title":"Efficient Training of Language Models to Fill in the Middle"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"5288b9f3a9f575543f44c39e1d3b78b3ca4c99da","title":"InCoder: A Generative Model for Code Infilling and Synthesis"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"b32a6f6ef7dd775e0f876b4713ceccebc56e651e","title":"A systematic evaluation of large language models of code"},{"paperId":"d407aa1cc3f9ce8478d3052344947fb49baa04d4","title":"CodeFill: Multi-token Code Completion by Jointly learning from Structure and Naming Sequences"},{"paperId":"775a9c722262c7b656876a5fef20f4577afd8981","title":"Multilingual training for Software Engineering"},{"paperId":"771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis"},{"paperId":"1b4c19168410fb2690d285b205ab2281793db81a","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages"},{"paperId":null,"title":"huggingface/tokenizers: Rust 0.13.2"},{"paperId":null,"title":"A framework for the evaluation of code generation models. https://github.com/bigcode-project/ bigcode-evaluation-harness"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"a38e0f993e4805ba8a9beae4c275c91ffcec01df","title":"Program Synthesis with Large Language Models"},{"paperId":"1ccd031f28dccfb226f6c0c588c93a97a50bf95f","title":"Measuring Coding Challenge Competence With APPS"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"3a1efac4a8aa2f40a8128e034c01f18660652dd5","title":"DOBF: A Deobfuscation Pre-Training Objective for Programming Languages"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"0d4b5c9a071557f4eb12f63f785dbc89071d4272","title":"How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models"},{"paperId":"db1afe3b3cd4cd90e41fbba65d3075dd5aebb61e","title":"The Pile: An 800GB Dataset of Diverse Text for Language Modeling"},{"paperId":null,"title":"2021), which is a benchmark constructed from CodeSearchNet Husain et al. (2019)"},{"paperId":null,"title":"A framework for few-shot language model evaluation"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"15a60130ea9cb4a00e0dac6d11a368894974cedf","title":"Unit Test Case Generation with Transformers and Focal Context"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"ad3222db224444e0998bcc9aa66341bfe5d81c51","title":"Learning and Evaluating Contextual Embedding of Source Code"},{"paperId":"a429c6c8407b10a89c9610ab5e9682ec89772356","title":"TypeWriter: neural type prediction with search-based validation"},{"paperId":"dc52b09089704ebd6f471177474bc29741c50023","title":"Fast Transformer Decoding: One Write-Head is All You Need"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"ea1b62a03ab16c4acca6f7041c6f096bedec244b","title":"Deep learning type inference"},{"paperId":"8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"116a877c774969b53399b1ccaa34d51bfb492ee4","title":"Seq2SQL: Generating Structured Queries from Natural Language using Reinforcement Learning"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"cf6023368a683572a090203406a7e4285566e9db","title":"Identifying and Filtering Near-Duplicate Documents"},{"paperId":null,"title":"We replace detected emails with"},{"paperId":null,"title":"Data pre-filtering This is the regular expression we used to pre-filter the annotation dataset for data containing emails"},{"paperId":null,"title":"For IP addresses, we used the same regular expression as the one used for PII detection"},{"paperId":null,"title":"Natural language processing with transformers"}],"id":"1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a","summary":null},{"url":"https://www.semanticscholar.org/paper/0b077c9577f4297dcf3da835e253d21965bbc6e0","title":"CoTexT: Multi-task Learning with Code-Text Transformer","venue":"NLP4PROG","year":2021,"referenceCount":25,"citationCount":60,"influentialCitationCount":11,"publicationDate":"18/05/2021","authors":"Long Phan,Hieu Tran,Daniel Le,H. Nguyen,J. Anibal,Alec Peltekian,Yanfang Ye","citations":[{"paperId":"3b01b5b497e1f359b11da45af029281ee6f64c2c","title":"Towards Enhancing In-Context Learning for Code Generation"},{"paperId":"f24dabf3317abaa3d7393ab7d48115f353749bde","title":"CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X"},{"paperId":"645960e8bcb2a55bc7e8816c49e60e8f98303acb","title":"Combining Contexts from Multiple Sources for Documentation-Specific Code Example Generation"},{"paperId":"443d898928eb8e32d2e6f8f287beaa63f5b00eb9","title":"JaCoText: A Pretrained Model for Java Code-Text Generation"},{"paperId":"2eb8342c2d16f28c823e0b747fa71a00f1218ed1","title":"Knowledge Transfer for Pseudo-code Generation from Low Resource Programming Language"},{"paperId":"aae61ba5b629eba965b7f49a685b3d9f1bfb358c","title":"Model-Agnostic Syntactical Information for Pre-Trained Programming Language Models"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"0fc64af26a442735c704ae094107fc6b090811f8","title":"Machine/Deep Learning for Software Engineering: A Systematic Literature Review"},{"paperId":"4fbf624b9a848425c81f21f1171c32426541330f","title":"On the Reliability and Explainability of Automated Code Generation Approaches"},{"paperId":"44f0eb7be111747f54a8669f74be0bb0dad96cff","title":"An Empirical Comparison of Pre-Trained Models of Source Code"},{"paperId":"44d53aa6df04dc99c441972cfd14b11ba12b1e2c","title":"End-to-End Transformer-Based Models in Textual-Based NLP"},{"paperId":"2aa32aa6dd4ad3798941c98762167999e5511eac","title":"CSGVD: A deep learning approach combining sequence and graph embedding for source code vulnerability detection"},{"paperId":"0bc9cedda48551847cc741b74c1fc299c5a9eed2","title":"Who Evaluates the Evaluators? On Automatic Metrics for Assessing AI-based Offensive Code Generators"},{"paperId":"bae76e1d13abe54f66dc140be53538b864578ba8","title":"A Survey on Pretrained Language Models for Neural Code Intelligence"},{"paperId":"9e3b52669d81dbf0d638a5f5d1d537c7087195d6","title":"When Neural Model Meets NL2Code: A Survey"},{"paperId":"257b7c33ee766dc1c1dc6ea155d1be6ab145403b","title":"An Empirical Study of Deep Learning Models for Vulnerability Detection"},{"paperId":"26335d23e56ac1ca14ee15c0da9f55eee744d9d6","title":"CPGBERT: An Effective Model for Defect Detection by Learning Program Semantics via Code Property Graph"},{"paperId":"5fd0533ba1068af1cb075d4eb71ee551691a71ac","title":"ExploitGen: Template-augmented exploit code generation based on CodeBERT"},{"paperId":"46d0a832fada6147bceb0bd4e39928e482733246","title":"How Important are Good Method Names in Neural Code Generation? A Model Robustness Perspective"},{"paperId":"20b60fb3993d2e9a5af04611f7bdf248e5a3a736","title":"Programming by Example and Text-to-Code Translation for Conversational Code Generation"},{"paperId":"8c618450ec7b52a4f7437d443d2a3b10c2342ef4","title":"Lighting up supervised learning in user review-based code localization: dataset and benchmark"},{"paperId":"8ca981a132ffb21099c8dd1c6bf2e0a5d1babb40","title":"Code Vulnerability Detection Based on Deep Sequence and Graph Models: A Survey"},{"paperId":"1e2a5dca6f310241dd8a9b56873edf8ca211c781","title":"Enriching Biomedical Knowledge for Low-resource Language Through Translation"},{"paperId":"54772ffae642a87b9a6122a6f1bae76b926a7230","title":"Enriching Biomedical Knowledge for Low-resource Language Through Large-Scale Translation"},{"paperId":"a807638436aadf2d56dd920f3a8de339dce44096","title":"COMBO: Pre-Training Representations of Binary Code Using Contrastive Learning"},{"paperId":"a7435722d8ab595da5a9c70ac9160f57d0dcd75a","title":"Enabling Transformers to Understand Low-Level Programs"},{"paperId":"7ffd5a29349962c6a49a3df2ba6e7b20788669bf","title":"Semantic-Preserving Adversarial Code Comprehension"},{"paperId":"88dd119dba5ee747851ade8f5d517b381614d918","title":"Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence"},{"paperId":"9360390b02b9a09ece9a2486055b17e18dc5d3f6","title":"Automatic Code Documentation Generation Using GPT-3"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"e9fc39f56abbc6b8aed1e05496d985e70345a95a","title":"An extensive study on pre-trained models for program understanding and generation"},{"paperId":"45263786d07f5751f7494fdeee3c8764836d02c4","title":"NatGen: generative pre-training by “naturalizing” source code"},{"paperId":"65ef841916d4f1cdd624a8454cb5c50fad6e14c9","title":"An Extractive-and-Abstractive Framework for Source Code Summarization"},{"paperId":"9d6e411065cd65b4291955c47b3f255ae668b81a","title":"StructCoder: Structure-Aware Transformer for Code Generation"},{"paperId":"05a4edf3a6c2439c83f23ffa9afe463ae26780d4","title":"Learning code summarization from a small and local dataset"},{"paperId":"5514b87e34db2b34bd9a9b995894243f91435efc","title":"Learning to Represent Programs with Code Hierarchies"},{"paperId":"1d5905b0c4a558604d562eee0b522bde63348c9f","title":"VulBERTa: Simplified Source Code Pre-Training for Vulnerability Detection"},{"paperId":"2417ab25a53e97410f44a20af69b82fff077fd53","title":"Deep Learning Meets Software Engineering: A Survey on Pre-Trained Models of Source Code"},{"paperId":"0d6e733e68c6eea2f0def4aab03181e9fb8f3b09","title":"ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language Generation"},{"paperId":"0121c151c96b32cd7851e4bcda2a468b279c1e6f","title":"CODE-MVP: Learning to Represent Source Code from Multiple Views with Contrastive Pre-Training"},{"paperId":"db783c480faf87b38e8806d4ef455dfde6e335aa","title":"Towards JavaScript program repair with Generative Pre-trained Transformer (GPT-2)"},{"paperId":"012378718c34f0b17b3fcd7316371f8f4e4fdde2","title":"Addressing Leakage in Self-Supervised Contextualized Code Retrieval"},{"paperId":"89e78d6f76b70c30804ecd3592fa05fccdc49b64","title":"Fix Bugs with Transformer through a Neural-Symbolic Edit Grammar"},{"paperId":"608df5bd7a0ad0f4d335ae4d071b8cfe60e2f3c5","title":"On the Effectiveness of Pretrained Models for API Learning"},{"paperId":"3ee317bfa4ba84f0df112ef66b7d890d8b445ada","title":"DualSC: Automatic Generation and Summarization of Shellcode via Transformer and Dual Learning"},{"paperId":"7b46b9da287429d19a00ca3f9219c1020f7c9df8","title":"A Survey on Artificial Intelligence for Source Code: A Dialogue Systems Perspective"},{"paperId":"532f32be1e918d6b75650947318e57fc8f4fb415","title":"CodeRetriever: Unimodal and Bimodal Contrastive Learning"},{"paperId":"9fd854f8315bdb02449500088d3cb8e6e84a0d09","title":"Cross-Domain Deep Code Search with Few-Shot Meta Learning"},{"paperId":"b2420b683fcc0a8b642394d36832b8f05ea049a3","title":"Cross-Domain Deep Code Search with Meta Learning"},{"paperId":"775a9c722262c7b656876a5fef20f4577afd8981","title":"Multilingual training for Software Engineering"},{"paperId":"467306672c89d4a0a7c6bc733814605c53bbfa97","title":"Towards Learning (Dis)-Similarity of Source Code from Program Contrasts"},{"paperId":"6f84d0cc33c7c58f74b28ddcc1cbda91ea608c9f","title":"C ODE S UMMARIZATION : D O T RANSFORMERS R E ALLY U C"},{"paperId":"cc5842ebb9ed13983b770cff6b99a1da27d91445","title":"Enriching Biomedical Knowledge for Vietnamese Low-resource Language Through Large-Scale Translation"},{"paperId":"74f57a9ffa73e1bee99300b177904c06199840aa","title":"CodeRetriever: A Large Scale Contrastive Pre-Training Method for Code Search"},{"paperId":"d388b3cbc820975da21c57c9fb9cef2d9c6d08f0","title":"CodeRetriever: Large-scale Contrastive Pre-training for Code Search"},{"paperId":"3b0cf543a730e674d4213d344ebc857fada76ead","title":"Understanding High-Level Properties of Low-Level Programs Through Transformers"},{"paperId":"219be4772aab5f13a172c100bc7f2441ba9192ad","title":"Modeling Hierarchical Syntax Structure with Triplet Position for Source Code Summarization"},{"paperId":"d4f16dae4ab1d21db3c0b0bd7b54f4b62e2d1b85","title":"The Effectiveness of Transformer Models for Analyzing Low-Level Programs"},{"paperId":"cb7c42c3f1335db36ac321c9a5830b97d46d5560","title":"Multilingual training for So ware Engineering"},{"paperId":"6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing"}],"references":[{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"a5b1d1cab073cb746a990b37d42dc7b67763f881","title":"TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"395de0bd3837fdf4b4b5e5f04835bcc69c279481","title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"00549af4bc3270e0f688acbf694f912d7ee39cad","title":"Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"a5ed9dfc0725bffb6428a2cc297a15265377906c","title":"Hello, It’s GPT-2 - How Can I Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems"},{"paperId":"e0c6abdbdecf04ffac65c440da77fb9d66bb474c","title":"XLNet: Generalized Autoregressive Pretraining for Language Understanding"},{"paperId":"87078d95bee341a1767034d9432fb34937ecf65a","title":"SciBERT: Pretrained Contextualized Embeddings for Scientific Text"},{"paperId":"156d217b0a911af97fa1b5a71dc909ccef7a8028","title":"SciBERT: A Pretrained Language Model for Scientific Text"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"3cac56572497ba51c71da66bd207e7a48b2c758f","title":"Mapping Language to Code in Programmatic Context"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"443516aeb2819d4d362ffe7d5418a54e5427a016","title":"ORANGE: a Method for Evaluating Automatic Evaluation Metrics for Machine Translation"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"ce9a21b93ba29d4145a8ef6bf401e77f261848de","title":"A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"}],"id":"0b077c9577f4297dcf3da835e253d21965bbc6e0","summary":"CoTexT is a pre-trained, transformer-based encoder-decoder model that learns the representative context between natural language (NL) and programming language (PL) using self-supervision to learn a general understanding of language and code."},{"url":"https://www.semanticscholar.org/paper/636f854b1a3a983e6803eae0277179596cc2cb95","title":"Summarize and Generate to Back-translate: Unsupervised Translation of Programming Languages","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":4,"influentialCitationCount":0,"publicationDate":"23/05/2022","authors":"Wasi Uddin Ahmad,Saikat Chakraborty,Baishakhi Ray,Kai-Wei Chang","citations":[{"paperId":"715e952081d0f47632f51bca968733a5f2e9adad","title":"MUFIN: Improving Neural Repair Models with Back-Translation"},{"paperId":"9ca243250b5ba4fa77bcda263db94f9c33d9599f","title":"On ML-Based Program Translation: Perils and Promises"},{"paperId":"fb243dfd1234b8f76dfda740a62402663da74085","title":"Exploring Data Augmentation for Code Generation Tasks"},{"paperId":"d6d9c368aae753c886c0beef888ebff6f3d0dca0","title":"Unsupervised Translation of Programming Language - A Survey Paper"}],"references":[{"paperId":"467306672c89d4a0a7c6bc733814605c53bbfa97","title":"Towards Learning (Dis)-Similarity of Source Code from Program Contrasts"},{"paperId":null,"title":"Spt-code: Sequence-to-sequence pre-training for learning the representation of source code"},{"paperId":"a30f912f8c5e2a2bfb06351d4578e1ba3fa37896","title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation"},{"paperId":"c2b98ea55333895f736b9267414b4c9b63b9d04b","title":"AVATAR: A Parallel Corpus for Java-Python Program Translation"},{"paperId":"c56aced0f0c5cfebefadb530cb08d736c3ac5c05","title":"Retrieval Augmented Code Generation and Summarization"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"7547680408358916e66917d03436fca7540a7528","title":"Project CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks"},{"paperId":"0646bb09db4d1ba24150e69b71edcd4aff691b3c","title":"Unified Pre-training for Program Understanding and Generation"},{"paperId":"3a1efac4a8aa2f40a8128e034c01f18660652dd5","title":"DOBF: A Deobfuscation Pre-Training Objective for Programming Languages"},{"paperId":"69a72ff5b30642d11c96635e99aadad3140d33a7","title":"CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation"},{"paperId":"4083958684292f6fa2f5c7fd4f9be975e80145b6","title":"GraphCodeBERT: Pre-training Code Representations with Data Flow"},{"paperId":"b88703f68f4abb9e69b86fb42dff85aa4a76fca4","title":"Multilingual Translation from Denoising Pre-Training"},{"paperId":"61a3de09c3430eb1a7da2cf5836c66e4520bb674","title":"Contrastive Learning for Source Code with Structural and Functional Properties"},{"paperId":"f23a0e443fe931aa2fed932421bf47c1a4fcf619","title":"CodeBLEU: a Method for Automatic Evaluation of Code Synthesis"},{"paperId":"91b21ce88d97ce37a70c8c5d725c35b5ba466638","title":"Unsupervised Translation of Programming Languages"},{"paperId":"0fe2636446cd686830da3d971b31a004d6094b3c","title":"CodeBERT: A Pre-Trained Model for Programming and Natural Languages"},{"paperId":"fbe25e4f069a19dc63daca27b7c98cff338663b9","title":"CodeSearchNet Challenge: Evaluating the State of Semantic Code Search"},{"paperId":"1906e3a2fda12641a42739e3fb6a8f8b1accc8dd","title":"SPoC: Search-based Pseudocode to Code"},{"paperId":"e3ef11877bdd08140fcabf358dd9fc5bef6b15e0","title":"Recommendations for Datasets for Source Code Summarization"},{"paperId":"faadd7d081c8d67e8c2567e8a5579e46cd6b2280","title":"fairseq: A Fast, Extensible Toolkit for Sequence Modeling"},{"paperId":"8fd47bff451220ce612463dbfb5bff2423fb06ab","title":"The FLORES Evaluation Datasets for Low-Resource Machine Translation: Nepali–English and Sinhala–English"},{"paperId":"ec4eba83f6b3266d9ae7cabb2b2cb1518f727edc","title":"Cross-lingual Language Model Pretraining"},{"paperId":"160563abbd75265b19afc8b4169bab9e1eb33d97","title":"Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":null,"title":"2019. BERT: Pre-training"},{"paperId":"776b4f1d6ed07f1623831eae2849562cf4381394","title":"Unsupervised Statistical Machine Translation"},{"paperId":"077ea3a496bd76bd22d76c432b79b9fa40e136e8","title":"Improving Automatic Source Code Summarization via Deep Reinforcement Learning"},{"paperId":"f1200a733652f744ba0d86ed38aafbdcbd9c2723","title":"Style Transfer as Unsupervised Machine Translation"},{"paperId":"807370352540f171685998aec7a5701f7110f147","title":"Understanding Back-Translation at Scale"},{"paperId":"49164d216b7e7968ded2d9863af161191f2c32e5","title":"Summarizing Source Code with Transferred API Knowledge"},{"paperId":"222d60e1669ea363b18a7d7d5eb893f8f6b24e6d","title":"Unsupervised Text Style Transfer using Language Models as Discriminators"},{"paperId":"48925fef94500cf19ee220ed74217816f1ab5e60","title":"Phrase-Based & Neural Unsupervised Machine Translation"},{"paperId":"6c6170ffb39cdc8cfffbeda9c7a2259eda5875f2","title":"Tree-to-tree Neural Networks for Program Translation"},{"paperId":"907a90967f68da4311802247408e0515e363f930","title":"CyCADA: Cycle-Consistent Adversarial Domain Adaptation"},{"paperId":"e3d772986d176057aca2f5e3eb783da53b559134","title":"Unsupervised Machine Translation Using Monolingual Corpora Only"},{"paperId":"c2a7afbb5609a723f8eea91bfde4b02579b048d6","title":"Unsupervised Neural Machine Translation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"66721162f712690bb10928132d402d9bd4460c1b","title":"Style Transfer from Non-Parallel Text by Cross-Alignment"},{"paperId":null,"title":"Unpaired image-to-image translation using cycle-consistent adversarial networks"},{"paperId":"23b559b5ab27f2fca6f56c0a7b6478bcf69db509","title":"Dual Learning for Machine Translation"},{"paperId":"f3b96ef2dc1fc5e14982f1b963db8db6a54183bb","title":"Improving Neural Machine Translation Models with Monolingual Data"},{"paperId":null,"title":"Github on bigquery: Analyze all the open source code"},{"paperId":"5539a60b8e30ca0e278829880cee2a1ec65fa677","title":"Using machine translation for converting Python 2 to Python 3 code"},{"paperId":"5fcd41ca42659ff792fc8ee7d535156e8e69f987","title":"On Using Monolingual Corpora in Neural Machine Translation"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5","title":"Neural Machine Translation by Jointly Learning to Align and Translate"},{"paperId":null,"title":"Neural machine translation by jointly 17Calculated using https://mlco2.github.io/impact, based on a total of 200 hours of training on RTX 2080 Ti and Amazon Web Services as the provider"},{"paperId":"4755b856dc08ac024ae935e7a6f9df325b00ae53","title":"Phrase-Based Statistical Translation of Programming Languages"},{"paperId":"5447a3b8701d59f3a2f1a7f7af030f687ba495c3","title":"Lexical statistical machine translation for language migration"},{"paperId":"843959ffdccf31c6694d135fad07425924f785b1","title":"Extracting and composing robust features with denoising autoencoders"},{"paperId":"50b8e8d48f4973cdeefa835807b4e1a8ca65ced3","title":"Pharaoh: A Beam Search Decoder for Phrase-Based Statistical Machine Translation Models"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":null,"title":"Bleu : a method for automatic eval - 746 uation of machine translation Retrieval 753 augmented code generation and summarization"}],"id":"636f854b1a3a983e6803eae0277179596cc2cb95","summary":"This work proposes a method for performing back-translation via code summarization and generation of natural language summaries given code snippets that performs competitively with state-of-the-art methods."}]}