{"papers":[{"url":"https://www.semanticscholar.org/paper/883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review","venue":"","year":2023,"referenceCount":72,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/03/2023","authors":"Zezhou Yang,Sirong Chen,Cuiyun Gao,Zhenhao Li,Ge Li,Rongcong Lv","id":"883a5338dee175ae61f323202a5aba80b2458e0f","summary":"The existing research results of each category of methods are systematically reviewed, summarized and commented and the overall literature review is summarized and provides a prospect on future research directions worthy of attention.","score":19},{"url":"https://www.semanticscholar.org/paper/57ffd52abfbba0eb76c8406c7331f1f8ca29aaff","title":"R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents","venue":"","year":2023,"referenceCount":89,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/03/2023","authors":"Daniel D. Johnson,Daniel Tarlow,Christian J. Walder","id":"57ffd52abfbba0eb76c8406c7331f1f8ca29aaff","summary":"This work proposes Randomized Utility-driven Synthesis of Uncertain REgions (R-U-SURE), an approach for building uncertainty-aware suggestions based on a decision-theoretic model of goal-conditioned utility, using random samples from a generative model as a proxy for the unobserved possible intents of the end user.","score":10},{"url":"https://www.semanticscholar.org/paper/1786a2f9140ed7211b21302977de64e948b92308","title":"Learning Performance-Improving Code Edits","venue":"ArXiv","year":2023,"referenceCount":62,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/02/2023","authors":"Aman Madaan,Alex Shypula,Uri Alon,Milad Hashemi,Parthasarathy Ranganathan,Yiming Yang,Graham Neubig,A. Yazdanbakhsh","id":"1786a2f9140ed7211b21302977de64e948b92308","summary":"This paper investigates the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits, and hypothesizes that language models can suggest such edits in ways that would be impractical for static analysis alone.","score":8},{"url":"https://www.semanticscholar.org/paper/0a39442979d6e678dd36bb443ad529c14e86a86e","title":"DocPrompting: Generating Code by Retrieving the Docs","venue":"","year":2022,"referenceCount":46,"citationCount":4,"influentialCitationCount":0,"publicationDate":"13/07/2022","authors":"Shuyan Zhou,Uri Alon,Frank F. Xu,Zhiruo Wang,Zhengbao Jiang,Graham Neubig","id":"0a39442979d6e678dd36bb443ad529c14e86a86e","summary":"DocPrompting is introduced: a natural-language-to-code generation approach that explicitly leverages documentation by retrieving the relevant documentation pieces given an NL intent, and generating code based on the NL intent and the retrieved documentation.","score":8},{"url":"https://www.semanticscholar.org/paper/66c52c6ef45cdd77c086996bcaaf01470e82dbd2","title":"Piloting Copilot and Codex: Hot Temperature, Cold Prompts, or Black Magic?","venue":"ArXiv","year":2022,"referenceCount":46,"citationCount":2,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Jean-Baptiste Döderlein,M. Acher,D. Khelladi,B. Combemale","id":"66c52c6ef45cdd77c086996bcaaf01470e82dbd2","summary":"The results showed that varying the input parameters can significantly improve the performance of language models, however, there is a tight dependency when varying the temperature, the prompt and the number of generated solutions, making potentially hard for developers to properly control the parameters to obtain an optimal result.","score":8},{"url":"https://www.semanticscholar.org/paper/6d269364de402d4a72ac30b0c8d81324f6849807","title":"LEVER: Learning to Verify Language-to-Code Generation with Execution","venue":"ArXiv","year":2023,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2023","authors":"Ansong Ni,Srini Iyer,Dragomir R. Radev,V. Stoyanov,Wen-tau Yih,Sida I. Wang,Xi Victoria Lin","id":"6d269364de402d4a72ac30b0c8d81324f6849807","summary":"This work proposes LEVER, a simple approach to improve language-to-code generation by learning to verify the generated programs with their execution results, which consistently improves over the base CodeLMs and achieves new state-of-the-art results on all of them.","score":8},{"url":"https://www.semanticscholar.org/paper/b2c4fdb49bdb23e6ad0ac3272029324157046ea7","title":"Automated Repair of Code from Language Models","venue":"","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Zhiyu Fan,Xiang Gao,M. Mirchev,Abhik Roychoudhury,Shin Hwei Tan","id":"b2c4fdb49bdb23e6ad0ac3272029324157046ea7","summary":"The study revealed that automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to auto-generated code, and given bug location information provided by a statistical fault localization approach, Codex edit mode is similar to or better than existing Java repair tools TBar and Recoder in correcting incorrect solutions.","score":7},{"url":"https://www.semanticscholar.org/paper/ec229b004ebaee27b9efa84b834825dbc2937538","title":"Is GitHub's Copilot as Bad As Humans at Introducing Vulnerabilities in Code?","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":8,"influentialCitationCount":0,"publicationDate":"10/04/2022","authors":"Owura Asare,M. Nagappan,Nirmal Asokan","id":"ec229b004ebaee27b9efa84b834825dbc2937538","summary":"A comparative empirical analysis of Copilot-generated code from a security perspective concludes that Copilot is not as bad as human developers at introducing vulnerabilities in code.","score":7},{"url":"https://www.semanticscholar.org/paper/0bcd59da541fdae66884afba8d25475a54a9da1a","title":"Automated Repair of Programs from Large Language Models","venue":"","year":2022,"referenceCount":49,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/05/2022","authors":"Zhiyu Fan,Xiang Gao,M. Mirchev,Abhik Roychoudhury,Shin Hwei Tan","id":"0bcd59da541fdae66884afba8d25475a54a9da1a","summary":"The study revealed that automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to auto-generated code, and given bug location information provided by a statistical fault localization approach, Codex edit mode is similar to or better than existing Java repair tools TBar and Recoder in correcting incorrect solutions.","score":7},{"url":"https://www.semanticscholar.org/paper/6032212d5790b6a580d68d469a9895aad6238c89","title":"Diverse Title Generation for Stack Overflow Posts with Multiple Sampling Enhanced Transformer","venue":"Journal of Systems and Software","year":2022,"referenceCount":81,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/08/2022","authors":"Fengji Zhang,Jin Liu,Yao Wan,Xiao Yu,Xiao Liu,J. Keung","id":"6032212d5790b6a580d68d469a9895aad6238c89","summary":"This work proposes M$_3$NSCT5, a novel approach to automatically generate multiple post titles from the given code snippets, using the maximal marginal multiple nucleus sampling strategy to generate multiple high-quality and diverse title candidates at a time for the developers to choose from.","score":7},{"url":"https://www.semanticscholar.org/paper/79680e20f0c8038039b243fb5fd385fbe967c799","title":"Controlling Large Language Models to Generate Secure and Vulnerable Code","venue":"ArXiv","year":2023,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/02/2023","authors":"Jingxuan He,Martin T. Vechev","id":"79680e20f0c8038039b243fb5fd385fbe967c799","summary":null,"score":7},{"url":"https://www.semanticscholar.org/paper/e73a16490c29530c37b49f6a30592790e7caaaa4","title":"Don't Complete It! Preventing Unhelpful Code Completion for Productive and Sustainable Neural Code Completion Systems","venue":"","year":2022,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/09/2022","authors":"Zhensu Sun,Xiaoning Du,Fu Song,Shangwen Wang,Mingze Ni,Li Li","id":"e73a16490c29530c37b49f6a30592790e7caaaa4","summary":"An early-rejection mechanism to turn down low-return prompts by foretelling the code completion qualities without sending them to the codepletion system is proposed and a lightweight Transformer-based estimator is proposed to demonstrate the feasibility of the mechanism.","score":6},{"url":"https://www.semanticscholar.org/paper/1b4c19168410fb2690d285b205ab2281793db81a","title":"A Scalable and Extensible Approach to Benchmarking NL2Code for 18 Programming Languages","venue":"ArXiv","year":2022,"referenceCount":27,"citationCount":5,"influentialCitationCount":1,"publicationDate":2022,"authors":"Federico Cassano,John Gouwar,Daniel Nguyen,S. Nguyen,Luna Phipps-Costin,Donald Pinckney,Ming-Ho Yee,Yangtian Zi,Carolyn Jane Anderson,Molly Q. Feldman,Arjun Guha,M. Greenberg,Abhinav Jangda","id":"1b4c19168410fb2690d285b205ab2281793db81a","summary":"It is shown that on several languages, Codex matches and even exceeds its performance on Python, and a general approach is described for easily adding support for new benchmarks and languages to MultiPL-E, the first multi-language parallel benchmark for natural-language-to-code-generation.","score":6},{"url":"https://www.semanticscholar.org/paper/95fa2b27ab7eb84738441ee16da97323538938f9","title":"I Speak, You Verify: Toward Trustworthy Neural Program Synthesis","venue":"ArXiv","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/09/2022","authors":"Darren Key,Wen-Ding Li,Kevin Ellis","id":"95fa2b27ab7eb84738441ee16da97323538938f9","summary":"An approach for improving the trustworthiness and overall accuracy of program synthesizers based on large language models for source code by analyzing the agreement between programs and predicates to judge both which program is most likely to be correct and whether the language model is able to solve the programming problem in the first place.","score":6},{"url":"https://www.semanticscholar.org/paper/ebd4bec684808aff360b5f255d15c0d112ba13d3","title":"Explicit Knowledge Transfer for Weakly-Supervised Code Generation","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/11/2022","authors":"Zhangir Azerbayev,Ansong Ni,Hailey Schoelkopf,Dragomir R. Radev","id":"ebd4bec684808aff360b5f255d15c0d112ba13d3","summary":"This paper proposes explicit knowledge transfer (EKT), which uses the few-shot capabilities of a teacher LLM to create NL-code pairs that are filter for correctness and fine-tune the student on, and finds that EKT not only yields better performance than training with expert iteration, but also outperforms knowledge distillation, another form of knowledge transfer.","score":6},{"url":"https://www.semanticscholar.org/paper/fba0b0817dbc8200b41a1de22654b54b778a11e9","title":"Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models","venue":"ArXiv","year":2023,"referenceCount":64,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/01/2023","authors":"Toufique Ahmed,Supriyo Ghosh,Chetan Bansal,T. Zimmermann,Xuchao Zhang,S. Rajmohan","id":"fba0b0817dbc8200b41a1de22654b54b778a11e9","summary":"This work does the first large-scale study to evaluate the effectiveness of GPT-3.x models for helping engineers root cause and mitigate production incidents and compares several large language models in zero-shot, fine-tuned and multi-task setting using semantic and lexical metrics.","score":6},{"url":"https://www.semanticscholar.org/paper/68edfd62d2619fb4af7c2469edb95b9e2fe4544a","title":"Execution-based Code Generation using Deep Reinforcement Learning","venue":"ArXiv","year":2023,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Parshin Shojaee,Aneesh Jain,S. Tipirneni,C. Reddy","id":"68edfd62d2619fb4af7c2469edb95b9e2fe4544a","summary":"PPOCoder is a new framework for code generation that combines pretrained PL models with Proximal Policy Optimization (PPO) deep reinforcement learning and employs execution feedback as the external source of knowledge into the model optimization, which is transferable across different code generation tasks and PLs.","score":6},{"url":"https://www.semanticscholar.org/paper/3d5463a16d85d9d1d0d8ebb4117a31aca3c240ff","title":"Fooling MOSS Detection with Pretrained Language Models","venue":"International Conference on Information and Knowledge Management","year":2022,"referenceCount":70,"citationCount":2,"influentialCitationCount":0,"publicationDate":"19/01/2022","authors":"Stella Rose Biderman,Edward Raff","id":"3d5463a16d85d9d1d0d8ebb4117a31aca3c240ff","summary":"It is found that a student using GPT-J can complete introductory level programming assignments without triggering suspicion from MOSS, a widely used software similarity and plagiarism detection tool.","score":5},{"url":"https://www.semanticscholar.org/paper/38115e80d805fb0fb8f090dc88ced4b24be07878","title":"CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis","venue":"","year":2022,"referenceCount":48,"citationCount":18,"influentialCitationCount":6,"publicationDate":"25/03/2022","authors":"Erik Nijkamp,Bo Pang,Hiroaki Hayashi,Lifu Tu,Haiquan Wang,Yingbo Zhou,S. Savarese,Caiming Xiong","id":"38115e80d805fb0fb8f090dc88ced4b24be07878","summary":"This work trains and releases a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER and model checkpoints, and investigates the multi-step paradigm for program synthesis.","score":5},{"url":"https://www.semanticscholar.org/paper/7107d06366b48b3593c8128ed2ca67e0b413628c","title":"Learning Math Reasoning from Self-Sampled Correct and Partially-Correct Solutions","venue":"","year":2022,"referenceCount":28,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/05/2022","authors":"Ansong Ni,J. Inala,Chenglong Wang,Oleksandr Polozov,Christopher Meek,Dragomir R. Radev,Jianfeng Gao","id":"7107d06366b48b3593c8128ed2ca67e0b413628c","summary":"It is shown that the use of self-sampled correct and partially-correct solutions can benefit learning and help guide the sampling process, leading to more efficient exploration of the solution space and the effectiveness of the method is shown.","score":5},{"url":"https://www.semanticscholar.org/paper/a889a606734cd47f802765866487c677497a70d6","title":"Lost at C: A User Study on the Security Implications of Large Language Model Code Assistants","venue":"","year":2022,"referenceCount":52,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/08/2022","authors":"Gustavo Sandoval,H. Pearce,Teo Nys,R. Karri,S. Garg,Brendan Dolan-Gavitt","id":"a889a606734cd47f802765866487c677497a70d6","summary":"A security-driven user study to assess code written by student programmers when assisted by LLMs and indicates that the security impact in this setting is small: AI-assisted users produce critical security bugs at a rate no greater than 10% more than the control.","score":5},{"url":"https://www.semanticscholar.org/paper/a7435722d8ab595da5a9c70ac9160f57d0dcd75a","title":"Enabling Transformers to Understand Low-Level Programs","venue":"IEEE Conference on High Performance Extreme Computing","year":2022,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/09/2022","authors":"Zifan Carl Guo,William S. Moses","id":"a7435722d8ab595da5a9c70ac9160f57d0dcd75a","summary":"This work applies transfer learning to low-level (LLVM) programs and study how low- level programs can be made more amenable to Transformer models through various techniques, including preprocessing, infix/prefix operators, and information deduplication.","score":5},{"url":"https://www.semanticscholar.org/paper/d3a7a4543d83f568f79d1febe8379465ff0140c9","title":"A Survey of Deep Learning for Mathematical Reasoning","venue":"ArXiv","year":2022,"referenceCount":216,"citationCount":3,"influentialCitationCount":1,"publicationDate":"20/12/2022","authors":"Pan Lu,Liang Qiu,Wenhao Yu,S. Welleck,Kai-Wei Chang","id":"d3a7a4543d83f568f79d1febe8379465ff0140c9","summary":"This survey paper reviews the key tasks, datasets, and methods at the intersec-tion of mathematical reasoning and deep learning over the past decade, and evaluates existing benchmarks and methods and discusses future research directions.","score":5},{"url":"https://www.semanticscholar.org/paper/d8405996b4d08c304098636aedd9e1c1a1e262ee","title":"The Premature Obituary of Programming","venue":"Communications of the ACM","year":2023,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/01/2023","authors":"D. Yellin","id":"d8405996b4d08c304098636aedd9e1c1a1e262ee","summary":"Why deep learning will not replace programming and why deep learning should not be considered as a programming language.","score":5},{"url":"https://www.semanticscholar.org/paper/6876fc47f4674fdf4f53208254b45dedc6de3755","title":"Adaptive Test Generation Using a Large Language Model","venue":"ArXiv","year":2023,"referenceCount":62,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/02/2023","authors":"Max Schäfer,Sarah Nadi,A. Eghbali,F. Tip","id":"6876fc47f4674fdf4f53208254b45dedc6de3755","summary":"TestPilot uses Codex, an off-the-shelf LLM, to automatically generate unit tests for a given program without requiring additional training or few-shot learning on examples of existing tests, and finds that TestPilot does not generate memorized tests.","score":5},{"url":"https://www.semanticscholar.org/paper/1c8e15f15d67c5974445634bb971e2275e957aff","title":"Security Implications of Large Language Model Code Assistants: A User Study","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":4,"influentialCitationCount":0,"publicationDate":2022,"authors":"Gustavo Sandoval,H. Pearce,Teo Nys,R. Karri,Brendan Dolan-Gavitt,S. Garg","id":"1c8e15f15d67c5974445634bb971e2275e957aff","summary":"A security-driven user study to assess code written by student programmers when assisted by LLMs and indicates that the security impact in this setting is small: AI-assisted users produce critical security bugs at a rate no greater than 10% more than the control.","score":4},{"url":"https://www.semanticscholar.org/paper/971e875e28f26240987d2c9470d1ee74ad204205","title":"Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/09/2022","authors":"Sungmin Kang,Juyeon Yoon,Shin Yoo","id":"971e875e28f26240987d2c9470d1ee74ad204205","summary":"Results show L IBRO has the potential to enhance developerency by automatically generating tests from bug reports, and is proposed as a framework that uses Large Language Models (LLMs), which have been shown to be capable of performing code-related tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/0b8772b7790c69f40897b5eb7f8fd57f24138f3d","title":"ContraGen: Effective Contrastive Learning For Causal Language Model","venue":"ArXiv","year":2022,"referenceCount":66,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/10/2022","authors":"Nihal Jain,Dejiao Zhang,Wasi Uddin Ahmad,Zijian Wang,Feng Nan,Xiaopeng Li,M. Tan,Ramesh Nallapati,Baishakhi Ray,Parminder Bhatia,Xiaofei Ma,Bing Xiang","id":"0b8772b7790c69f40897b5eb7f8fd57f24138f3d","summary":"It is shown that C ONTRA G EN can effectively enhance both uniformity and discrimination of the representations and lead to the desired improvement on various language understanding tasks where discriminative representations are crucial for attaining good performance.","score":4},{"url":"https://www.semanticscholar.org/paper/825333b7efe2cade106eaf36c7e731f757974806","title":"How Readable is Model-generated Code? Examining Readability and Visual Inspection of GitHub Copilot","venue":"37th IEEE/ACM International Conference on Automated Software Engineering","year":2022,"referenceCount":28,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Naser Al Madi","id":"825333b7efe2cade106eaf36c7e731f757974806","summary":"The results suggest that model generated code is comparable in complexity and readability to code written by human pair programmers, and eye tracking data suggests, to a statistically significant level, that programmers direct less visual attention to model generate code.","score":4},{"url":"https://www.semanticscholar.org/paper/39e40821b7207125e54e6ed7112e55cd38c6f0c3","title":"Language Models of Code are Few-Shot Commonsense Learners","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":38,"citationCount":13,"influentialCitationCount":1,"publicationDate":"13/10/2022","authors":"Aman Madaan,Shuyan Zhou,Uri Alon,Yiming Yang,Graham Neubig","id":"39e40821b7207125e54e6ed7112e55cd38c6f0c3","summary":"This paper shows that when this task is frame as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than L Ms of natural language, even when the downstream task does not involve source code at all.","score":4},{"url":"https://www.semanticscholar.org/paper/a6ef5bad716091fb1888bf365f6129628ab3a5ee","title":"ObSynth: An Interactive Synthesis System for Generating Object Models from Natural Language Specifications","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/10/2022","authors":"Alex Gu,Tamara Mitrovska,D. Vélez,Jacob Andreas,Armando Solar-Lezama","id":"a6ef5bad716091fb1888bf365f6129628ab3a5ee","summary":"ObSynth is introduced, an interactive system leveraging the domain knowledge em-bedded in large language models (LLMs) to help users design object models from high level natural language prompts, showing that it often synthesizes objects, methods, and methods users might have otherwise omitted.","score":4},{"url":"https://www.semanticscholar.org/paper/ec7324a15009a9bd0b676f6b17762f759cf5dd9a","title":"Large Language Models Are Human-Level Prompt Engineers","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":19,"influentialCitationCount":2,"publicationDate":"03/11/2022","authors":"Yongchao Zhou,Andrei Ioan Muresanu,Ziwen Han,Keiran Paster,Silviu Pitis,Harris Chan,Jimmy Ba","id":"ec7324a15009a9bd0b676f6b17762f759cf5dd9a","summary":"It is shown that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts.","score":4},{"url":"https://www.semanticscholar.org/paper/71280dba5bda65c162f9deaffed7d3d20692ca0a","title":"SecurityEval dataset: mining vulnerability examples to evaluate machine learning-based code generation techniques","venue":"","year":2022,"referenceCount":28,"citationCount":3,"influentialCitationCount":1,"publicationDate":"07/11/2022","authors":"Mohammed Latif Siddiq,msiddiq","id":"71280dba5bda65c162f9deaffed7d3d20692ca0a","summary":"SecurityEval, an evaluation dataset that contains 130 samples for 75 vulnerability types, which are mapped to the Common Weakness Enumeration (CWE) and demonstrated using one open-source and one closed-source code generation model to evaluate.","score":4},{"url":"https://www.semanticscholar.org/paper/20b60fb3993d2e9a5af04611f7bdf248e5a3a736","title":"Programming by Example and Text-to-Code Translation for Conversational Code Generation","venue":"ArXiv","year":2022,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Eli Whitehouse,William Gerard,Yauhen Klimovich,Marc Franco-Salvador","id":"20b60fb3993d2e9a5af04611f7bdf248e5a3a736","summary":"Modular Programs for Text-guided Hierarchical Synthesis (MPaTHS), a method for integrating Programming by Example and text-to-code systems which uses an accessible natural language interface for synthesizing general programs, is proposed.","score":4},{"url":"https://www.semanticscholar.org/paper/c192fb33f308f19ac8a5c4c2d623d56385b839be","title":"Systematic Literature Review on Solving Competitive Programming Problem with Artificial Intelligence (AI)","venue":"2022 1st International Conference on Software Engineering and Information Technology (ICoSEIT)","year":2022,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/11/2022","authors":"Francis Alexander,Edwin Ario Abdiwijaya,Felix Pherry,A. A. Gunawan,Anderies","id":"c192fb33f308f19ac8a5c4c2d623d56385b839be","summary":"It can be concluded that the code auto-completion and code-generation tools that are available now still do not meet the necessary benchmark which is solving CP tasks, and AI still has a long way to go before competing at the highest level of CP.","score":4},{"url":"https://www.semanticscholar.org/paper/30624a18720bf93a85dc3efe570df271a8c9f4c3","title":"Program Repair","venue":"ArXiv","year":2022,"referenceCount":105,"citationCount":2,"influentialCitationCount":0,"publicationDate":"23/11/2022","authors":"Xiang Gao,Yannic Noller,Abhik Roychoudhury","id":"30624a18720bf93a85dc3efe570df271a8c9f4c3","summary":"Keeping intermediate rules, instead of producing the most generalized rule by generalizing all concrete transformations, enables us to apply the most suitable rule to transform a given code.","score":4},{"url":"https://www.semanticscholar.org/paper/8175ce2cbb99b6a394bdac152ae39d413f4f1380","title":"Codex Hacks HackerRank: Memorization Issues and a Framework for Code Synthesis Evaluation","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"publicationDate":"06/12/2022","authors":"Anjan Karmakar,Julian Aron Prenner,Marco D'Ambros,R. Robbes","id":"8175ce2cbb99b6a394bdac152ae39d413f4f1380","summary":"This work evaluates the code synthesis capabilities of the Codex model based on a set of 115 Python problem statements from a popular competitive programming portal: HackerRank, and proposes a framework for code-synthesis evaluation using variations of problem statements based on mutations.","score":4},{"url":"https://www.semanticscholar.org/paper/9e8f0d9ba4e7af673c8b214b3764e020706dd1f3","title":"Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments","venue":"ArXiv","year":2022,"referenceCount":66,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Yu Gu,Xiang Deng,Yu Su","id":"9e8f0d9ba4e7af673c8b214b3764e020706dd1f3","summary":"Pangu is proposed, a generic framework for grounded language understanding that capitalizes on the discriminative ability of LMs instead of their generative ability, and enables, for the first time, effective few-shot in-context learning for KBQA with large LMs such as Codex.","score":4},{"url":"https://www.semanticscholar.org/paper/12a4e62c43b829dabdb8afc60eee76aa80fa3f6e","title":"Fuzzing Deep-Learning Libraries via Large Language Models","venue":"ArXiv","year":2022,"referenceCount":65,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/12/2022","authors":"Yinlin Deng,Chun Xia,Haoran Peng,Chenyuan Yang,Lingming Zhang","id":"12a4e62c43b829dabdb8afc60eee76aa80fa3f6e","summary":"LLMFuzz is the first automated approach to directly leveraging Large Pre-trained Language Models (LLMs) to generate input programs for fuzzing DL libraries, and is able to detect 65 bugs, with 41 already confirmed as previously unknown bugs.","score":4},{"url":"https://www.semanticscholar.org/paper/0566c1c3eeeef5c968fced6d80b77fe22d02bbd9","title":"Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language","venue":"Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1","year":2022,"referenceCount":26,"citationCount":5,"influentialCitationCount":0,"publicationDate":"27/10/2022","authors":"Paul Denny,Viraj Kumar,Nasser Giacaman","id":"0566c1c3eeeef5c968fced6d80b77fe22d02bbd9","summary":"Evaluating the performance of Copilot on a publicly available dataset of 166 programming problems finds that it successfully solves around half of these problems on its very first attempt, and that it solves 60% of the remaining problems using only natural language changes to the problem description.","score":4},{"url":"https://www.semanticscholar.org/paper/c72ac81c4ac414314b52cf8f5b77370f8d0875d4","title":"Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models","venue":"ArXiv","year":2023,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/01/2023","authors":"Tung Phung,J. Cambronero,Sumit Gulwani,Tobias Kohn,R. Majumdar,A. Singla,Gustavo Soares","id":"c72ac81c4ac414314b52cf8f5b77370f8d0875d4","summary":"The key idea behind PyFiXV is to use a novel run-time validation mechanism to decide whether the generated feedback is suitable for sharing with the student; notably, this validation mechanism also provides a precision knob to educators.","score":4},{"url":"https://www.semanticscholar.org/paper/0885556b71b24a641b4ffe139afd4d2712228cff","title":"Beware of the Unexpected: Bimodal Taint Analysis","venue":"ArXiv","year":2023,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/01/2023","authors":"Yiu Wai Chow,Max Schäfer,Michael Pradel","id":"0885556b71b24a641b4ffe139afd4d2712228cff","summary":"Fluffy is presented, a bimodal taint analysis that combines static analysis, which reasons about data flow, with machine learning, which probabilistically determines which flows are potentially problematic.","score":4},{"url":"https://www.semanticscholar.org/paper/13a66fc8689724e295548ceac9e5425fc46cc093","title":"SkCoder: A Sketch-based Approach for Automatic Code Generation","venue":"ArXiv","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/02/2023","authors":"Jia Li,Yongming Li,Ge Li,Zhi Jin,Yiyang Hao,Xing Hu","id":"13a66fc8689724e295548ceac9e5425fc46cc093","summary":"This paper proposes a sketch-based code generation approach named SkCoder to mimic developers' code reuse behavior, which retrieves a similar code snippet, extracts relevant parts as a code sketch, and edits the sketch into the desired code.","score":4},{"url":"https://www.semanticscholar.org/paper/7c817fa57edd087820d3b4d16e4d1f40d4cb5200","title":"Generation Probabilities Are Not Enough: Exploring the Effectiveness of Uncertainty Highlighting in AI-Powered Code Completions","venue":"ArXiv","year":2023,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/02/2023","authors":"Helena Vasconcelos,Gagan Bansal,Adam Fourney,Q. Liao,Jennifer Wortman Vaughan","id":"7c817fa57edd087820d3b4d16e4d1f40d4cb5200","summary":"The question of whether conveying information about uncertainty enables programmers to more quickly and accurately produce code when collaborating with an AI-powered code completion tool, and if so, what measure of uncertainty best fits programmers' needs is explored.","score":4},{"url":"https://www.semanticscholar.org/paper/f5d93cf5d81575aeee61faeddde4437fbcb74cd0","title":"The Programmer's Assistant: Conversational Interaction with a Large Language Model for Software Development","venue":"ArXiv","year":2023,"referenceCount":111,"citationCount":2,"influentialCitationCount":0,"publicationDate":"14/02/2023","authors":"Steven I. Ross,Fernando Martinez,Stephanie Houde,Michael J. Muller,Justin D. Weisz","id":"f5d93cf5d81575aeee61faeddde4437fbcb74cd0","summary":"This work developed a prototype system -- the Programmer's Assistant -- in order to explore the utility of conversational interactions grounded in code, as well as software engineers' receptiveness to the idea of conversing with, rather than invoking, a code-fluent LLM.","score":4},{"url":"https://www.semanticscholar.org/paper/57e849d0de13ed5f91d086936296721d4ff75a75","title":"LLaMA: Open and Efficient Foundation Language Models","venue":"ArXiv","year":2023,"referenceCount":79,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/02/2023","authors":"Hugo Touvron,Thibaut Lavril,Gautier Izacard,Xavier Martinet,Marie-Anne Lachaux,Timothée Lacroix,Baptiste Rozière,Naman Goyal,Eric Hambro,Faisal Azhar,Aur'elien Rodriguez,Armand Joulin,Edouard Grave,Guillaume Lample","id":"57e849d0de13ed5f91d086936296721d4ff75a75","summary":"LLaMA, a collection of foundation language models ranging from 7B to 65B parameters, is introduced and it is shown that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets.","score":4},{"url":"https://www.semanticscholar.org/paper/d388b3cbc820975da21c57c9fb9cef2d9c6d08f0","title":"CodeRetriever: Large-scale Contrastive Pre-training for Code Search","venue":"","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Xiaonan Li,Yeyun Gong,Yelong Shen,Xipeng Qiu,Hang Zhang,Bolun Yao,Weizhen Qi,Daxin Jiang,Weizhu Chen,Nan Duan","id":"d388b3cbc820975da21c57c9fb9cef2d9c6d08f0","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/359550c0c36186facbf37a38a4d3e64f39ea9f48","title":"Who Evaluates the Evaluators? On Automatic Metrics for Assessing AI-based Offensive Code Generators","venue":"ArXiv","year":2022,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/12/2022","authors":"Cristina Improta,Pietro Liguori,R. Natella,B. Cukic,Domenico Cotroneo","id":"359550c0c36186facbf37a38a4d3e64f39ea9f48","summary":"This work analyzes a large set of output similarity metrics on offensive code generators and applies the metrics on two state-of-the-art NMT models using two datasets containing offensive assembly and Python code with their descriptions in the English language.","score":3},{"url":"https://www.semanticscholar.org/paper/e9a64e58855dbbc725203d0202ceb9e7f8b7bb36","title":"A Survey of Learning-based Automated Program Repair","venue":"ArXiv","year":2023,"referenceCount":185,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/01/2023","authors":"Quanjun Zhang,Chunrong Fang,Yuxiang Ma,Weisong Sun,Zhenyu Chen","id":"e9a64e58855dbbc725203d0202ceb9e7f8b7bb36","summary":"This work presents a meta-modelling system that automates the very labor-intensive and therefore time-heavy and therefore expensive and expensive process of manually cataloging and cataloging individual neurons in the brain.","score":3},{"url":"https://www.semanticscholar.org/paper/b1eebb2df3b9ff7ff2b00fb1a786f6ada2caebce","title":"Towards a Mathematics Formalisation Assistant using Large Language Models","venue":"ArXiv","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"Ayush Agrawal,Siddhartha Gadgil,Navin Goyal,Ashvni Narayanan,Anand Tadipatri","id":"b1eebb2df3b9ff7ff2b00fb1a786f6ada2caebce","summary":"The abilities of a large language model (Codex) to help with formalisation in the Lean theorem prover are explored, finding that with careful inputdependent prompt selection and postprocessing, Codex is able to formalise short mathematical statements at undergrad level with nearly 75% accuracy for 120 theorem statements.","score":3},{"url":"https://www.semanticscholar.org/paper/49fede098a0d2a48e8100b30189224fc6f5eb25b","title":"Language Model Crossover: Variation through Few-Shot Prompting","venue":"ArXiv","year":2023,"referenceCount":90,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/02/2023","authors":"Elliot Meyerson,M. Nelson,Herbie Bradley,Arash Moradi,Amy K. Hoover,J. Lehman","id":"49fede098a0d2a48e8100b30189224fc6f5eb25b","summary":"The conclusion is that language model crossover is a promising method for evolving genomes representable as text through evolving binary bit-strings, sentences, equations, text-to-image prompts, and Python code.","score":3},{"url":"https://www.semanticscholar.org/paper/6df98ac2300c6e9c232440147ba976b4f501ca67","title":"C ODEX HACKS H ACKER R ANK : B ENEFITS AND R ISKS OF L ARGE -S CALE S OURCE C ODE M ODELS","venue":"","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","id":"6df98ac2300c6e9c232440147ba976b4f501ca67","summary":"These studies evaluate Codex on code synthesis, similar to the approach, but their evaluation efforts remain limited to math problems.","score":3},{"url":"https://www.semanticscholar.org/paper/8d282bf295d655e38b63ddbc7cdc94b188df8bb2","title":"G ENERATING S EQUENCES BY L EARNING TO [S ELF -]C ORRECT","venue":"","year":2022,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"","id":"8d282bf295d655e38b63ddbc7cdc94b188df8bb2","summary":null,"score":3},{"url":"https://www.semanticscholar.org/paper/7497360b0f411a44aa6afbd8b830050c40ec8aed","title":"Dataset of Student Solutions to Algorithm and Data Structure Programming Assignments","venue":"International Conference on Language Resources and Evaluation","year":2022,"referenceCount":18,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Fynn Petersen-Frey,Marcus Soll,Louis Kobras,Melf Johannsen,Peter Kling,Chris Biemann","id":"7497360b0f411a44aa6afbd8b830050c40ec8aed","summary":"This paper presents a dataset containing source code solutions to algorithmic programming exercises solved by hundreds of Bachelor-level students at the University of Hamburg, and plans to extend the dataset with tasks and solutions from upcoming courses.","score":3},{"url":"https://www.semanticscholar.org/paper/4f278ab5ad629267e06196e273252262854c1c57","title":"BF++: a language for general-purpose program synthesis","venue":"","year":2021,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/01/2021","authors":"Vadim Liventsev,A. Harma,M. Petkovi'c","id":"4f278ab5ad629267e06196e273252262854c1c57","summary":"A new programming language, BF ++ is proposed, designed speciﬁcally for automatic programming of agents in a Partially Observable Markov Decision Process (POMDP) setting and apply neural program synthesis to solve standard OpenAI Gym benchmarks.","score":3},{"url":"https://www.semanticscholar.org/paper/92173d081b15824d22a9ef070e118744ceee8052","title":"Show Your Work: Scratchpads for Intermediate Computation with Language Models","venue":"ArXiv","year":2021,"referenceCount":30,"citationCount":119,"influentialCitationCount":12,"publicationDate":"30/11/2021","authors":"Maxwell Nye,Anders Andreassen,Guy Gur-Ari,H. Michalewski,Jacob Austin,David Bieber,David Dohan,Aitor Lewkowycz,Maarten Bosma,D. Luan,Charles Sutton,Augustus Odena","id":"92173d081b15824d22a9ef070e118744ceee8052","summary":"Surprisingly, large pre-trained language models are able to perform complex multistep computations—even in the few-shot regime—when asked to perform the operation “step by step”, showing the results of intermediate computations.","score":3},{"url":"https://www.semanticscholar.org/paper/a5731122200fbb8b37f048010a1e1ca4474aa606","title":"Examining Zero-Shot Vulnerability Repair with Large Language Models","venue":"","year":2021,"referenceCount":56,"citationCount":8,"influentialCitationCount":0,"publicationDate":"03/12/2021","authors":"H. Pearce,B. Tan,Baleegh Ahmad,R. Karri,Brendan Dolan-Gavitt","id":"a5731122200fbb8b37f048010a1e1ca4474aa606","summary":"This work examines the use of large language models for code (such as OpenAI’s Codex and AI21's Jurassic J-1) for zero-shot vulnerability repair, and investigates challenges in the design of prompts that coax LLMs into generating repaired versions of insecure code.","score":3},{"url":"https://www.semanticscholar.org/paper/9a2ca811882ed7513f83014b9de4fb3b4ab218c4","title":"C OMPOSITIONAL G ENERALIZATION AND D ECOMPOSITION IN N EURAL P ROGRAM S YNTHESIS","venue":"","year":null,"referenceCount":0,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"","id":"9a2ca811882ed7513f83014b9de4fb3b4ab218c4","summary":"A suite of generalization tasks, which measure different types of compositional generalization that are desirable for program synthesis and are particularly difﬁcult for current sequence to sequence models, are proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/a2b6e1f7d8a7963d321f29fca7c01eeb1ebd7f0f","title":"P ATCH G ENERATION WITH L ANGUAGE M ODELS : F EASIBILITY AND S CALING B EHAVIOR","venue":"","year":2022,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Sophia Kolak,Ruben Martins,Claire Le Goues,V. Hellendoorn","id":"a2b6e1f7d8a7963d321f29fca7c01eeb1ebd7f0f","summary":"This work highlights a noticeable correlation of model size with test-passing accuracy and patch ranking quality, and the propensity for especially the largest models to generate candidate patches that closely resemble (if not exactly match), the original developer patch.","score":3},{"url":"https://www.semanticscholar.org/paper/78863000eb70945cc8d791d45d4a3fe8a6521cb6","title":"Open-ended Knowledge Tracing for Computer Science Education","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":55,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Naiming Liu,Zichao Wang","id":"78863000eb70945cc8d791d45d4a3fe8a6521cb6","summary":"An initial solution to the OKT problem is developed, a student knowledge-guided code generation approach that combines program synthesis methods using language models with student knowledge tracing methods and a series of quantitative and qualitative experiments on a real-world student code dataset to validate and demonstrate the promise of OKT.","score":3},{"url":"https://www.semanticscholar.org/paper/56e6d62c638a24411f12d15cdc8821a31fc495c8","title":"Source Code Generation from Descriptions in a Natural Language","venue":"","year":2022,"referenceCount":70,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Bc. Jan Pašek","id":"56e6d62c638a24411f12d15cdc8821a31fc495c8","summary":"This work introduces CodeFormer, a Python source code generator pretrained on a massive GitHub crawl consisting of 230M Python functions, and releases the resulting model, built on BART architecture, which generates Python functions based on descriptions in English.","score":3},{"url":"https://www.semanticscholar.org/paper/1aed58bd07026492194672adec494dc37c894a28","title":"Leveraging Automated Unit Tests for Unsupervised Code Translation","venue":"International Conference on Learning Representations","year":2021,"referenceCount":60,"citationCount":19,"influentialCitationCount":2,"publicationDate":"13/10/2021","authors":"Baptiste Rozière,J Zhang,François Charton,M. Harman,Gabriel Synnaeve,Guillaume Lample","id":"1aed58bd07026492194672adec494dc37c894a28","summary":"This work proposes to leverage an automated unit-testing system to filter out invalid translations, thereby creating a fully tested parallel corpus, and finds that fine-tuning an unsupervised model with this filtered data set significantly reduces the noise in the translations so-generated, comfortably outperforming the state of the art for all language pairs studied.","score":3},{"url":"https://www.semanticscholar.org/paper/52db8674337e5d86dcb96d013734befc8c3d4581","title":"Large Language Models are not Models of Natural Language: they are Corpus Models.","venue":"IEEE Access","year":2021,"referenceCount":63,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/12/2021","authors":"C. Veres","id":"52db8674337e5d86dcb96d013734befc8c3d4581","summary":"It is argued that the term language model is misleading because deep learning models are not theoretical models of language and proposed the adoption of corpus model instead, which better reflects the genesis and contents of the model.","score":3},{"url":"https://www.semanticscholar.org/paper/7b5aa186ca8abc585607c5ec91562e127a398601","title":"Open-Ended Knowledge Tracing","venue":"ArXiv","year":2022,"referenceCount":64,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/02/2022","authors":"Naiming Liu,Zichao Wang,Richard Baraniuk,Andrew S. Lan","id":"7b5aa186ca8abc585607c5ec91562e127a398601","summary":"This paper develops an initial solution to the OKT problem, a student knowledge-guided code generation approach that combines program synthesis methods using language models with student knowledge tracing methods and conducts a series of quantitative and qualitative experiments on a real-world student code dataset to validate OKT and demonstrate its promise in educational applications.","score":3},{"url":"https://www.semanticscholar.org/paper/1f11601f9eb8c0bee1d1d2cc7f4fa187e8c5e525","title":"The impact of lexical and grammatical processing on generating code from natural language","venue":"Findings","year":2022,"referenceCount":19,"citationCount":4,"influentialCitationCount":2,"publicationDate":"28/02/2022","authors":"Nathanael Beau,Benoit Crabb'e","id":"1f11601f9eb8c0bee1d1d2cc7f4fa187e8c5e525","summary":"The paper highlights the importance of the lexical substitution component in the current natural language to code systems with a state-of-the-art architecture that relies on BERT encoder and a grammar-based decoder for which a formalization is provided.","score":3},{"url":"https://www.semanticscholar.org/paper/771371fb288da26a9812f5808535847a0a9c9a80","title":"A Conversational Paradigm for Program Synthesis","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":61,"influentialCitationCount":23,"publicationDate":2022,"authors":"Erik Nijkamp,Bo Pang,Hiroaki Hayashi,Lifu Tu,Haiquan Wang,Yingbo Zhou,S. Savarese,Caiming Xiong","id":"771371fb288da26a9812f5808535847a0a9c9a80","summary":"This work proposes and trains C ODE G EN, an interactive code generation model for program synthesis, and suggests that the capacity of conversational program synthesis scales as a function of the model size and data size.","score":3},{"url":"https://www.semanticscholar.org/paper/094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways","venue":"ArXiv","year":2022,"referenceCount":173,"citationCount":601,"influentialCitationCount":88,"publicationDate":"05/04/2022","authors":"Aakanksha Chowdhery,Sharan Narang,Jacob Devlin,Maarten Bosma,Gaurav Mishra,Adam Roberts,P. Barham,Hyung Won Chung,Charles Sutton,Sebastian Gehrmann,Parker Schuh,Kensen Shi,Sasha Tsvyashchenko,Joshua Maynez,Abhishek Rao,Parker Barnes,Yi Tay,Noam M. Shazeer,Vinodkumar Prabhakaran,Emily Reif,Nan Du,B. Hutchinson,Reiner Pope,James Bradbury,Jacob Austin,M. Isard,Guy Gur-Ari,Pengcheng Yin,Toju Duke,Anselm Levskaya,S. Ghemawat,Sunipa Dev,H. Michalewski,Xavier García,Vedant Misra,Kevin Robinson,L. Fedus,Denny Zhou,Daphne Ippolito,D. Luan,Hyeontaek Lim,Barret Zoph,A. Spiridonov,Ryan Sepassi,David Dohan,Shivani Agrawal,Mark Omernick,Andrew M. Dai,T. S. Pillai,Marie Pellat,Aitor Lewkowycz,Erica Moreira,Rewon Child,Oleksandr Polozov,Katherine Lee,Zongwei Zhou,Xuezhi Wang,Brennan Saeta,Mark Díaz,Orhan Firat,Michele Catasta,Jason Wei,K. Meier-Hellstern,D. Eck,J. Dean,Slav Petrov,Noah Fiedel","id":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","summary":"A 540-billion parameter, densely activated, Transformer language model, which is called PaLM achieves breakthrough performance, outperforming the state-of-the-art on a suite of multi-step reasoning tasks, and outperforming average human performance on the recently released BIG-bench benchmark.","score":3},{"url":"https://www.semanticscholar.org/paper/6a250b904965732840a75b6a13e35ac15f5cce4d","title":"Compositional Generalization and Decomposition in Neural Program Synthesis","venue":"ArXiv","year":2022,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/04/2022","authors":"Kensen Shi,Joey Hong,M. Zaheer,Pengcheng Yin,Charles Sutton","id":"6a250b904965732840a75b6a13e35ac15f5cce4d","summary":"A suite of generalization tasks, which measure different types of compositional generalization that are desirable for program synthesis and are particularly difﬁcult for current sequence to sequence models, are proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/1f87dc41bdf2c4c78e2dce9c5c8adfef5e25a70c","title":"Passport: Improving Automated Formal Verification Using Identifiers","venue":"ArXiv","year":2022,"referenceCount":73,"citationCount":4,"influentialCitationCount":0,"publicationDate":"21/04/2022","authors":"Alex Sanchez-Stern,E. First,Timothy Zhou,Zhanna Kaufman,Yuriy Brun,T. Ringer","id":"1f87dc41bdf2c4c78e2dce9c5c8adfef5e25a70c","summary":"Passport is a fully-automated proof-synthesis tool that encodes one additional aspect of that rich proof data: identifiers, suggesting that modeling identifiers can play a significant role in improving proof synthesis, leading to higher-quality software.","score":3},{"url":"https://www.semanticscholar.org/paper/f7820b52e3cdff6625e6bd0430a8d48ca66cca3f","title":"Self-Programming Artificial Intelligence Using Code-Generating Language Models","venue":"","year":2022,"referenceCount":31,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/04/2022","authors":"Alex Sheng,Shankar Padmanabhan","id":"f7820b52e3cdff6625e6bd0430a8d48ca66cca3f","summary":"It is empirically show that a self-programming AI implemented using a code generation model can successfully modify its own source code to improve performance and program sub-models to perform auxiliary tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/9a7d4c2f4309a2a38ea8e5b23c6d616fa0952d44","title":"Neural language models for network configuration: Opportunities and reality check","venue":"Computer Communications","year":2022,"referenceCount":62,"citationCount":4,"influentialCitationCount":0,"publicationDate":"03/05/2022","authors":"Zied Ben-Houidi,Dario Rossi","id":"9a7d4c2f4309a2a38ea8e5b23c6d616fa0952d44","summary":"Recent advances in deep learning applied to programming languages, for the purpose of code verification, synthesis and translation are surveyed, in particularly, their training requirements and expected performance are reviewed, and qualitatively assess whether similar techniques can benefit corresponding use-cases in networking.","score":3},{"url":"https://www.semanticscholar.org/paper/0efa0441da820b1905572666ba1974a06a9663fb","title":"NaturalProver: Grounded Mathematical Proof Generation with Language Models","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":7,"influentialCitationCount":0,"publicationDate":"25/05/2022","authors":"S. Welleck,Jiacheng Liu,Ximing Lu,Hannaneh Hajishirzi,Yejin Choi","id":"0efa0441da820b1905572666ba1974a06a9663fb","summary":"N ATURAL P ROVER is capable of proving some theorems that require short (2-6 step) proofs, and providing next-step suggestions that are rated as correct and useful over 40% of the time, which is to the authors' knowledge the first demonstration of these capabilities using neural language models.","score":3},{"url":"https://www.semanticscholar.org/paper/0d08ffccc982781e310bb184397bbe64b9aef157","title":"Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models","venue":"International Computing Education Research Workshop","year":2022,"referenceCount":99,"citationCount":21,"influentialCitationCount":2,"publicationDate":"03/06/2022","authors":"Sami Sarsa,Paul Denny,Arto Hellas,Juho Leinonen","id":"0d08ffccc982781e310bb184397bbe64b9aef157","summary":"The analysis suggests that there is significant value in massive generative machine learning models as a tool for instructors, although there remains a need for some oversight to ensure the quality of the generated content before it is delivered to students.","score":3},{"url":"https://www.semanticscholar.org/paper/2edc8efcda27c944a46f367acf6a5280b8f65525","title":"FixEval: Execution-based Evaluation of Program Fixes for Competitive Programming Problems","venue":"ArXiv","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Md. Mahim Anjum Haque,W. Ahmad,Ismini Lourentzou,Chris Brown","id":"2edc8efcda27c944a46f367acf6a5280b8f65525","summary":"This work introduces F IX E VAL, a benchmark comprising of buggy code submissions to competitive programming problems and their respective ﬁxes, and believes it provides a step towards real-world automatic bugﬁxing and model-generated code evaluation.","score":3},{"url":"https://www.semanticscholar.org/paper/ab0e3d3e4d42369de5933a3b4c237780b41c0d77","title":"Solving Quantitative Reasoning Problems with Language Models","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":88,"influentialCitationCount":16,"publicationDate":"29/06/2022","authors":"Aitor Lewkowycz,Anders Andreassen,David Dohan,Ethan Dyer,H. Michalewski,V. Ramasesh,Ambrose Slone,Cem Anil,Imanol Schlag,Theo Gutman-Solo,Yuhuai Wu,Behnam Neyshabur,Guy Gur-Ari,Vedant Misra","id":"ab0e3d3e4d42369de5933a3b4c237780b41c0d77","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/99f85119f113b5498517928eff74a904b69e37b7","title":"CCTEST: Testing and Repairing Code Completion Systems","venue":"ArXiv","year":2022,"referenceCount":81,"citationCount":1,"influentialCitationCount":0,"publicationDate":"17/08/2022","authors":"Zongjie Li,Chaozheng Wang,Zhibo Liu,Haoxuan Wang,Shuai Wang,Cuiyun Gao","id":"99f85119f113b5498517928eff74a904b69e37b7","summary":"This research proposes CCT EST, a framework to test and repair code completion systems in black-box settings, which features a set of novel mutation strategies, namely program structure-consistent (PSC) mutations, to generate mutated code completion inputs.","score":3},{"url":"https://www.semanticscholar.org/paper/9360390b02b9a09ece9a2486055b17e18dc5d3f6","title":"Automatic Code Documentation Generation Using GPT-3","venue":"International Conference on Automated Software Engineering","year":2022,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/09/2022","authors":"Junaed Younus Khan,Gias Uddin","id":"9360390b02b9a09ece9a2486055b17e18dc5d3f6","summary":"Codec is a GPT-3 based model pre-trained on both natural and programming languages that outperforms existing techniques even with basic settings like one-shot learning and achieves an overall BLEU score of 20.6 for six different programming languages.","score":3},{"url":"https://www.semanticscholar.org/paper/2e72aaf89aea0ee494c6020ff537dd074586311e","title":"Code as Policies: Language Model Programs for Embodied Control","venue":"ArXiv","year":2022,"referenceCount":60,"citationCount":12,"influentialCitationCount":2,"publicationDate":"16/09/2022","authors":"Jacky Liang,Wenlong Huang,F. Xia,Peng Xu,Karol Hausman,Brian Ichter,Peter R. Florence,Andy Zeng","id":"2e72aaf89aea0ee494c6020ff537dd074586311e","summary":"This paper presents code as policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms.","score":3},{"url":"https://www.semanticscholar.org/paper/f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab","title":"Binding Language Models in Symbolic Languages","venue":"ArXiv","year":2022,"referenceCount":63,"citationCount":17,"influentialCitationCount":1,"publicationDate":"06/10/2022","authors":"Zhoujun Cheng,Tianbao Xie,Peng Shi,Chengzu Li,R.K. Nadkarni,Yushi Hu,Caiming Xiong,Dragomir R. Radev,M. Ostendorf,Luke Zettlemoyer,N. A. Smith,Tao Yu","id":"f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab","summary":"Binder is a training-free neural-symbolic framework that maps the task input to a program, which allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions.","score":3},{"url":"https://www.semanticscholar.org/paper/62f0db3a5ad5c795ec18fc7a6e7b01836809df57","title":"Language Models are Multilingual Chain-of-Thought Reasoners","venue":"ArXiv","year":2022,"referenceCount":50,"citationCount":19,"influentialCitationCount":2,"publicationDate":"06/10/2022","authors":"Freda Shi,Mirac Suzgun,Markus Freitag,Xuezhi Wang,Suraj Srivats,Soroush Vosoughi,Hyung Won Chung,Yi Tay,Sebastian Ruder,Denny Zhou,Dipanjan Das,Jason Wei","id":"62f0db3a5ad5c795ec18fc7a6e7b01836809df57","summary":"It is shown that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment, and that models have strikingly strong mult bilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili.","score":3},{"url":"https://www.semanticscholar.org/paper/0c78a473e33a81246d5c0fbbda7e7de168814c18","title":"FlexType: A Plug-and-Play Framework for Type Inference Models","venue":"International Conference on Automated Software Engineering","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Sivani Voruganti,Kevin Jesse,Prem Devanbu","id":"0c78a473e33a81246d5c0fbbda7e7de168814c18","summary":"This work introduces FlexType, an IDE extension that can be used on both JavaScript and TypeScript to infer types in an interactive or automatic fashion and believes the interactive Visual Studio Code extension is inherently useful in both TypeScript and JavaScript especially when resolving types is taxing for the developer.","score":3},{"url":"https://www.semanticscholar.org/paper/663a41c866d49ce052801fbc88947d39764cad29","title":"Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them","venue":"ArXiv","year":2022,"referenceCount":55,"citationCount":27,"influentialCitationCount":5,"publicationDate":"17/10/2022","authors":"Mirac Suzgun,Nathan Scales,Nathanael Scharli,Sebastian Gehrmann,Yi Tay,Hyung Won Chung,Aakanksha Chowdhery,Quoc V. Le,E. Chi,Denny Zhou,Jason Wei","id":"663a41c866d49ce052801fbc88947d39764cad29","summary":"It is found that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex to surpass it on 17 of the23 tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/8ffe7b7eeddbd4c22d642b0a48379d17e61c3bab","title":"Soft-Labeled Contrastive Pre-training for Function-level Code Representation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":42,"citationCount":2,"influentialCitationCount":0,"publicationDate":"18/10/2022","authors":"Xiaonan Li,Daya Guo,Yeyun Gong,Yun Lin,Yelong Shen,Xipeng Qiu,Daxin Jiang,Weizhu Chen,Nan Duan","id":"8ffe7b7eeddbd4c22d642b0a48379d17e61c3bab","summary":"This paper presents SCodeR, aoft-labeled contrastive pre-training framework with two positive sample construction methods to learn functional-level code representation and shows the effectiveness of the proposed pre- training method.","score":3},{"url":"https://www.semanticscholar.org/paper/4c2534f9b03ac2f3810c07abc398a11bcf47258e","title":"Transformers Learn Shortcuts to Automata","venue":"ArXiv","year":2022,"referenceCount":104,"citationCount":4,"influentialCitationCount":0,"publicationDate":"19/10/2022","authors":"Bingbin Liu,J. Ash,Surbhi Goel,A. Krishnamurthy,Cyril Zhang","id":"4c2534f9b03ac2f3810c07abc398a11bcf47258e","summary":"The theoretical results completely characterize shortcut solutions, whereby a shallow Transformer with only o(T ) layers can exactly replicate the computation of an automaton on an input sequence of length T .","score":3},{"url":"https://www.semanticscholar.org/paper/f031ba42cf82f106200bb03fbb91dd5671a59b9c","title":"Practical Program Repair in the Era of Large Pre-trained Language Models","venue":"ArXiv","year":2022,"referenceCount":69,"citationCount":4,"influentialCitationCount":0,"publicationDate":"25/10/2022","authors":"Chun Xia,Yuxiang Wei,Lingming Zhang","id":"f031ba42cf82f106200bb03fbb91dd5671a59b9c","summary":"This study demonstrates that directly applying state-of-the-art PLMs can already substantially outperform all existing APR techniques on all the authors' datasets and shows that PLM-based APR can be further substantially boosted via: increasing the sample size, and incorporating inﬁx template information.","score":3},{"url":"https://www.semanticscholar.org/paper/621009f1c30951b7c952c65c45ef0064a204e91e","title":"Early Experience with Transformer-Based Similarity Analysis for DataRaceBench","venue":"International Workshop on Software Correctness for HPC Applications","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Winson X. Chen,T. Vanderbruggen,Pei-Hung Lin,C. Liao,M. Emani","id":"621009f1c30951b7c952c65c45ef0064a204e91e","summary":"The challenges and the solutions when applying transformer-based similarity analysis to new source codes which are unseen by pre-trained transformers are explored, and comparative experiments of different variants of similarity analysis are used to comment on the strengths and limitations of the transformer- based approach and point out future research directions.","score":3},{"url":"https://www.semanticscholar.org/paper/327f1561b544b0a3b9d8d5d0e6d82c2a5911fca9","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model","venue":"ArXiv","year":2022,"referenceCount":157,"citationCount":71,"influentialCitationCount":14,"publicationDate":"09/11/2022","authors":"Teven Le Scao,Angela Fan,Christopher Akiki,Elizabeth-Jane Pavlick,Suzana Ili'c,Daniel Hesslow,Roman Castagn'e,A. Luccioni,Franccois Yvon,Matthias Gallé,J. Tow,Alexander M. Rush,Stella Rose Biderman,Albert Webson,Pawan Sasanka Ammanamanchi,Thomas Wang,Benoît Sagot,Niklas Muennighoff,Albert Villanova del Moral,Olatunji Ruwase,Rachel Bawden,Stas Bekman,Angelina McMillan-Major,Iz Beltagy,Huu Nguyen,Lucile Saulnier,Samson Tan,Pedro Ortiz Suarez,Victor Sanh,Hugo Laurenccon,Yacine Jernite,Julien Launay,Margaret Mitchell,Colin Raffel,Aaron Gokaslan,Adi Simhi,Aitor Soroa Etxabe,Alham Fikri Aji,Amit Alfassy,Anna Rogers,Ariel Kreisberg Nitzav,Canwen Xu,Chenghao Mou,Chris C. Emezue,Christopher Klamm,Colin Leong,Daniel Alexander van Strien,David Ifeoluwa Adelani,Dragomir R. Radev,Eduardo G. Ponferrada,Efrat Levkovizh,Ethan Kim,E. Natan,F. Toni,Gérard Dupont,Germán Kruszewski,Giada Pistilli,Hady ElSahar,Hamza Benyamina,Hieu Tran,Ian Yu,Idris Abdulmumin,Isaac Johnson,Itziar Gonzalez-Dios,Javier de la Rosa,Jenny Chim,Jesse Dodge,Jian Zhu,Jonathan Chang,Jorg Frohberg,Josephine L. Tobing,J. Bhattacharjee,Khalid Almubarak,Kimbo Chen,Kyle Lo,Leandro von Werra,Leon Weber,Long Phan,Loubna Ben Allal,Ludovic Tanguy,Manan Dey,M. Muñoz,Maraim Masoud,Mar'ia Grandury,Mario vSavsko,Max Huang,Maximin Coavoux,Mayank Singh,Mike Tian-Jian Jiang,Minh Chien Vu,M. A. Jauhar,Mustafa Ghaleb,Nishant Subramani,Nora Kassner,Nurulaqilla Khamis,Olivier Nguyen,Omar Espejel,Ona de Gibert,Paulo Villegas,Peter Henderson,Pierre Colombo,Priscilla Amuok,Quentin Lhoest,Rheza Harliman,Rishi Bommasani,R. L'opez,Rui Ribeiro,Salomey Osei,Sampo Pyysalo,Sebastian Nagel,Shamik Bose,Shamsuddeen Hassan Muhammad,Shanya Sharma,S. Longpre,Somaieh Nikpoor,Stanislav Silberberg,S. Pai,S. Zink,Tiago Timponi Torrent,Timo Schick,Tristan Thrush,V. Danchev,Vassilina Nikoulina,Veronika Laippala,Violette Lepercq,V. Prabhu,Zaid Alyafeai,Zeerak Talat,Arun Raja,Benjamin Heinzerling,Chenglei Si,Elizabeth Salesky,Sabrina J. Mielke,Wilson Y. Lee,Abheesht Sharma,Andrea Santilli,Antoine Chaffin,Arnaud Stiegler,Debajyoti Datta,Eliza Szczechla,Gunjan Chhablani,Han Wang,Harshit Pandey,Hendrik Strobelt,Jason Alan Fries,Jos Rozen,Leo Gao,Lintang Sutawika,M Saiful Bari,Maged S. Al-shaibani,Matteo Manica,Nihal V. Nayak,Ryan Teehan,Samuel Albanie,Sheng Shen,Srulik Ben-David,Stephen H. Bach,Taewoon Kim,T. Bers,Thibault Févry,Trishala Neeraj,Urmish Thakker,Vikas Raunak,Xiang Tang,Zheng Xin Yong,Zhiqing Sun,Shaked Brody,Y. Uri,Hadar Tojarieh,Adam Roberts,Hyung Won Chung,Jaesung Tae,Jason Phang,Ofir Press,Conglong Li,D. Narayanan,Hatim Bourfoune,J. Casper,Jeff Rasley,Max Ryabinin,Mayank Mishra,Minjia Zhang,M. Shoeybi,Myriam Peyrounette,N. Patry,Nouamane Tazi,Omar Sanseviero,Patrick von Platen,Pierre Cornette,Pierre Franccois Lavall'ee,R. Lacroix,Samyam Rajbhandari,Sanchit Gandhi,Shaden Smith,S. Requena,Suraj Patil,Tim Dettmers,Ahmed Baruwa,Amanpreet Singh,Anastasia Cheveleva,Anne-Laure Ligozat,Arjun Subramonian,Aur'elie N'ev'eol,Charles Lovering,Daniel H Garrette,D. Tunuguntla,Ehud Reiter,Ekaterina Taktasheva,E. Voloshina,Eli Bogdanov,Genta Indra Winata,Hailey Schoelkopf,Jan-Christoph Kalo,Jekaterina Novikova,J. Forde,Jordan Clive,Jungo Kasai,Ken Kawamura,Liam Hazan,Marine Carpuat,Miruna Clinciu,Najoung Kim,Newton Cheng,Oleg Serikov,Omer Antverg,Oskar van der Wal,Rui Zhang,Ruochen Zhang,Sebastian Gehrmann,S. Pais,Tatiana Shavrina,Thomas Scialom,Tian Yun,Tomasz Limisiewicz,Verena Rieser,Vitaly Protasov,V. Mikhailov,Yada Pruksachatkun,Yonatan Belinkov,Zachary Bamberger,Zdenvek Kasner,A. Rueda,A. Pestana,A. Feizpour,Ammar Khan,Amy Faranak,A. Santos,A. Hevia,Antigona Unldreaj,Arash Aghagol,Arezoo Abdollahi,A. Tammour,Azadeh HajiHosseini,Bahareh Behroozi,B. Ajibade,B. Saxena,Carlos Muñoz Ferrandis,Danish Contractor,D. Lansky,Davis David,Douwe Kiela,D. A. Nguyen,Edward Tan,Emily Baylor,Ezinwanne Ozoani,Fatim T Mirza,Frankline Ononiwu,Habib Rezanejad,H.A. Jones,Indrani Bhattacharya,Irene Solaiman,Irina Sedenko,I. Nejadgholi,J. Passmore,Joshua Seltzer,Julio Bonis Sanz,Karen Fort,L. Dutra,Mairon Samagaio,Maraim Elbadri,M. Mieskes,Marissa Gerchick,Martha Akinlolu,Michael McKenna,Mike Qiu,M. Ghauri,Mykola Burynok,Nafis Abrar,Nazneen Rajani,Nour Elkott,N. Fahmy,O. Samuel,Ran An,R. Kromann,Ryan Hao,S. Alizadeh,Sarmad Shubber,Silas L. Wang,Sourav Roy,S. Viguier,Thanh-Cong Le,Tobi Oyebade,T. Le,Yoyo Yang,Z. Nguyen,Abhinav Ramesh Kashyap,Alfredo Palasciano,A. Callahan,Anima Shukla,Antonio Miranda-Escalada,A. Singh,Benjamin Beilharz,Bo Wang,C. Brito,Chenxi Zhou,Chirag Jain,Chuxin Xu,Clémentine Fourrier,Daniel Le'on Perin'an,Daniel Molano,Dian Yu,Enrique Manjavacas,Fabio Barth,Florian Fuhrimann,Gabriel Altay,Giyaseddin Bayrak,Gully A. Burns,Helena U. Vrabec,I. Bello,Isha Dash,J. Kang,John Giorgi,J. Golde,J. Posada,Karthi Sivaraman,Lokesh Bulchandani,Lu Liu,Luisa Shinzato,Madeleine Hahn de Bykhovetz,Maiko Takeuchi,Marc Pàmies,M. A. Castillo,Marianna Nezhurina,Mario Sanger,M. Samwald,Michael Cullan,Michael Weinberg,M. Wolf,Mina Mihaljcic,Minna Liu,M. Freidank,Myungsun Kang,Natasha Seelam,N. Dahlberg,N. Broad,N. Muellner,Pascale Fung,Patricia Haller,R. Chandrasekhar,R. Eisenberg,Robert Martin,Rodrigo L. Canalli,Rosaline Su,Ruisi Su,Samuel Cahyawijaya,Samuele Garda,Shlok S Deshmukh,Shubhanshu Mishra,Sid Kiblawi,Simon Ott,Sinee Sang-aroonsiri,Srishti Kumar,Stefan Schweter,S. Bharati,T. A. Laud,Th'eo Gigant,Tomoya Kainuma,Wojciech Kusa,Yanis Labrak,Yashasvi Bajaj,Y. Venkatraman,Yifan Xu,Ying Xu,Yun-chao Xu,Z. Tan,Zhongli Xie,Zifan Ye,M. Bras,Younes Belkada,Thomas Wolf","id":"327f1561b544b0a3b9d8d5d0e6d82c2a5911fca9","summary":"BLOOM is a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers and achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning.","score":3},{"url":"https://www.semanticscholar.org/paper/a4bdc300db297756f36bedee2859b62df8e268c2","title":"Follow the Wisdom of the Crowd: Effective Text Generation via Minimum Bayes Risk Decoding","venue":"ArXiv","year":2022,"referenceCount":83,"citationCount":4,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"Mirac Suzgun,Luke Melas-Kyriazi,Dan Jurafsky","id":"a4bdc300db297756f36bedee2859b62df8e268c2","summary":"This work presents crowd sampling, a family of decoding methods based on Bayesian risk minimization, to ad-dress this diversity-quality trade-off in open-ended natural-language generation.","score":3},{"url":"https://www.semanticscholar.org/paper/0731c710fc09fe7c039aee3e9bff006bf94565aa","title":"XTest: A Parallel Multilingual Corpus with Test Cases for Code Translation and Its Evaluation*","venue":"2022 25th International Conference on Computer and Information Technology (ICCIT)","year":2022,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/12/2022","authors":"Israt Jahan Rithy,Hasib Hossain Shakil,Niloy Mondal,Fatema Sultana,F. Shah","id":"0731c710fc09fe7c039aee3e9bff006bf94565aa","summary":"This paper introduces XTest: A Parallel Multilingual Corpus with Test Cases for Code Translation, which contains parallel programs in 9 languages, Problem statement and test cases and built 30 systems to translate code between some high-resourced and low-resourcing programming languages.","score":3},{"url":"https://www.semanticscholar.org/paper/42630c03d3817b1153d245f20742ad4b30a80b75","title":"JEMMA: An Extensible Java Dataset for ML4Code Applications","venue":"ArXiv","year":2022,"referenceCount":93,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/12/2022","authors":"Anjan Karmakar,Miltiadis Allamanis,R. Robbes","id":"42630c03d3817b1153d245f20742ad4b30a80b75","summary":"JEMMA is introduced, which is a largescale, diverse, and high-quality dataset targeted at ML4Code applications, and becomes a workbench that researchers can use to experiment with novel representations and tasks operating on source code.","score":3},{"url":"https://www.semanticscholar.org/paper/bf5fbd690f24f873df86d1b0a06579cf42f7dc36","title":"Dialog2API: Task-Oriented Dialogue with API Description and Example Programs","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Raphael Shu,Elman Mansimov,Tamer Alkhouli,Nikolaos Pappas,Salvatore Romeo,Arshit Gupta,Saab Mansour,Yi Zhang,D. Roth","id":"bf5fbd690f24f873df86d1b0a06579cf42f7dc36","summary":"An approach tailored for the Dialog2API, where the dialogue states are represented by a stack of programs, with most recently mentioned program on the top of the stack, is proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/2abed82162c47a0cc32cd62afcf46b0745541017","title":"Experiences from Using Code Explanations Generated by Large Language Models in a Web Software Development E-Book","venue":"Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1","year":2022,"referenceCount":46,"citationCount":3,"influentialCitationCount":0,"publicationDate":"04/11/2022","authors":"S. MacNeil,Andrew Tran,Arto Hellas,Joanne Kim,Sami Sarsa,Paul Denny,Seth Bernstein,Juho Leinonen","id":"2abed82162c47a0cc32cd62afcf46b0745541017","summary":"The results show that all explanation types were viewed by students and that the majority of students perceived the code explanations as helpful to them, however, student engagement varied by code snippet complexity, explanation type, and code snippet length.","score":3},{"url":"https://www.semanticscholar.org/paper/dca3bc28a7d404b28780a813ea7072eda809e6c0","title":"Programming Is Hard - Or at Least It Used to Be: Educational Opportunities and Challenges of AI Code Generation","venue":"Proceedings of the 54th ACM Technical Symposium on Computer Science Education V. 1","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/12/2022","authors":"Brett A. Becker,Paul Denny,James Finnie-Ansley,Andrew Luxton-Reilly,J. Prather,E. Santos","id":"dca3bc28a7d404b28780a813ea7072eda809e6c0","summary":"It is argued that the community needs to act quickly in deciding what possible opportunities can and should be leveraged and how, while also working on overcoming otherwise mitigating the possible challenges.","score":3},{"url":"https://www.semanticscholar.org/paper/c532f1df90925e5c69789f0cd99248d8a2a2e5bc","title":"My AI Wants to Know if This Will Be on the Exam: Testing OpenAI’s Codex on CS2 Programming Exercises","venue":"IFAC Symposium on Advances in Control Education","year":2023,"referenceCount":27,"citationCount":3,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"James Finnie-Ansley,Paul Denny,Andrew Luxton-Reilly,E. Santos,J. Prather,Brett A. Becker","id":"c532f1df90925e5c69789f0cd99248d8a2a2e5bc","summary":"This paper presents results detailing how Codex performs on more advanced CS2 exam questions taken from past exams, and compares these results to those of students who took the same exams under normal conditions, demonstrating that Codex outscores most students.","score":3},{"url":"https://www.semanticscholar.org/paper/63396fceb84286b02796dc58e55c07ec1095c4dc","title":"FLAME: A small language model for spreadsheet formulas","venue":"ArXiv","year":2023,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Harshit Joshi,Abishai Ebenezer,J. Cambronero,Sumit Gulwani,Aditya Kanade,Vu Le,Ivan Radivcek,Gust Verbruggen","id":"63396fceb84286b02796dc58e55c07ec1095c4dc","summary":"This work presents FLAME, a T5-based model trained on Excel formulas that leverages domain insights to achieve competitive performance with a substantially smaller model (60M parameters) and two orders of magnitude less training data.","score":3},{"url":"https://www.semanticscholar.org/paper/782f3d43b37790a83c98d5fd3ef142b296f20616","title":"CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code Models","venue":"ArXiv","year":2023,"referenceCount":76,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"Changan Niu,Chuanyi Li,Vincent Ng,Bin Luo","id":"782f3d43b37790a83c98d5fd3ef142b296f20616","summary":"A large-scale benchmark that includes 216 existing code-related tasks is proposed and it is demonstrated that the cross-task generalization of models can be largely improved by in-context learning methods such as few-shot learning and learning from task instructions, which shows the promising prospects of conducting cross- task learning research on this benchmark.","score":3},{"url":"https://www.semanticscholar.org/paper/038f249ab708cebae2a58265b768b9b1cbadad3a","title":"The Impact of AI on Developer Productivity: Evidence from GitHub Copilot","venue":"ArXiv","year":2023,"referenceCount":17,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/02/2023","authors":"Sida Peng,Eirini Kalliamvakou,Peter Cihon,Mert Demirer","id":"038f249ab708cebae2a58265b768b9b1cbadad3a","summary":"Observed heterogenous effects show promise for AI pair programmers to help people transition into software development careers.","score":3},{"url":"https://www.semanticscholar.org/paper/f03f39f735186c4359b719724be6e1c2eb912fef","title":"Studying the effect of AI Code Generators on Supporting Novice Learners in Introductory Programming","venue":"ArXiv","year":2023,"referenceCount":93,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/02/2023","authors":"Majeed Kazemitabaar,J. Chow,Carl Ka To Ma,B. Ericson,David Weintrop,Tovi Grossman","id":"f03f39f735186c4359b719724be6e1c2eb912fef","summary":"Using OpenAI Codex significantly increased code-authoring performance while not decreasing performance on manual code-modification tasks, and learners with access to Codex during the training phase performed slightly better on the evaluation post-tests conducted one week later, although this difference did not reach statistical significance.","score":3},{"url":"https://www.semanticscholar.org/paper/9325c4bee1de9dc9b91cfb43fa65320817d2eea4","title":"Complex QA and language models hybrid architectures, Survey","venue":"ArXiv","year":2023,"referenceCount":346,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/02/2023","authors":"Xavier Daull,P. Bellot,Emmanuel Bruno,Vincent Martin,Elisabeth Murisasco","id":"9325c4bee1de9dc9b91cfb43fa65320817d2eea4","summary":"Current solutions and promising strategies are reviewed, using elements such as hybrid LLM architectures, human-in-the-loop reinforcement learning, prompting adaptation, neuro-symbolic and structured knowledge grounding, program synthesis, and others, and an overview of the current research and trends in the area of complex QA.","score":3},{"url":"https://www.semanticscholar.org/paper/cfc15e31cad7ae1c3f3337f675a3c57ac5614295","title":"Learning Deep Semantics for Test Completion","venue":"ArXiv","year":2023,"referenceCount":89,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/02/2023","authors":"Pengyu Nie,Rahul Banerjee,Junyi Jessy Li,R. Mooney,Miloš Gligorić","id":"cfc15e31cad7ae1c3f3337f675a3c57ac5614295","summary":"This work formalizes the novel task of test completion to automatically complete the next statement in a test method based on the context of prior statements and the code under test, and develops TeCo -- a deep learning model using code semantics for test completion.","score":3},{"url":"https://www.semanticscholar.org/paper/35d083ff4dc2805c477d0aedc8158fc7a1aefeda","title":"EvoPrompting: Language Models for Code-Level Neural Architecture Search","venue":"ArXiv","year":2023,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/02/2023","authors":"Angelica Chen,David Dohan,David R. So","id":"35d083ff4dc2805c477d0aedc8158fc7a1aefeda","summary":"EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.","score":3},{"url":"https://www.semanticscholar.org/paper/b0335eb2b9a1684fc16ff79234f0b206b30da169","title":"A Study of Editor Features in a Creative Coding Classroom","venue":"ArXiv","year":2023,"referenceCount":107,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"Andrew M. McNutt,Anton Outkine,Ravi Chugh","id":"b0335eb2b9a1684fc16ff79234f0b206b30da169","summary":"An experimental editor is implemented and used to teach a sequence of college and high-school creative coding courses to identify opportunities to improve creativity- and novice-focused IDEs and highlight ten-sions in their design.","score":2},{"url":"https://www.semanticscholar.org/paper/532f32be1e918d6b75650947318e57fc8f4fb415","title":"CodeRetriever: Unimodal and Bimodal Contrastive Learning","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":5,"influentialCitationCount":0,"publicationDate":2022,"authors":"Xiaonan Li,Yeyun Gong,Yelong Shen,Xipeng Qiu,Hang Zhang,Bolun Yao,Weizhen Qi,Daxin Jiang,Weizhu Chen,Nan Duan","id":"532f32be1e918d6b75650947318e57fc8f4fb415","summary":"The CodeRetriever model, which combines the unimodal and bimodal contrastive learning to train functionlevel code semantic representations, specifically for the code search task, achieves the new state-ofthe-art performance with significant improvement over existing code pre-trained models.","score":2},{"url":"https://www.semanticscholar.org/paper/b109588511459bf46e94cb4eb68b5cf79b092795","title":"Antecedent Predictions Are More Important Than You Think: An Effective Method for Tree-Based Code Generation","venue":"","year":2022,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/08/2022","authors":"Yihong Dong,Ge Li,Zhi Jin","id":"b109588511459bf46e94cb4eb68b5cf79b092795","summary":"This paper designs an AST-to-Vector (AST2Vec) method, that maps AST node positions to two-dimensional vectors, to model the position information of AST nodes, and implements and trains an Antecedent Prioritized Tree-based code generation model called APT, which improves the performance of existing Seq2Tree methods.","score":2},{"url":"https://www.semanticscholar.org/paper/5fd0533ba1068af1cb075d4eb71ee551691a71ac","title":"ExploitGen: Template-augmented exploit code generation based on CodeBERT","venue":"Journal of Systems and Software","year":2022,"referenceCount":38,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Guang Yang,Yu Zhou,Xiang Chen,Xiangyu Zhang,Tingting Han,Taolue Chen","id":"5fd0533ba1068af1cb075d4eb71ee551691a71ac","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/0bc2753f59e653de718b5c7a2a0a7e00d13778c7","title":"NL2CMD: An Updated Workflow for Natural Language to Bash Commands Translation","venue":"ArXiv","year":2023,"referenceCount":75,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/02/2023","authors":"Quchen Fu,Zhongwei Teng,Marco Georgaklis,Jules White,Douglas,C. Schmidt","id":"0bc2753f59e653de718b5c7a2a0a7e00d13778c7","summary":"A state-of-the-art translation model used to generate Bash Commands from the corresponding English text is described and a new NL2CMD dataset is introduced that is automatically generated, involves minimal human intervention, and is over six times larger than prior datasets.","score":2},{"url":"https://www.semanticscholar.org/paper/ce3f027b68dad014a58aa35f52380932c8d0b209","title":"Do Users Write More Insecure Code with AI Assistants?","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":6,"influentialCitationCount":0,"publicationDate":"07/11/2022","authors":"Neil Perry,Megha Srivastava,Deepak Kumar,D. Boneh","id":"ce3f027b68dad014a58aa35f52380932c8d0b209","summary":null,"score":2},{"url":"https://www.semanticscholar.org/paper/22df866f9605d27d1e5cca9b3ab721f33673e158","title":"ProgramTransformer: A tool for generating semantically equivalent transformed programs","venue":"Softw. Impacts","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/10/2022","authors":"Md Rafiqul Islam Rabin,Mohammad Amin Alipour","id":"22df866f9605d27d1e5cca9b3ab721f33673e158","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/4fbf624b9a848425c81f21f1171c32426541330f","title":"On the Reliability and Explainability of Automated Code Generation Approaches","venue":"ArXiv","year":2023,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/02/2023","authors":"Yue Liu,C. Tantithamthavorn,Yonghui Liu,Li Li","id":"4fbf624b9a848425c81f21f1171c32426541330f","summary":"A thorough empirical study of five code generation models on four representative code generation datasets to assess the limits and capabilities of automatic code generation approaches and reveals that state-of-the-art approaches suffer from severe data duplication and input insensitivity, which are subtle issues with significant implications.","score":2},{"url":"https://www.semanticscholar.org/paper/05ae2f22e150e47ff8030aa3024158a28c98d51d","title":"Do Machine Learning Models Produce TypeScript Types that Type Check?","venue":"ArXiv","year":2023,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/02/2023","authors":"Ming-Ho Yee,Arjun Guha","id":"05ae2f22e150e47ff8030aa3024158a28c98d51d","summary":"It is argued that accuracy can be misleading, and this paper presents TypeWeaver, a TypeScript type migration tool that can be used with an arbitrary type prediction model and evaluates it with three models from the literature.","score":2},{"url":"https://www.semanticscholar.org/paper/862c0b672c9defded3111924310a07760cfa27ff","title":"Can NMT Understand Me? Towards Perturbation-based Evaluation of NMT Models for Code Generation","venue":"2022 IEEE/ACM 1st International Workshop on Natural Language-Based Software Engineering (NLBSE)","year":2022,"referenceCount":58,"citationCount":1,"influentialCitationCount":0,"publicationDate":"29/03/2022","authors":"Pietro Liguori,Cristina Improta,S. D. Vivo,R. Natella,B. Cukic,Domenico Cotroneo","id":"862c0b672c9defded3111924310a07760cfa27ff","summary":"This work identifies a set of perturbations and metrics tailored for the robustness assessment of NMT models, and presents a preliminary experimental evaluation, showing what type of perturbedations affect the model the most and deriving useful insights for future directions.","score":2},{"url":"https://www.semanticscholar.org/paper/24c5450d8fa785e5f85d9427d2d65cf66476ac3a","title":"Toward General Design Principles for Generative AI Applications","venue":"ArXiv","year":2023,"referenceCount":115,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/01/2023","authors":"Justin D. Weisz,Michael J. Muller,Jessica He,Stephanie Houde","id":"24c5450d8fa785e5f85d9427d2d65cf66476ac3a","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/608df5bd7a0ad0f4d335ae4d071b8cfe60e2f3c5","title":"On the Effectiveness of Pretrained Models for API Learning","venue":"IEEE International Conference on Program Comprehension","year":2022,"referenceCount":51,"citationCount":1,"influentialCitationCount":0,"publicationDate":"05/04/2022","authors":"M. Hadi,Imam Nur Bani Yusuf,Ferdian Thung,K. Luong,Lingxiao Jiang,F. Fard,David Lo","id":"608df5bd7a0ad0f4d335ae4d071b8cfe60e2f3c5","summary":"This work uses a dataset that contains 7 million annotations collected from GitHub to evaluate the effectiveness of recent Pre-trained Transformer based Models (PTMs) for the API learning task and identifies two different tokenization approaches that can contribute to a significant boost in PTMs' performance for theAPI sequence generation task.","score":2},{"url":"https://www.semanticscholar.org/paper/2ad6b59ff14d6bec3f14d8b6480025bbebc50e46","title":"Optimal Neural Program Synthesis from Multimodal Specifications","venue":"Conference on Empirical Methods in Natural Language Processing","year":2020,"referenceCount":48,"citationCount":8,"influentialCitationCount":0,"publicationDate":"04/10/2020","authors":"Xi Ye,Qiaochu Chen,Işil Dillig,Greg Durrett","id":"2ad6b59ff14d6bec3f14d8b6480025bbebc50e46","summary":"The experimental results on a multimodal synthesis dataset show that the proposed optimal neural synthesis approach substantially outperforms prior state-of-the-art techniques in terms of accuracy %, finds model-optimal programs more frequently, and explores fewer states during search.","score":2},{"url":"https://www.semanticscholar.org/paper/a23e0cf74f10b6a3061ab71497fe2c8c476fecd1","title":"Conversational Automated Program Repair","venue":"ArXiv","year":2023,"referenceCount":30,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"Chun Xia,Lingming Zhang","id":"a23e0cf74f10b6a3061ab71497fe2c8c476fecd1","summary":null,"score":2},{"url":"https://www.semanticscholar.org/paper/8bfc22de7fe66286ad9ae705d677246757fbf8a8","title":"Large Language Models Can Be Easily Distracted by Irrelevant Context","venue":"ArXiv","year":2023,"referenceCount":57,"citationCount":1,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Freda Shi,Xinyun Chen,Kanishka Misra,Nathan Scales,David Dohan,E. Chi,Nathanael Scharli,Denny Zhou","id":"8bfc22de7fe66286ad9ae705d677246757fbf8a8","summary":"This work investigates the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context, and introduces Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description.","score":2},{"url":"https://www.semanticscholar.org/paper/d4f16dae4ab1d21db3c0b0bd7b54f4b62e2d1b85","title":"The Effectiveness of Transformer Models for Analyzing Low-Level Programs","venue":"","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Zifan Carl,William S. Moses","id":"d4f16dae4ab1d21db3c0b0bd7b54f4b62e2d1b85","summary":"It is shown that transformer models can translate C to LLVM-IR with high accuracy, by training on a parallel corpus of functions extract from 1 million compilable, open-sourced C programs (AnghaBench) and its corresponding LL VM-IR after compiling with Clang.","score":2},{"url":"https://www.semanticscholar.org/paper/cc80244d69c2c5bb7fe79fc6ca2561f2bed830ca","title":"Learning to Represent Programs with Heterogeneous Graphs","venue":"IEEE International Conference on Program Comprehension","year":2020,"referenceCount":70,"citationCount":16,"influentialCitationCount":3,"publicationDate":"28/09/2020","authors":"Wenhan Wang,Kechi Zhang,Ge Li,Zhi Jin","id":"cc80244d69c2c5bb7fe79fc6ca2561f2bed830ca","summary":"This paper proposes the heterogeneous program graph (HPG), which provides the types of the nodes and the edges explicitly, and employs the heterogeneity transformer (HGT) architecture to generate representations based on HPG, considering the type of information during processing.","score":2},{"url":"https://www.semanticscholar.org/paper/c125b0be73c8493ebc27beb572f6c1b21d6b4ae4","title":"Static Prediction of Runtime Errors by Learning to Execute Programs with External Resource Descriptions","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":2,"influentialCitationCount":0,"publicationDate":"07/03/2022","authors":"David Bieber,Rishab Goel,Daniel Zheng,H. Larochelle,Daniel Tarlow","id":"c125b0be73c8493ebc27beb572f6c1b21d6b4ae4","summary":"Surprisingly, it is shown that the model can also predict the location of the error, despite being trained only on labels indicating the presence/absence and kind of error.","score":2},{"url":"https://www.semanticscholar.org/paper/703f79763d534dbf9674132cec890f432dcc19ec","title":"A Survey of Deep Learning Models for Structural Code Understanding","venue":"ArXiv","year":2022,"referenceCount":180,"citationCount":1,"influentialCitationCount":0,"publicationDate":"03/05/2022","authors":"Ruoting Wu,Yu-xin Zhang,Qibiao Peng,Liang Chen,Zibin Zheng","id":"703f79763d534dbf9674132cec890f432dcc19ec","summary":"This survey presents a comprehensive overview of the structures formed from code data, categorizing the models for understanding code in recent years into two groups: sequence-based and graph-based models, and makes some suggestions for future research in structural code understanding field.","score":2},{"url":"https://www.semanticscholar.org/paper/5514b87e34db2b34bd9a9b995894243f91435efc","title":"Learning to Represent Programs with Code Hierarchies","venue":"ArXiv","year":2022,"referenceCount":66,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/05/2022","authors":"Minh Nguyen,Nghi D. Q. Bui","id":"5514b87e34db2b34bd9a9b995894243f91435efc","summary":"A method for representing code as a hierarchy ( Code Hierarchy), in which different code components are represented separately at various levels of granular- ity, and a novel pretraining objective called Miss- ing Subtree Prediction to complement this method.","score":2},{"url":"https://www.semanticscholar.org/paper/713bd2971116098211ef06336dfbe91a69854404","title":"Probing Semantic Grounding in Language Models of Code with Representational Similarity Analysis","venue":"International Conference on Advanced Data Mining and Applications","year":2022,"referenceCount":18,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/07/2022","authors":"Shounak Naik,Rajaswa Patil,Swati Agarwal,V. Baths","id":"713bd2971116098211ef06336dfbe91a69854404","summary":"This paper probes representations from the CodeBERT model for semantic grounding by using the data from the IBM CodeNet dataset, and shows that using bimodalinputs over unimodal inputs gives better semantic grounding and sample eﬃciency during semantic ﬁne-tuning.","score":2},{"url":"https://www.semanticscholar.org/paper/27e3ca4fd7b8290a0e12ca4fd2b7ad5bcd5900f2","title":"Are Transformers All That Karel Needs?","venue":"","year":2021,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Abhay Garg,Anand Sriraman,Kunal Pagarey,S. Karande","id":"27e3ca4fd7b8290a0e12ca4fd2b7ad5bcd5900f2","summary":"By changing the base architecture to a transformer based one, speciﬁcally GPT2, this work is able to apply simple execution guidance on top to achieve a generalization accurary of 89.64%, which is within 2.36 percentage points of the current state-of-the-art on Karel which uses ensembling.","score":2},{"url":"https://www.semanticscholar.org/paper/27d16a7f2ce8f2b787b34ff1f9b4fece079700c3","title":"Figuring out Figures: Using Textual References to Caption Scientific Figures","venue":"","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Stanley Cao,Kevin Liu","id":"27d16a7f2ce8f2b787b34ff1f9b4fece079700c3","summary":"This work uses the S CI C AP datasets curated by Hsu et al. and uses a variant of a CLIP+GPT-2 encoder-decoder model with cross-attention to generate captions conditioned on the image, and uses SciBERT to encode the textual metadata and uses this encoding alongside the figure embedding.","score":2},{"url":"https://www.semanticscholar.org/paper/317208b423d24d52ba04221cfb46956962364e22","title":"Extracting Meaningful Attention on Source Code: An Empirical Study of Developer and Neural Model Code Exploration","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/10/2022","authors":"Matteo Paltenghi,Rahul Pandita,Austin Z. Henley,Albert Ziegler","id":"317208b423d24d52ba04221cfb46956962364e22","summary":"This work empirically evaluates attention-agnostic heuris-tics and ten attention-based post processing approaches of the attention signal against the ground truth of developers exploring code, including the novel concept of follow-up attention which exhibits the highest agreement.","score":2},{"url":"https://www.semanticscholar.org/paper/e5993b3afe6384b5e6f90093989773ad1f868f71","title":"Towards Top-Down Deep Code Generation in Limited Scopes","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/09/2022","authors":"Jian Gu,H. Gall","id":"e5993b3afe6384b5e6f90093989773ad1f868f71","summary":"A semantic pyramid framework (SPF) is proposed as the approach, focusing on softwares of high modularity and low complexity, and introduces a three-layer semantic pyramid (SP) to associate text data and code data.","score":2},{"url":"https://www.semanticscholar.org/paper/bab6893ee48d168d27c227c3b0867f6d471fbea8","title":"Language Models are not Models of Language","venue":"ArXiv","year":2021,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"C. Veres","id":"bab6893ee48d168d27c227c3b0867f6d471fbea8","summary":"It is argued that the term language model is misleading because deep learning models are not theoretical models of language and proposed the adoption of corpus model instead, which better reflects the genesis and contents of the model.","score":2},{"url":"https://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5","title":"Measuring Mathematical Problem Solving With the MATH Dataset","venue":"NeurIPS Datasets and Benchmarks","year":2021,"referenceCount":65,"citationCount":94,"influentialCitationCount":20,"publicationDate":"05/03/2021","authors":"Dan Hendrycks,Collin Burns,Saurav Kadavath,Akul Arora,Steven Basart,Eric Tang,D. Song,J. Steinhardt","id":"57d1e7ac339e783898f2c3b1af55737cbeee9fc5","summary":"This work introduces MATH, a new dataset of 12, 500 challenging competition mathematics problems which can be used to teach models to generate answer derivations and explanations, and shows that accuracy remains relatively low, even with enormous Transformer models.","score":2},{"url":"https://www.semanticscholar.org/paper/98485ce6532d69f34a8ec67de6b09a39532bd221","title":"Communicating Natural Programs to Humans and Machines","venue":"ArXiv","year":2021,"referenceCount":87,"citationCount":9,"influentialCitationCount":0,"publicationDate":"15/06/2021","authors":"Samuel Acquaviva,Yewen Pu,Marta Kryven,Catherine Wong,Gabrielle Ecanow,Maxwell Nye,Theo Sechopoulos,Michael Henry Tessler,J. Tenenbaum","id":"98485ce6532d69f34a8ec67de6b09a39532bd221","summary":"LARC, the Language-complete ARC is presented, a collection of natural language descriptions by a group of human participants who instruct each other on how to solve ARC tasks using language alone, which contains successful instructions for 88% of the ARC tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/58a6ca2ae28a618126f71a07262cb958a8c37904","title":"Latent Execution for Neural Program Synthesis","venue":"","year":2021,"referenceCount":60,"citationCount":2,"influentialCitationCount":0,"publicationDate":"29/06/2021","authors":"Xinyun Chen,D. Song,Yuandong Tian","id":"58a6ca2ae28a618126f71a07262cb958a8c37904","summary":"LaSynth learns the latent representation to approximate the execution of partially generated programs, even if they are incomplete in syntax, and significantly improves the performance of next token prediction over existing approaches, facilitating search.","score":2},{"url":"https://www.semanticscholar.org/paper/5436193122dff271796bca07df7cecb7a8d6dea6","title":"Natural language-guided programming","venue":"SIGPLAN symposium on New ideas, new paradigms, and reflections on programming and software","year":2021,"referenceCount":53,"citationCount":6,"influentialCitationCount":1,"publicationDate":"11/08/2021","authors":"Geert Heyman,Rafael Huysegems,P. Justen,Tom Van Cutsem","id":"5436193122dff271796bca07df7cecb7a8d6dea6","summary":"The key idea is to adapt code autocompletion tools such that they take into account not only the developer’s already-written code but also the intent of the task the developer is trying to achieve next, formulated in plain natural language.","score":2},{"url":"https://www.semanticscholar.org/paper/4885e616e85d420576196b2578525cbc501137ec","title":"Programming and execution models for next generation code intelligence systems (keynote)","venue":"ESEC/SIGSOFT FSE","year":2021,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/08/2021","authors":"M. Mezini","id":"4885e616e85d420576196b2578525cbc501137ec","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/3f97c2067cde9377e50b3160bbd7982c94abd88a","title":"An Empirical Cybersecurity Evaluation of GitHub Copilot's Code Contributions","venue":"ArXiv","year":2021,"referenceCount":29,"citationCount":23,"influentialCitationCount":1,"publicationDate":2021,"authors":"H. Pearce,Baleegh Ahmad,Benjamin Tan,Brendan Dolan-Gavitt,R. Karri","id":"3f97c2067cde9377e50b3160bbd7982c94abd88a","summary":"This work systematically investigates the prevalence and conditions that can cause GitHub Copilot to recommend insecure code, and explores Copilot’s performance on three distinct code generation axes—examining how it performs given diversity of weaknesses, diversity of prompts, and diversity of domains.","score":2},{"url":"https://www.semanticscholar.org/paper/a176b0de62840f7118006277d94bbc1547162a4d","title":"Learning to Synthesize Programs as Interpretable and Generalizable Policies","venue":"Neural Information Processing Systems","year":2021,"referenceCount":131,"citationCount":15,"influentialCitationCount":2,"publicationDate":"31/08/2021","authors":"Dweep Trivedi,Jesse Zhang,Shao-Hua Sun,Joseph J. Lim","id":"a176b0de62840f7118006277d94bbc1547162a4d","summary":"Experimental results demonstrate that the proposed framework not only learns to reliably synthesize task-solving programs but also outperforms DRL and program synthesis baselines while producing interpretable and more generalizable policies.","score":2},{"url":"https://www.semanticscholar.org/paper/a8863de15a5ee8eed98107f423138a1a8f5a2ba8","title":"Multi-modal program inference: a marriage of pre-trained language models and component-based synthesis","venue":"Proc. ACM Program. Lang.","year":2021,"referenceCount":77,"citationCount":17,"influentialCitationCount":0,"publicationDate":"03/09/2021","authors":"Kia Rahmani,Mohammad Raza,Sumit Gulwani,Vu Le,Dan Morris,Arjun Radhakrishna,Gustavo Soares,A. Tiwari","id":"a8863de15a5ee8eed98107f423138a1a8f5a2ba8","summary":"This work presents an approach that combines PTMs with component-based synthesis (CBS): PTMs are used to generate candidates programs from the natural language description of the task, which are then used to guide the CBS procedure to find the program that matches the precise examples-based specification.","score":2},{"url":"https://www.semanticscholar.org/paper/bc9598dc4ed0472d8b59b87ed3a139f8347d40ee","title":"Towards A Measure Of General Machine Intelligence","venue":"ArXiv","year":2021,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/09/2021","authors":"Gautham Venkatasubramanian,Sibesh Kar,Abhimanyu Singh,Shubham Mishra,Dushyant Yadav,Shreyansh Chandak","id":"bc9598dc4ed0472d8b59b87ed3a139f8347d40ee","summary":"A common language of instruction is proposed, a programming language that allows the expression of programs in the form of directed acyclic graphs across a wide variety of real-world domains and computing platforms and evaluates the suitability of some well-known models as general intelligence systems by calculating their g-index scores.","score":2},{"url":"https://www.semanticscholar.org/paper/05c2e1ee203be217f100d2da05bdcc52004f00b6","title":"Unsolved Problems in ML Safety","venue":"ArXiv","year":2021,"referenceCount":217,"citationCount":69,"influentialCitationCount":6,"publicationDate":"28/09/2021","authors":"Dan Hendrycks,Nicholas Carlini,J. Schulman,J. Steinhardt","id":"05c2e1ee203be217f100d2da05bdcc52004f00b6","summary":"This work provides a new roadmap for ML Safety and presents four problems ready for research, namely withstanding hazards, identifying hazards, steering ML systems, and reducing deployment hazards.","score":2},{"url":"https://www.semanticscholar.org/paper/6c2d43e71e240e354b5790a38da78a291ceffe7c","title":"Learning to Superoptimize Real-world Programs","venue":"ArXiv","year":2021,"referenceCount":37,"citationCount":3,"influentialCitationCount":0,"publicationDate":"28/09/2021","authors":"Alex Shypula,P. Yin,Jeremy Lacomis,Claire Le Goues,E. Schwartz,Graham Neubig","id":"6c2d43e71e240e354b5790a38da78a291ceffe7c","summary":"A framework to learn to superoptimize real-world programs by using neural sequence-to-sequence models, and an approach to implement and outperforms a standard policy gradient learning approach on this dataset.","score":2},{"url":"https://www.semanticscholar.org/paper/dace03e57056d736f9e24937bdf486e894f8e866","title":"Is neural machine translation approach accurate enough for coding assistance?","venue":"BCNC@SPLASH","year":2021,"referenceCount":20,"citationCount":2,"influentialCitationCount":1,"publicationDate":"15/10/2021","authors":"Yuka Akinobu,Momoka Obara,Teruno Kajiura,Shiho Takano,Miyu Tamura,Mayu Tomioka,Kimio Kuramitsu","id":"dace03e57056d736f9e24937bdf486e894f8e866","summary":"A transcompiler-based back-translation, a data augmentation method that generates parallel corpora from numerous source code repositories and the resulting BLEU indicates that the proposed model is accurate enough to allow coding assistance in the future.","score":2},{"url":"https://www.semanticscholar.org/paper/21e8e76386aaaa00e0971af70ce84a8a544e1aa1","title":"Cascaded Fast and Slow Models for Efficient Semantic Code Search","venue":"ArXiv","year":2021,"referenceCount":24,"citationCount":4,"influentialCitationCount":0,"publicationDate":"15/10/2021","authors":"Akhilesh Deepak Gotmare,Junnan Li,Shafiq R. Joty,S. Hoi","id":"21e8e76386aaaa00e0971af70ce84a8a544e1aa1","summary":"An efficient and accurate semantic code search framework with cascaded fast and slow models, in which a fast transformer encoder model is learned to optimize a scalable index for fast retrieval followed by learning a slow classification-based re-ranking model to improve the performance of the top K results from the fast retrieval.","score":2},{"url":"https://www.semanticscholar.org/paper/570a6a5b8ec2827c3f33bb1b1bd027190a0d3e07","title":"Neural Program Generation Modulo Static Analysis","venue":"Neural Information Processing Systems","year":2021,"referenceCount":56,"citationCount":11,"influentialCitationCount":0,"publicationDate":"26/10/2021","authors":"Rohan Mukherjee,Yeming Wen,Dipak Chaudhari,T. Reps,Swarat Chaudhuri,C. Jermaine","id":"570a6a5b8ec2827c3f33bb1b1bd027190a0d3e07","summary":"The neurosymbolic method allows a deep generative model to symbolically compute, using calls to a static-analysis tool, long-distance semantic relationships in the code that it has already generated, and learns to generate programs conditioned on them.","score":2},{"url":"https://www.semanticscholar.org/paper/9bed7e0205963eaa332721b25f024cd1e876c30d","title":"User-Driven Support for Visualization Prototyping in D3","venue":"","year":2021,"referenceCount":75,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/12/2021","authors":"Hannah K. Bako,Alisha Varma,Anuoluwapo Faboro,Mahreen Haider,Favour Nerrise,B. Kenah,John P. Dickerson,L. Battle","id":"9bed7e0205963eaa332721b25f024cd1e876c30d","summary":"The structural similarities revealed by templates are used to design resilient support features for prototyping D3 visualizations: recommendations to suggest complementary interactions for a user's D3 program; and code augmentation to implement recommended interactions with a single click, even when users deviate from pre-defined templates.","score":2},{"url":"https://www.semanticscholar.org/paper/091fa84bdc07dcb22a34060c3996d8c58d71cd20","title":"Towards Neural Functional Program Evaluation","venue":"ArXiv","year":2021,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/12/2021","authors":"Torsten Scholak,Jonathan Pilault,Joey Velez-Ginorio","id":"091fa84bdc07dcb22a34060c3996d8c58d71cd20","summary":"A new program generation mechanism is introduced that allows control over syntactic sugar for semantically equivalent programs in transformer-based language models for program evaluation of simple functional programming languages.","score":2},{"url":"https://www.semanticscholar.org/paper/75c8f2916a2067f7549cf58ea8c9061565eb1dab","title":"C ROSS B EAM : L EARNING TO S EARCH IN B OTTOM -U P P ROGRAM S YNTHESIS","venue":"","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"H. Dai,Kevin Ellis,Charles Sutton","id":"75c8f2916a2067f7549cf58ea8c9061565eb1dab","summary":"This work uses a neural model to learn a hands-on search policy for bottom-up synthesis, instead of relying on a combinatorial search algorithm, and observes that CROSSBEAM learns to search efficiently, exploring much smaller portions of the program space compared to the state-of-the-art.","score":2},{"url":"https://www.semanticscholar.org/paper/9bf75110ea0923bbed49256b5491f1ec284019ec","title":"From BERT to GPT-3 Codex: Harnessing the Potential of Very Large Language Models for Data Management","venue":"Proceedings of the VLDB Endowment","year":2022,"referenceCount":35,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Immanuel Trummer","id":"9bf75110ea0923bbed49256b5491f1ec284019ec","summary":"The goal of the tutorial is to introduce database researchers to the latest generation of language models, and to their use cases in the domain of data management.","score":2},{"url":"https://www.semanticscholar.org/paper/ddab94478a7647ee136b1f6b5076417db3074d0f","title":"Machine Programming: Turning Data into Programmer Productivity","venue":"Proceedings of the VLDB Endowment","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"A. Wasay,Nesime Tatbul,Justin Emile Gottschlich","id":"ddab94478a7647ee136b1f6b5076417db3074d0f","summary":"An introduction to machine programming is introduced introducing its three pillars: intention, invention, and adaptation, and an overview of the data ecosystem central to all machine programming systems is provided, highlighting challenges and novel opportunities relevant to the data systems community.","score":2},{"url":"https://www.semanticscholar.org/paper/2443179d421e1faf7474add557b45add554723c7","title":"Formal Premise Selection With Language Models","venue":"","year":2022,"referenceCount":34,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Szymon Tworkowski,Maciej Miku la,Tomasz Odrzygóźdź,K. Czechowski,Szymon Antoniak,Albert Qiaochu Jiang,Christian Szegedy,Lukasz Kucinski,Piotr Mi loś,Yuhuai Wu","id":"2443179d421e1faf7474add557b45add554723c7","summary":"This work provides a solution to the problem of selecting a useful premise to prove a new theorem by combining a premise selection model with a language model, and shows that this retrieval-augmented prover achieves significant improvements in proof rates compared to the language model alone.","score":2},{"url":"https://www.semanticscholar.org/paper/b98d6fe8f0ef02ec0d1bb2bcfb924c8f01feb7d4","title":"Convergent Representations of Computer Programs in Human and Artificial Neural Networks","venue":"","year":2022,"referenceCount":79,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Shashank Srikant,Benjamin Lipkin,Anna A. Ivanova,Evelina Fedorenko,Una-May O’Reilly","id":"b98d6fe8f0ef02ec0d1bb2bcfb924c8f01feb7d4","summary":"Analysis of brain recordings derived from functional magnetic resonance imaging studies of programmers comprehending Python code suggests at least two distinct neural mechanisms mediating computer program comprehension and evaluation, prompting the design of code model objectives that go beyond static language modeling.","score":2},{"url":"https://www.semanticscholar.org/paper/15ef2d1b88f54fa32a32927463a7116219b89529","title":"L EARNING TO S UPEROPTIMIZE R EAL - WORLD P ROGRAMS","venue":"","year":null,"referenceCount":0,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"","id":"15ef2d1b88f54fa32a32927463a7116219b89529","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/4797e960f8e7a2b47ae0d95c7071ef84fa5d4b5b","title":"Code Generation Using Machine Learning: A Systematic Review","venue":"IEEE Access","year":2022,"referenceCount":117,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Enrique Dehaerne,Bappaditya Dey,Sandip Halder,S. De Gendt,Wannes Meert","id":"4797e960f8e7a2b47ae0d95c7071ef84fa5d4b5b","summary":"This review provides a broad and detailed overview of studies for code generation using ML, and summarizes the applications, models, datasets, results, limitations, and future work of 37 publications.","score":2},{"url":"https://www.semanticscholar.org/paper/660ca9e15e19409903a0605f0584d0f263c35c67","title":"S YNCHROMESH : R ELIABLE C ODE G ENERATION FROM P RE - TRAINED L ANGUAGE M ODELS","venue":"","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Gabriel Poesia,A. Tiwari,Gustavo Soares,Christopher Meek","id":"660ca9e15e19409903a0605f0584d0f263c35c67","summary":"A framework for substantially improving the reliability of pre-trained models for code generation and observing substantial complementary gains from CSD and TST in prediction accuracy and in effectively preventing run-time errors is proposed.","score":2},{"url":"https://www.semanticscholar.org/paper/ba5d21b7c65c6598c7bd39a5d992308c205df374","title":"A S YSTEMATIC E VALUATION OF L ARGE L ANGUAGE M ODELS OF C ODE","venue":"","year":2022,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Graham Neubig,V. Hellendoorn","id":"ba5d21b7c65c6598c7bd39a5d992308c205df374","summary":"It is found that existing open-source models do achieve close results in some programming languages, although targeted mainly for natural language modeling, and a new model, PolyCoder, is released that was trained on 249GB of code across 12 programming languages on a single machine.","score":2},{"url":"https://www.semanticscholar.org/paper/6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","title":"Asleep at the Keyboard? Assessing the Security of GitHub Copilot’s Code Contributions","venue":"IEEE Symposium on Security and Privacy","year":2021,"referenceCount":35,"citationCount":38,"influentialCitationCount":6,"publicationDate":"20/08/2021","authors":"H. Pearce,Baleegh Ahmad,Benjamin Tan,Brendan Dolan-Gavitt,R. Karri","id":"6e5eb6167d9766fbb4d14611d15cf95d7b75fb9d","summary":"This work systematically investigates the prevalence and conditions that can cause GitHub Copilot to recommend insecure code, and explores Copilot’s performance on three distinct code generation axes—examining how it performs given diversity of weaknesses, diversity of prompts, and diversity of domains.","score":2},{"url":"https://www.semanticscholar.org/paper/ff0b2681d7b05e16c46dfb71d980cc2f605907cd","title":"Finetuned Language Models Are Zero-Shot Learners","venue":"International Conference on Learning Representations","year":2021,"referenceCount":167,"citationCount":369,"influentialCitationCount":88,"publicationDate":"03/09/2021","authors":"Jason Wei,Maarten Bosma,Vincent Zhao,Kelvin Guu,A. Yu,Brian Lester,Nan Du,Andrew M. Dai,Quoc V. Le","id":"ff0b2681d7b05e16c46dfb71d980cc2f605907cd","summary":"It is shown that instruction tuning —ﬁnetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks and outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze.","score":2},{"url":"https://www.semanticscholar.org/paper/6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a","title":"Few-Shot Semantic Parsing with Language Models Trained on Code","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":18,"citationCount":19,"influentialCitationCount":3,"publicationDate":"16/12/2021","authors":"Richard Shin,Benjamin Van Durme","id":"6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a","summary":"This paper evaluates OpenAI Codex on Overnight and SMCalFlow and finds that unlike GPT-3, Codex performs similarly when targeting meaning representations directly, perhaps because meaning representations are structured similar to code in these datasets.","score":2},{"url":"https://www.semanticscholar.org/paper/53c0abe83fe9b4fdaf2208295d8504fcf5241694","title":"UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":118,"citationCount":96,"influentialCitationCount":13,"publicationDate":"16/01/2022","authors":"Tianbao Xie,Chen Henry Wu,Peng Shi,Ruiqi Zhong,Torsten Scholak,Michihiro Yasunaga,Chien-Sheng Wu,Ming Zhong,Pengcheng Yin,Sida I. Wang,Victor Zhong,Bailin Wang,Chengzu Li,Connor Boyle,Ansong Ni,Ziyu Yao,Dragomir R. Radev,Caiming Xiong,Lingpeng Kong,Rui Zhang,Noah A. Smith,Luke Zettlemoyer,Tao Yu","id":"53c0abe83fe9b4fdaf2208295d8504fcf5241694","summary":"The UnifiedSKG framework is proposed, which unifies 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset.","score":2},{"url":"https://www.semanticscholar.org/paper/92a8f7f09f3705cb5a6009a42220a6f01ea084e8","title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents","venue":"International Conference on Machine Learning","year":2022,"referenceCount":55,"citationCount":100,"influentialCitationCount":13,"publicationDate":"18/01/2022","authors":"Wenlong Huang,P. Abbeel,Deepak Pathak,Igor Mordatch","id":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","summary":"This paper investigates the possibility of grounding high-level tasks, expressed in natural language, to a chosen set of actionable steps and proposes a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions.","score":2},{"url":"https://www.semanticscholar.org/paper/b62d63580b81a2cbb20c3c1593dd62d118e4cb07","title":"Synchromesh: Reliable code generation from pre-trained language models","venue":"International Conference on Learning Representations","year":2022,"referenceCount":31,"citationCount":33,"influentialCitationCount":5,"publicationDate":"26/01/2022","authors":"Gabriel Poesia,Oleksandr Polozov,Vu Le,A. Tiwari,Gustavo Soares,Christopher Meek,Sumit Gulwani","id":"b62d63580b81a2cbb20c3c1593dd62d118e4cb07","summary":"A framework for substantially improving the reliability of pre-trained models for code generation and observing substantial complementary gains from CSD and TST in prediction accuracy and in effectively preventing run-time errors is proposed.","score":2},{"url":"https://www.semanticscholar.org/paper/7428f9b16a82839e2cb6e6c7a77c1ffeab898813","title":"HEAT: Hyperedge Attention Networks","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":3,"influentialCitationCount":0,"publicationDate":"28/01/2022","authors":"Dobrik Georgiev,Marc Brockschmidt,Miltiadis Allamanis","id":"7428f9b16a82839e2cb6e6c7a77c1ffeab898813","summary":"This work presents HEAT, a neural model capable of representing typed and qualified hypergraphs, where each hyperedge explicitly qualifies how participating nodes con-tribute is treated, which can be viewed as a generalization of both message passing neural networks and Transformers.","score":2},{"url":"https://www.semanticscholar.org/paper/1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models","venue":"ArXiv","year":2022,"referenceCount":103,"citationCount":371,"influentialCitationCount":99,"publicationDate":"28/01/2022","authors":"Jason Wei,Xuezhi Wang,Dale Schuurmans,Maarten Bosma,E. Chi,Quoc Le,Denny Zhou","id":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","summary":"Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/763792c655e591c5d61f67d7ac9cbecbcb5f4508","title":"Pop Quiz! Can a Large Language Model Help With Reverse Engineering?","venue":"ArXiv","year":2022,"referenceCount":27,"citationCount":4,"influentialCitationCount":0,"publicationDate":"02/02/2022","authors":"H. Pearce,B. Tan,P. Krishnamurthy,F. Khorrami,R. Karri,Brendan Dolan-Gavitt","id":"763792c655e591c5d61f67d7ac9cbecbcb5f4508","summary":"An extensive quantitative analysis of the measured performance of the language model on a set of program purpose identification and information extraction tasks shows that LLMs are not yet ready for zero-shot reverse engineering.","score":2},{"url":"https://www.semanticscholar.org/paper/9ac5e1ffa8f837671a89354d4e298b1adeb08d79","title":"Enabling Automatic Repair of Source Code Vulnerabilities Using Data-Driven Methods","venue":"2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/02/2022","authors":"Anastasiia Grishina","id":"9ac5e1ffa8f837671a89354d4e298b1adeb08d79","summary":"This work proposes ways to improve code representations for vulnerability repair from three perspectives: input data type, data-driven models, and downstream tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/9cbc044e315cdefe9a255119037ac7c23e9abdd5","title":"Predictability and Surprise in Large Generative Models","venue":"Conference on Fairness, Accountability and Transparency","year":2022,"referenceCount":92,"citationCount":31,"influentialCitationCount":2,"publicationDate":"15/02/2022","authors":"Deep Ganguli,Danny Hernandez,Liane Lovitt,Nova DasSarma,T. Henighan,Andy Jones,Nicholas Joseph,John Kernion,Benjamin Mann,Amanda Askell,Yuntao Bai,Anna Chen,Tom Conerly,Dawn Drain,Nelson Elhage,Sheer El Showk,Stanislav Fort,Zac Hatfield-Dodds,Scott Johnston,S. Kravec,Neel Nanda,Kamal Ndousse,Catherine Olsson,Daniela Amodei,Dario Amodei,Tom B. Brown,Jared Kaplan,Sam McCandlish,C. Olah,Jack Clark","id":"9cbc044e315cdefe9a255119037ac7c23e9abdd5","summary":"This paper highlights a counterintuitive property of large-scale generative models, which have a paradoxical combination of predictable loss on a broad training distribution, and unpredictable specific capabilities, inputs, and outputs, and analyzed how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment.","score":2},{"url":"https://www.semanticscholar.org/paper/76f023c3a819fc58989a064a1b50825b11fce95d","title":"Capturing Failures of Large Language Models via Human Cognitive Biases","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":7,"influentialCitationCount":0,"publicationDate":"24/02/2022","authors":"Erik Jones,J. Steinhardt","id":"76f023c3a819fc58989a064a1b50825b11fce95d","summary":"The results indicate that experimental methodology from cognitive science can help characterize how machine learning systems behave, and draw inspiration from human cognitive biases as motivation to generate hypotheses for problems that models may have and develop experiments that elicit these problems.","score":2},{"url":"https://www.semanticscholar.org/paper/d26fe2a7a7cc940d8485488e97460b144dc7d69e","title":"From Natural Language to Simulations: Applying GPT-3 Codex to Automate Simulation Modeling of Logistics Systems","venue":"SSRN Electronic Journal","year":2022,"referenceCount":60,"citationCount":2,"influentialCitationCount":0,"publicationDate":"24/02/2022","authors":"I. Jackson,M. J. Sáenz","id":"d26fe2a7a7cc940d8485488e97460b144dc7d69e","summary":"This work demonstrated that the framework built on top of the GPT-3 Codex, a Transformer-based language model, could produce functionally valid simulations of queuing and inventory control systems given the verbal description.","score":2},{"url":"https://www.semanticscholar.org/paper/2b556fcf2ac634f03c1fb0ace5e602e829418e65","title":"ReACC: A Retrieval-Augmented Code Completion Framework","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":70,"citationCount":15,"influentialCitationCount":2,"publicationDate":"15/03/2022","authors":"Shuai Lu,Nan Duan,Hojae Han,Daya Guo,Seung-won Hwang,Alexey Svyatkovskiy","id":"2b556fcf2ac634f03c1fb0ace5e602e829418e65","summary":"This work proposes a retrieval-augmented code completion framework, leveraging both lexical copying and referring to code with similar semantics by retrieval, and adopts a stage-wise training approach that combines a source code retriever and an auto-regressive language model for programming language.","score":2},{"url":"https://www.semanticscholar.org/paper/c347093e2dca530ce347526380b0b7aedf03a6b2","title":"CrossBeam: Learning to Search in Bottom-Up Program Synthesis","venue":"International Conference on Learning Representations","year":2022,"referenceCount":56,"citationCount":5,"influentialCitationCount":0,"publicationDate":"20/03/2022","authors":"Kensen Shi,H. Dai,Kevin Ellis,Charles Sutton","id":"c347093e2dca530ce347526380b0b7aedf03a6b2","summary":"This work uses a neural model to learn a hands-on search policy for bottom-up synthesis, instead of relying on a combinatorial search algorithm, and observes that CROSSBEAM learns to search efficiently, exploring much smaller portions of the program space compared to the state-of-the-art.","score":2},{"url":"https://www.semanticscholar.org/paper/fb5c11bbf63884f75d2da615fbf37a3bcfa2bd20","title":"Wordcraft: Story Writing With Large Language Models","venue":"International Conference on Intelligent User Interfaces","year":2022,"referenceCount":41,"citationCount":19,"influentialCitationCount":1,"publicationDate":"22/03/2022","authors":"Ann Yuan,Andy Coenen,Emily Reif,Daphne Ippolito","id":"fb5c11bbf63884f75d2da615fbf37a3bcfa2bd20","summary":"This work built Wordcraft, a text editor in which users collaborate with a generative language model to write a story, and shows that large language models enable novel co-writing experiences.","score":2},{"url":"https://www.semanticscholar.org/paper/237f5ca6fcccef2b77a2212b34fb06a1dbd09b72","title":"Evaluating Prompts Across Multiple Choice Tasks In a Zero-Shot Setting","venue":"ArXiv","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/03/2022","authors":"Gabriel Orlanski","id":"237f5ca6fcccef2b77a2212b34fb06a1dbd09b72","summary":"Collect and standardize prompts from a diverse range of tasks for use with tasks they were not designed for and evaluate these prompts across multiple choice datasets for a quantitative analysis of how certain attributes of a prompt affect performance.","score":2},{"url":"https://www.semanticscholar.org/paper/89e78d6f76b70c30804ecd3592fa05fccdc49b64","title":"Fix Bugs with Transformer through a Neural-Symbolic Edit Grammar","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/04/2022","authors":"Yaojie Hu,Xingjian Shi,Qiang Zhou,Lee Pike","id":"89e78d6f76b70c30804ecd3592fa05fccdc49b64","summary":"NSEdit per- 017 forms robustly when programs vary from pack- 018 ages to packages and when buggy programs are 019 concrete, and achieved a new state-of-the-art 015 accuracy on the Tufano small dataset 016 of the CodeXGLUE benchmark.","score":2},{"url":"https://www.semanticscholar.org/paper/012378718c34f0b17b3fcd7316371f8f4e4fdde2","title":"Addressing Leakage in Self-Supervised Contextualized Code Retrieval","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/04/2022","authors":"Johannes Villmow,Viola Campos,A. Ulges,Ulrich Schwanecke","id":"012378718c34f0b17b3fcd7316371f8f4e4fdde2","summary":"This work proposes a novel approach based on mutual identifier masking, dedentation, and the selection of syntax-aligned targets for contextualized code retrieval that improves retrieval substantially, and yields new state-of-the-art results for code clone and defect detection.","score":2},{"url":"https://www.semanticscholar.org/paper/2ba7104f7b93d77940312664f3467b8f090d6d16","title":"On Distribution Shift in Learning-based Bug Detectors","venue":"International Conference on Machine Learning","year":2022,"referenceCount":49,"citationCount":5,"influentialCitationCount":1,"publicationDate":"21/04/2022","authors":"Jingxuan He,Luca Beurer-Kellner,Martin T. Vechev","id":"2ba7104f7b93d77940312664f3467b8f090d6d16","summary":"This work proposes to train a bug detector in two phases, first on a synthetic bug distribution to adapt the model to the bug detection domain, and then on a real bug distributionTo drive the model towards the real distribution, which leverage a multi-task hierarchy, focal loss, and contrastive learning to further boost performance.","score":2},{"url":"https://www.semanticscholar.org/paper/6050454e0446a3068617f73b0301453f3f67844d","title":"Stylette: Styling the Web with Natural Language","venue":"International Conference on Human Factors in Computing Systems","year":2022,"referenceCount":65,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/04/2022","authors":"Tae Soo Kim,Yoonseo Choi,D. Choi,Juho Kim","id":"6050454e0446a3068617f73b0301453f3f67844d","summary":"Stylette is a browser extension that enables users to change the style of websites by expressing goals in natural language, and shows that Stylette lowered the learning curve, helping participants perform styling changes 35% faster than those using developer tools.","score":2},{"url":"https://www.semanticscholar.org/paper/4054fc9e8776dc0324cfc215462d606eb75916c0","title":"Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models","venue":"CHI Extended Abstracts","year":2022,"referenceCount":53,"citationCount":40,"influentialCitationCount":6,"publicationDate":"27/04/2022","authors":"Ganesha Upadhyaya,Anastasia Reinhardt,Hridesh Rajan,Miryung Kim,Elena L. Glassman,B. Hartmann,Joseph Pinedo","id":"4054fc9e8776dc0324cfc215462d606eb75916c0","summary":"It was found that, while Copilot did not necessarily improve the task completion time or success rate, most participants preferred to use Copilot in daily programming tasks, since Copilot often provided a useful starting point and saved the effort of searching online.","score":2},{"url":"https://www.semanticscholar.org/paper/4bc040835fbff57ce6612306d794b8c6c8226086","title":"Discovering the Syntax and Strategies of Natural Language Programming with Generative Language Models","venue":"International Conference on Human Factors in Computing Systems","year":2022,"referenceCount":36,"citationCount":12,"influentialCitationCount":2,"publicationDate":"29/04/2022","authors":"Ellen Jiang,Edwin Toh,A. Molina,Kristen Olson,Claire Kayacik,Aaron Donsbach,Carrie J. Cai,Michael Terry","id":"4bc040835fbff57ce6612306d794b8c6c8226086","summary":"A natural language code synthesis tool, GenLine, backed by a large generative language model and a set of task-specific prompts that create or change code is presented, indicating that while naturallanguage code synthesis can sometimes provide a magical experience, participants still faced challenges.","score":2},{"url":"https://www.semanticscholar.org/paper/f8aa0cab09bc0668276e2cb5690bae47fa50d350","title":"An Initial Look at Self-Reprogramming Artificial Intelligence","venue":"ArXiv","year":2022,"referenceCount":14,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Alex Sheng","id":"f8aa0cab09bc0668276e2cb5690bae47fa50d350","summary":"This paper develops and experimentally validate the first fully self-reprogramming AI system with the ability to continuously modify and rewrite its own neural network source code.","score":2},{"url":"https://www.semanticscholar.org/paper/1f3a97d255a6d0b2d5535af75737ed43b1e96fc4","title":"Is GitHub Copilot a Substitute for Human Pair-programming? An Empirical Study","venue":"2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)","year":2022,"referenceCount":11,"citationCount":11,"influentialCitationCount":0,"publicationDate":"01/05/2022","authors":"Saki Imai","id":"1f3a97d255a6d0b2d5535af75737ed43b1e96fc4","summary":"The results suggest that although Copilot increases productivity as measured by Lines of code added, the quality of code produced is inferior by having more lines of code deleted in the subsequent trial.","score":2},{"url":"https://www.semanticscholar.org/paper/cdfe9580f63070f311151444f9df32818cc858bf","title":"An Empirical Evaluation of GitHub Copilot's Code Suggestions","venue":"IEEE Working Conference on Mining Software Repositories","year":2022,"referenceCount":29,"citationCount":22,"influentialCitationCount":3,"publicationDate":"01/05/2022","authors":"N. Nguyen,Sarah Nadi","id":"cdfe9580f63070f311151444f9df32818cc858bf","summary":"Overall, Copilot's suggestions have low complexity with no notable differences between the programming languages and some potential Copilot shortcomings are found.","score":2},{"url":"https://www.semanticscholar.org/paper/5922f437512158970c417f4413bface021df5f78","title":"A Generalist Agent","venue":"ArXiv","year":2022,"referenceCount":102,"citationCount":147,"influentialCitationCount":23,"publicationDate":"12/05/2022","authors":"S. Reed,Konrad Zolna,Emilio Parisotto,Sergio Gomez Colmenarejo,Alexander Novikov,Gabriel Barth-Maron,Mai Gimenez,Yury Sulsky,Jackie Kay,J. T. Springenberg,Tom Eccles,Jake Bruce,Ali Razavi,Ashley D. Edwards,N. Heess,Yutian Chen,R. Hadsell,Oriol Vinyals,Mahyar Bordbar,N. D. Freitas","id":"5922f437512158970c417f4413bface021df5f78","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/c61ce808818308566124df2c8725c98d6bd38dc3","title":"A Precis of Language Models are not Models of Language","venue":"ArXiv","year":2022,"referenceCount":16,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/05/2022","authors":"C. Veres","id":"c61ce808818308566124df2c8725c98d6bd38dc3","summary":"It is shown that despite their many successes at performing linguistic tasks, Large Neural Language Models are ill suited as comprehensive models of natural language.","score":2},{"url":"https://www.semanticscholar.org/paper/58aacb967cc7fc25cfc9d51b7ad3e57ac00d119b","title":"Can Foundation Models Wrangle Your Data?","venue":"Proceedings of the VLDB Endowment","year":2022,"referenceCount":92,"citationCount":7,"influentialCitationCount":1,"publicationDate":"20/05/2022","authors":"A. Narayan,Ines Chami,Laurel J. Orr,Christopher R'e","id":"58aacb967cc7fc25cfc9d51b7ad3e57ac00d119b","summary":"It is found that large FMs generalize and achieve SoTA performance on data cleaning and integration tasks, even though they are not trained for these data tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/f1b6b34b4440a77ba86493f7062e8974062508c5","title":"Applying genetic programming to PSB2: the next generation program synthesis benchmark suite","venue":"Genetic Programming and Evolvable Machines","year":2022,"referenceCount":62,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/06/2022","authors":"Thomas Helmuth,Peter Kelly","id":"f1b6b34b4440a77ba86493f7062e8974062508c5","summary":"25 new general program synthesis benchmark problems that make up PSB2, a new benchmark suite curated from a variety of sources, including programming katas and college courses are described.","score":2},{"url":"https://www.semanticscholar.org/paper/8c7fd98b6bc4f32772e76471afe8babc323f10d3","title":"CitySpec: An Intelligent Assistant System for Requirement Specification in Smart Cities","venue":"International Conference on Smart Computing","year":2022,"referenceCount":18,"citationCount":4,"influentialCitationCount":0,"publicationDate":"01/06/2022","authors":"Zirong Chen,Isaac Li,Haoxiang Zhang,S. Preum,J. Stankovic,Meiyi Ma","id":"8c7fd98b6bc4f32772e76471afe8babc323f10d3","summary":"This work builds CitySpec, the first intelligent assistant system for requirement specification in smart cities, and builds a translation model and enhance it through requirement synthesis and develops a novel online learning framework with validation under uncertainty.","score":2},{"url":"https://www.semanticscholar.org/paper/40edfa97cd02268fccff75eb9c693b11c1a968b2","title":"Formal Specifications from Natural Language","venue":"ArXiv","year":2022,"referenceCount":97,"citationCount":2,"influentialCitationCount":0,"publicationDate":"04/06/2022","authors":"Christopher Hahn,Frederik Schmitt,Julia J. Tillman,Niklas Metzger,Julian Siber,B. Finkbeiner","id":"40edfa97cd02268fccff75eb9c693b11c1a968b2","summary":"These experiments show that language models maintain their generalization capabilities from pre-trained knowledge of natural language to generalize, e.g., to new variable names or operator descriptions, and achieve competitive performance, and even outperform the state-of-the-art for translating into regular expressions.","score":2},{"url":"https://www.semanticscholar.org/paper/a64871352f8ac7f8aa226fda5cce70251a18a4fc","title":"Assessing Project-Level Fine-Tuning of ML4SE Models","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/06/2022","authors":"Egor Bogomolov,Sergey Zhuravlev,Egor Spirin,T. Bryksin","id":"a64871352f8ac7f8aa226fda5cce70251a18a4fc","summary":"It is shown that per-project fine-tuning can greatly improve the models’ quality as they capture the project’s domain and naming conventions.","score":2},{"url":"https://www.semanticscholar.org/paper/9aed848be4e9e401b0e61a2e5d60dbdafa0c6cc1","title":"A Dataset and Benchmark for Automatically Answering and Generating Machine Learning Final Exams","venue":"ArXiv","year":2022,"referenceCount":15,"citationCount":2,"influentialCitationCount":1,"publicationDate":"11/06/2022","authors":"Sarah Zhang,Reece Shuttleworth,Derek Austin,Yann Hicke,Leonard Tang,Sathwik Karnik,Darnell Granberry,Iddo Drori","id":"9aed848be4e9e401b0e61a2e5d60dbdafa0c6cc1","summary":"A student survey comparing the quality, appropriateness, andulty of machine- generated questions with human-written questions shows that across multiple aspects, machine-generated questions are indistinguishable from human-generated Questions and are suitable for ﬁnal exams.","score":2},{"url":"https://www.semanticscholar.org/paper/45263786d07f5751f7494fdeee3c8764836d02c4","title":"NatGen: generative pre-training by “naturalizing” source code","venue":"ESEC/SIGSOFT FSE","year":2022,"referenceCount":66,"citationCount":12,"influentialCitationCount":0,"publicationDate":"15/06/2022","authors":"Saikat Chakraborty,Toufique Ahmed,Yangruibo Ding,Prem Devanbu,Baishakhi Ray","id":"45263786d07f5751f7494fdeee3c8764836d02c4","summary":"This paper proposes a new pre-training objective, “Naturalizing” of source code, exploiting code’s bimodal, dual-channel (formal & natural channels) nature, and introduces six classes of semantic preserving transformations to introduce unnatural forms of code, and forces the model to produce more natural original programs written by developers.","score":2},{"url":"https://www.semanticscholar.org/paper/52ca0f589fdcde1c45fd6dfb1b72248d4ecaefc0","title":"Code Translation with Compiler Representations","venue":"ArXiv","year":2022,"referenceCount":43,"citationCount":2,"influentialCitationCount":0,"publicationDate":"30/06/2022","authors":"Marc Szafraniec,Baptiste Rozière,Hugh Leather Francois Charton,Patrick Labatut,Gabriel Synnaeve","id":"52ca0f589fdcde1c45fd6dfb1b72248d4ecaefc0","summary":"This paper proposes to augment code translation with IRs, speciﬁcally LLVM IR, with results on the C++, Java, Rust, and Go languages, and improves upon the state of the art for unsupervised code translation.","score":2},{"url":"https://www.semanticscholar.org/paper/53661ff6fdbfb8557c5b19895fad151792c62da7","title":"Few-shot training LLMs for project-specific code-summarization","venue":"International Conference on Automated Software Engineering","year":2022,"referenceCount":25,"citationCount":2,"influentialCitationCount":0,"publicationDate":"09/07/2022","authors":"Toufique Ahmed,Prem Devanbu","id":"53661ff6fdbfb8557c5b19895fad151792c62da7","summary":"This paper investigates the use few-shot training with the very large GPT (Generative Pre-trained Transformer) Codex model, and finds evidence suggesting that one can significantly surpass state-of-the-art models for code-summarization, leveraging project-specific training.","score":2},{"url":"https://www.semanticscholar.org/paper/f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","title":"Exploring Length Generalization in Large Language Models","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":21,"influentialCitationCount":5,"publicationDate":"11/07/2022","authors":"Cem Anil,Yuhuai Wu,Anders Andreassen,Aitor Lewkowycz,Vedant Misra,V. Ramasesh,Ambrose Slone,Guy Gur-Ari,Ethan Dyer,Behnam Neyshabur","id":"f843233f76a5dff07bfa93a71a1cf13d8aa6a94a","summary":"It is shown that combining pretrained large language models’ in-context learning abilities with scratchpad prompting results in a dramatic improvement in length generalization, and is run to identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.","score":2},{"url":"https://www.semanticscholar.org/paper/e37155d21818513bd40d64ee212099aac82bd6f8","title":"Less training, more repairing please: revisiting automated program repair via zero-shot learning","venue":"ESEC/SIGSOFT FSE","year":2022,"referenceCount":77,"citationCount":9,"influentialCitationCount":1,"publicationDate":"17/07/2022","authors":"Chun Xia,Lingming Zhang","id":"e37155d21818513bd40d64ee212099aac82bd6f8","summary":"This paper proposesAlphaRepair, the first cloze-style (or infilling-style) APR approach to directly leveraging large pre-trained code models for APR without any fine-tuning/retraining on historical bug fixes, and implements AlphaRepair as a practical multilingual APR tool based on the recent CodeBERT model.","score":2},{"url":"https://www.semanticscholar.org/paper/2f2750b48a6f958ff12cba90e99695123d1e2f47","title":"Using pre-trained language models to resolve textual and semantic merge conflicts (experience paper)","venue":"International Symposium on Software Testing and Analysis","year":2022,"referenceCount":29,"citationCount":2,"influentialCitationCount":0,"publicationDate":"18/07/2022","authors":"Jialu Zhang,Todd Mytkowicz,Mike Kaufman,R. Piskac,Shuvendu K. Lahiri","id":"2f2750b48a6f958ff12cba90e99695123d1e2f47","summary":"The feasibility of automatically repairing merge conflicts using k-shot learning with pre-trained large neural language models (LM) such as GPT-3 is explored, and LMs provide the state-of-the-art (SOTA) performance on semantic merge conflict resolution for Edge compared to earlier symbolic approaches.","score":2},{"url":"https://www.semanticscholar.org/paper/e9fc39f56abbc6b8aed1e05496d985e70345a95a","title":"An extensive study on pre-trained models for program understanding and generation","venue":"International Symposium on Software Testing and Analysis","year":2022,"referenceCount":87,"citationCount":7,"influentialCitationCount":1,"publicationDate":"18/07/2022","authors":"Zhengran Zeng,Hanzhuo Tan,Haotian Zhang,Jing Li,Yuqun Zhang,Lingming Zhang","id":"e9fc39f56abbc6b8aed1e05496d985e70345a95a","summary":"The first study for natural language-programming language pre-trained model robustness via adversarial attacks is performed and it is found that a simple random attack approach can easily fool the state-of-the-art pre- trained models and thus incur security issues.","score":2},{"url":"https://www.semanticscholar.org/paper/1e3ec5709b2ca43233c566cd77dbabbd6892bfa6","title":"Neurosymbolic repair for low-code formula languages","venue":"Proc. ACM Program. Lang.","year":2022,"referenceCount":69,"citationCount":3,"influentialCitationCount":0,"publicationDate":"24/07/2022","authors":"Rohan Bavishi,Harshit Joshi,Jos'e Pablo Cambronero S'anchez,Anna Fariha,Sumit Gulwani,Vu Le,Ivan Radicek,A. Tiwari","id":"1e3ec5709b2ca43233c566cd77dbabbd6892bfa6","summary":"LaMirage, a LAst-MIle RepAir-engine GEnerator that combines symbolic and neural techniques to perform last-mile repair in low-code formula languages, is developed and compared to state-of-the-art neural and symbolic approaches on 400 real Excel and Power Fx formulas, where LaMirage outperforms all baselines.","score":2},{"url":"https://www.semanticscholar.org/paper/ec4c8d99eb1c028c43af6d8bbf727392d351cb59","title":"Efficient Training of Language Models to Fill in the Middle","venue":"ArXiv","year":2022,"referenceCount":60,"citationCount":17,"influentialCitationCount":1,"publicationDate":"28/07/2022","authors":"Mohammad Bavarian,Heewoo Jun,N. Tezak,J. Schulman,C. McLeavey,Jerry Tworek,Mark Chen","id":"ec4c8d99eb1c028c43af6d8bbf727392d351cb59","summary":"There is extensive evidence that training models with a large fraction of data transformed in this way does not harm the original left-to-right generative capability, as measured by perplexity and sampling evaluations across a wide range of scales.","score":2},{"url":"https://www.semanticscholar.org/paper/def2e28863338cb20782eb2015a39d32df697ed6","title":"Learning to Improve Code Efficiency","venue":"ArXiv","year":2022,"referenceCount":35,"citationCount":2,"influentialCitationCount":0,"publicationDate":"09/08/2022","authors":"Bing Chen,Daniel Tarlow,Kevin Swersky,M. Maas,P. Heiber,Ashish Naik,Milad Hashemi,Parthasarathy Ranganathan","id":"def2e28863338cb20782eb2015a39d32df697ed6","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/8c2d9d2aa891e3b53bbd9166c1b804a0741fc44a","title":"Finding Reusable Machine Learning Components to Build Programming Language Processing Pipelines","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":1,"influentialCitationCount":0,"publicationDate":"11/08/2022","authors":"Patrick Flynn,T. Vanderbruggen,C. Liao,Pei-Hung Lin,M. Emani,Xipeng Shen","id":"8c2d9d2aa891e3b53bbd9166c1b804a0741fc44a","summary":"To improve theability, accessibility, interoperability and reusability (FAIRness) of machine learning components, a set of representative papers in the domain of machineLearning-based PLP are collected and analyzed.","score":2},{"url":"https://www.semanticscholar.org/paper/568eb10d17f1643228303670fe0f1d6608bd6f4d","title":"Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models","venue":"ArXiv","year":2022,"referenceCount":92,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/08/2022","authors":"Xingyu Xie,Pan Zhou,Huan Li,Zhouchen Lin,Shuicheng Yan","id":"568eb10d17f1643228303670fe0f1d6608bd6f4d","summary":"Extensive experimental results show that Adan consistently surpasses the corresponding SoTA optimizers on vision, language, and RL tasks and sets new SoTAs for many popular networks and frameworks, e.g., ResNet, ConvNext, ViT, Swin, MAE, DETR, GPT-2, Transformer-XL, and BERT.","score":2},{"url":"https://www.semanticscholar.org/paper/9b61de7038290751377b64293baaf42f3e7cf441","title":"An Empirical Evaluation of Competitive Programming AI: A Case Study of AlphaCode","venue":"International Workshop on Software Clones","year":2022,"referenceCount":35,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/08/2022","authors":"Sila Lertbanjongngam,Bodin Chinthanet,T. Ishio,R. Kula,P. Leelaprute,Bundit Manaskasemsak,A. Rungsawang,Kenichi Matsumoto","id":"9b61de7038290751377b64293baaf42f3e7cf441","summary":"An empirical study to find code similarities and performance differences between AlphaCode-generated codes and human codes shows that the generated codes are similar to human codes and the generated code performs on par with or worse than the human code in terms of execution time and memory usage.","score":2},{"url":"https://www.semanticscholar.org/paper/befde3e07ce97f02f12fe92ab27e99f23ccd17aa","title":"Evaluating Progress in Automatic Chest X-Ray Radiology Report Generation","venue":"medRxiv","year":2022,"referenceCount":33,"citationCount":2,"influentialCitationCount":0,"publicationDate":"31/08/2022","authors":"F. Yu,M. Endo,R. Krishnan,I. Pan,A. Tsai,E. P. Reis,E. Fonseca,H. M. H. Lee,Z. H. Abad,A. Y. Ng,C. Langlotz,V. Venugopal,P. Rajpurkar","id":"befde3e07ce97f02f12fe92ab27e99f23ccd17aa","summary":"This study quantitatively examines the correlation between automated metrics and the scoring of reports by radiologists, and proposes a composite metric, called RadCliQ, that is able to rank the quality of reports similarly to radiologists and better than existing metrics.","score":2},{"url":"https://www.semanticscholar.org/paper/6b5d1e50894b1f28e4798cf20e9ffa88b9ec011a","title":"How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models","venue":"ArXiv","year":2022,"referenceCount":22,"citationCount":2,"influentialCitationCount":0,"publicationDate":"03/09/2022","authors":"Hai Dang,Lukas Mecke,Florian Lehmann,Sven Goller,D. Buschek","id":"6b5d1e50894b1f28e4798cf20e9ffa88b9ec011a","summary":"This work discusses the key opportunities and challenges for interactive creative applications that use prompting as a new paradigm for Human-AI interaction and proposes four design goals for user interfaces that support prompting.","score":2},{"url":"https://www.semanticscholar.org/paper/372dd97de200970859315bce7e150fe50baecad5","title":"Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence","venue":"ArXiv","year":2022,"referenceCount":58,"citationCount":6,"influentialCitationCount":0,"publicationDate":"07/09/2022","authors":"Junjie Wang,Yuxiang Zhang,Lin Zhang,Ping Yang,Xinyu Gao,Ziwei Wu,Xiaoqun Dong,Junqing He,Jianheng Zhuo,Qi Yang,Yongfeng Huang,Xiayu Li,Yan-Ze Wu,Junyu Lu,Xinyu Zhu,Weifeng Chen,Ting-Ting Han,Kunhao Pan,Rui Wang,Hao Wang,Xiaojun Wu,Zhong Zeng,Chong-An Chen,Ruyi Gan,Jiaxing Zhang","id":"372dd97de200970859315bce7e150fe50baecad5","summary":"F Fengshenbang aims to re-evaluate the open-source community of Chinese pre-trained large-scale models, prompting the development of the entire Chinese large- scale model community, and invites companies, colleges, and research institutions to collaborate with us to build the large- Scale open- source model-based ecosystem.","score":2},{"url":"https://www.semanticscholar.org/paper/363758e9e296adc9391ed731e834809cf5d4c19b","title":"Are machine programming systems using right source-code measures to select code repositories?","venue":"MaLTeSQuE@ESEC/SIGSOFT FSE","year":2022,"referenceCount":32,"citationCount":1,"influentialCitationCount":0,"publicationDate":"24/09/2022","authors":"N. Hasabnis","id":"363758e9e296adc9391ed731e834809cf5d4c19b","summary":"A framework to rank open-source repositories on quality, maintainability, and popularity by leveraging existing research on this topic is developed and some correlation between the quality measures used in GitRank and ControlFlag's performance is revealed, suggesting that some of the measures used by GitRank are applicable to ControlFlag.","score":2},{"url":"https://www.semanticscholar.org/paper/cd6496bc404e18a24f634e3dded2ed1cdca03e0f","title":"Learning to Learn with Generative Models of Neural Network Checkpoints","venue":"ArXiv","year":2022,"referenceCount":77,"citationCount":9,"influentialCitationCount":0,"publicationDate":"26/09/2022","authors":"William S. Peebles,Ilija Radosavovic,Tim Brooks,Alexei A. Efros,J. Malik","id":"cd6496bc404e18a24f634e3dded2ed1cdca03e0f","summary":"This model is a conditional diffusion transformer that, given an initial input parameter vector and a prompted loss, error, or return, predicts the distribution over parameter updates that achieve the desired metric.","score":2},{"url":"https://www.semanticscholar.org/paper/55e3fe05598be7c3dd357d51166869f6571b824f","title":"Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/10/2022","authors":"Mohammad Reza Taesiri,F. Macklon,Yihe Wang,Hengshuo Shen,C. Bezemer","id":"55e3fe05598be7c3dd357d51166869f6571b824f","summary":"This study explores the possibil-ity of leveraging the zero-shot capabilities of large language models for video game bug detection by formulating the bug detection problem as a question-answering task, and shows thatLarge language models can identify which event is buggy in a sequence of textual descriptions of events from a game.","score":2},{"url":"https://www.semanticscholar.org/paper/259b7a01700c39d5669e88d1434873ea38a13528","title":"In-Context Policy Iteration","venue":"ArXiv","year":2022,"referenceCount":39,"citationCount":1,"influentialCitationCount":0,"publicationDate":"07/10/2022","authors":"Ethan Brooks,Logan Walls,Richard L. Lewis,Satinder Singh","id":"259b7a01700c39d5669e88d1434873ea38a13528","summary":"An algorithm, ICPI, that learns to perform RL tasks without expert demonstrations or gradients, and is presented as a policy-iteration method in which the prompt content is the entire locus of learning.","score":2},{"url":"https://www.semanticscholar.org/paper/5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","title":"Scaling Instruction-Finetuned Language Models","venue":"ArXiv","year":2022,"referenceCount":104,"citationCount":53,"influentialCitationCount":17,"publicationDate":"20/10/2022","authors":"Hyung Won Chung,Le Hou,S. Longpre,Barret Zoph,Yi Tay,W. Fedus,Eric Li,Xuezhi Wang,M. Dehghani,Siddhartha Brahma,Albert Webson,S. Gu,Zhuyun Dai,Mirac Suzgun,Xinyun Chen,Aakanksha Chowdhery,Dasha Valter,Sharan Narang,Gaurav Mishra,A. Yu,Vincent Zhao,Yanping Huang,Andrew M. Dai,Hongkun Yu,Slav Petrov,E. Chi,J. Dean,Jacob Devlin,Adam Roberts,Denny Zhou,Quoc V. Le,Jason Wei","id":"5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","summary":"This result shows that instruction and UL2 continued pre-training are complementary compute-eﬃcient methods to improve the performance of language models without increasing model scale.","score":2},{"url":"https://www.semanticscholar.org/paper/4634362d75606287955260ef1788171286efbeaa","title":"Draft, Sketch, and Prove: Guiding Formal Theorem Provers with Informal Proofs","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":4,"influentialCitationCount":0,"publicationDate":"21/10/2022","authors":"Albert Qiaochu Jiang,S. Welleck,J. Zhou,Wenda Li,Jiacheng Liu,M. Jamnik,Timothée Lacroix,Yuhuai Wu,Guillaume Lample","id":"4634362d75606287955260ef1788171286efbeaa","summary":"Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems, is introduced.","score":2},{"url":"https://www.semanticscholar.org/paper/472d87be3dc298102e058be55a814cc6d2085b39","title":"Towards Deceptive Defense in Software Security with Chaff Bugs","venue":"International Symposium on Recent Advances in Intrusion Detection","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Zhenghao Hu,Yu Hu,Brendan Dolan-Gavitt","id":"472d87be3dc298102e058be55a814cc6d2085b39","summary":"A new defensive technique called chaff bugs is proposed, which instead targets the bug discovery and exploit creation stages of this process, and can serve as an effective deterrent against both human attackers and automated bug-finding tools.","score":2},{"url":"https://www.semanticscholar.org/paper/ac3d7ae8b4acb137492d4a8d8bcff480b89fa000","title":"Programming Pedagogy and Assessment in the Era of AI/ML: A Position Paper","venue":"Compute","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/11/2022","authors":"A. Raman,Viraj Kumar","id":"ac3d7ae8b4acb137492d4a8d8bcff480b89fa000","summary":"This work surveys recent research on automated systems for writing code, and examines the components of the code-writing task using a six-step framework proposed in the literature, and identifies the impact of automated systems at each step.","score":2},{"url":"https://www.semanticscholar.org/paper/632ab7663e6d64578ceda1d1df9ec525b503bacb","title":"Steps towards prompt-based creation of virtual worlds","venue":"ArXiv","year":2022,"referenceCount":41,"citationCount":1,"influentialCitationCount":0,"publicationDate":"10/11/2022","authors":"Jasmine Roberts,Andrzej Banburski-Fahey,J. Lanier","id":"632ab7663e6d64578ceda1d1df9ec525b503bacb","summary":"This work shows that prompt-based methods can both accelerate in-VR level editing, as well as can become part of gameplay rather than just part of game development.","score":2},{"url":"https://www.semanticscholar.org/paper/048ed70192de0232086eb32a95ffb3be8d336c76","title":"Metaphors We Learn By","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/11/2022","authors":"R. Memisevic","id":"048ed70192de0232086eb32a95ffb3be8d336c76","summary":"This essay relates parameter sharing (“weight sharing”) to analogy making and the school of thought of cognitive metaphor, and discusses how recurrent and auto-regressive models can be thought of as extending analogy making from static features to dynamic skills and procedures.","score":2},{"url":"https://www.semanticscholar.org/paper/403028df4fe52786f1748f1c314b6eb2cf867197","title":"CLAWSAT: Towards Both Robust and Accurate Code Models","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Jinghan Jia,Shashank Srikant,Tamara Mitrovska,Chuang Gan,Shiyu Chang,Sijia Liu,Una-May O’Reilly","id":"403028df4fe52786f1748f1c314b6eb2cf867197","summary":"This is the first systematic study to explore and exploit the robustness and accuracy benefits of (multi-view) code obfuscations in code models and demonstrate the effectiveness of adversarial learning in CLAW.","score":2},{"url":"https://www.semanticscholar.org/paper/6257d48954680fb4e7df9b663e0ac50db4404046","title":"On the Security Vulnerabilities of Text-to-SQL Models","venue":"ArXiv","year":2022,"referenceCount":75,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/11/2022","authors":"Xutan Peng,Yipeng Zhang,Jingfeng Yang,Mark Stevenson","id":"6257d48954680fb4e7df9b663e0ac50db4404046","summary":"This is the first demonstration that NLP models can be exploited as attack vectors in the wild and verified that straightforward backdoor attacks on Text-to-SQL systems achieve a 100% success rate without affecting their performance.","score":2},{"url":"https://www.semanticscholar.org/paper/6d4a12ea469ff08634eeb24c47b265a7dca2fce2","title":"PADL: Language-Directed Physics-Based Character Control","venue":"ACM SIGGRAPH Conference and Exhibition on Computer Graphics and Interactive Techniques in Asia","year":2022,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/11/2022","authors":"Jordan Juravsky,Yunrong Guo,S. Fidler,X. B. Peng","id":"6d4a12ea469ff08634eeb24c47b265a7dca2fce2","summary":"PADL, which leverages recent innovations in NLP in order to take steps towards developing language-directed controllers for physics-based character animation, is presented and it is shown that the framework can be applied to effectively direct a simulated humanoid character to perform a diverse array of complex motor skills.","score":2},{"url":"https://www.semanticscholar.org/paper/ea55ae4c82b45e3857f37406c94e9f642a2b3d84","title":"Genetic Programming with Local Scoring","venue":"ArXiv","year":2022,"referenceCount":9,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/11/2022","authors":"Max Vistrup","id":"ea55ae4c82b45e3857f37406c94e9f642a2b3d84","summary":"New techniques for synthesizing programs through se-quences of mutations, including a method of local scoring assigning a score to each expression in a program, and cyclic evolution in which programs evolve programs through phases of expansion and reduction are presented.","score":2},{"url":"https://www.semanticscholar.org/paper/4290a70025f29d7054c550c75ae6b24c38a79d12","title":"SE Factual Knowledge in Frozen Giant Code Model: A Study on FQN and its Retrieval","venue":"ArXiv","year":2022,"referenceCount":74,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"Qing Huang,Dianshu Liao,Zhenchang Xing,Zhiqiang Yuan,Qinghua Lu,Xiwei Xu,Jiaxing Lu","id":"4290a70025f29d7054c550c75ae6b24c38a79d12","summary":"This work conducts the first systematic study on the SE factual knowledge in the state-of-the-art PCM CoPilot, focusing on APIs’ Fully Qualiﬁed Names (FQNs), the fundamental knowledge for effective code analysis, search and reuse.","score":2},{"url":"https://www.semanticscholar.org/paper/ae441f7305dc2cd58c708528b3ecee3501cc5c46","title":"Plansformer: Generating Symbolic Plans using Transformers","venue":"ArXiv","year":2022,"referenceCount":36,"citationCount":1,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"Vishal Pallagani,Bharath Muppasani,K. Murugesan,F. Rossi,L. Horesh,Biplav Srivastava,F. Fabiano,Andrea Loreggia","id":"ae441f7305dc2cd58c708528b3ecee3501cc5c46","summary":"The use of LLMs for automated planning - a branch of AI concerned with the realization of action sequences (plans) to achieve a goal, typically executed by intelligent agents, autonomous robots, and unmanned vehicles are explored.","score":2},{"url":"https://www.semanticscholar.org/paper/3b8ccc7ec80b8775de603e248ac1ca2b919d6b70","title":"Chatbots in a Botnet World","venue":"ArXiv","year":2022,"referenceCount":29,"citationCount":3,"influentialCitationCount":0,"publicationDate":"18/12/2022","authors":"Forrest McKee,David Noever","id":"3b8ccc7ec80b8775de603e248ac1ca2b919d6b70","summary":"Questions-and-answer formats provide a novel experimental platform for investigating cybersecurity questions and illustrate cases that support the broad gain of functionality, including self-replication and self-modification, evasion, and strategic understanding of complex cybersecurity goals.","score":2},{"url":"https://www.semanticscholar.org/paper/6756fcd998caeb7b23702e08559e63710179334c","title":"Reasoning with Language Model Prompting: A Survey","venue":"ArXiv","year":2022,"referenceCount":150,"citationCount":7,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng,Chuanqi Tan,Fei Huang,Huajun Chen","id":"6756fcd998caeb7b23702e08559e63710179334c","summary":"This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting and introduces research works with comparisons and summaries and provides systematic resources to help beginners.","score":2},{"url":"https://www.semanticscholar.org/paper/c2a259bb42cedad59469797b2175f7ca062c2bd7","title":"User-Driven Support for Rapid Visualization Prototyping in D3","venue":"","year":2023,"referenceCount":74,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Hannah K. Bako,Alisha Varma,Anuoluwapo Faboro,Mahreen Haider,Favour Nerrise,B. Kenah,John P. Dickerson,L. Battle","id":"c2a259bb42cedad59469797b2175f7ca062c2bd7","summary":"The structural similarities revealed by templates are used to design resilient support features for prototyping D3 visualizations: recommendations to suggest complementary interactions for a users’ D3 program; and code augmentation to implement recommended interactions with a single click, even when users deviate from pre-defined templates.","score":2},{"url":"https://www.semanticscholar.org/paper/490f1e8ff352bded30bcde01d5b4769d6c2d2dd5","title":"Adversarial Attacks on Neural Models of Code via Code Difference Reduction","venue":"ArXiv","year":2023,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/01/2023","authors":"Zhao Tian,Junjie Chen,Zhi Jin","id":"490f1e8ff352bded30bcde01d5b4769d6c2d2dd5","summary":"This work proposes a novel adversarial attack technique, CODA, that uses the code differences between the target input and reference inputs to guide the generation of adversarial examples and considers both structure differences and identiﬁer differences to preserve the original semantics.","score":2},{"url":"https://www.semanticscholar.org/paper/468992bf970c37bd1fef58b78a6c2fcd8c018868","title":"Scaling Laws for Generative Mixed-Modal Language Models","venue":"ArXiv","year":2023,"referenceCount":41,"citationCount":3,"influentialCitationCount":0,"publicationDate":"10/01/2023","authors":"Armen Aghajanyan,L. Yu,Alexis Conneau,Wei-Ning Hsu,Karen Hambardzumyan,Susan Zhang,Stephen Roller,Naman Goyal,Omer Levy,Luke Zettlemoyer","id":"468992bf970c37bd1fef58b78a6c2fcd8c018868","summary":"New mixed-modal scaling laws that unify the contributions of individual modalities and the interactions between them are reported, and the optimal synergy and competition due to data and model size is explicitly model as an additive term to previous uni-modAL scaling laws.","score":2},{"url":"https://www.semanticscholar.org/paper/1f22de83d912176cb8857efa1c6d65b14d6a2f5c","title":"ChatGPT is not all you need. A State of the Art Review of large Generative AI models","venue":"ArXiv","year":2023,"referenceCount":34,"citationCount":12,"influentialCitationCount":0,"publicationDate":"11/01/2023","authors":"Roberto Gozalo-Brizuela,E.C. Garrido-Merchán","id":"1f22de83d912176cb8857efa1c6d65b14d6a2f5c","summary":"This work consists on an attempt to describe in a concise way the main models are sectors that aresector that are aﬀected by generative AI and to provide a taxonomy of the main generative models published recently.","score":2},{"url":"https://www.semanticscholar.org/paper/907a77639069bb7dd270f017068745706133cffc","title":"Inaccessible Neural Language Models Could Reinvigorate Linguistic Nativism","venue":"ArXiv","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/01/2023","authors":"Patrick Perrine","id":"907a77639069bb7dd270f017068745706133cffc","summary":"This work argues that this lack of accessibility could instill a nativist bias in researchers new to computational linguistics, and calls upon researchers to open source their LLM code wherever possible to allow both empircist and hybrid approaches to remain accessible.","score":2},{"url":"https://www.semanticscholar.org/paper/a20875e70a38cb053cd34e170038c4746f85dac9","title":"A Case Study in Engineering a Conversational Programming Assistant's Persona","venue":"ArXiv","year":2023,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/01/2023","authors":"Steven I. Ross,Michael J. Muller,Fernando Martinez,Stephanie Houde,Justin D. Weisz","id":"a20875e70a38cb053cd34e170038c4746f85dac9","summary":"The Programmer’s Assistant is an experimental prototype software development environment that integrates a chatbot with a code editor that establishes a conversational interaction pattern, a set of conventions.","score":2},{"url":"https://www.semanticscholar.org/paper/074baf835aec44a100990178859b35451975f339","title":"Navigating Complexity in Software Engineering: A Prototype for Comparing GPT-n Solutions","venue":"ArXiv","year":2023,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/01/2023","authors":"Christoph Treude","id":"074baf835aec44a100990178859b35451975f339","summary":"A work-in-progress prototype “GPTC OMPARE” is presented, which allows programmers to visually compare multiple source code solutions generated by GPT-n models for the same programming- related query by highlighting their similarities and differences.","score":2},{"url":"https://www.semanticscholar.org/paper/1d34b6cffe67077cdd4df41950b1195f09ae0cb8","title":"Hierarchical Programmatic Reinforcement Learning via Learning to Compose Programs","venue":"ArXiv","year":2023,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"Guanhui. Liu,En-Pei Hu,Pu-Jen Cheng,Hung-yi Lee,Shao-Hua Sun","id":"1d34b6cffe67077cdd4df41950b1195f09ae0cb8","summary":"This work proposes to learn a meta-policy that composes a series of programs sampled from the learned program embedding space that can produce program policies that describe out-of-distributionally complex behaviors and directly assign credits to programs that induce desired behaviors.","score":2},{"url":"https://www.semanticscholar.org/paper/861916af6428277a3ce2e18034e4b40dc6616eb9","title":"On the Robustness of Code Generation Techniques: An Empirical Study on GitHub Copilot","venue":"ArXiv","year":2023,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/02/2023","authors":"A. Mastropaolo,L. Pascarella,Emanuela Guglielmi,Matteo Ciniselli,Simone Scalabrino,R. Oliveto,G. Bavota","id":"861916af6428277a3ce2e18034e4b40dc6616eb9","summary":"This paper presents an empirical study in which it is aimed at understanding whether different but semantically equivalent natural language descriptions result in the same recommended function, and investigates the extent to which predictions generated by Copilot changed.","score":2},{"url":"https://www.semanticscholar.org/paper/2af6a21a1b682ceb585165359d3605e89f4cf6b0","title":"Fixing Hardware Security Bugs with Large Language Models","venue":"ArXiv","year":2023,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/02/2023","authors":"Baleegh Ahmad,Shailja Thakur,B. Tan,R. Karri,H. Pearce","id":"2af6a21a1b682ceb585165359d3605e89f4cf6b0","summary":"The results show that LLMs can repair hardware security bugs and the framework designed and implemented are an important step towards the ultimate goal of an automated end-to-end bug repair framework.","score":2},{"url":"https://www.semanticscholar.org/paper/2b5fb3dd226ad6f7b6b1d4b00402f969fb853af9","title":"KNOD: Domain Knowledge Distilled Tree Decoder for Automated Program Repair","venue":"ArXiv","year":2023,"referenceCount":65,"citationCount":1,"influentialCitationCount":0,"publicationDate":"03/02/2023","authors":"Nan Jiang,Thibaud Lutellier,Yiling Lou,Lin Tan,Dan Goldwasser,Xiangyu Zhang","id":"2b5fb3dd226ad6f7b6b1d4b00402f969fb853af9","summary":"KNOD is proposed, which incorporates domain knowledge to guide patch generation in a direct and comprehensive way and is evaluated on three widely-used benchmarks, outperforming all existing APR tools.","score":2},{"url":"https://www.semanticscholar.org/paper/fb243dfd1234b8f76dfda740a62402663da74085","title":"Exploring Data Augmentation for Code Generation Tasks","venue":"ArXiv","year":2023,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/02/2023","authors":"Pinzhen Chen,Gerasimos Lampouras","id":"fb243dfd1234b8f76dfda740a62402663da74085","summary":"Focusing on data utilization for downstream tasks, this work proposes and adapt augmentation methods that yield consistent improvements in code translation and summarization by up to 6.9% and 7.5% respectively.","score":2},{"url":"https://www.semanticscholar.org/paper/8927db4ee890bf42608752bb840bc9d7db556da1","title":"ChatGPT and Software Testing Education: Promises & Perils","venue":"ArXiv","year":2023,"referenceCount":25,"citationCount":2,"influentialCitationCount":0,"publicationDate":"07/02/2023","authors":"Sajed Jalil,Suzzana Rafi,Thomas D. LaToza,Kevin Moran,Wing Lam","id":"8927db4ee890bf42608752bb840bc9d7db556da1","summary":"How well ChatGPT performs when tasked with solving common questions in a popular software testing curriculum is examined, and the potential promise, and dangers related to the use ofChatGPT by students and instructors are discussed.","score":2},{"url":"https://www.semanticscholar.org/paper/d2170504c4ad9403bea118ae8debdfda95978546","title":"The Wisdom of Hindsight Makes Language Models Better Instruction Followers","venue":"ArXiv","year":2023,"referenceCount":51,"citationCount":1,"influentialCitationCount":0,"publicationDate":"10/02/2023","authors":"Tianjun Zhang,Fangchen Liu,Justin Wong,P. Abbeel,Joseph Gonzalez","id":"d2170504c4ad9403bea118ae8debdfda95978546","summary":"HIR is proposed, a novel algorithm for aligning language models with instructions by converting feedback to instruction by relabeling the original one and training the model for better alignment in a supervised manner and it outperforms the baseline algorithms and is comparable to or even surpasses supervised finetuning.","score":2},{"url":"https://www.semanticscholar.org/paper/9cba7821f5731df6da540852643fbfa023d5f1a8","title":"Impact of Code Language Models on Automated Program Repair","venue":"ArXiv","year":2023,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/02/2023","authors":"Nan Jiang,Kevin Liu,Thibaud Lutellier,Lin Tan","id":"9cba7821f5731df6da540852643fbfa023d5f1a8","summary":"This work is the first to evaluate ten CLMs on four APR benchmarks, which shows that surprisingly, the best CLM, as is, fixes 72% more bugs than the state-of-the-art deep-learning (DL)-based APR techniques.","score":2},{"url":"https://www.semanticscholar.org/paper/eb619cbfb34127f28f1cab03eb22234267f892ea","title":"PAC Prediction Sets for Large Language Models of Code","venue":"ArXiv","year":2023,"referenceCount":20,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/02/2023","authors":"Adam Khakhar,Stephen Mell,O. Bastani","id":"eb619cbfb34127f28f1cab03eb22234267f892ea","summary":"This work proposes a solution that considers a restricted set of prediction sets that can compactly be represented as partial programs, which are programs with portions replaced with holes in uncertain parts of the generated code.","score":2},{"url":"https://www.semanticscholar.org/paper/3f83582c08a62e5bd02398fafc93f7eaf1e4b84e","title":"Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints","venue":"ArXiv","year":2023,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/02/2023","authors":"Albert Lu,Hongxin Zhang,Yanzhe Zhang,Xuezhi Wang,Diyi Yang","id":"3f83582c08a62e5bd02398fafc93f7eaf1e4b84e","summary":"This paper takes a prompt-centric approach to analyzing and bounding the abilities of open-ended generative models, and presents a generic methodology of analysis with two challenging prompt constraint types: structural and stylistic.","score":2},{"url":"https://www.semanticscholar.org/paper/7678d1862140d84f9c15d95e7d4d085857f332eb","title":"Conversational Text-to-SQL: An Odyssey into State-of-the-Art and Challenges Ahead","venue":"ArXiv","year":2023,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/02/2023","authors":"S. Parthasarathi,Lu Zeng,Dilek Z. Hakkani-Tür","id":"7678d1862140d84f9c15d95e7d4d085857f332eb","summary":"With multi-tasking over coherent tasks with discrete prompts during training, this work improves over specialized text-to-SQL T5-family models and applies a query plan model and a schema linking algorithm as rerankers based on Oracle analyses over n-best hypotheses.","score":2},{"url":"https://www.semanticscholar.org/paper/a8e0ba16346b72c3a04dd0b1da84bc5f28900174","title":"Using GitHub Copilot to Solve Simple Programming Problems","venue":"","year":2023,"referenceCount":15,"citationCount":1,"influentialCitationCount":1,"publicationDate":"02/03/2023","authors":"M. Wermelinger","id":"a8e0ba16346b72c3a04dd0b1da84bc5f28900174","summary":"Evaluating Copilot to see if and how it differs from Codex, and looking qualitatively at the generated suggestions, to understand the limitations of Copilot, finds the most performant Codex model quantitatively.","score":2},{"url":"https://www.semanticscholar.org/paper/9ca243250b5ba4fa77bcda263db94f9c33d9599f","title":"On ML-Based Program Translation: Perils and Promises","venue":"ArXiv","year":2023,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/02/2023","authors":"Aniketh Malyala,K. Zhou,Baishakhi Ray,Saikat Chakraborty","id":"9ca243250b5ba4fa77bcda263db94f9c33d9599f","summary":"A rule-based program mutation engine, which pre-processes the input code if the input follows specific patterns and post-process the output if the output follows certain patterns, and shows that it can form a hybrid program translator and significantly improve the state-of-the-art.","score":1},{"url":"https://www.semanticscholar.org/paper/6c761cfdb031701072582e434d8f64d436255da6","title":"AMMUS : A Survey of Transformer-based Pretrained Models in Natural Language Processing","venue":"ArXiv","year":2021,"referenceCount":301,"citationCount":62,"influentialCitationCount":7,"publicationDate":"12/08/2021","authors":"Katikapalli Subramanyam Kalyan,A. Rajasekharan,S. Sangeetha","id":"6c761cfdb031701072582e434d8f64d436255da6","summary":"This comprehensive survey paper explains various core concepts like pretraining, Pretraining methods, pretraining tasks, embeddings and downstream adaptation methods, presents a new taxonomy of T-PTLMs and gives brief overview of various benchmarks including both intrinsic and extrinsic.","score":1},{"url":"https://www.semanticscholar.org/paper/cb7c42c3f1335db36ac321c9a5830b97d46d5560","title":"Multilingual training for So ware Engineering","venue":"","year":2021,"referenceCount":82,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Toufique Ahmed,Prem Devanbu","id":"cb7c42c3f1335db36ac321c9a5830b97d46d5560","summary":"Evidence suggesting that human-written code in different languages (which performs the same function), is rather similar, and particularly preserving of identifier naming patterns is presented, to find evidence that available multilingual training data (across different languages) can be used to amplify performance.","score":1},{"url":"https://www.semanticscholar.org/paper/219be4772aab5f13a172c100bc7f2441ba9192ad","title":"Modeling Hierarchical Syntax Structure with Triplet Position for Source Code Summarization","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":53,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"Juncai Guo,Jin Liu,Yao Wan,Li Li,Pingyi Zhou","id":"219be4772aab5f13a172c100bc7f2441ba9192ad","summary":"CODESCRIBE is proposed to model the hierarchical syntax structure of code by introducing a novel triplet position for code summarization and leverages the graph neural network and Transformer to preserve the structural and sequential information of code, respectively.","score":1},{"url":"https://www.semanticscholar.org/paper/cc5842ebb9ed13983b770cff6b99a1da27d91445","title":"Enriching Biomedical Knowledge for Vietnamese Low-resource Language Through Large-Scale Translation","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Long Phan,Tai Dang,Hieu Tran,Vy Phan,Lam D. Chau,Trieu H. Trinh","id":"cc5842ebb9ed13983b770cff6b99a1da27d91445","summary":"A state-of-the-art translation model in English-Vietnamese is used to translate and produce both pretrained and supervised data in the biomedical domains, and ViPubmedT5, a pretrained Encoder-Decoder Transformer model trained on 20 million translated abstracts from the high-quality public PubMed corpus is introduced.","score":1},{"url":"https://www.semanticscholar.org/paper/467306672c89d4a0a7c6bc733814605c53bbfa97","title":"Towards Learning (Dis)-Similarity of Source Code from Program Contrasts","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":39,"citationCount":5,"influentialCitationCount":1,"publicationDate":"08/10/2021","authors":"Yangruibo Ding,Luca Buratti,Saurabh Pujar,Alessandro Morari,Baishakhi Ray,Saikat Chakraborty","id":"467306672c89d4a0a7c6bc733814605c53bbfa97","summary":"This work presents DISCO (DIS-similarity of COde), a novel self-supervised model focusing on identifying (dis)similar functionalities of source code, and proposes to pre-train the Transformer model with such automatically generated program contrasts to better identify similar code in the wild and differentiate vulnerable programs from benign ones.","score":1},{"url":"https://www.semanticscholar.org/paper/775a9c722262c7b656876a5fef20f4577afd8981","title":"Multilingual training for Software Engineering","venue":"International Conference on Software Engineering","year":2021,"referenceCount":85,"citationCount":17,"influentialCitationCount":1,"publicationDate":"03/12/2021","authors":"Toufique Ahmed,Prem Devanbu","id":"775a9c722262c7b656876a5fef20f4577afd8981","summary":"Evidence suggesting that human-written code in different languages, is rather similar, and particularly preserving of identifier naming patterns, is presented, to find evidence that available multilingual training data (across different languages) can be used to amplify performance.","score":1},{"url":"https://www.semanticscholar.org/paper/b2420b683fcc0a8b642394d36832b8f05ea049a3","title":"Cross-Domain Deep Code Search with Meta Learning","venue":"International Conference on Software Engineering","year":2022,"referenceCount":46,"citationCount":2,"influentialCitationCount":1,"publicationDate":"01/01/2022","authors":"Yitian Chai,Hongyu Zhang,Beijun Shen,Xiaodong Gu","id":"b2420b683fcc0a8b642394d36832b8f05ea049a3","summary":"Experimental results show that CDCS significantly outperforms conventional pre-trained code models that are directly fine-tuned in domain-specific languages, and it is particularly effective for scarce data.","score":1},{"url":"https://www.semanticscholar.org/paper/9fd854f8315bdb02449500088d3cb8e6e84a0d09","title":"Cross-Domain Deep Code Search with Few-Shot Meta Learning","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":5,"influentialCitationCount":0,"publicationDate":2022,"authors":"Yitian Chai,Hongyu Zhang,Beijun Shen,Xiaodong Gu","id":"9fd854f8315bdb02449500088d3cb8e6e84a0d09","summary":"Experimental results show that CDCS significantly outperforms conventional pre-trained code models that are directly fine-tuned in domain-specific languages, and it is particularly effective for scarce data.","score":1},{"url":"https://www.semanticscholar.org/paper/3ee317bfa4ba84f0df112ef66b7d890d8b445ada","title":"DualSC: Automatic Generation and Summarization of Shellcode via Transformer and Dual Learning","venue":"IEEE International Conference on Software Analysis, Evolution, and Reengineering","year":2022,"referenceCount":79,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/02/2022","authors":"Guang Yang,Xiang Chen,Yanlin Zhou,Chi Yu","id":"3ee317bfa4ba84f0df112ef66b7d890d8b445ada","summary":"This study formalizes automatic shellcode generation and summarization as dual tasks, uses a shallow Transformer for model construction, and design a normalization method Adjust_QKNorm to adapt these low-resource tasks, and proposes a rule-based repair component to improve the performance of automatic shell code generation.","score":1},{"url":"https://www.semanticscholar.org/paper/db783c480faf87b38e8806d4ef455dfde6e335aa","title":"Towards JavaScript program repair with Generative Pre-trained Transformer (GPT-2)","venue":"APR","year":2022,"referenceCount":45,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/05/2022","authors":"Márk Lajkó,Viktor Csuvik,László Vidács","id":"db783c480faf87b38e8806d4ef455dfde6e335aa","summary":"The GPT-2 model was able to learn how to write syntactically correct source code almost on every attempt, although it failed to learn good bug-fixes in some cases, resulting in an overall accuracy up to 17.25%.","score":1},{"url":"https://www.semanticscholar.org/paper/0d6e733e68c6eea2f0def4aab03181e9fb8f3b09","title":"ViT5: Pretrained Text-to-Text Transformer for Vietnamese Language Generation","venue":"North American Chapter of the Association for Computational Linguistics","year":2022,"referenceCount":27,"citationCount":5,"influentialCitationCount":1,"publicationDate":"13/05/2022","authors":"Long Phan,Hieu Tran,H. Nguyen,Trieu H. Trinh","id":"0d6e733e68c6eea2f0def4aab03181e9fb8f3b09","summary":"The experiments show that ViT5 significantly outperforms existing models and achieves state-of-the-art results on Vietnamese Text Summarization, and the importance of context length during the self-supervised pretraining on downstream performance across different settings is shown.","score":1},{"url":"https://www.semanticscholar.org/paper/2417ab25a53e97410f44a20af69b82fff077fd53","title":"Deep Learning Meets Software Engineering: A Survey on Pre-Trained Models of Source Code","venue":"International Joint Conference on Artificial Intelligence","year":2022,"referenceCount":76,"citationCount":7,"influentialCitationCount":0,"publicationDate":"24/05/2022","authors":"Changan Niu,Chuanyi Li,Bin Luo,Vincent Ng","id":"2417ab25a53e97410f44a20af69b82fff077fd53","summary":"An overview of this rapidly advancing field of research in software engineering is provided and reflects on future research directions.","score":1},{"url":"https://www.semanticscholar.org/paper/1d5905b0c4a558604d562eee0b522bde63348c9f","title":"VulBERTa: Simplified Source Code Pre-Training for Vulnerability Detection","venue":"IEEE International Joint Conference on Neural Network","year":2022,"referenceCount":40,"citationCount":5,"influentialCitationCount":2,"publicationDate":"25/05/2022","authors":"Hazim Hanif,S. Maffeis","id":"1d5905b0c4a558604d562eee0b522bde63348c9f","summary":"The evaluation results show that VulBERTa achieves state-of-the-art performance and outperforms existing approaches across different datasets, despite its conceptual simplicity, and limited cost in terms of size of training data and number of model parameters.","score":1},{"url":"https://www.semanticscholar.org/paper/05a4edf3a6c2439c83f23ffa9afe463ae26780d4","title":"Learning code summarization from a small and local dataset","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/06/2022","authors":"Toufique Ahmed,Prem Devanbu","id":"05a4edf3a6c2439c83f23ffa9afe463ae26780d4","summary":"This work compares several models and training approaches, and finds that the maximalist hybrid setting provides consistent, substantial gains over the state-of-the-art, on many different projects in both Java and Python.","score":1},{"url":"https://www.semanticscholar.org/paper/9d6e411065cd65b4291955c47b3f255ae668b81a","title":"StructCoder: Structure-Aware Transformer for Code Generation","venue":"ArXiv","year":2022,"referenceCount":28,"citationCount":4,"influentialCitationCount":1,"publicationDate":"10/06/2022","authors":"S. Tipirneni,Ming Zhu,C. Reddy","id":"9d6e411065cd65b4291955c47b3f255ae668b81a","summary":"This work develops an encoder-decoder Transformer model where both the encoder and decoder are trained to recognize the syntax and data flow in the source and target codes, respectively, and achieves state-of-the-art performance on code translation and text-to-code generation tasks in the CodeXGLUE benchmark.","score":1},{"url":"https://www.semanticscholar.org/paper/65ef841916d4f1cdd624a8454cb5c50fad6e14c9","title":"An Extractive-and-Abstractive Framework for Source Code Summarization","venue":"ArXiv","year":2022,"referenceCount":87,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/06/2022","authors":"Weisong Sun,Chunrong Fang,Yuchen Chen,Quanjun Zhang,Guanhong Tao,Tingxu Han,Yifei Ge,Yudu You,Bin Luo","id":"65ef841916d4f1cdd624a8454cb5c50fad6e14c9","summary":"A novel extractive-and-abstractive framework to generate human-written-like summaries with preserved factual details, called EACS, which significantly outperforms state-of-the-art techniques in terms of all three widely used metrics.","score":1},{"url":"https://www.semanticscholar.org/paper/7ffd5a29349962c6a49a3df2ba6e7b20788669bf","title":"Semantic-Preserving Adversarial Code Comprehension","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":36,"citationCount":1,"influentialCitationCount":0,"publicationDate":"12/09/2022","authors":"Yiyang Li,Hongqi Wu,Hai Zhao","id":"7ffd5a29349962c6a49a3df2ba6e7b20788669bf","summary":"Semantic-Preserving Adversarial Code Embeddings (SPACE) is proposed to find the worst-case semantic-preserving attacks while forcing the model to predict the correct labels under these worst cases.","score":1},{"url":"https://www.semanticscholar.org/paper/a807638436aadf2d56dd920f3a8de339dce44096","title":"COMBO: Pre-Training Representations of Binary Code Using Contrastive Learning","venue":"ArXiv","year":2022,"referenceCount":71,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/10/2022","authors":"Yifan Zhang,Chen Huang,Yueke Zhang,Kevin Cao,Scott Thomas Andersen,Huajie Shao,Kevin Leach,Yu Huang","id":"a807638436aadf2d56dd920f3a8de339dce44096","summary":"Results show that COMBO facilitates representation learning of binary code visualized by distribution analysis, and improves the performance on all three downstream tasks by 5.45% on average compared to state-of-the-art large-scale language representation models.","score":1},{"url":"https://www.semanticscholar.org/paper/54772ffae642a87b9a6122a6f1bae76b926a7230","title":"Enriching Biomedical Knowledge for Low-resource Language Through Large-Scale Translation","venue":"","year":2022,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/10/2022","authors":"Long Phan,Tai Dang,Hieu Tran,Trieu H. Trinh,Vy Phan,Lam D. Chau,Minh-Thang Luong","id":"54772ffae642a87b9a6122a6f1bae76b926a7230","summary":"A state-of-the-art translation model in English-Vietnamese is used to translate and produce both pretrained and supervised data in the biomedical domains, and ViPubmedT5, a pretrained Encoder-Decoder Transformer model trained on 20 million translated abstracts from the high-quality public PubMed corpus is introduced.","score":1},{"url":"https://www.semanticscholar.org/paper/1e2a5dca6f310241dd8a9b56873edf8ca211c781","title":"Enriching Biomedical Knowledge for Low-resource Language Through Translation","venue":"bioRxiv","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Long Phan,Tai Dang,Hieu Tran,Vy Phan,Lam D. Chau,Trieu H. Trinh","id":"1e2a5dca6f310241dd8a9b56873edf8ca211c781","summary":"A state-of-theart translation model in English-Vietnamese is made use to translate and produce both pretrained as well as supervised data in the biomedical domains, and ViPubmedT5, a pretrained Encoder-Decoder Transformer model trained on 20 million translated abstracts from the high-quality public PubMed corpus is introduced.","score":1},{"url":"https://www.semanticscholar.org/paper/8ca981a132ffb21099c8dd1c6bf2e0a5d1babb40","title":"Code Vulnerability Detection Based on Deep Sequence and Graph Models: A Survey","venue":"Security and Communication Networks","year":2022,"referenceCount":64,"citationCount":1,"influentialCitationCount":0,"publicationDate":"14/10/2022","authors":"Bolun Wu,Futai Zou","id":"8ca981a132ffb21099c8dd1c6bf2e0a5d1babb40","summary":"This survey focuses on code vulnerability detection approaches based on deep sequence modeling and graph modeling technologies, and investigates how these two methods are applied to facilitate codeulnerability detection.","score":1},{"url":"https://www.semanticscholar.org/paper/8c618450ec7b52a4f7437d443d2a3b10c2342ef4","title":"Lighting up supervised learning in user review-based code localization: dataset and benchmark","venue":"ESEC/SIGSOFT FSE","year":2022,"referenceCount":78,"citationCount":1,"influentialCitationCount":0,"publicationDate":"07/11/2022","authors":"Xinwen Hu,Yu-tang Guo,Jianjie Lu,Zheling Zhu,Chuanyi Li,Jidong Ge,LiGuo Huang,B. Luo","id":"8c618450ec7b52a4f7437d443d2a3b10c2342ef4","summary":"This paper introduces a large-scale human-labeled ground truth dataset, including the annotation process and statistical analysis, and can provide a basis for in-depth exploration of the supervised learning-based Review2Code solutions.","score":1},{"url":"https://www.semanticscholar.org/paper/257b7c33ee766dc1c1dc6ea155d1be6ab145403b","title":"An Empirical Study of Deep Learning Models for Vulnerability Detection","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/12/2022","authors":"Benjamin Steenhoek,Md. Mahbubur Rahman,Richard Jiles,Wei Le","id":"257b7c33ee766dc1c1dc6ea155d1be6ab145403b","summary":"This paper surveyed and reproduced 9 state-of-the-art (SOTA) deep learning models on 2 widely used vulnerability detection datasets: Devign and MSR and investigated 6 research questions in three areas, namely model capabilities, training data, and model interpretation.","score":1},{"url":"https://www.semanticscholar.org/paper/2aa32aa6dd4ad3798941c98762167999e5511eac","title":"CSGVD: A deep learning approach combining sequence and graph embedding for source code vulnerability detection","venue":"Journal of Systems and Software","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/01/2023","authors":"Wei Tang,Mingwei Tang,Minchao Ban,Ziguo Zhao,Mingjun Feng","id":"2aa32aa6dd4ad3798941c98762167999e5511eac","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/44d53aa6df04dc99c441972cfd14b11ba12b1e2c","title":"End-to-End Transformer-Based Models in Textual-Based NLP","venue":"Applied Informatics","year":2023,"referenceCount":163,"citationCount":1,"influentialCitationCount":0,"publicationDate":"05/01/2023","authors":"Abir Rahali,Moulay A. Akhloufi","id":"44d53aa6df04dc99c441972cfd14b11ba12b1e2c","summary":"This paper presents a literature review on Transformer-based (TB) models, providing a detailed overview of each model in comparison to the Transformer’s standard architecture, and classifies them based on their architecture and training mode.","score":1},{"url":"https://www.semanticscholar.org/paper/44f0eb7be111747f54a8669f74be0bb0dad96cff","title":"An Empirical Comparison of Pre-Trained Models of Source Code","venue":"ArXiv","year":2023,"referenceCount":72,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/02/2023","authors":"Changan Niu,Chuanyi Li,Vincent Ng,Dongxiao Chen,Jidong Ge,B. Luo","id":"44f0eb7be111747f54a8669f74be0bb0dad96cff","summary":"The first systematic empirical comparison of 19 recently- developed pre-trained models of source code on 13 SE tasks is performed, and a recently-developed 4-dimensional categorization of pre- trained models is adopted to gain additional insights into these models.","score":1},{"url":"https://www.semanticscholar.org/paper/172c965f60403f0da32f170070026d9582301562","title":"MTEB: Massive Text Embedding Benchmark","venue":"ArXiv","year":2022,"referenceCount":75,"citationCount":3,"influentialCitationCount":1,"publicationDate":"13/10/2022","authors":"Niklas Muennighoff,Nouamane Tazi,Loic Magne,Nils Reimers","id":"172c965f60403f0da32f170070026d9582301562","summary":"Through the benchmarking of 33 models on MTEB, it is found that no particular text embedding method dominates across all tasks, suggesting that the field has yet to converge on a universal text embeddedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/6436ff048987866680254827461dfeacca4621d4","title":"Exploring AI Ethics of ChatGPT: A Diagnostic Analysis","venue":"ArXiv","year":2023,"referenceCount":95,"citationCount":5,"influentialCitationCount":1,"publicationDate":"30/01/2023","authors":"Terry Yue Zhuo,Yujin Huang,Chunyang Chen,Zhenchang Xing","id":"6436ff048987866680254827461dfeacca4621d4","summary":"A qualitative research method on OpenAI's ChatGPT is performed to better understand the practical features of ethical dangers in recent LLMs, and finds that a significant number of ethical risks cannot be addressed by existing benchmarks, and hence illustrate them via additional case studies.","score":1},{"url":"https://www.semanticscholar.org/paper/1586dd245ae4cca00ea519fb35f27e2a0980476d","title":"Teaching Structured Vision&Language Concepts to Vision&Language Models","venue":"ArXiv","year":2022,"referenceCount":83,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Sivan Doveh,Assaf Arbelle,Sivan Harary,R. Panda,Roei Herzig,Eli Schwartz,Donghyun Kim,R. Giryes,R. Feris,S. Ullman,Leonid Karlinsky","id":"1586dd245ae4cca00ea519fb35f27e2a0980476d","summary":"Various techniques based on language structure understanding can be used to manipulate the textual part of off-the-shelf paired VL datasets that makes more effective use of existing VL pre-training datasets and does not require any additional data.","score":1},{"url":"https://www.semanticscholar.org/paper/6d4e540e1bed26679097139bf90c8652919e4e5c","title":"Explanation Regeneration via Information Bottleneck","venue":"ArXiv","year":2022,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Qintong Li,Zhiyong Wu,Lingpeng Kong,Wei Bi","id":"6d4e540e1bed26679097139bf90c8652919e4e5c","summary":"This work develops an information bottleneck method EIB that regenerates the free-text explanation by polishing the single-pass output from the pretrained language model but retaining the information that supports the contents being explained.","score":1},{"url":"https://www.semanticscholar.org/paper/6db13f58ff662eefa823a660fa86faf8ddf75533","title":"Controllable Text Generation with Language Constraints","venue":"ArXiv","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Howard Chen,Huihan Li,Danqi Chen,Karthik Narasimhan","id":"6db13f58ff662eefa823a660fa86faf8ddf75533","summary":"A solution to leverage a language model’s own internal knowledge to guide generation and propose three forms of guidance (binary veriﬁer, top-k token, textual example), and employ pre-tuning approaches to distill the guidance to tackle diverse natural language constraints.","score":1},{"url":"https://www.semanticscholar.org/paper/bbe93c90b7b87939cd064c805858feca61a3234d","title":"Self-Instruct: Aligning Language Model with Self Generated Instructions","venue":"ArXiv","year":2022,"referenceCount":54,"citationCount":7,"influentialCitationCount":3,"publicationDate":"20/12/2022","authors":"Yizhong Wang,Yeganeh Kordi,Swaroop Mishra,Alisa Liu,Noah A. Smith,Daniel Khashabi,Hannaneh Hajishirzi","id":"bbe93c90b7b87939cd064c805858feca61a3234d","summary":"S ELF -I NSTRUCT provides an almost annotation-free method for aligning pre-trained language models with instructions, and is released to facili-tate future studies on instruction tuning.","score":1},{"url":"https://www.semanticscholar.org/paper/430aaacce4147faef7940d1908cf715c3938e51f","title":"Using Language Models to Convert Between Natural Language and Game Commands","venue":"","year":null,"referenceCount":13,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"Stefan Papazov,W. Gill,Marta Garcia Ferreiro,Andrew Zhu,Lara J. Martin,Chris Callison-Burch","id":"430aaacce4147faef7940d1908cf715c3938e51f","summary":"This paper looks at enhancing a Discord Bot called Avrae that is developed by D&D Beyond to help with online play, and uses GPT-3’s few shot learning and fine tuning capabilities to achieve 64% accuracy.","score":1},{"url":"https://www.semanticscholar.org/paper/134a83d99b4913af4ed265310dfaa670e2689ce0","title":"Translating Natural Language to Bash Commands using Deep Neural Networks","venue":"","year":2022,"referenceCount":9,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Daniel Jenson,Yingxiao Liu","id":"134a83d99b4913af4ed265310dfaa670e2689ce0","summary":"It was found that while cross-entropy loss decreased steadily for all models, only T5 was able to continue learning the structure of Bash commands, and after post-processing, all models improved, but only T4 and BART exceeded the performance of the GPT-3 baseline model.","score":1},{"url":"https://www.semanticscholar.org/paper/199e744034a49edbb616dc5f003bb7ebe09bea3c","title":"Deep Learning Models on CPUs: A Methodology for Efficient Training","venue":"ArXiv","year":2022,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/06/2022","authors":"Quchen Fu,R. Chukka,Keith Achorn,Thomas Atta-fosu,Deepak R. Canchi,Zhongwei Teng,Jules White,Douglas C. Schmidt","id":"199e744034a49edbb616dc5f003bb7ebe09bea3c","summary":"A generic training optimization method that guides the workflow and explores several case studies where it identified performance issues and then optimized the Intel Extension for PyTorch, resulting in an overall 2x training performance increase for the RetinaNet-ResNext50 model.","score":1},{"url":"https://www.semanticscholar.org/paper/a5feb4d4eb7a4db7a7226f518c5c978fc6779f1f","title":"CitySpec with Shield: A Secure Intelligent Assistant for Requirement Formalization","venue":"ArXiv","year":2023,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/02/2023","authors":"Zirong Chen,Issa Li,Haoxiang Zhang,S. Preum,J. Stankovic,Meiyi Ma","id":"a5feb4d4eb7a4db7a7226f518c5c978fc6779f1f","summary":"CitySpec, the first intelligent assistant system for requirement specification in smart cities, is built and shows its strong usability and adaptability to different domains, and also its robustness to malicious inputs.","score":1},{"url":"https://www.semanticscholar.org/paper/9ed21b38ed48773048a6749c48748d3b88974a17","title":"A large-scale empirical study of commit message generation: models, datasets and evaluation","venue":"Empirical Software Engineering","year":2022,"referenceCount":67,"citationCount":1,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Wei Tao,Yanlin Wang,Ensheng Shi,Lun Du,Shi Han,Hongyu Zhang,Dongmei Zhang,Wenqiang Zhang","id":"9ed21b38ed48773048a6749c48748d3b88974a17","summary":"This paper conducts a systematic and in-depth analysis of the state-of-the-art models and datasets for automatic commit message generation and collects a large-scale, information-rich, multi-programming-language, MCMD.","score":1},{"url":"https://www.semanticscholar.org/paper/e772bf6ad9b87a333291f427d328e24cc57f23a1","title":"Human perceiving behavior modeling in evaluation of code generation models","venue":"IEEE Games Entertainment Media Conference","year":2022,"referenceCount":14,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"S. Kovalchuk,Vadim Lomshakov,Artem Aliev","id":"e772bf6ad9b87a333291f427d328e24cc57f23a1","summary":"A 5-level Likert scale assessment of the model output using a perceiving model based on the Theory of Planned Behavior (TPB) shows an extension of model assessment as well as a deeper understanding of the quality and applicability of generated code for practical question answering.","score":1},{"url":"https://www.semanticscholar.org/paper/cbbacf9ba52942fd6a43588295385e2ab39545b0","title":"Interacting with next-phrase suggestions: How suggestion systems aid and influence the cognitive processes of writing","venue":"","year":2022,"referenceCount":49,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/08/2022","authors":"Advait Bhat,Saaket Agashe,Niharika Mohile,Parth Oberoi,R. Jangir,Anirudha N. Joshi","id":"cbbacf9ba52942fd6a43588295385e2ab39545b0","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/dda0f7f086fc875d583604f8b0cf4a8678bc4de4","title":"Bootstrapping Multilingual Semantic Parsers using Large Language Models","venue":"ArXiv","year":2022,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"Abhijeet Awasthi,Nitish Gupta,Bidisha Samanta,Shachi Dave,Sunita Sarawagi,P. Talukdar","id":"dda0f7f086fc875d583604f8b0cf4a8678bc4de4","summary":"The effectiveness and flexibility offered by large language models (LLMs) for translating English datasets into several languages via few-shot prompting are demonstrated and it is shown that the method of translating data using LLMs outperforms a strong translate-train baseline on 41 out of 50 languages.","score":1},{"url":"https://www.semanticscholar.org/paper/1b13cf2971199d48722192b8c290d4c4eb63ca80","title":"Semi-Supervised Code Generation by Editing Intents","venue":"","year":2018,"referenceCount":15,"citationCount":0,"influentialCitationCount":0,"publicationDate":2018,"authors":"Edgar Chen","id":"1b13cf2971199d48722192b8c290d4c4eb63ca80","summary":"This work proposes a deep generative model to rewrite and augment utterances such as StackOverflow questions into an acceptable form by adding variable names and other important information.","score":1},{"url":"https://www.semanticscholar.org/paper/a9d240d911e338e6083cee83570c10af6f9dafb8","title":"A Comprehensive Study of StaQC for Deep Code Summarization","venue":"","year":2018,"referenceCount":51,"citationCount":3,"influentialCitationCount":0,"publicationDate":2018,"authors":"Jayavardhan Reddy Peddamail,Ziyu Yao,Zhen Wang,Huan Sun","id":"a9d240d911e338e6083cee83570c10af6f9dafb8","summary":"StaQC is described, a large-scale and high-quality dataset of <NL, code> pairs in Python and SQL domain, systematically mined from the Stack Overflow forum (SO), showing that StaQC helps achieve substantially better results.","score":1},{"url":"https://www.semanticscholar.org/paper/8e773b1840b894603c06b677a0f15ebcf0f26378","title":"Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task","venue":"Conference on Empirical Methods in Natural Language Processing","year":2018,"referenceCount":45,"citationCount":436,"influentialCitationCount":119,"publicationDate":"24/09/2018","authors":"Tao Yu,Rui Zhang,Kai-Chou Yang,Michihiro Yasunaga,Dongxu Wang,Zifan Li,James Ma,Irene Z Li,Qingning Yao,Shanelle Roman,Zilin Zhang,Dragomir R. Radev","id":"8e773b1840b894603c06b677a0f15ebcf0f26378","summary":"This work defines a new complex and cross-domain semantic parsing and text-to-SQL task so that different complicated SQL queries and databases appear in train and test sets and experiments with various state-of-the-art models show that Spider presents a strong challenge for future research.","score":1},{"url":"https://www.semanticscholar.org/paper/e6b035ff8d839810593769b96edb18dd188c118c","title":"Reinforcing Diversity Company Policies: Insights from StackOverflow Developers Survey","venue":"International Conference on Enterprise Information Systems","year":2019,"referenceCount":29,"citationCount":2,"influentialCitationCount":0,"publicationDate":2019,"authors":"K. Silveira,S. Musse,I. Manssour,R. Vieira,R. Prikladnicki","id":"e6b035ff8d839810593769b96edb18dd188c118c","summary":"This work is interested in understanding the pain points in software engineering regarding diversity and provide insights to support the attraction, hiring and retention policies for more diverse software engineering environments and proposes a discussion about the unconscious bias, stereotypes, and impostor syndrome.","score":1},{"url":"https://www.semanticscholar.org/paper/970d34f38106023cfdacfb0bf59b7f3f64dcc4c3","title":"A Multi-Modal Intelligent Agent that Learns from Demonstrations and Natural Language Instructions","venue":"","year":2019,"referenceCount":146,"citationCount":0,"influentialCitationCount":0,"publicationDate":2019,"authors":"Toby Jia-Jun Li","id":"970d34f38106023cfdacfb0bf59b7f3f64dcc4c3","summary":"The preliminary lab usability evaluation results showed that the prototype of SUGILITE allowed users with little or no programming expertise to successfully teach the agent common smartphone tasks, as well as the appropriate conditionals for triggering these actions and the relevant concepts for determining these conditions.","score":1},{"url":"https://www.semanticscholar.org/paper/8cd26c197ad9635bd7508f01bdad67b6562099dd","title":"Deep Code-Comment Understanding and Assessment","venue":"IEEE Access","year":2019,"referenceCount":29,"citationCount":4,"influentialCitationCount":0,"publicationDate":2019,"authors":"Deze Wang,Yong Guo,Wei Dong,Zhiming Wang,Haoran Liu,Shanshan Li","id":"8cd26c197ad9635bd7508f01bdad67b6562099dd","summary":"This paper converts the quality assessment of code comments into a classification problem based on the multi-input neural network and concatenates the code vectors as the input of the Multiple-Layer Perceptron classifier for the comment quality assessment.","score":1},{"url":"https://www.semanticscholar.org/paper/4fbddbe0f23f76bd779f8d1e524374d6bf1bea81","title":"Learning to Map Natural Language to General Purpose Source Code","venue":"","year":2019,"referenceCount":161,"citationCount":0,"influentialCitationCount":0,"publicationDate":2019,"authors":"Srini Iyer","id":"4fbddbe0f23f76bd779f8d1e524374d6bf1bea81","summary":"This dissertation presents efficient deep learning models and training paradigms to map language to general purpose source code that will enable numerous applications for non-expert users as well as developers.","score":1},{"url":"https://www.semanticscholar.org/paper/4fdbf9af4058ff17c31db8bc8ca751d69b90ae43","title":"Semantic Parser and Natural Language Generator via Dual Information Maximization","venue":"","year":2019,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":2019,"authors":"Hai Ye","id":"4fdbf9af4058ff17c31db8bc8ca751d69b90ae43","summary":"This paper proposes the method of dual information maximization (DIM) to regularize the learning process, where DIM empirically maximizes the variational lower bounds of expected joint distributions of NL and MRs.","score":1},{"url":"https://www.semanticscholar.org/paper/3092325d55f6aa9ba28b0841bdcfd61991a38d48","title":"A Neural Model for Generating Natural Language Summaries of Program Subroutines","venue":"International Conference on Software Engineering","year":2019,"referenceCount":59,"citationCount":172,"influentialCitationCount":42,"publicationDate":"05/02/2019","authors":"Alexander LeClair,Siyuan Jiang,Collin McMillan","id":"3092325d55f6aa9ba28b0841bdcfd61991a38d48","summary":"This paper presents a neural model that combines words from code with code structure from an AST, which allows the model to learn code structure independent of the text in code.","score":1},{"url":"https://www.semanticscholar.org/paper/f44ceb54fa773920b767c1e93ea0bc8725f248df","title":"Automatic Acquisition of Annotated Training Corpora for Test-Code Generation","venue":"Inf.","year":2019,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/02/2019","authors":"Magdalena Kacmajor,John D. Kelleher","id":"f44ceb54fa773920b767c1e93ea0bc8725f248df","summary":"The experiments show that a neural MT model trained on a generated dataset can generate syntactically correct and semantically relevant short Java functions from quasi-natural language descriptions of functionality.","score":1},{"url":"https://www.semanticscholar.org/paper/dfd252415b37208617da78d09cfd87480b9800eb","title":"Modeling Vocabulary for Big Code Machine Learning","venue":"ArXiv","year":2019,"referenceCount":80,"citationCount":22,"influentialCitationCount":1,"publicationDate":"03/04/2019","authors":"Hlib Babii,A. Janes,R. Robbes","id":"dfd252415b37208617da78d09cfd87480b9800eb","summary":"It is shown that a subset of decisions have decisive characteristics, allowing to train accurate Neural Language Models quickly on a large corpus of 10,106 projects.","score":1},{"url":"https://www.semanticscholar.org/paper/a57131646a16445df5ddbc86da917fb497cc27da","title":"Cleaning StackOverflow for Machine Translation","venue":"IEEE Working Conference on Mining Software Repositories","year":2019,"referenceCount":44,"citationCount":4,"influentialCitationCount":1,"publicationDate":"26/05/2019","authors":"Musfiqur Rahman,Peter C. Rigby,Dharani Palani,T. Nguyen","id":"a57131646a16445df5ddbc86da917fb497cc27da","summary":"This paper cleans StackOverflow, one of the most popular online discussion forums for programmers, to generate a parallel English-Code corpus from Android posts, and creates a simple maximum likelihood MT model to show that English words in the corpus map to a small number of specific code elements.","score":1},{"url":"https://www.semanticscholar.org/paper/1fbaed00dbda975a6209761857dd1a78618c6585","title":"Jointly Learning Semantic Parser and Natural Language Generator via Dual Information Maximization","venue":"Annual Meeting of the Association for Computational Linguistics","year":2019,"referenceCount":50,"citationCount":19,"influentialCitationCount":1,"publicationDate":"03/06/2019","authors":"Hai Ye,Wenjie Li,Lu Wang","id":"1fbaed00dbda975a6209761857dd1a78618c6585","summary":"A novel method of dual information maximization (DIM) is proposed to regularize the learning process, where DIM empirically maximizes the variational lower bounds of expected joint distributions of NL and MRs.","score":1},{"url":"https://www.semanticscholar.org/paper/735ce0447e459e13f89ae751b19323d76a2af786","title":"Program Synthesis and Semantic Parsing with Learned Code Idioms","venue":"Neural Information Processing Systems","year":2019,"referenceCount":43,"citationCount":59,"influentialCitationCount":7,"publicationDate":"26/06/2019","authors":"Richard Shin,Miltiadis Allamanis,Marc Brockschmidt,Oleksandr Polozov","id":"735ce0447e459e13f89ae751b19323d76a2af786","summary":"PATOIS is a system that allows a neural program synthesizer to explicitly interleave high-level and low-level reasoning at every generation step, and it accomplishes this by automatically mining common code idioms from a given corpus and incorporating them into the underlying language for neural synthesis.","score":1},{"url":"https://www.semanticscholar.org/paper/3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c","title":"Reranking for Neural Semantic Parsing","venue":"Annual Meeting of the Association for Computational Linguistics","year":2019,"referenceCount":31,"citationCount":47,"influentialCitationCount":10,"publicationDate":"01/07/2019","authors":"Pengcheng Yin,Graham Neubig","id":"3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c","summary":"This paper presents a simple approach to quickly iterate and improve the performance of an existing neural semantic parser by reranking an n-best list of predicted MRs, using features that are designed to fix observed problems with baseline models.","score":1},{"url":"https://www.semanticscholar.org/paper/561b279dd17a232f7466a5dda89d879f75a2bf3b","title":"Identifying Algorithm Names in Code Comments","venue":"ArXiv","year":2019,"referenceCount":18,"citationCount":2,"influentialCitationCount":0,"publicationDate":"10/07/2019","authors":"Jakapong Klainongsuang,Yusuf Sulistyo Nugroho,Hideaki Hata,Bundit Manaskasemsak,A. Rungsawang,P. Leelaprute,Ken-ichi Matsumoto","id":"561b279dd17a232f7466a5dda89d879f75a2bf3b","summary":"This paper proposes an automatic method of algorithm name identification by extracting important N-gram words containing the word `algorithm' in the last from active FLOSS projects written in seven programming languages.","score":1},{"url":"https://www.semanticscholar.org/paper/98ec93df77d6f672b4f682cbe315fedf0e2d4ee7","title":"PUMICE: A Multi-Modal Agent that Learns Concepts and Conditionals from Natural Language and Demonstrations","venue":"ACM Symposium on User Interface Software and Technology","year":2019,"referenceCount":64,"citationCount":50,"influentialCitationCount":6,"publicationDate":"30/08/2019","authors":"Toby Jia-Jun Li,Marissa Radensky,Justin Jia,Kirielle Singarajah,Tom Michael Mitchell,B. Myers","id":"98ec93df77d6f672b4f682cbe315fedf0e2d4ee7","summary":"A new multi-modal domain-independent approach that combines natural language programming and programming-by-demonstration to allow users to first naturally describe tasks and associated conditions at a high level, and then collaborate with the agent to recursively resolve any ambiguities or vagueness through conversations and demonstrations is described.","score":1},{"url":"https://www.semanticscholar.org/paper/fa5b139b08ef9ce0529d68c929f786412edc2898","title":"JuICe: A Large Scale Distantly Supervised Dataset for Open Domain Context-based Code Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2019,"referenceCount":33,"citationCount":36,"influentialCitationCount":9,"publicationDate":"01/10/2019","authors":"R. Agashe,Srini Iyer,Luke Zettlemoyer","id":"fa5b139b08ef9ce0529d68c929f786412edc2898","summary":"To study code generation conditioned on a long context history, JuICe is presented, a corpus of 1.5 million examples with a curated test set of 3.7K instances based on online programming assignments that provides refined human-curated data, open-domain code, and an order of magnitude more training data.","score":1},{"url":"https://www.semanticscholar.org/paper/5ad7ce7810964bde2c86e07d63156270e0b17e0b","title":"Extracting Code-relevant Description Sentences Based on Structural Similarity","venue":"Asia-Pacific Symposium on Internetware","year":2019,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/10/2019","authors":"Yingkui Cao,Yanzhen Zou,Bing Xie","id":"5ad7ce7810964bde2c86e07d63156270e0b17e0b","summary":"To quantify the relevance between code line and natural language sentence, the authors represent them with structure trees and calculate their structural similarity.","score":1},{"url":"https://www.semanticscholar.org/paper/539268757e7d29201a13e5ca90454eb151d3d022","title":"Abstraction, Generalization, and Embodiment in Neural Program Synthesis","venue":"","year":2020,"referenceCount":129,"citationCount":0,"influentialCitationCount":0,"publicationDate":2020,"authors":"Richard Shin","id":"539268757e7d29201a13e5ca90454eb151d3d022","summary":"This paper presents a meta-modelling framework that automates the very labor-intensive and therefore time-heavy and expensive process of designing and implementing neural program Synthesis systems.","score":1}]}